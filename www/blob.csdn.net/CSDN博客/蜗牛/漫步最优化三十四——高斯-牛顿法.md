
# 漫步最优化三十四——高斯-牛顿法 - 蜗牛 - CSDN博客


2017年10月28日 21:33:06[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：718



$\textbf{你的温柔像羽毛，}$
$\textbf{秘密躺在我怀抱。}$
$\textbf{你的微笑像拥抱，}$
$\textbf{只有我能看到。}$
$\textbf{你的香味在徘徊，}$
$\textbf{我舍不得离开。}$
$\textbf{我坚持学单纯的小孩，}$
$\textbf{只想看守这份爱。}$
$\textbf{——畅宝宝的傻逼哥哥}$
对于许多优化问题，其目标函数是函数向量的形式

$$
\mathbf{f}=[f_1(\mathbf{x})\ f_2(\mathbf{x})\ \cdots\ f_m(\mathbf{x})]^T
$$
其中$f_p(\mathbf{x}),p=1,2,\ldots,m$是$\mathbf{x}$的函数且互相独立，要求的解就是同时使$f_p(\mathbf{x})$为零的点$\mathbf{x}$。
对于这样的问题，我们可以构造如下的实值函数

$$
F=\sum_{p=1}^mf_p(\mathbf{x})^2=\mathbf{f}^T\mathbf{f}
$$
如果用多维无约束算法最小化$F$，那么相当于用最小二乘最小化函数$f_p(\mathbf{x})$。
一种求解上面问题的方法是高斯-牛顿法，这个方法基于前面介绍的牛顿法。
因为有多个函数$f_p(\mathbf{x})$且每个都依赖于$x_i,i=1,2,\ldots,n$，那么存在一个梯度矩阵

$$
\mathbf{J}=
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1}&\frac{\partial f_1}{\partial x_2}&\cdots&\frac{\partial f_1}{\partial x_n}\\
\frac{\partial f_2}{\partial x_1}&\frac{\partial f_2}{\partial x_2}&\cdots&\frac{\partial f_2}{\partial x_n}\\
\vdots&\vdots&&\vdots\\
\frac{\partial f_m}{\partial x_1}&\frac{\partial f_m}{\partial x_2}&\cdots&\frac{\partial f_m}{\partial x_n}
\end{bmatrix}
$$
称为雅可比矩阵，函数的总数$m$可以超过变量的总数$n$，即雅可比不需要必须为方阵。
$F$分别对$x_i$求导可得

$$
\frac{\partial F}{\partial x_i}=\sum_{i=1}^m2f_p(\mathbf{x})\frac{\partial f_p}{\partial x_i}\quad\text{for}\ i=1,2,\ldots,n
$$
用矩阵形式表示为

$$
\begin{bmatrix}
\frac{\partial F}{\partial x_1}\\
\frac{\partial F}{\partial x_2}\\
\vdots\\
\frac{\partial F}{\partial x_n}\\
\end{bmatrix}=
2
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1}&\frac{\partial f_2}{\partial x_1}&\cdots&\frac{\partial f_m}{\partial x_1}\\
\frac{\partial f_1}{\partial x_2}&\frac{\partial f_2}{\partial x_2}&\cdots&\frac{\partial f_m}{\partial x_2}\\
\vdots&\vdots&&\vdots\\
\frac{\partial f_1}{\partial x_n}&\frac{\partial f_2}{\partial x_n}&\cdots&\frac{\partial f_m}{\partial x_n}
\end{bmatrix}
\begin{bmatrix}
f_1(\mathbf{x})\\
f_2(\mathbf{x})\\
\vdots\\
f_m(\mathbf{x})
\end{bmatrix}
$$
因此$F$的梯度(用$\mathbf{g}_F$表示)可以表示为

$$
\mathbf{g}_F=2\mathbf{J}^T\mathbf{f}
$$
假设$f_p(\mathbf{x})\in C^2$，那么

$$
\frac{\partial^2F}{\partial x_i\partial x_j}=2\sum_{p=1}^m\frac{\partial f_p}{\partial x_i}\frac{\partial f_p}{\partial x_j}+2\sum_{p=1}^mf_p(\mathbf{x})\frac{\partial f_p}{\partial x_i\partial x_j}
$$
其中$i,j=1,2,\ldots,n$。如果$f_p(\mathbf{x})$的二阶导可以忽略，则我们有

$$
\frac{\partial^2F}{\partial x_i\partial x_j}\approx2\sum_{p=1}^m\frac{\partial f_p}{\partial x_i}\frac{\partial f_p}{\partial x_j}
$$
所以$F$的海森矩阵(用$\mathbf{H}_F$表示)可以化简为

$$
\mathbf{H}_F\approx2\mathbf{J}^T\mathbf{J}
$$
因为现在$F$的梯度与海森矩阵已知，所以接下来就能用牛顿法求问题的解，类比前面可得

$$
\begin{align*}
\mathbf{x}_{k+1}
&=\mathbf{x}_k-\alpha_k(2\mathbf{J}^T\mathbf{J})^{-1}(2\mathbf{J}^T\mathbf{f})\\
&=\mathbf{x}_k-\alpha_k(\mathbf{J}^T\mathbf{J})^{-1}(\mathbf{J}^T\mathbf{f})
\end{align*}
$$
其中$\alpha_k$是最小化$F(\mathbf{x}_k+\alpha\mathbf{d}_k)$的$\alpha$值，随着$k$不断增大，相邻的线搜索会使$F_k$减小且$\mathbf{x}_k$越来越接近$\mathbf{x}^*$。当$\mathbf{x}_k$位于$\mathbf{x}^*$的邻域时，函数$f_p(\mathbf{x}_k)$可以用泰勒级数的线性近似准确表示，上面的海森矩阵就是$F_k$海森矩阵的准确表示并且该方法会迅速收敛。如果函数$f_p(\mathbf{x})$是线性的，$F$是二次的，那么上面的海森矩阵就是海森矩阵，一次迭代就能求出问题的解。
如果$\mathbf{H}_F$是奇异的，那么该方法与牛顿法类似也会失效。然而可以用牛顿法当中提到的修正法来修正。

