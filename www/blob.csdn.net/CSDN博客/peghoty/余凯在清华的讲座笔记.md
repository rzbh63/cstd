
# 余凯在清华的讲座笔记 - peghoty - CSDN博客


2014年04月08日 11:27:29[皮果提](https://me.csdn.net/peghoty)阅读数：4306标签：[Deep Learning																](https://so.csdn.net/so/search/s.do?q=Deep Learning&t=blog)[百度																](https://so.csdn.net/so/search/s.do?q=百度&t=blog)[余凯																](https://so.csdn.net/so/search/s.do?q=余凯&t=blog)[深度学习																](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)[Big Data																](https://so.csdn.net/so/search/s.do?q=Big Data&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=余凯&t=blog)个人分类：[机器学习																](https://blog.csdn.net/peghoty/article/category/1824627)[深度学习																](https://blog.csdn.net/peghoty/article/category/1451403)[
							](https://blog.csdn.net/peghoty/article/category/1824627)
[
				](https://so.csdn.net/so/search/s.do?q=余凯&t=blog)
[
			](https://so.csdn.net/so/search/s.do?q=余凯&t=blog)
[
		](https://so.csdn.net/so/search/s.do?q=百度&t=blog)
[
	](https://so.csdn.net/so/search/s.do?q=Deep Learning&t=blog)
本文来自[Rorschach](http://my.csdn.net/huangbo10)的博客：[ http://blog.csdn.net/huangbo10/article/details/22944007](http://blog.csdn.net/huangbo10/article/details/22944007)

2014.4.4，余凯在清华 FIT 楼做了“Deep Learning Unfolds the Big Data Era”的讲座。感觉这个讲座还是比较 high-level 的，毕竟 90 分钟也很难把这么大的问题讲清楚。

根据我的理解，讲座主要分成 4 部分：
1. Deep Learning 怎样被工业界看重并火得一塌糊涂；
2. 分析了一下 shallow model 和 deep model 的区别；
3. 介绍了百度在 DL 方面的研究和产品；
4. DL 的发展趋势，百度可能的发展方向。

# 第一部分：DL 是怎么火起来的

余凯首先以百度为例讲了深度学习的重要性：百度的商业模式是搜索引擎+卖广告，而机器学习算法能够将广告与潜在用户进行精准匹配，帮百度赚钱，turn data into value。在这方面百度需要解决的问题包括：自然图像中的光学字符识别（OCR in natural images）、语音识别（speech
 recognition and understanding）、基于内容的图像检索（content-based image retrieval）。尤其是鉴于最近**可穿戴设备**的快速发展，语音识别可能在未来扮演非常重要的角色。

随后他又画了那张很出名的技术发展的曲线，即先升后降再稳步提升。现在处于第一个上升期的代表包括**3D 打印**、**自动驾驶**等等，第二个上升期的则是**虹膜识别**等等。

现在 DL 为什么这么火呢？06 年 Hinton 的文章通过 pre-training 较好地解决了多层网络难以训练的问题，重新唤起了人们对于 neural network 的兴趣。但更重要的是在随后的几年里，人们利用 DL
 模型在**语音识别**和**图像分类**上获得了突破性的进展。之所以说是突破性的，是因为其性能提升之大是以往小修小补远不能及的。尤其是在图像领域，DL 算法甚至不需要 human engineered feature，直接以原始像素作为输入就行。这改变了以往人们对于特征提取的认识。套用邓侃的一句话就是：这个领域离钱太近了。（这里还讲了一个小八卦：最早用 DL 在语音上取得学术成果的是微软，但最先将其产品化的却是谷歌，随后是百度。）

# 第二部分：shallow
 model 和 deep model 的比较

余凯将机器学习的模型归为三类：
shallow model，包括 linear regression、logistic regression 等；
shallow model with one hidden layer，包括 SVM、boosting、MLP 等；
还有就是 deep model。

其中 shallow model with one hidden layer，其实都可以认为（是余凯认为）是将数据用某种方法投影到隐含的特征空间中。学习过程就是学习这种映射关系，对隐层的不同假设就形成了不同的模型。余凯将 manifold learning
 和 semi-supervised learning 也归为这一类，我可以理解；但是他将PCA 也归为这一类，我没太明白为什么。

浅层的线性模型在以往获得了很大的成功（我记得 Andrew Ng 在 Machine Learning 课上讲完 linear regression 和 logistic regression 之后说：OK，你们现在已经比硅谷大部分工程师都牛逼了），不过说白了，浅层模型研究的问题是：已经有了好的特征我们怎么分类。问题是好的特征哪里来？过去人们靠专家拍脑袋想特征，比如
 SIFT。但是当数据变得更复杂、数据量更大的时候这种办法就不行了，所以要让模型自动提取特征。

余凯给了一幅图像处理的流程图，从左到右依次为：low-levelsensing
 ->pre-processing->featureextraction->featureselection->prediction/classificationetc.。前面的 4 个环节都属于**dirty work**，但这些工作又对最后的结果影响极大。实践中这些环节是最耗费时间的，且往往是通过人工选取的方式，典型的例子就是 SIFT 和 MFCC。正如前面所说，在图像领域我们已经能让模型自动地选取特征，但是在语音领域我们还没到这个程度。

DL 能够自动习得从信号到语义的层次化的数据表示（参考谷歌的猫脸），这就是最吸引人的部分。

# 第三部分：百度在
 DL 方面的工作

这块比较有意思，有研究也有产品。余凯非常自豪地说，现在用户在百度上的每一个 query、每一次点击都会用 NN 处理。成果方面，百度把语音识别的相对出错率降低了 25%，OCR 的相对错误率降低了 30%，图像检索则是世界最好的，“way better
 than Google”。从给出的几个例子上来看确实是这样。

然后讲了一点怎样把 DL  应用于**文本检索**，这块也是我一直比较好奇的。大致结构是（在网上没找到图，读者脑补一下吧……）：先将 query 和 document 映射成向量，要求这个向量可以反映出语义上的相关性（有没有参考谷歌的
 word2vec？）；分别对两个向量做 pooling；将 pooling 后的向量送入 DNN 进行打分，得到 query 和 doc 的相关度。对于有标记的训练集，我们可以知道类似 score( query, doc1 ) > score( query, doc2 ) 这样的数据，所以只要定义一个合适的cost function 就可以愉快地训练了。

作为搜索服务提供者，百度当然不缺数据。据余凯说，百度使用的图片大概有 100 million，用于 OCR 的数据也差不多，语音是 10 billion，CTR（点击预估）甚至达到 100 billion。怪不得去百度实习过的同学都看不上学术圈这点玩具一样的数据集……

之后的东西就很水了，提了一下**百度的分布式学习框架 PADDLE**，强调他们最看重的就是 scalability。

讲到这里余凯又总结了一下为什么 DL 现在会这么成功，包括：
○ Big data 解决了 vanishing gradient 的问题，一定程度上缓解了 local optimal 的问题（能 fit 这么多数据说明这个 local optimal
 还是足够好的）
○ GPU
○ Large scale parallel computation
○ pre-training（现在不重要，如第一条所述，数据足够就行了）
○ RELU，drop out，normalization 等等 trick

# 第四部分：深度学习和百度的未来

从**生物学的角度**分析，高等动物相比低等动物，在神经元连接稠密程度上增长不大，但神经元数量要多得多。因此 DL 下一步的发展方向也许还是**更大规模的网络**。

从**技术角度**讲，需要解决的问题包括：
1.大规模海量数据并行训练平台；
2.对于结构和非结构数据的深度学习建模技术；
3.对于语言、语义、知识的建模、学习、表示、集成；
4.线上模型压缩加速技术，软件+硬件。这些不光要靠算法的改进，也依赖于高性能计算、脑科学的发展。

从研究**哲学的角度**，余凯强调了两点：
1. DL 不是一个 blackbox；
2. DL 不是 AI。在他看来，DL 是一种语言、一种框架，就像 graphical model。我们**只有在先验知识的指导下才能更好的设计网络的结构**，以期反映问题的本质。与过去不同的是，过去我们将先验知识应用于 feature
 engineering，现在则是用于 model structure。同时需要认识到，一个完整的 AI 应该包括四方面的能力：利用知识，学习，解决问题，创造。机器学习仅仅是学习的一部分，而 DL 也只是机器学习的一部分。

# 第五部分：提问环节

只有一个比较有意思的问题，是一个不知道是研究生还是老师的漂亮姐姐问的：对于一个已经训练好的模型，如果出现了一个新的需要识别的类别，怎样将其快速加入到模型中？余凯给出的解决方法是**保留下面几层，只重新训练接近输出层的几层**。这也是可以理解的，输入的底层特征都差不多，应该避免重复劳动。

以上就是我梳理的笔记。这么写一遍感觉又加深了自己的理解，以后多来几次。

