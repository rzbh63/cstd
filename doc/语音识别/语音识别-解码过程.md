# 语音识别-解码过程

2017年02月23日 17:49:27

 

quheDiegooo

 

阅读数：9195

 

标签： [语音识别](http://so.csdn.net/so/search/s.do?q=%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB&t=blog)[算法](http://so.csdn.net/so/search/s.do?q=%E7%AE%97%E6%B3%95&t=blog)[解码](http://so.csdn.net/so/search/s.do?q=%E8%A7%A3%E7%A0%81&t=blog) 更多

个人分类： [语音训练](https://blog.csdn.net/quheDiegooo/article/category/6731972)[语音识别](https://blog.csdn.net/quheDiegooo/article/category/6693504)



上一篇讨论了语音识别中的训练过程，本章讨论语音识别中，解码的过程。

解码的过程就是在给定声学特征的情况下，找到最可能对应的词组的过程，再次看如下求解目的的公式：

![img](https://img-blog.csdn.net/20170223175422001?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

其中似然概率是在一系列给定声学frame情况下，计算每个对应的分类器得分，然后相乘得出的概率，使得其值变得很小，而P(W)比较大，这样就导致

P(w)权重太大了，所以需要对齐进行缩放，以平衡贡献值，所以把上面公式改写如下：

![img](https://img-blog.csdn.net/20170223180011800?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

因为P(w)小于1，使LMSF大于1，（5-15），这样就减小了P(w)对整个公式的贡献，以达到缩放的目的。

但是在P(w)中以上惩罚对词插入的情况下是有副作用的，所以改写如下：

![img](https://img-blog.csdn.net/20170223181130713?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

在log形式展开，最后解码的目标就是如下公式所示：

![img](https://img-blog.csdn.net/20170223181215758?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

有了上述的目标公式，接下来就要讨论，如何解码取其最大值

解码中最常用的是Viterbi算法，首先看一下语音识别中HMM模型：



*Q* = *q*1*q*2 ...*q**N   对应subphone的状态序列*

*A* = *a*01*a*02 ...*a**n*1 ...*a**nn   状态转移矩阵（自环和转到下一个状态）*

*B* = *b**i*(*o**t* )   观测似然，或者叫做发射概率，代表在t时刻，状态i产生声音倒谱特征O的概率

其中A和B由上一章中的嵌入式训练得到。下图为识别数字的HMM结构图。

![img](https://img-blog.csdn.net/20170223182903517?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

首先我看用前向算法*O*(*N*2*T*) 来进行解码的过程，

举例如下：英文字母five，有对应状态[f], [ay], 和[v] ，观测序列O,如下所示：

​     ![img](https://img-blog.csdn.net/20170223184239836?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

![img](https://img-blog.csdn.net/20170223184310696?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

首先我们引入α*t* ( *j*)  ，记做：在看见前t个观测值之后，处于状态j的概率。

![img](https://img-blog.csdn.net/20170223200903216?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

这里*q**t* = *j 表示在状态序列中，t时刻状态为j,*

*αt ( j) 可以理解为，所有能到到达当前状态的所有路径的概率之和，即：*

*![img](https://img-blog.csdn.net/20170223201335847?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)*

*其中，**α**t**−**1**(**i**) 表示前一个（t-1）之前的路径的概率，*



*a**ij 表示概率转移矩阵，表示从状态*qi  到当前状态q j 的概率



*b**j*(*o**t*) 叫做状态观测似然，也叫作发射概率，表示给定状态j，观测到ot  的概率

前向算法如下所示：

![img](https://img-blog.csdn.net/20170223203825162?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



*q**F 表示结束状态。*

*假设矩阵A自环概率概率为0.5，假设矩阵B如下所示：![img](https://img-blog.csdn.net/20170223204120736?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)*

*则单词“five”的前向算法过程如下所示：*

*![img](https://img-blog.csdn.net/20170223204239033?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)*

*接着讨论Viterbi算法，*

*Viterbi算法是返回最可能的状态序列，虽然不是最可能的次序列，但是也足够接近了，其时间复杂度为*

*O*(*N*2*T*) ，

使得vt ( j) 记做：在给定 λ 的情况下，在看到前t个观测特征Ot，且通过了最可能的q1...qt−1 状态序列的情况下，HMM当前状态为j的概率，即：

![img](https://img-blog.csdn.net/20170223205312398?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

根据动态规划算法，可以理解为t-1时刻的在状态i时候的最大概率路径到当前时刻t时候，状态为j的概率,记做：

![img](https://img-blog.csdn.net/20170223205648055?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



*v**t*−1(*i*) 表示：前一时刻，Viterbit路径的概率；



*a**i j 表示：状态*qi 到状态q j的概率 ；



*b**j*(*o**t*) 表示：状态观测似然，表示为给定状态j，产生观测向量ot  的概率。

根据上式可知，Viterbi算法是在给定观测序列o = (o1o2o3 ...oT ) 情况下，找到最优的状态序列q =(q1q2q3 ...qT ) 的过程

，同时找到对应的最大的概率。对比上面的前向算法可知，他们目标都是一致的，但是Viterbi算法是求其最大值。

Viterbi具体解码算法如下所示：（假设起始状态为0，结束状态为qF，是非发射状态）

![img](https://img-blog.csdn.net/20170223210521310?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

例子：

*假设矩阵A自环概率概率为0.5，假设**矩阵B如下所示：![img](https://img-blog.csdn.net/20170223204120736?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)*

*则对应的计算数值如下所示：*

*![img](https://img-blog.csdn.net/20170223211320486?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)*

*Viterbi解码的真正用处不仅仅是在词内解码，更重要的是可以解码一串词，为了使Viterbi能够进行词间进行解码，我们得增加矩阵A，使其不仅要有词内的状态转移概率，还需要增加从一个词末尾，到另一个词开始的状态转移概率。*

*下图补充了2-gram间的转移概率，*

*![img](https://img-blog.csdn.net/20170223213316213?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)*

下图展示了2-gram词间解码的过程：

![img](https://img-blog.csdn.net/20170223213807134?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

一旦一句话的Viterbi解码计算完毕，就可以用过后项指针回溯，来找到最大概率的状态序列，即最大概率的词序列。

如下图所示：最后的回溯词序列为w1wN ···w2 

![img](https://img-blog.csdn.net/20170223214415174?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXVoZURpZWdvb28=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

实际上Viterbi解码过程中需要进行beam剪枝，通过设定beam的宽度θ 来进行beam剪枝。

Beam剪枝算法我们以后章节继续讨论。

以上就是语音识别过程中，解码的所有细节，谢谢！





