
# 迁移学习之最大分类器差异的无监督域适应 - 喜欢打酱油的老鸟 - CSDN博客


2019年04月08日 10:13:30[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：62标签：[迁移学习																](https://so.csdn.net/so/search/s.do?q=迁移学习&t=blog)[最大化分类器差异																](https://so.csdn.net/so/search/s.do?q=最大化分类器差异&t=blog)[交叉熵																](https://so.csdn.net/so/search/s.do?q=交叉熵&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=最大化分类器差异&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=迁移学习&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)
[
																								](https://so.csdn.net/so/search/s.do?q=迁移学习&t=blog)


[https://www.toutiao.com/a6675993553413341700/](https://www.toutiao.com/a6675993553413341700/)
介绍一篇CVPR2018的论文
**Maximum Classifier Discrepancy for Unsupervised Domain Adaptation**
文章的背景是迁移学习，下面我们先介绍一下迁移学习，然后介绍域适应
# 一、迁移学习
目前，图片分类、语义分割等任务的正确率已经非常高了。这得益于人工标注的数据集和深度神经网络的快速发展。但是每次我们要训练一个新的任务时，都需要建立一个新的数据集。这个代价非常的昂贵。
比如我们利用现有的人脸数据集，能很好的识别“谁的脸”，但是现在有一个新的任务是“识别这张脸的年纪”。我们就需要重新标注数据集。又或者存在一个很多年前标注的电脑的数据集，由于时代发展电脑的外观发生的巨大的变化。所以旧的数据集并不能帮助我们很好的识别现在的电脑。所以我们需要重新建立一个数据集。
由上面的例子可以看出 新任务与旧数据集之间的矛盾。我们需要另一种手段去减少标注数据的代价。这时迁移学习应运而生了。
迁移学习是指：
**给定源域和目标域的数据集和任务，利用源域的数据集和任务，辅助目标域的训练。**
在上面的例子中，源域的数据集就是旧的电脑的数据集，有标注。源域的任务是识别电脑。目标域的任务也是识别电脑，数据集只有新电脑的图片，没有标注。这种特殊的迁移学习任务又称“**域适应**”。
换句话说，我的目标任务是没有标注好的数据集的。我要利用现有的，但是不怎么适用的数据集来解决我的目标任务。这就是这篇文章的主题
# 域适应存在的问题
传统的域适应仅区分源域的特征和目标域的特征，并不考虑每个类的边界
完全对齐源域和目标域的分布是一件很困难的事情
一般情况下，我们希望源域和目标域提取出来的特征越接近越好。举个例子，假设源域都是红苹果，目标域是黄苹果。如果一个特征提取器都把苹果的轮廓提取出来，那么分类器就能识别目标域的苹果。如果特征提取器把颜色提取出来了，分类器就很可能认为黄色的不是苹果。这样就错了。这其实是分布对齐的一个过程。
但是往往我们的数据集包含很多类，，如果只对齐源域和目标域的分布，而忽略每个类的差异。可想而知效果也不会太好。
![迁移学习之最大分类器差异的无监督域适应](http://p3.pstatp.com/large/pgc-image/67938bcfd429411aa6eed0327d2a5170)
对比
# 简介
**介绍**
本文使用任务特异性的分类器（两个）作为判别器。该判别器可以考虑每个类独特的性质——边界。使用一个特征分类器作为生成器，用于对齐源域和目标域的分布。使用对抗的思想进行训练
**思路**
![迁移学习之最大分类器差异的无监督域适应](http://p3.pstatp.com/large/pgc-image/fb075a9993224c00a289b31b45a173e6)
先看源域（图中虚线部分），可以构造出两个不同的分类器F1和F2，并且都能正确对源域分类（即画一条线完美的分开class A和B）再看目标域（实线部分），靠近源域的地方被划分到正确的类别中。比如对于分类器F1，它将橙色阴影部分划分为 A类，这是错的。
对F1来说，类别B（橙色部分）的边界线是橙色与线相交的部分。边界以上是B类，以下是A类。所以当目标域穿过边界线时，错误就会产生。同理F2也是这样。
两个分类器在目标域上的分歧，即阴影部分，定义了每个类别的边界。**当我们最大化分类器的分歧时，就会得到针对每个类别的决策边界**
当然，我们还希望对齐源域和目标域的分布，即两个橙色的圈能重合，还有两个蓝色的圈能重合。所以我们希望特征提取器提取的特征是相似的，所以在训练是需要最小化分类器分歧。这样就形成了对抗。
以上就是这篇了论文的思路，很简单是不是
# 方法
![迁移学习之最大分类器差异的无监督域适应](http://p9.pstatp.com/large/pgc-image/956b817f48d842c592c0e6b69a89766b)
如上图所示，分为三个阶段
最常见的方法，用交叉熵训练两个分类器
最大化分类器差异，训练F1，F2
最小化分类器差异，训练G

