
# bagging boosting 随机森林 GBDT对比 - bitcarmanlee的博客 - CSDN博客


2018年06月30日 19:21:54[bitcarmanlee](https://me.csdn.net/bitcarmanlee)阅读数：1170



## 1.bagging
产生n个样本的方法可以采用Bootstraping法，这是一种有放回的抽样方法，产生n个样本。在统计学中，Bootstraping 是依靠替换随机采样的任意试验或度量.普通的决策树会受到高方差的困扰。这意味着如果我们把训练数据随机分成两部分，并且给二者都训练一个决策树，我们得到的结果可能就会相当不同。Bootstrap 聚集，或者叫做Bagging(袋装)，是减少统计学习方法的方差的通用过程。
而最终结果采用Bagging的策略来获得，即多数投票机制。
bagging算法的流程大致如下：
1.从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
2.每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
3.如果最终是分类问题，则将上步得到的k个模型采用投票的方式得到分类结果；如果是回归问题，计算上述模型的平均值作为最后的结果。在此过程中，所有模型的权重都相等。
## 2.boosting
前面提到，bagging的核心思想是针对样本进行有放回的抽样，那么boosting的核心思想则是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。
boosting类型的算法主要面临有两个核心问题：
1.在每一轮如何改变训练数据的权值或概率分布？
boosting一般的做法是通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
2.通过什么方式来组合弱分类器？
通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。而提升树(GBDT)通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。
## 3.bagging VS boosting
简单总结一下Bagging与Boosting的对比：
在样本的选择上：
bagging:训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
boosting:每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
样本权重上：
bagging:使用均匀取样，每个样例的权重相等。
boosting:根据错误率不断调整样例的权值，错误率越大则权重越大。
弱分类器的权重：
bagging:所有弱分类器的权重相等。
boosting:每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
并行化：
bagging:因为每棵树都是独立的，所以并行运算很容易。
boosting:每棵树只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
## 4.随机森林
鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的。
随机森林的生成流程大致如下：
1.从样本集中通过重采样的方式产生n个样本
2.假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点
3.重复m次，产生m棵决策树
4.多数投票机制来进行预测
随机森林的随机性主要体现在两个方面：
1.数据集的随机选取：从原始的数据集中采取有放回的抽样（bagging），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。
2.待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。
随机森林的优点：
1.实现简单，训练速度快，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的，可以并行这点对于数据量大的场景就很重要。
2.能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的。这一点对于大数据量也很重要，因为大数据集上通常会面临维度灾难(Dimensional disaster)。
3.相比其他算法，不是很怕特征缺失，因为待选特征也是随机选取；
4.相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
5.对于不平衡的数据集，可以平衡误差；
6.训练完成后可以给出哪些特征比较重要，这一点对于提高模型的可解释性也很重要。
随机森林的不足之处在于：
1.在噪声过大的分类和回归问题还是容易过拟合；
2.相比于单一决策树，它的随机性让我们难以对模型进行解释。
## 5.GBDT
GBDT是以决策树为基学习器的迭代算法，注意GBDT里的决策树都是回归树而不是分类树。GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。
GBDT的优点：
1.GBDT适用面广，离散或连续的数据都可以处理。
2.GBDT几乎可用于所有回归问题（线性/非线性），也可以用于二分类问题。
缺点：
由于弱分类器的依赖关系，GBDT的并行训练比较难。
## 6.RF VS GBDT
1.RF是基于bagging思想，而GBDT是基于boosting思想。Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此boosting的分类精度要优于bagging。
2.随机森林的树可以并行生成，但GBDT只能串行生成。
3.组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
4.随机森林对异常值不太敏感，而GBDT对异常值比较敏感。
5.随机森林是采用投票取多数的方式获取最终结果，而GBDT是将所有结果累加或进行加权累加。
6.随机森林因为是对所有样本进行抽样，所以方差(variance)比较小，但是偏差(bias)比较大,所以随机森林树的深度一般会比较深以减小偏差。GBDT因为对错误分类样本有加权，所以偏差(bias)比较小，而方差(variance)比较打大，所以GBDT树的深度一般比较小，通过减小模型复杂度的方式来减小方差。
参考文献：
1.[https://zhuanlan.zhihu.com/p/25496196](https://zhuanlan.zhihu.com/p/25496196)N问GBDT（1-12答案）

