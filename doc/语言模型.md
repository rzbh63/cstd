# 语言模型



# （一） 工具和使用简介

2015年08月30日 21:13:58 [xmucas](https://me.csdn.net/xmdxcsj) 阅读数：5602



## 一、常用工具

\1.      Kenlm <https://kheafield.com/code/kenlm/>

C++版本，最大特点是速度快、占用内存少

\2.      Srilm <http://www.speech.sri.com/projects/srilm/>

SRI（Standford ResearchInstitute）开发，使用比较广泛，c++版本

\3.      IRSTLM <http://sourceforge.net/projects/irstlm/>

IRSTLM是意大利TrentoFBK-IRST实验室开发的语言模型训练工具包，其开发的目的是处理较大规模的训练数据

\4.      MITLM <https://code.google.com/p/mitlm/>

\5.      BerkeleyLM <https://code.google.com/p/berkeleylm/>

Java版本，速度和kenlm差不多，内存比srilm小

参考：

http://52opencourse.com/111/斯坦福大学自然语言处理第四课-语言模型（language-modeling）

<http://www.52nlp.cn/language-model-training-tools-srilm-details>

 

## 二、Srilm使用

\1.      构建语言模型

【ngram-count】：用于统计ngram的个数和生成最终的语言模型

【ngram-merge】：用于merge多个ngram count文件，适用于内存不足以容下所有语料的情况

\2.      语言模型得分

【ngram】：用于语言模型打分、计算困惑度（perplexity）、产生句子和模型插值

\3.      语言模型插值

【ngram】：用于语言模型打分、计算困惑度（perplexity）、产生句子和模型插值

 

\4.      语言模型剪枝

 

## 三、Srilm使用的文件类型

\1.      Ngram

最常用的arpa文件，格式如下：

\data\

ngram1=n1

ngram2=n2

...

ngramN=nN

 

\1-grams:

p       w               [bow]

...

 

\2-grams:

p       w1 w2                [bow]

...

 

\N-grams:

p       w1 ... wN

...

 

\end\

\2.      Classes

基于class的语言模型的格式，如下：

```
class [p] word1 word2 ...
```

其中class是语言模型中使用的class name，p是该条class定义的概率，word是该class对应的具体word

\3.      Psfg

Probabilistic finite-state grammars是一种被SRIDecipher（SRI的解码器）使用的有限状态机。

name name

nodes Nw1 ... wN

initial i

final f

transitionsT

n1 n2 p

...

\4.      Nbest

用于nbest的重打分







# （二） 评估和类别

2015年11月26日 09:26:21

 

xmucas

 

阅读数：5205

 

标签： [语言模型](http://so.csdn.net/so/search/s.do?q=%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&t=blog) 更多

个人分类： [语言模型](https://blog.csdn.net/xmdxcsj/article/category/5981983)



一、Evaluation

1、  熵 entropy

l  熵（entropy）又称自信息，self-information

描述一个随机变量的不确定性的数量，熵越大，不确定性越大，正确估计其值的可能性越小。越不确定的随机变量越需要大的信息量以确定其值。

![img](https://img-blog.csdn.net/20151126112307535?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

p(x)表示x的分布概率

l  相对熵（relativeentropy）又称KL距离，Kullback-Leibler divergence

衡量相同事件空间里两个概率分布相对差距的测度，当p=q的时候，相对熵为0，当p和q差距变大时，交叉熵也变大。

![img](https://img-blog.csdn.net/20151126112324082?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

p(x)和q(x)代表x的两种概率分布

l  交叉熵（crossentropy）

衡量估计模型和真实概率分布之间的差异

![img](https://img-blog.csdn.net/20151126112333202?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



定义语言L=(Xi)~p(x)与其模型q的交叉熵为：

![img](https://img-blog.csdn.net/20151126112714382?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

表示L的语句(应该表示一句话)，q(x)表示估计模型，由于无法获取真实模型的概率，需要作出一个假设：假定L是稳态遍历的随机过程，即当n无穷大的时候，所有句子的概率和为1。

![img](https://img-blog.csdn.net/20151126112356016?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

2、  困惑度perplexity

![img](https://img-blog.csdn.net/20151126112435019?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

相比如交叉熵的优势在于：

交叉熵的值在6.6位和7.64位之间，对应的ppl在100到200之间，ppl值更容易记。

交叉熵下降2%，对应于Ppl值10%的提升，描述同样的提升，ppl的数值更漂亮。

最重要的一点是，ppl更容易计算，最小的ppl值意味着训练的模型最接近真实模型。

劣势在于：

Ppl适合直接比较两个模型，交叉熵适合描述一个模型的提升。在描述模型的提升的时候，通常使用相比于基线降低的相对百分比，如下图：

![img](https://img-blog.csdn.net/20151126092541874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

 

3、  字错误率 worderror rate

![img](https://img-blog.csdn.net/20151126112447305?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

S表示替换个数，D表示删除个数，I表示插入个数，N表示reference中字的个数。

WER通常用在语音识别结果的评估上面，有标准答案，适合比较，缺点就是过于死板，没有同义词的容错机制。

适合用于同一个任务的不同技术比较，不适合在不同系统不同技术之间的对比。

二、N-gram model



影响n-gram模型性能最重要的两点是阶数和平滑。其中KN平滑算法性能最佳。

n-gram最大的优势在于速度和可靠性，计算简单。

局限性有以下几点：

跨领域的脆弱性——不同领域的语言使用规律不同

独立性假设的无效性——n-gram假设当前词只和前n-1个词有关

语言模型规模——随着阶数的增加，n-gram会呈指数型增长，所以无法使用高阶数的语言模型。基于神经网络的LM可以解决该问题。

语言模型面临两个问题如下：

1、 高阶问题



THE SKY ABOVE OUR HEADS IS BLUE

如果要刻画BLUE和SKY之间的联系，需要6gram的语言模型，将导致语言模型过大。

2、  相似词问题

PARTY WILL BE ON <DAY OF WEEK>

由于训练语料的问题，可能导致Friday的概率明显高于Monday。

三、其他语言模型



1、 Cache LM



在文本中刚刚出现的一些词在后面的句子中再次出现的可能性较大。通过原始n-gram的概率和cache的n-gram概率的线性插值，获得最终的概率。

优势在于可以降低PPL值，劣势在于PPL值降低的同时WER会升高，原因在于识别结果错误的时候会形成错误的反馈。

2、 Class based LM



当训练数据量小时候，为了解决数据稀疏，可以引入classes，然后根据classes训练gram。

该类语言模型的难点在于高的计算复杂度和如何构造class。

3、  Structured LM

在一个句子中的相关词，可能没有出现在临近的位置。Structured LM将句子看做一种树结构，叶子代表词，节点代表连接符号。

4、 Decision Trees andRandomForestLM



使用决策树通过问问题的方式构建LM，但是很难找到好的决策树。计算复杂度较高，在某种程度上面类似于class based LM。

5、 Maximum Entropy LM



将来自不同信息源的语言模型进行结合（类似于插值），获得更好的语言模型。

6、  Neural Network LM

Frederick Jelinek: "Every time I _re alinguist out of my group, the accuracy goes up."

7、  NNLM

训练trick：

每次训练句子随机化，可以减少训练的epoch。

学习率，当提升明显的时候，学习率保持不变，当没有明显提升的时候，学习率减半。

权重矩阵初始化为均值为0、方差为0.1的随机数分布。

 

参考文献：

《statistic language models based on neural networks》

http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf

《统计自然语言处理》宗成庆





# （三） RNN

2015年11月29日 18:09:46 [xmucas](https://me.csdn.net/xmdxcsj) 阅读数：1993



------

## 概况

### feedforward NN based LM

单词使用1-of-V表示，其中V表示词典的大小，单词对应的位置为1，其他为0 
输入：历史词序列，输出是当前词。比如词典大小为50k，使用5-gram，那么输入维度为50k*4 
projection：将50k降维到30，即为30*4 
隐层：100-300 
输出：大小为V，代表概率值$P(w_t|w_{t-4},w_{t-3},w_{t-2},w_{t-1})$

### recurrent NN based LM

相比于feedforward，RNN可以利用到更多的历史信息，但是需要更多的空间用来保存历史信息。 
输入：当前词wtwt，使用1-of-v表示 
输出：下一个词wt+1wt+1的概率，$P(w_{t+1}|w_{t},s(t-1))$表示上一时刻隐层的输出。

------

## trick

### loss function

使用CE，使用MSE容易陷入局部最优，有待论证。

### learninng rate

开始学习率设为0.1，如果在validation set上面提升明显，学习率不变；如果不明显，学习率减半。

### L2 regularization

使用doubleprecision，更新权重的时候的系数，有待论证。

### explode

BPTT会导致梯度过大，导致权重过大，使得训练失败。可以使用限制梯度值的方法，将大小限制在[-15,15]，否则大数据量无法训练成功。

### computational complexity

如果每一个example都unfolding的话，会导致训练复杂度过大，可以选择N个example进行一次unfolding；参数更新online比batch要好，最实用的是使用min-batch（大小设置为10-20）。

### vocabulary trunction

减小词典的大小，Bengio将非常用词归结到一类，然后根据ngram模型对这些词分配概率。可以降低计算量，但是准确率下降较大。

### Factorization of the Output Layer

可以采用对输出结果进行分类的方式。可以根据词频对单词进行分类以减小输出层的大小，称为frequenct binning.

------

## dynamic evaluation

使用测试数据实时更新模型参数，所以在计算前向的同时，还需要计算BPTT，计算量太大。

------

## Combination of Neural Network Models

根据不同的训练策略训练处的RNN模型可能收敛于不同的局部最优，可以使用多个模型同时计算前向，然后做一下平均。

------

## reference

[STATISTICAL LANGUAGE MODELS BASED ON NEURAL NETWORKS](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf)





# （四） RNNLM



