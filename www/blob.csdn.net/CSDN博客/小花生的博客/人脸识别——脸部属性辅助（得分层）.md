
# 人脸识别——脸部属性辅助（得分层） - 小花生的博客 - CSDN博客


2018年11月11日 20:27:39[Peanut_范](https://me.csdn.net/u013841196)阅读数：695



###### 《A Face Recognition Signature Combining Patch-based Features with Soft Facial Attributes》
2018，L. Zhang, P. Dou, I.A. Kakadiaris.
### 1.引言：
这个signature（签名）有两部分组成：**the existing patch-based features**（基于补丁的特征） 和**the soft facial attributes**（软的面部属性）。在匹配器中，**匹配得分的计算：是patch-based features和facial attributes相结合来得到最后的匹配分数，对于不同的面部属性具有不同的权重系数。**
本文提出的signature在UR2D系统上进行评估，在 UHDB31和 IJB-A dataset上超过之前的结果，Rank-1 的准确率分别提升了4%和0.37%。
*参考：X. Xu, H. Le, P. Dou, Y. Wu, I. A. Kakadiaris, Evaluation of a 3D-aided*
*pose invariant 2D face recognition system, in: Proc. International Joint Conference on Biometrics, Denver, CO, 2017, pp. 446–455.*
*网址：**[https://www.researchgate.net/project/3D-Aided-2D-Face-Recognition](https://www.researchgate.net/project/3D-Aided-2D-Face-Recognition)*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111195637149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*总的来说，传统的深度人脸识别网络有两种限制：*
*1.学习到的是隐含特征，并不是人类可读的（human-readable）信息和具有区分力的信息是编码在高维特征空间。*
*2.显式的面部属性特征是被低估的，而这些特征通常可以提高识别表现。*
*2.Signature：*
*本文提出了一个新的人脸识别签名，它包含两个部分：**从UR2D系统中提取的基于补丁的特征和用一个最新的CNN提取的软的脸部属性（40个脸部属性）。*
*1. Patch-based feature component:*
$$
S^{P}
$$
*给定输入人脸图像，UR2D的管道如下：face detection, landmark detection, pose estimation, 3D reconstruction, texture lifting, and signature extraction。*
*在UR2D系统中，可以提取两类签名:姿态鲁棒脸部签名（PRFS）和深姿态鲁棒脸部签名（DPRFS）。*
*PRFS和DPRFS都是基于补丁的签名。*
*在PRFS，面部纹理图像和遮挡掩模图像首先被划分为64个非重叠的局部贴片。然后，在每个局部贴片上提取判别的DFD特征。此外，计算自遮挡编码。*
*在DPRFS中，首先将人脸纹理图像和遮挡掩模图像分成八个部分重叠的局部斑块。由于表情的变化，口部区域被忽略了。*
*Signature由两部分组成：特征矩阵和遮挡编码。*
*基于DPRFS的签名生成过程：*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111200059268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*参考：《Fully Associative Patch-based 1-to-N Matcher for Face Recognition》*
*[
](https://img-blog.csdnimg.cn/20181111200059268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)**2. Soft facial attribute component:*
$$
S^{A}
$$
*输入一张2D的图像I，使用CNNs提取人脸属性。*
*网络结构：*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111200231545.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*Labels：40 facial attributes*
*1）5_o_Clock_Shadow：刚长出的双颊胡须  2）Arched_Eyebrows：柳叶眉*
*3）Attractive：吸引人的  4）Bags_Under_Eyes：眼袋  5）Bald：秃头  6）Bangs：刘海*
*7）Big_Lips：大嘴唇  8）Big_Nose：大鼻子  9）Black_Hair：黑发 10）Blond_Hair：金发*
*11）Blurry：模糊的 12）Brown_Hair：棕发 13）Bushy_Eyebrows：浓眉*
*14）Chubby：圆胖的 15）Double_Chin：双下巴 16）Eyeglasses：眼镜*
*17）Goatee：山羊胡子 18）Gray_Hair：灰发或白发 19）Heavy_Makeup：浓妆*
*20）High_Cheekbones：高颧骨 21）Male：男性 22）Mouth_Slightly_Open：微微张开嘴巴*
*23）Mustache：胡子，髭 24）Narrow_Eyes：细长的眼睛 25）No_Beard：无胡子*
*26）Oval_Face：椭圆形的脸 27）Pale_Skin：苍白的皮肤 28）Pointy_Nose：尖鼻子*
*29）Receding_Hairline：发际线后移 30）Rosy_Cheeks：红润的双颊*
*31）Sideburns：连鬓胡子 32）Smiling：微笑 33）Straight_Hair：直发*
*34）Wavy_Hair：卷发 35）Wearing_Earrings：戴着耳环 36）Wearing_Hat：戴着帽子*
*37）Wearing_Lipstick：涂了唇膏 38）Wearing_Necklace：戴着项链*
*39）Wearing_Necktie：戴着领带 40）Young：年轻人*
*损失函数：sigmoid function，*
$$
p_{i}=\frac{1}{1+e^{-a_{i}}}
$$
*让*![](https://img-blog.csdnimg.cn/201811112010342.png)*[,表示人脸属性层输出的结果，概率值](https://img-blog.csdnimg.cn/201811112010342.png)*![在这里插入图片描述](https://img-blog.csdnimg.cn/2018111120111816.png)*[，](https://img-blog.csdnimg.cn/2018111120111816.png)*
*在P上设置一个阈值0.5，一个二分类的向量为*![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201156693.png)
*让*![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201215536.png)*[（40x2），表示facial attribute signature component。](https://img-blog.csdnimg.cn/20181111201215536.png)*
*算法流程：*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201255479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*3.Signature Matching*
*1. Patch-based feature component matching*
*在UR2D system中，对于gallery和probe图像使用cosine得分来衡量相似度。*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201336648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*2. Soft facial attribute component matching*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201410580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*λ代表人脸属性部分所占的权重。*
*之前的匹配问题所有的人脸属性都被平等对待，然而，不同的人脸属性可能有不同的权重。*
*Eg：“Bags under eyes”的权重应该比“Eye glasses”属性权重大；*
*“Receding hairline”的权重应该比“Black hair” or ‘Blond hair”所占的比重大。*
*让*![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201435671.png)*[向量来表示每个属性的权重，为](https://img-blog.csdnimg.cn/20181111201435671.png)*![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201507843.png)*[和](https://img-blog.csdnimg.cn/20181111201507843.png)*![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201520403.png)*[,相似性计算是：](https://img-blog.csdnimg.cn/20181111201520403.png)*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201616773.png)*[=](https://img-blog.csdnimg.cn/20181111201616773.png)*![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201628236.png)
*[
](https://img-blog.csdnimg.cn/20181111201616773.png)**最后同权值属性的匹配得分是：*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201650438.png)
*算法流程：*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201719191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*4.实验：*
*The datasets used for testing are the UHDB31 dataset 和 the IJB-A dataset signature matcher的权重λ设置为0.1，在third dataset CASIA WebFace上进行网格搜索获得。Range 0f = {0.1,0.2,…,1}。*
*The proposed signature with facial attribute is represented as UR2D-A.*
*The weight vector of the Weighted attribute matcher (UR2DA-W) is decided by the training accuracy of each attribute.*
*The weight vector of the weighted Probe attribute matcher (UR2D-A-P) is decided by the attribute confidence scores of each probe image.*
*1. Constrained face recognition*
*The UHDB31 dataset contains 29,106 color face images of 77 subjects with 21 poses and 18 illuminations.*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201804608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*To evaluate the performance of cross pose face recognition, the front pose (pose-11) face images are used as gallery the remaining images from 20 poses are used as probe.*
*Table2和3是同PRFS特征和DPRFS的结果：*
*The accuracy improvements range from1% to 8%.*
*The accuracy improvements range from 1% to 4%.*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201842820.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201900812.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*2.Unconstrained face recognition*
*The IJB-A dataset contains images and videos from 500 subjects captured from “in the wild” environment.*
*According to the IJB-A protocol, it splits galleries and probes into 10 splits.*
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111201930566.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
*with PRFS signature, the proposed UR2D-A signature can improve the accuracy under all the splits. The average accuracy is improved by 3.19% and 3.29% with UR2D-A-VGG-Face and UR2DA-*
*ResNet, respectively.*
*Under DPRFS, the proposed UR2D-A signature also achieves better performance. The average accuracy is improved by 0.21% and 0.33% with VGG-Face and ResNet, respectively.*
*3. Sensitivity Analysis*
*对于不同的匹配器评估不同的λ值，范围{0.1,0.2，…,1}。*
*It can be observed that UR2D-A, UR2D-A-W and UR2D-A-P perform similarly on the two datasets. Different methods achieve the best result with different λvalues. Also, the performance of DPRFS is less sensitive to than that of PRFS.*
![在这里插入图片描述](https://img-blog.csdnimg.cn/2018111120200357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111202011771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111202020537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181111202031809.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70)
---
*注：博众家之所长，集群英之荟萃。*

