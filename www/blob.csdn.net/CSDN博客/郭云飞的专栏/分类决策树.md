
# 分类决策树 - 郭云飞的专栏 - CSDN博客


2017年10月11日 18:08:56[guoyunfei20](https://me.csdn.net/guoyunfei20)阅读数：190


决策树是一个简单易用的机器学习算法，具有很好的实用性。在风险评估、数据分类、专家系统中都能见到决策树的身影。决策树其实是一系列的*if-then*规则的集合，它有可读性良好，分类速度快等优点。
把决策树看成是一些**if-then**规则的集合，在每一层树上根据属性的值判断走势，至到遇到叶节点，叶节点对应的就是该数据的归类。
决策时的几个名词：
根节点（最重要的特征）；
父节点与子节点是一对，先有父节点，才会有子节点；
叶节点（最终标签）。
决策树生成算法遇到的第一个问题就是**特征选择问题**，即第一次、第二次...第n次应该根据哪个属性来划分数据。这里需要引入几个概念：
**熵：**
是衡量变量不确定性的一种度量方式。熵越大表示变量的不确定性越大，反之，则越小。定义熵的大小为：
![](http://latex.codecogs.com/gif.latex?H(X)=-%5Csum_%7Bi=1%7D%5E%7Bn%7Dp_ilogp_i)
从熵的计算公式可以看出——**0 ≤ H(X) ≤ logn**
当*p**i*等于0或1时，*H(X)*=0,所以说熵越小不确定性越小，当熵等于0时，就丧失了不确定性。
**条件熵：**
条件熵表示在已知随机变量X的条件下随机变量Y的不确定性大小，记为:
![](http://latex.codecogs.com/gif.latex?H(Y/X)=%5Csum_%7Bi=1%7D%5E%7Bn%7DP(X=x_i)H(Y/X=x_i))
**信息增益：**
定义为集合**D**的经验熵*H(D)*与特征*A*给定的条件下*D*的条件熵*H(D/A)*之差：**g(D,A) = H(D) - H(D/A)**
OK
回到生成决策树上面，我们现在需要的是计算使得**g(D,A)**值最大的特征A，即求出:
![](http://latex.codecogs.com/gif.latex?%5Cunderset%7BA%7D%7Bargmax%7Dg(D,A))
除了上面介绍的信息增益，还有一种作为特征选择的准则：**信息增益比**。由于在计算信息增益的过程中，如果一个特征它的取值较多的话，对应的信息增益值也会比其他特征对应的信息增益值高，这样就会导致在特征选择上出现不公平的现象。信息增益比计算公式：
![](http://latex.codecogs.com/gif.latex?g_%7BR%7D(D,A)%20=%20%5Cfrac%7Bg(D/A)%7D%7BH_%7BA%7D(D)%7D)，其中
![](http://latex.codecogs.com/gif.latex?H_%7BA(D)%7D%20=%20-%5Csum_%7Bi=1%7D%5E%7Bn%7D%5Cfrac%7B%5Cleft%20%7C%20D_i%20%5Cright%20%7C%7D%7B%5Cleft%20%7C%20D%20%5Cright%20%7C%7Dlog_%7B2%7D%5Cfrac%7B%5Cleft%20%7C%20D_i%20%5Cright%20%7C%7D%7B%5Cleft%20%7C%20D%20%5Cright%20%7C%7D)
生成决策树的算法有三类：
ID3
C4.5
CART（分类回归树）
ID3是基础，C4.5算法是从ID3的基础上发展而来的（在ID3的算法上进行改进，引入了**信息增益比**来作为特征选择的准则）。CART(分类回归树)是一种二叉决策树，它既能用于分类也可以用于回归。CART一般用基尼指数*(Gini index)*最小化准则进行特征选择。
**剪枝：**
在机器学习中有一个很常见的问题——过拟合，就是指模型在对训练数据的预测结果上往往能达到很高的准确率，但对新的未知数据却达不到那么高的准确率。在决策树的生成过程中往往会出现过拟合现象，所以需要对决策树进行剪枝。
剪枝的过程就是通过极小化决策树的损失函数：
![](http://latex.codecogs.com/gif.latex?C_%7B%5Calpha%20%7D(T)%20=%20%5Csum_%7Bt=1%7D%5E%7B%5Cleft%20%7C%20T%20%5Cright%20%7C%7DN_tH_t(T)+%5Calpha%5Cleft%20%7C%20T%20%5Cright%20%7C)
*|T|*表示叶结点的个数，*t*表示树的叶结点，*N**t*代表叶结点*t*的样本点数；![](http://latex.codecogs.com/gif.latex?H_t(T))为叶结点t的经验熵，参数*α*用来控制模型与训练数据的拟合程度跟模型复杂度*|T|*之间的影响。

一个Python实例（包含全部数据集和代码）见*GitHub*：

```python
https://github.com/conancome/decesion-tree-implement
```
README信息：
a implement of decision tree with python
decision_tree.py : build decison tree with C4.5 algorithm  // 采用了C4.5生成决策树算法
irisTrain.txt : train datasets
irisTest.txt : test datasets
**再论决策树之——决策时和线性模型、回归模型的区别**
树形模型是一个一个特征进行处理，而线性模型是所有特征给予权重相加得到一个新的值。
决策树与逻辑回归的分类区别也在于此。逻辑回归是将所有特征变换为概率后，通过大于某一概率阈值的划分为一类，小于某一概率阈值的为另一类；而决策树是对每一个特征做一个划分。另外逻辑回归只能找到线性分割（输入特征*x*与*logit*之间是线性的，除非对*x*进行多维映射），而决策树可以找到非线性分割。
树形模型更加接近人的思维方式，可以产生可视化的分类规则，产生的模型具有可解释性（可以抽取规则）。树模型拟合出来的函数其实是分区间的阶梯函数。
**再论决策树之****——决策树思想，实际上就是寻找最纯净的划分方法**
这个最纯净在数学上叫纯度，纯度通俗点理解就是目标变量要分得足够开（y=1的和y=0的混到一起就会不纯）。另一种理解是分类误差率的一种衡量。实际决策树算法往往用到的是，纯度的另一面也即不纯度，下面是不纯度的公式：
![](https://img-blog.csdn.net/20171012114702083?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
不纯度的选取有多种方法，每种方法也就形成了不同的决策树方法，比如ID3算法使用信息增益作为不纯度；C4.5算法使用信息增益率（比）作为不纯度；CART算法使用基尼系数作为不纯度。













