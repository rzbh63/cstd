
# 自然语言处理5 -- 词向量 - 谢杨易的博客 - CSDN博客

2018年08月27日 10:23:43[谢杨易](https://me.csdn.net/u013510838)阅读数：1239


系列文章，请多关注
[Tensorflow源码解析1 – 内核架构和源码结构](https://blog.csdn.net/u013510838/article/details/84103503)
[带你深入AI（1） - 深度学习模型训练痛点及解决方法](https://blog.csdn.net/u013510838/article/details/79835563)
[自然语言处理1 – 分词](https://blog.csdn.net/u013510838/article/details/81673016)
[自然语言处理2 – jieba分词用法及原理](https://blog.csdn.net/u013510838/article/details/81738431)
[自然语言处理3 – 词性标注](https://blog.csdn.net/u013510838/article/details/81907121)
[自然语言处理4 – 句法分析](https://blog.csdn.net/u013510838/article/details/81976427)
[自然语言处理5 – 词向量](https://blog.csdn.net/u013510838/article/details/82108381)
[自然语言处理6 – 情感分析](https://blog.csdn.net/u013510838/article/details/82558797)
# 1 概述
词向量和分词一样，也是自然语言处理中的基础性工作。词向量一方面解决了词语的编码问题，另一方面也解决了词的同义关系，使得基于LSTM等深度学习模型的自然语言处理成为了可能。和分词不同，中英文文本，均需要进行词向量编码。
# 2 词向量工具
2013年Google开源了word2vec工具，它可以进行词向量训练，加载已有模型进行增量训练，求两个词向量相似度，求与某个词接近的词语，等等。功能十分丰富，基本能满足我们对于词向量的需求。下面详细讲解怎么使用word2vec
### 2.1 模型训练
词向量模型训练只需要有训练语料即可，语料越丰富准确率越高，属于无监督学习。后面会讲词向量训练算法和代码实现，这儿先说怎么利用word2vec工具进行词向量模型训练。
```python
# gensim是自然语言处理的一个重要Python库，它包括了Word2vec
```
```python
import
```
```python
gensim
```
```python
from
```
```python
gensim
```
```python
.
```
```python
models
```
```python
import
```
```python
word2vec
```
```python
# 语句，由原始语句经过分词后划分为的一个个词语
```
```python
sentences
```
```python
=
```
```python
[
```
```python
[
```
```python
'网商银行'
```
```python
,
```
```python
'体验'
```
```python
,
```
```python
'好'
```
```python
]
```
```python
,
```
```python
[
```
```python
'网商银行'
```
```python
,
```
```python
'转账'
```
```python
,
```
```python
'快'
```
```python
]
```
```python
]
```
```python
# 使用word2vec进行训练
```
```python
# min_count: 词语频度，低于这个阈值的词语不做词向量
```
```python
# size:每个词对应向量的维度，也就是向量长度
```
```python
# workers：并行训练任务数
```
```python
model
```
```python
=
```
```python
word2vec
```
```python
.
```
```python
Word2Vec
```
```python
(
```
```python
sentences
```
```python
,
```
```python
size
```
```python
=
```
```python
256
```
```python
,
```
```python
min_count
```
```python
=
```
```python
1
```
```python
)
```
```python
# 保存词向量模型，下次只需要load就可以用了
```
```python
model
```
```python
.
```
```python
save
```
```python
(
```
```python
"word2vec_atec"
```
```python
)
```
### 2.2 增量训练
有时候我们语料不是很丰富，但都是针对的某个垂直场景的，比如网商银行相关的语料。此时我们训练词向量时，可以先基于一个已有的模型进行增量训练，这样就可以得到包含特定语料的比较准确的词向量了。
```python
# 先加载已有模型
```
```python
model
```
```python
=
```
```python
gensim
```
```python
.
```
```python
models
```
```python
.
```
```python
Word2Vec
```
```python
.
```
```python
load
```
```python
(
```
```python
"word2vec_atec"
```
```python
)
```
```python
# 进行增量训练
```
```python
corpus
```
```python
=
```
```python
[
```
```python
[
```
```python
'网商银行'
```
```python
,
```
```python
'余利宝'
```
```python
,
```
```python
'收益'
```
```python
,
```
```python
'高'
```
```python
]
```
```python
,
```
```python
[
```
```python
'贷款'
```
```python
,
```
```python
'发放'
```
```python
,
```
```python
'快'
```
```python
]
```
```python
]
```
```python
# 新增语料
```
```python
model
```
```python
.
```
```python
build_vocab
```
```python
(
```
```python
corpus
```
```python
,
```
```python
update
```
```python
=
```
```python
True
```
```python
)
```
```python
# 训练该行
```
```python
model
```
```python
.
```
```python
train
```
```python
(
```
```python
corpus
```
```python
,
```
```python
total_examples
```
```python
=
```
```python
model
```
```python
.
```
```python
corpus_count
```
```python
,
```
```python
epochs
```
```python
=
```
```python
model
```
```python
.
```
```python
iter
```
```python
)
```
```python
# 保存增量训练后的新模型
```
```python
model
```
```python
.
```
```python
save
```
```python
(
```
```python
"../data/word2vec_atec"
```
```python
)
```
### 2.3 求词语相似度
可以利用词向量来求两个词语的相似度。词向量的余弦夹角越小，则相似度越高。
```python
# 验证词相似程度
```
```python
print
```
```python
model
```
```python
.
```
```python
wv
```
```python
.
```
```python
similarity
```
```python
(
```
```python
'花呗'
```
```python
.
```
```python
decode
```
```python
(
```
```python
'utf-8'
```
```python
)
```
```python
,
```
```python
'借呗'
```
```python
.
```
```python
decode
```
```python
(
```
```python
'utf-8'
```
```python
)
```
```python
)
```
### 2.4 求与词语相近的多个词语
```python
for
```
```python
i
```
```python
in
```
```python
model
```
```python
.
```
```python
most_similar
```
```python
(
```
```python
u
```
```python
"我"
```
```python
)
```
```python
:
```
```python
print
```
```python
i
```
```python
[
```
```python
0
```
```python
]
```
```python
,
```
```python
i
```
```python
[
```
```python
1
```
```python
]
```
# 3 词向量训练算法
词向量可以通过使用大规模语料进行无监督学习训练得到，常用的算法有CBOW连续词袋模型和skip-gram跳字模型。二者没有本质的区别，算法框架完全相同。区别在于，CBOW利用上下文来预测中心词。而skip-gram则相反，利用中心词来预测上下文。比如对于语料{“The”, “cat”, “jump”, “over”, “the”, “puddle”} ，CBOW利用上下文{“The”, “cat”, “over”, “the”, “puddle”} 预测中心词“jump”，而skip-gram则利用jump来预测上下文的词，比如jump->cat, jump->over。一般来说，CBOW适合小规模训练语料，对其进行平滑处理。skip-gram适合大规模训练语料，可以基于滑窗随机选择上下文词语。word2vec模型训练时默认采用skip-gram。
# 4 词向量训练代码实现
下面来看一个基于skip-gram的词向量训练的代码实现，这样就能够skip-gram算法有比较深刻的理解。CBOW算法和skip-gram基本相同。代码来自TensorFlow官方教程[https://www.tensorflow.org/tutorials/word2vec](https://www.tensorflow.org/tutorials/word2vec)
```python
# -*- coding: utf-8 -*-
```
```python
from
```
```python
__future__
```
```python
import
```
```python
absolute_import
```
```python
from
```
```python
__future__
```
```python
import
```
```python
division
```
```python
from
```
```python
__future__
```
```python
import
```
```python
print_function
```
```python
import
```
```python
collections
```
```python
import
```
```python
math
```
```python
import
```
```python
os
```
```python
import
```
```python
random
```
```python
import
```
```python
zipfile
```
```python
import
```
```python
numpy
```
```python
as
```
```python
np
```
```python
from
```
```python
six
```
```python
.
```
```python
moves
```
```python
import
```
```python
urllib
```
```python
from
```
```python
six
```
```python
.
```
```python
moves
```
```python
import
```
```python
xrange
```
```python
# pylint: disable=redefined-builtin
```
```python
import
```
```python
tensorflow
```
```python
as
```
```python
tf
```
```python
# 1 下载语料文件，并校验文件字节数是否正确
```
```python
url
```
```python
=
```
```python
'http://mattmahoney.net/dc/'
```
```python
def
```
```python
maybe_download
```
```python
(
```
```python
filename
```
```python
,
```
```python
expected_bytes
```
```python
)
```
```python
:
```
```python
if
```
```python
not
```
```python
os
```
```python
.
```
```python
path
```
```python
.
```
```python
exists
```
```python
(
```
```python
filename
```
```python
)
```
```python
:
```
```python
urllib
```
```python
.
```
```python
request
```
```python
.
```
```python
urlretrieve
```
```python
(
```
```python
url
```
```python
+
```
```python
filename
```
```python
,
```
```python
filename
```
```python
)
```
```python
statinfo
```
```python
=
```
```python
os
```
```python
.
```
```python
stat
```
```python
(
```
```python
filename
```
```python
)
```
```python
if
```
```python
(
```
```python
statinfo
```
```python
.
```
```python
st_size
```
```python
==
```
```python
expected_bytes
```
```python
)
```
```python
:
```
```python
print
```
```python
(
```
```python
"get text and verified"
```
```python
)
```
```python
else
```
```python
:
```
```python
raise
```
```python
Exception
```
```python
(
```
```python
"text size is not correct"
```
```python
)
```
```python
return
```
```python
filename
filename
```
```python
=
```
```python
maybe_download
```
```python
(
```
```python
"text8.zip"
```
```python
,
```
```python
31344016
```
```python
)
```
```python
# 2 语料处理，弄成一个个word组成的list, 以空格作为分隔符。
```
```python
# 如果是中文语料，这一步还需要进行分词
```
```python
def
```
```python
read_data
```
```python
(
```
```python
filename
```
```python
)
```
```python
:
```
```python
with
```
```python
zipfile
```
```python
.
```
```python
ZipFile
```
```python
(
```
```python
filename
```
```python
)
```
```python
as
```
```python
f
```
```python
:
```
```python
data
```
```python
=
```
```python
tf
```
```python
.
```
```python
compat
```
```python
.
```
```python
as_str
```
```python
(
```
```python
f
```
```python
.
```
```python
read
```
```python
(
```
```python
f
```
```python
.
```
```python
namelist
```
```python
(
```
```python
)
```
```python
[
```
```python
0
```
```python
]
```
```python
)
```
```python
)
```
```python
.
```
```python
split
```
```python
(
```
```python
)
```
```python
return
```
```python
data
vocabulay
```
```python
=
```
```python
read_data
```
```python
(
```
```python
filename
```
```python
)
```
```python
print
```
```python
(
```
```python
"total word size %d"
```
```python
%
```
```python
len
```
```python
(
```
```python
vocabulay
```
```python
)
```
```python
)
```
```python
print
```
```python
(
```
```python
"100 words at first: "
```
```python
,
```
```python
vocabulay
```
```python
[
```
```python
0
```
```python
:
```
```python
100
```
```python
]
```
```python
)
```
```python
# 3 词表制作，根据出现频率排序，序号代表这个单词。词语编码的一种常用方式
```
```python
def
```
```python
build_dataset
```
```python
(
```
```python
words
```
```python
,
```
```python
n_words
```
```python
)
```
```python
:
```
```python
count
```
```python
=
```
```python
[
```
```python
[
```
```python
"UNK"
```
```python
,
```
```python
-
```
```python
1
```
```python
]
```
```python
]
```
```python
count
```
```python
.
```
```python
extend
```
```python
(
```
```python
collections
```
```python
.
```
```python
Counter
```
```python
(
```
```python
words
```
```python
)
```
```python
.
```
```python
most_common
```
```python
(
```
```python
n_words
```
```python
-
```
```python
1
```
```python
)
```
```python
)
```
```python
dictionay
```
```python
=
```
```python
dict
```
```python
(
```
```python
)
```
```python
for
```
```python
word
```
```python
,
```
```python
_
```
```python
in
```
```python
count
```
```python
:
```
```python
# 利用按照出现频率排序好的词语的位置，来代表这个词语
```
```python
dictionay
```
```python
[
```
```python
word
```
```python
]
```
```python
=
```
```python
len
```
```python
(
```
```python
dictionay
```
```python
)
```
```python
# data包含语料库中的所有词语，低频的词语标注为UNK。这些词语都是各不相同的
```
```python
data
```
```python
=
```
```python
list
```
```python
(
```
```python
)
```
```python
unk_count
```
```python
=
```
```python
0
```
```python
for
```
```python
word
```
```python
in
```
```python
words
```
```python
:
```
```python
if
```
```python
word
```
```python
in
```
```python
dictionay
```
```python
:
```
```python
index
```
```python
=
```
```python
dictionay
```
```python
[
```
```python
word
```
```python
]
```
```python
else
```
```python
:
```
```python
index
```
```python
=
```
```python
0
```
```python
unk_count
```
```python
+=
```
```python
1
```
```python
data
```
```python
.
```
```python
append
```
```python
(
```
```python
index
```
```python
)
```
```python
count
```
```python
[
```
```python
0
```
```python
]
```
```python
[
```
```python
1
```
```python
]
```
```python
=
```
```python
unk_count
```
```python
# unk的个数
```
```python
# 将key value reverse一下，使用数字来代表这个词语
```
```python
reversed_dictionary
```
```python
=
```
```python
dict
```
```python
(
```
```python
zip
```
```python
(
```
```python
dictionay
```
```python
.
```
```python
values
```
```python
(
```
```python
)
```
```python
,
```
```python
dictionay
```
```python
.
```
```python
keys
```
```python
(
```
```python
)
```
```python
)
```
```python
)
```
```python
return
```
```python
data
```
```python
,
```
```python
count
```
```python
,
```
```python
dictionay
```
```python
,
```
```python
reversed_dictionary
VOC_SIZE
```
```python
=
```
```python
50000
```
```python
data
```
```python
,
```
```python
count
```
```python
,
```
```python
dictionary
```
```python
,
```
```python
reversed_dictionary
```
```python
=
```
```python
build_dataset
```
```python
(
```
```python
vocabulay
```
```python
,
```
```python
VOC_SIZE
```
```python
)
```
```python
del
```
```python
vocabulay
```
```python
print
```
```python
(
```
```python
"most common words"
```
```python
,
```
```python
count
```
```python
[
```
```python
0
```
```python
:
```
```python
5
```
```python
]
```
```python
)
```
```python
# 打印前10个单词的数字序号
```
```python
print
```
```python
(
```
```python
"sample data"
```
```python
,
```
```python
data
```
```python
[
```
```python
:
```
```python
10
```
```python
]
```
```python
,
```
```python
[
```
```python
reversed_dictionary
```
```python
[
```
```python
i
```
```python
]
```
```python
for
```
```python
i
```
```python
in
```
```python
data
```
```python
[
```
```python
:
```
```python
10
```
```python
]
```
```python
]
```
```python
)
```
```python
# 4 生成训练的batch label对
```
```python
data_index
```
```python
=
```
```python
0
```
```python
# skip_window表示与target中心词相关联的上下文的长度。整个Buffer为 (2 * skip_window + 1)，从skip_window中随机选取num_skips个单词作为label
```
```python
# 最后形成 target->label1 target->label2的batch label对组合
```
```python
def
```
```python
generate_batch
```
```python
(
```
```python
batch_size
```
```python
,
```
```python
num_skips
```
```python
,
```
```python
skip_window
```
```python
)
```
```python
:
```
```python
global
```
```python
data_index
    batch
```
```python
=
```
```python
np
```
```python
.
```
```python
ndarray
```
```python
(
```
```python
shape
```
```python
=
```
```python
(
```
```python
batch_size
```
```python
)
```
```python
,
```
```python
dtype
```
```python
=
```
```python
np
```
```python
.
```
```python
int32
```
```python
)
```
```python
labels
```
```python
=
```
```python
np
```
```python
.
```
```python
ndarray
```
```python
(
```
```python
shape
```
```python
=
```
```python
(
```
```python
batch_size
```
```python
,
```
```python
1
```
```python
)
```
```python
,
```
```python
dtype
```
```python
=
```
```python
np
```
```python
.
```
```python
int32
```
```python
)
```
```python
# 将skip_window的数据组合放入Buffer中
```
```python
span
```
```python
=
```
```python
2
```
```python
*
```
```python
skip_window
```
```python
+
```
```python
1
```
```python
buffer
```
```python
=
```
```python
collections
```
```python
.
```
```python
deque
```
```python
(
```
```python
maxlen
```
```python
=
```
```python
span
```
```python
)
```
```python
for
```
```python
_
```
```python
in
```
```python
range
```
```python
(
```
```python
span
```
```python
)
```
```python
:
```
```python
buffer
```
```python
.
```
```python
append
```
```python
(
```
```python
data
```
```python
[
```
```python
data_index
```
```python
]
```
```python
)
```
```python
data_index
```
```python
=
```
```python
(
```
```python
data_index
```
```python
+
```
```python
1
```
```python
)
```
```python
%
```
```python
len
```
```python
(
```
```python
data
```
```python
)
```
```python
# 防止超出data数组范围，因为batch可以取很多次迭代。所以可以循环重复
```
```python
# num_skips表示一个Buffer中选取几个batch->label对，每一对为一个batch，故需要batch_size // num_skips个Buffer
```
```python
for
```
```python
i
```
```python
in
```
```python
range
```
```python
(
```
```python
batch_size
```
```python
//
```
```python
num_skips
```
```python
)
```
```python
:
```
```python
target
```
```python
=
```
```python
skip_window
        targets_to_avoid
```
```python
=
```
```python
[
```
```python
skip_window
```
```python
]
```
```python
# 一个Buffer内部寻找num_skips个label
```
```python
for
```
```python
j
```
```python
in
```
```python
range
```
```python
(
```
```python
num_skips
```
```python
)
```
```python
:
```
```python
# 寻找label的位置，总共会有num_skips个label
```
```python
while
```
```python
target
```
```python
in
```
```python
targets_to_avoid
```
```python
:
```
```python
# 中间那个为batch，不能选为target.也不能重复选target
```
```python
target
```
```python
=
```
```python
random
```
```python
.
```
```python
randint
```
```python
(
```
```python
0
```
```python
,
```
```python
span
```
```python
-
```
```python
1
```
```python
)
```
```python
targets_to_avoid
```
```python
.
```
```python
append
```
```python
(
```
```python
target
```
```python
)
```
```python
# 中心位置为batch，随机选取的num_skips个其他位置的为label
```
```python
batch
```
```python
[
```
```python
i
```
```python
*
```
```python
num_skips
```
```python
+
```
```python
j
```
```python
]
```
```python
=
```
```python
buffer
```
```python
[
```
```python
skip_window
```
```python
]
```
```python
#
```
```python
labels
```
```python
[
```
```python
i
```
```python
*
```
```python
num_skips
```
```python
+
```
```python
j
```
```python
,
```
```python
0
```
```python
]
```
```python
=
```
```python
buffer
```
```python
[
```
```python
target
```
```python
]
```
```python
# 遍历选取的label
```
```python
# 一个Buffer内的num_skips找完之后，向后移动一位，将单词加入Buffer内，并将Buffer内第一个单词移除，从而形成新的Buffer
```
```python
buffer
```
```python
.
```
```python
append
```
```python
(
```
```python
data
```
```python
[
```
```python
data_index
```
```python
]
```
```python
)
```
```python
data_index
```
```python
=
```
```python
(
```
```python
data_index
```
```python
+
```
```python
1
```
```python
)
```
```python
%
```
```python
len
```
```python
(
```
```python
data
```
```python
)
```
```python
# 所有batch都遍历完之后，重新调整data_index指针位置
```
```python
data_index
```
```python
=
```
```python
(
```
```python
data_index
```
```python
+
```
```python
len
```
```python
(
```
```python
data
```
```python
)
```
```python
-
```
```python
span
```
```python
)
```
```python
%
```
```python
len
```
```python
(
```
```python
data
```
```python
)
```
```python
return
```
```python
batch
```
```python
,
```
```python
labels
batch
```
```python
,
```
```python
labels
```
```python
=
```
```python
generate_batch
```
```python
(
```
```python
batch_size
```
```python
=
```
```python
8
```
```python
,
```
```python
num_skips
```
```python
=
```
```python
2
```
```python
,
```
```python
skip_window
```
```python
=
```
```python
1
```
```python
)
```
```python
for
```
```python
i
```
```python
in
```
```python
range
```
```python
(
```
```python
8
```
```python
)
```
```python
:
```
```python
print
```
```python
(
```
```python
batch
```
```python
[
```
```python
1
```
```python
]
```
```python
,
```
```python
reversed_dictionary
```
```python
[
```
```python
batch
```
```python
[
```
```python
i
```
```python
]
```
```python
]
```
```python
,
```
```python
"->"
```
```python
,
```
```python
labels
```
```python
[
```
```python
i
```
```python
,
```
```python
0
```
```python
]
```
```python
,
```
```python
reversed_dictionary
```
```python
[
```
```python
labels
```
```python
[
```
```python
i
```
```python
,
```
```python
9
```
```python
]
```
```python
]
```
```python
)
```
```python
# 5 构造训练模型
```
```python
batch_size
```
```python
=
```
```python
128
```
```python
embedding_size
```
```python
=
```
```python
128
```
```python
# 词向量为128维，也就是每一个word转化为的vec是128维的
```
```python
skip_window
```
```python
=
```
```python
1
```
```python
# 滑窗大小为1， 也就是每次取中心词前后各一个词
```
```python
num_skips
```
```python
=
```
```python
2
```
```python
# 每次取上下文的两个词
```
```python
# 模型验证集, 对前100个词进行验证，每次验证16个词
```
```python
valid_size
```
```python
=
```
```python
16
```
```python
valid_window
```
```python
=
```
```python
100
```
```python
valid_examples
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
choice
```
```python
(
```
```python
valid_window
```
```python
,
```
```python
valid_size
```
```python
,
```
```python
replace
```
```python
=
```
```python
False
```
```python
)
```
```python
# 噪声词数量
```
```python
num_sampled
```
```python
=
```
```python
64
```
```python
graph
```
```python
=
```
```python
tf
```
```python
.
```
```python
Graph
```
```python
(
```
```python
)
```
```python
with
```
```python
graph
```
```python
.
```
```python
as_default
```
```python
(
```
```python
)
```
```python
:
```
```python
train_inputs
```
```python
=
```
```python
tf
```
```python
.
```
```python
placeholder
```
```python
(
```
```python
tf
```
```python
.
```
```python
int32
```
```python
,
```
```python
shape
```
```python
=
```
```python
[
```
```python
batch_size
```
```python
]
```
```python
)
```
```python
train_labels
```
```python
=
```
```python
tf
```
```python
.
```
```python
placeholder
```
```python
(
```
```python
tf
```
```python
.
```
```python
int32
```
```python
,
```
```python
shape
```
```python
=
```
```python
[
```
```python
batch_size
```
```python
,
```
```python
1
```
```python
]
```
```python
)
```
```python
valid_dataset
```
```python
=
```
```python
tf
```
```python
.
```
```python
constant
```
```python
(
```
```python
valid_examples
```
```python
,
```
```python
dtype
```
```python
=
```
```python
tf
```
```python
.
```
```python
int32
```
```python
)
```
```python
# 验证集
```
```python
with
```
```python
tf
```
```python
.
```
```python
device
```
```python
(
```
```python
"/cpu:0"
```
```python
)
```
```python
:
```
```python
# 构造embeddings, 50000个词语，每个词语为长度128的向量
```
```python
embeddings
```
```python
=
```
```python
tf
```
```python
.
```
```python
Variable
```
```python
(
```
```python
tf
```
```python
.
```
```python
random_uniform
```
```python
(
```
```python
[
```
```python
VOC_SIZE
```
```python
,
```
```python
embedding_size
```
```python
]
```
```python
,
```
```python
-
```
```python
1.0
```
```python
,
```
```python
1.0
```
```python
)
```
```python
)
```
```python
embed
```
```python
=
```
```python
tf
```
```python
.
```
```python
nn
```
```python
.
```
```python
embedding_lookup
```
```python
(
```
```python
embeddings
```
```python
,
```
```python
train_inputs
```
```python
)
```
```python
nce_weights
```
```python
=
```
```python
tf
```
```python
.
```
```python
Variable
```
```python
(
```
```python
tf
```
```python
.
```
```python
truncated_normal
```
```python
(
```
```python
[
```
```python
VOC_SIZE
```
```python
,
```
```python
embedding_size
```
```python
]
```
```python
,
```
```python
stddev
```
```python
=
```
```python
1.0
```
```python
/
```
```python
math
```
```python
.
```
```python
sqrt
```
```python
(
```
```python
embedding_size
```
```python
)
```
```python
)
```
```python
)
```
```python
nce_biases
```
```python
=
```
```python
tf
```
```python
.
```
```python
Variable
```
```python
(
```
```python
tf
```
```python
.
```
```python
zeros
```
```python
(
```
```python
[
```
```python
VOC_SIZE
```
```python
]
```
```python
)
```
```python
)
```
```python
# 利用nce loss将多分类问题转化为二分类问题，从而使得词向量训练成为可能，不然分类会是上万的量级
```
```python
loss
```
```python
=
```
```python
tf
```
```python
.
```
```python
reduce_mean
```
```python
(
```
```python
tf
```
```python
.
```
```python
nn
```
```python
.
```
```python
nce_loss
```
```python
(
```
```python
weights
```
```python
=
```
```python
nce_weights
```
```python
,
```
```python
biases
```
```python
=
```
```python
nce_biases
```
```python
,
```
```python
labels
```
```python
=
```
```python
train_labels
```
```python
,
```
```python
inputs
```
```python
=
```
```python
embed
```
```python
,
```
```python
# inputs为经过embeddings词向量之后的train_inputs
```
```python
num_sampled
```
```python
=
```
```python
num_sampled
```
```python
,
```
```python
# 噪声词
```
```python
num_classes
```
```python
=
```
```python
VOC_SIZE
```
```python
,
```
```python
)
```
```python
)
```
```python
optimizer
```
```python
=
```
```python
tf
```
```python
.
```
```python
train
```
```python
.
```
```python
GradientDescentOptimizer
```
```python
(
```
```python
1.0
```
```python
)
```
```python
.
```
```python
minimize
```
```python
(
```
```python
loss
```
```python
)
```
```python
# 归一化embeddings
```
```python
norm
```
```python
=
```
```python
tf
```
```python
.
```
```python
sqrt
```
```python
(
```
```python
tf
```
```python
.
```
```python
reduce_sum
```
```python
(
```
```python
tf
```
```python
.
```
```python
square
```
```python
(
```
```python
embeddings
```
```python
)
```
```python
,
```
```python
1
```
```python
,
```
```python
keep_dims
```
```python
=
```
```python
True
```
```python
)
```
```python
)
```
```python
normalized_embeddings
```
```python
=
```
```python
embeddings
```
```python
/
```
```python
norm
    valid_embeddings
```
```python
=
```
```python
tf
```
```python
.
```
```python
nn
```
```python
.
```
```python
embedding_lookup
```
```python
(
```
```python
normalized_embeddings
```
```python
,
```
```python
valid_dataset
```
```python
)
```
```python
similarity
```
```python
=
```
```python
tf
```
```python
.
```
```python
matmul
```
```python
(
```
```python
valid_embeddings
```
```python
,
```
```python
normalized_embeddings
```
```python
,
```
```python
transpose_b
```
```python
=
```
```python
True
```
```python
)
```
```python
init
```
```python
=
```
```python
tf
```
```python
.
```
```python
global_variables_initializer
```
```python
(
```
```python
)
```
```python
# 6 训练
```
```python
num_steps
```
```python
=
```
```python
100000
```
```python
with
```
```python
tf
```
```python
.
```
```python
Session
```
```python
(
```
```python
graph
```
```python
=
```
```python
graph
```
```python
)
```
```python
as
```
```python
session
```
```python
:
```
```python
init
```
```python
.
```
```python
run
```
```python
(
```
```python
)
```
```python
average_loss
```
```python
=
```
```python
0
```
```python
for
```
```python
step
```
```python
in
```
```python
xrange
```
```python
(
```
```python
num_steps
```
```python
)
```
```python
:
```
```python
# 构建batch，并进行feed
```
```python
batch_inputs
```
```python
,
```
```python
batch_labels
```
```python
=
```
```python
generate_batch
```
```python
(
```
```python
batch_size
```
```python
,
```
```python
num_skips
```
```python
,
```
```python
skip_window
```
```python
)
```
```python
feed_dict
```
```python
=
```
```python
{
```
```python
train_inputs
```
```python
:
```
```python
batch_inputs
```
```python
,
```
```python
train_labels
```
```python
:
```
```python
batch_labels
```
```python
}
```
```python
# run optimizer和loss，跑模型
```
```python
_
```
```python
,
```
```python
loss_val
```
```python
=
```
```python
session
```
```python
.
```
```python
run
```
```python
(
```
```python
[
```
```python
optimizer
```
```python
,
```
```python
loss
```
```python
]
```
```python
,
```
```python
feed_dict
```
```python
=
```
```python
feed_dict
```
```python
)
```
```python
average_loss
```
```python
+=
```
```python
loss_val
```
```python
if
```
```python
step
```
```python
%
```
```python
2000
```
```python
==
```
```python
0
```
```python
and
```
```python
step
```
```python
>
```
```python
0
```
```python
:
```
```python
average_loss
```
```python
/=
```
```python
2000
```
```python
print
```
```python
(
```
```python
"average loss at step "
```
```python
,
```
```python
step
```
```python
,
```
```python
": "
```
```python
,
```
```python
average_loss
```
```python
)
```
```python
average_loss
```
```python
=
```
```python
0
```
```python
# 1万步，验证一次
```
```python
if
```
```python
step
```
```python
%
```
```python
10000
```
```python
==
```
```python
0
```
```python
:
```
```python
sim
```
```python
=
```
```python
similarity
```
```python
.
```
```python
eval
```
```python
(
```
```python
)
```
```python
for
```
```python
i
```
```python
in
```
```python
xrange
```
```python
(
```
```python
valid_size
```
```python
)
```
```python
:
```
```python
valid_word
```
```python
=
```
```python
reversed_dictionary
```
```python
[
```
```python
valid_examples
```
```python
[
```
```python
i
```
```python
]
```
```python
]
```
```python
top_k
```
```python
=
```
```python
8
```
```python
nearest
```
```python
=
```
```python
(
```
```python
-
```
```python
sim
```
```python
[
```
```python
i
```
```python
,
```
```python
:
```
```python
]
```
```python
)
```
```python
.
```
```python
argsort
```
```python
(
```
```python
)
```
```python
[
```
```python
1
```
```python
:
```
```python
top_k
```
```python
+
```
```python
1
```
```python
]
```
```python
log_str
```
```python
=
```
```python
"Nearest to %s:"
```
```python
%
```
```python
valid_word
```
```python
for
```
```python
k
```
```python
in
```
```python
xrange
```
```python
(
```
```python
top_k
```
```python
)
```
```python
:
```
```python
close_word
```
```python
=
```
```python
reversed_dictionary
```
```python
[
```
```python
nearest
```
```python
[
```
```python
k
```
```python
]
```
```python
]
```
```python
log_str
```
```python
=
```
```python
'%s %s,'
```
```python
%
```
```python
(
```
```python
log_str
```
```python
,
```
```python
close_word
```
```python
)
```
```python
print
```
```python
(
```
```python
log_str
```
```python
)
```
```python
final_embeddings
```
```python
=
```
```python
normalized_embeddings
```
```python
.
```
```python
eval
```
```python
(
```
```python
)
```
流程还是很简单的，关键在第四步batch的构建，和第五步训练模型的构建，步骤如下
下载语料文件，并校验文件字节数是否正确。这儿只是一个demo，语料也很小，只有100M。如果想得到比较准确的词向量，一般需要通过爬虫获取维基百科 网易新闻等既丰富又相对准确的语料素材。一般需要几十上百G的corpus，即语料。谷歌根据不同的语料预训练了一些词向量，参考[https://github.com/Embedding/Chinese-Word-Vectors](https://github.com/Embedding/Chinese-Word-Vectors)
语料处理，文本切割为一个个词语。英文的话以空格为分隔符进行切分即可（有误差，但还好）。中文的话需要通过分词工具进行分割。
词表制作，词语预编码。根据词语出现频率排序，序号代表这个单词。词语编码的一种常用方式。
生成训练的batch label对。这是比较关键的一步，也是体现skip-gram算法的一步。
先取出滑窗范围的一组词，如滑窗大小为5，则取出5个词。
位于中心的词为中心词，比如滑窗大小为5，则第三个词为中心词。其他词则称为上下文。
从上下文中随机取出num_skip个词，比如num_skip为2，则从4个上下文词语中取2个。通过随机选取提高了一定的泛化性
得到num_skip个中心词->上下文的x->y词组
将滑窗向右移动一个位置，继续这些步骤，直到滑窗到达文本最后
构造训练模型，这一步也很关键。利用nce loss将多分类问题转化为二分类问题，optimizer优化方法采用随机梯度下降。
开始真正的训练。这一步比较常规化。送入第四步构建的batch进行feed，跑optimizer和loss，并进行相关信息打印即可。训练结束后，即可得到调整完的词向量模型。

# 5 总结
基于深度学习的词向量训练方法，具有算法简单通用，语料获取容易，泛化性好的优点。通过学习官方代码，可以对skip-gram等词向量训练算法有比较深入的理解。词向量在文本分析，文本摘要，情感分析等领域都是必须的预处理，可以大大提高自然语言处理的准确度。
系列文章，请多关注
[Tensorflow源码解析1 – 内核架构和源码结构](https://blog.csdn.net/u013510838/article/details/84103503)
[带你深入AI（1） - 深度学习模型训练痛点及解决方法](https://blog.csdn.net/u013510838/article/details/79835563)
[自然语言处理1 – 分词](https://blog.csdn.net/u013510838/article/details/81673016)
[自然语言处理2 – jieba分词用法及原理](https://blog.csdn.net/u013510838/article/details/81738431)
[自然语言处理3 – 词性标注](https://blog.csdn.net/u013510838/article/details/81907121)
[自然语言处理4 – 句法分析](https://blog.csdn.net/u013510838/article/details/81976427)
[自然语言处理5 – 词向量](https://blog.csdn.net/u013510838/article/details/82108381)
[自然语言处理6 – 情感分析](https://blog.csdn.net/u013510838/article/details/82558797)

