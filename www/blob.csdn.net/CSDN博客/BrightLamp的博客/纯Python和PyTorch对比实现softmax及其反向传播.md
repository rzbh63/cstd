
# 纯Python和PyTorch对比实现softmax及其反向传播 - BrightLamp的博客 - CSDN博客


2018年11月14日 16:45:19[BrightLampCsdn](https://me.csdn.net/oBrightLamp)阅读数：328所属专栏：



## 摘要
本文使用纯 Python 和 PyTorch 对比实现softmax函数及其反向传播.
## 相关
*原理和详细解释, 请参考文章 :*
softmax函数详解及反向传播中的梯度求导
*系列文章索引 :*
[https://blog.csdn.net/oBrightLamp/article/details/85067981](https://blog.csdn.net/oBrightLamp/article/details/85067981)
## 正文
```python
import
```
```python
numpy
```
```python
as
```
```python
np
```
```python
import
```
```python
torch
```
```python
class
```
```python
Solfmax
```
```python
:
```
```python
def
```
```python
__init__
```
```python
(
```
```python
self
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
softmax
```
```python
=
```
```python
None
```
```python
self
```
```python
.
```
```python
grad
```
```python
=
```
```python
None
```
```python
self
```
```python
.
```
```python
dnx
```
```python
=
```
```python
None
```
```python
def
```
```python
__call__
```
```python
(
```
```python
self
```
```python
,
```
```python
nx
```
```python
)
```
```python
:
```
```python
shifted_x
```
```python
=
```
```python
nx
```
```python
-
```
```python
np
```
```python
.
```
```python
max
```
```python
(
```
```python
nx
```
```python
)
```
```python
ex
```
```python
=
```
```python
np
```
```python
.
```
```python
exp
```
```python
(
```
```python
shifted_x
```
```python
)
```
```python
sum_ex
```
```python
=
```
```python
np
```
```python
.
```
```python
sum
```
```python
(
```
```python
ex
```
```python
)
```
```python
self
```
```python
.
```
```python
solfmax
```
```python
=
```
```python
ex
```
```python
/
```
```python
sum_ex
```
```python
return
```
```python
self
```
```python
.
```
```python
solfmax
```
```python
def
```
```python
get_grad
```
```python
(
```
```python
self
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
grad
```
```python
=
```
```python
self
```
```python
.
```
```python
solfmax
```
```python
[
```
```python
:
```
```python
,
```
```python
np
```
```python
.
```
```python
newaxis
```
```python
]
```
```python
*
```
```python
self
```
```python
.
```
```python
solfmax
```
```python
[
```
```python
np
```
```python
.
```
```python
newaxis
```
```python
,
```
```python
:
```
```python
]
```
```python
for
```
```python
i
```
```python
in
```
```python
range
```
```python
(
```
```python
len
```
```python
(
```
```python
self
```
```python
.
```
```python
grad
```
```python
)
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
grad
```
```python
[
```
```python
i
```
```python
,
```
```python
i
```
```python
]
```
```python
-=
```
```python
self
```
```python
.
```
```python
solfmax
```
```python
[
```
```python
i
```
```python
]
```
```python
self
```
```python
.
```
```python
grad
```
```python
=
```
```python
-
```
```python
self
```
```python
.
```
```python
grad
```
```python
return
```
```python
self
```
```python
.
```
```python
grad
```
```python
def
```
```python
backward
```
```python
(
```
```python
self
```
```python
,
```
```python
dl
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
get_grad
```
```python
(
```
```python
)
```
```python
self
```
```python
.
```
```python
dnx
```
```python
=
```
```python
np
```
```python
.
```
```python
sum
```
```python
(
```
```python
self
```
```python
.
```
```python
grad
```
```python
*
```
```python
dl
```
```python
,
```
```python
axis
```
```python
=
```
```python
1
```
```python
)
```
```python
return
```
```python
self
```
```python
.
```
```python
dnx

np
```
```python
.
```
```python
random
```
```python
.
```
```python
seed
```
```python
(
```
```python
123
```
```python
)
```
```python
np
```
```python
.
```
```python
set_printoptions
```
```python
(
```
```python
precision
```
```python
=
```
```python
8
```
```python
,
```
```python
suppress
```
```python
=
```
```python
True
```
```python
,
```
```python
linewidth
```
```python
=
```
```python
120
```
```python
)
```
```python
d_loss
```
```python
=
```
```python
np
```
```python
.
```
```python
array
```
```python
(
```
```python
[
```
```python
11
```
```python
,
```
```python
12
```
```python
,
```
```python
13
```
```python
,
```
```python
14
```
```python
,
```
```python
15
```
```python
,
```
```python
16
```
```python
,
```
```python
17
```
```python
,
```
```python
18
```
```python
,
```
```python
19
```
```python
]
```
```python
,
```
```python
dtype
```
```python
=
```
```python
float
```
```python
)
```
```python
d_loss_tensor
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
d_loss
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
softmax_numpy
```
```python
=
```
```python
Solfmax
```
```python
(
```
```python
)
```
```python
x_numpy
```
```python
=
```
```python
np
```
```python
.
```
```python
array
```
```python
(
```
```python
[
```
```python
1
```
```python
,
```
```python
2
```
```python
,
```
```python
3
```
```python
,
```
```python
4
```
```python
,
```
```python
5
```
```python
,
```
```python
6
```
```python
,
```
```python
7
```
```python
,
```
```python
8
```
```python
,
```
```python
9
```
```python
]
```
```python
,
```
```python
dtype
```
```python
=
```
```python
float
```
```python
)
```
```python
soft_numpy
```
```python
=
```
```python
softmax_numpy
```
```python
(
```
```python
x_numpy
```
```python
)
```
```python
x_grad_numpy
```
```python
=
```
```python
softmax_numpy
```
```python
.
```
```python
backward
```
```python
(
```
```python
d_loss
```
```python
)
```
```python
x_tensor
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
x_numpy
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
soft_tensor
```
```python
=
```
```python
torch
```
```python
.
```
```python
nn
```
```python
.
```
```python
functional
```
```python
.
```
```python
softmax
```
```python
(
```
```python
x_tensor
```
```python
,
```
```python
dim
```
```python
=
```
```python
0
```
```python
)
```
```python
soft_tensor
```
```python
.
```
```python
backward
```
```python
(
```
```python
d_loss_tensor
```
```python
)
```
```python
x_grad_tensor
```
```python
=
```
```python
x_tensor
```
```python
.
```
```python
grad
```
```python
print
```
```python
(
```
```python
soft_numpy
```
```python
)
```
```python
print
```
```python
(
```
```python
soft_tensor
```
```python
.
```
```python
data
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
)
```
```python
print
```
```python
(
```
```python
)
```
```python
print
```
```python
(
```
```python
x_grad_numpy
```
```python
)
```
```python
print
```
```python
(
```
```python
x_grad_tensor
```
```python
.
```
```python
data
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
)
```
```python
"""
代码输出 :
[ 0.00021208  0.00057649  0.00156706  0.00425972  0.01157912  0.03147531  0.08555877  0.23257286  0.63219858]
[ 0.00021208  0.00057649  0.00156706  0.00425972  0.01157912  0.03147531  0.08555877  0.23257286  0.63219858]
[-0.00157344 -0.00370057 -0.00849213 -0.01882428 -0.03959057 -0.07614301 -0.12141937 -0.09747922  0.36722258]
[-0.00157344 -0.00370057 -0.00849213 -0.01882428 -0.03959057 -0.07614301 -0.12141937 -0.09747922  0.36722258]
"""
```

