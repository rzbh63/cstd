
# CS229part3广义线性模型 - OraYang的博客 - CSDN博客

2017年08月30日 17:14:04[OraYang](https://me.csdn.net/u010665216)阅读数：366标签：[广义线性回归																](https://so.csdn.net/so/search/s.do?q=广义线性回归&t=blog)[普通最小二乘																](https://so.csdn.net/so/search/s.do?q=普通最小二乘&t=blog)[softmax回归																](https://so.csdn.net/so/search/s.do?q=softmax回归&t=blog)[逻辑回归																](https://so.csdn.net/so/search/s.do?q=逻辑回归&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=softmax回归&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=普通最小二乘&t=blog)个人分类：[Machine_learning																](https://blog.csdn.net/u010665216/article/category/7066495)
[
																					](https://so.csdn.net/so/search/s.do?q=普通最小二乘&t=blog)所属专栏：[机器学习](https://blog.csdn.net/column/details/16605.html)[
							](https://so.csdn.net/so/search/s.do?q=普通最小二乘&t=blog)
[
																	](https://so.csdn.net/so/search/s.do?q=广义线性回归&t=blog)


**译者注：翻译斯坦福CS229对应讲义，如有错误，欢迎留言指正~（持续更新中）**
**专栏地址：****[传送门](http://blog.csdn.net/column/details/16605.html)**
到目前为止，我们已经看到一个回归示例和一个分类示例。在回归示例中，我们有![](https://img-blog.csdn.net/20170830171352079)，在分类示例中，我们有![](https://img-blog.csdn.net/20170830171352229)，其中要给x和的函数µ 和 φ合适的定义。在本节中，我们将展示这两种方法都是一个更广泛的模型的特例，称为广义线性模型GLMs。我们还将展示GLM家族中的其他模型如何派生和应用其他分类和回归问题。
指数族
为了得到GLMs，我们将通过定义指数簇开始。如果一类分布可以写成
![](https://img-blog.csdn.net/20170830171352701)
那么我们就说它属于指数簇。这里η被称为分布的自然参数（也被称为规范参数）；T(y)是充分统计量（对于我们考虑的分布，它通常是T(y)=y）。a(η)是配分函数。本质上起标准化常数的作用，这确保了p(y; η)的和或积分分布在y到1之间。固定的T,a,b定义了由η参数化的分布簇。当我们改变η时，我们得到不同的分布簇。我们现在展示伯努利分布与高斯分布是指数簇分布的例子。均值为φ的伯努利分布写成Bernoulli(φ)，指定了一个y
 ∈ {0, 1}的分布。所以p(y = 1; φ) = φ; p(y = 0; φ) = 1 –φ，当我们改变φ时，我们得到了不同均值的伯努利分布。我们现在证明，通过变化φ得到的一类伯努利分布也是在指数簇中。例如选择T,a,b那么公式6就成了一类伯努利分布。我们将伯努利分布写成：
![](https://img-blog.csdn.net/20170830171353014)
因此自然参数由![](https://img-blog.csdn.net/20170830171353347)给出，有趣的是，如果我们不求η的定义，通过已知η求φ，会得到：
φ=![](https://img-blog.csdn.net/20170830171353786)这就是我们熟悉的sigmoid函数！
当我们把逻辑函数派生成GLM时，这个将会再次出现。为了完成伯努利分布公式到指数簇的转变，我们也有：
![](https://img-blog.csdn.net/20170830171354136)
这表明伯努利分布可以通过合适的T,a,b写成公式（6）的形式。现在让我们继续考虑高斯分布。回想一下，当推导得到线性回归时,的值对和的选择没有影响。因此我们可以给赋值任意值而不改变任何东西。为了简化下面的推导，我们设置=1。然后我们有：
![](https://img-blog.csdn.net/20170830171354411)
因此我们可以看到高斯属于指数簇，有
![](https://img-blog.csdn.net/20170830171354747)
也有很多其他的分布是指数簇的成员：多项式分布（我们接下来会看到），泊松分布（为计数建模），伽马和指数分布（为非负连续变量建模，例如时间间隔），β与狄利克雷分布（概率分布）等等。下一节，我们将描述构建模型的一般方法，其中y(给定x和θ)来自这些分布。
构造广义线性模型
假设你想构建一个模型，来估计在任意给定的时间内到达你的商店客户的数量（或者你网站的访问量），该模型基于特定的特征x，例如商店促销，最近的广告，天气，星期天等等。我们知道泊松分布通常能对访问者数量建立很好的模型。知道这点后，我们如何想到针对我们这个问题的模型呢？幸运的是，泊松分布是一个指数簇分布，因此我们可以应用广义线性模型。在这一节，我们将描述构建模型解决诸如此类问题的方法。
更一般地，考虑一个分类或回归问题，在问题中我们将要去预测一些随机变量y的值，其中y是x的函数。为了得到针对这个问题的GLM,我们将给出给定x的条件下y的条件分布以及我们模型的三个假设：
y | x; θ ∼指数簇（η）。例如：给定x和，y的分布服从一些关于η指数簇的分布。
给定x,我们的目标是预测给定x情况下的T(y)的期望值。在大多数的例子中，我们有T(y)=y。这意味着我们想通过学习的假设h预测h(x)输出满足h(x) = E[y|x]。（请注意对于逻辑回归与线性回归中(x)的选择都满足该假设。例如，在逻辑回归中，我们有![](https://img-blog.csdn.net/20170830171355171)=
 E[y|x; θ]）
自然参数η及输入x是线性相关的：![](https://img-blog.csdn.net/20170830171355490)。（或者，如果η是矢量值，那么![](https://img-blog.csdn.net/20170830171355668)）
上面第三个假设看上去是最不合理的，所以在设计GLMs过程中，最好能把它看成一个设计选择，而不是一个假设。这三个假设或设计选择将允许我们推出一门优雅地学习算法即GLMs，它有许多可取的特性，比如易于学习。而且，生成的模型对构建y的不同类型的分布通常是有效的；例如，我们将简短地证明逻辑回归与普通最小二乘都可以由GLMs推导得到。
普通最小二乘
为了证明普通最小二乘是GLM模型的特例，考虑目标变量y(在GLM术语中也被称为响应变量)是连续的，我们为给定x的y的条件分布构建模型，就像高斯分布：N (µ, )。（此处可能会依赖x）因此我们让上面的指数簇分布成为高斯分布。正如我们之前看到的，在将高斯分布作为指数簇分布的公式中，我们有 η。因此我们有
![](https://img-blog.csdn.net/20170830171356295)
第一个等式来自上面的假设2；第二个量服从y|x; θ ∼ N (µ,σ2)，因此它的期望值由µ给定；第三个量服从假设1（我们之前的推导证明了在高斯分布的公式中），最后一个等式来自于假设3。
逻辑回归
我们现在考虑逻辑回归。此处我们对而分类感兴趣，因此y ∈ {0, 1}。考虑到y是个二进制，因此选择伯努利分布给条件分布y建模看上去很自然。在我们伯努利分布的公式中，我们有
![](https://img-blog.csdn.net/20170830171356481)。而且注意![](https://img-blog.csdn.net/20170830171356789)，然后E[y|x; θ] = φ。因此通过最小二乘的一个类似推导，我们得到：
![](https://img-blog.csdn.net/20170830171356959)
因此这给了我们![](https://img-blog.csdn.net/20170830171357286)形式的假设函数。如果你之前想要知道我们是如何得到逻辑函数![](https://img-blog.csdn.net/20170830171357425)的形式的，这给了一个答案：一旦我们假设以x为条件的y是伯努利分布，它得到了GLMs和指数簇分布定义的结果。为了引入更多的术语，函数g给出了分布的均值为自然参数（![](https://img-blog.csdn.net/20170830171357766)）的函数被称为正则响应函数。它的逆,被称为正则联系函数。因此，高斯簇的正则响应函数只是恒等函数，以及伯努利的正则响应函数是逻辑函数。
Softmax回归
让我们再看一个GLM的例子。考虑一个分类问题，在这个问题中，变量y可以取任意一个k值,y∈{1, 2, . . . , k}。例如，而不是将邮件分为两类垃圾邮件或非垃圾邮件——这是一个二分类问题——我们可能想把它分为三类，例如垃圾邮件，个人邮件及工作邮件。这个响应变量仍然是离散，但是现在可以取超过两个值。因此我们可以根据一个多项式分布来为其建模。
让我们推导一个GLM来为这种多项式数据建模。为了达到这个，我们首先要把多项式描述成指数簇分布。为了参数化多项式K个可能的输出，可以使用k个参数φ1, . . . , φk，指定每个输出的概率。但是这些参数是冗余的，或者更正式地说，他们不是独立的（因为知道任意k-1个φ决定了最后那个，他们必须满足![](https://img-blog.csdn.net/20170830171357951)）,因此我们仅用k-1一个参数来参数化多项式，其中![](https://img-blog.csdn.net/20170830171358286)，![](https://img-blog.csdn.net/20170830171358427)为了符号便利化，我们也让![](https://img-blog.csdn.net/20170830171358764)，但是我们应该记住，这不是一个参数，并且它由φ1,
 . . . , φk−1指定。为了把多项式描述成指数簇分布，我们将会定义![](https://img-blog.csdn.net/20170830171358913)，如下：
![](https://img-blog.csdn.net/20170830171359249)
不像我们先前的例子，此处我们没有T(y)=y;并且T(y)现在是一个k-1维的向量而不是一个实数。我们将写来表示向量T(y)第i个元素。
我们一个非常有用的符号。一个指数函数1{•},如果它的参数为真那么则取值1，否则取值0。
例如，1{2=3}=0 ，1{3=5-2}=1。因此我们也可以将T(y)和y间的关系写成![](https://img-blog.csdn.net/20170830171359425)。此外这有![](https://img-blog.csdn.net/20170830171359666)。
我们现在准备证明多项式是指数簇的成员。我们有：
![](https://img-blog.csdn.net/20170830171400043)
其中：
![](https://img-blog.csdn.net/20170830171400237)
这就完成了将多项式表示成指数簇分布。
联系函数由![](https://img-blog.csdn.net/20170830171400578)给定(i=1,2,,,k)
为了方便，我们也定义了![](https://img-blog.csdn.net/20170830171400744)。为了对联系函数做逆变换并推导出响应函数，我们因此有
![](https://img-blog.csdn.net/20170830171401081)
这解释了![](https://img-blog.csdn.net/20170830171401255)，可以把它代入公式（7）从而得到响应函数
![](https://img-blog.csdn.net/20170830171401603)
从η到φ的映射被称为softmax函数。
为了完成我们的模型，我们使用了之前给出的假设3，η与线性相关。因此有，![](https://img-blog.csdn.net/20170830171401777)
（i=1,2,3,,,k-1）,其中![](https://img-blog.csdn.net/20170830171402203)，是我们模型的参数。为了符号便利性，我们也可以定义=0，因此正如前面给出的，![](https://img-blog.csdn.net/20170830171402533)。因此我们的模型假设给定x的y的条件分布如下：
![](https://img-blog.csdn.net/20170830171403069)
这个模型适用于分类问题，其中y∈ {1, . . . , k},被称为softmax回归。它是逻辑回归的推广。我们的假设将输出：
![](https://img-blog.csdn.net/20170830171403428)
换句话说，我们的假设，对i=1,2,,,,k的每个至将会输出p(y=i|x；)的估计概率（尽管上述假设只有k-1维，但是p(y = k|x; θ)可以由![](https://img-blog.csdn.net/20170830171403646)获得。
最后我们讨论参数拟合。与我们的普通最小二乘和逻辑回归的推导类似，如果我们有一个m个样本的训练集![](https://img-blog.csdn.net/20170830171403975)，并去学习这个模型的参数，我们将通过写下log似然函数来开始：
![](https://img-blog.csdn.net/20170830171404141)
为了获得上式的第二行，我们利用了公式（8）对p(y|x; θ)的定义，我们现在可以利用梯度上升或牛顿法最大化ℓ(θ)来获得最大似然估计的参数。

