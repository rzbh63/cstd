
# 2.12 主成分分析（下） - 蜗牛 - CSDN博客


2015年11月17日 01:35:18[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：471标签：[深度学习																](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)[线性代数																](https://so.csdn.net/so/search/s.do?q=线性代数&t=blog)[麻省理工																](https://so.csdn.net/so/search/s.do?q=麻省理工&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=线性代数&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)个人分类：[PRML																](https://blog.csdn.net/u010182633/article/category/3186993)
[
																								](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)


**声明：该文章翻译自MIT出版的《DEEP LEARNING》，博主会定期更新文章内容。由于博主能力有限，中间有过错之处希望大家给予批评指正，一起学习交流。**
为了进一步分析，我们必须替换$g(c)$的定义：
$$
\boldsymbol{c}^\ast=\rm{\mathop{argmin}_{c}}-2\boldsymbol{x}^T\boldsymbol{Dc}+\boldsymbol{c}^T\boldsymbol{D}^T\boldsymbol{Dc}
$$

$$
=\rm{\mathop{argmin}_{c}}-2\boldsymbol{x}^T\boldsymbol{Dc}+\boldsymbol{c}^T\boldsymbol{I_lc}
$$
(对$\boldsymbol{D}$施加正交和单位范数约束)
$$
=\rm{\mathop{argmin}_{c}}-2\boldsymbol{x}^T\boldsymbol{Dc}+\boldsymbol{c}^T\boldsymbol{c}
$$
我们可以用矢量微积分解决这个最优化问题（该部分内容参见4.3）：
$$
\nabla\rm(-2\boldsymbol{x}^T\boldsymbol{Dc}+\boldsymbol{c}^T\boldsymbol{c})=0
$$

$$
-2\rm\boldsymbol{D}^T\boldsymbol{x}+2\boldsymbol{c}=0
$$

$$
\boldsymbol{c}=\rm\boldsymbol{D}^T\boldsymbol{x}\tag{2.2}
$$
这是一个好消息：我们可以只用一个矩阵向量操作来最优化编码$\boldsymbol{x}$。为了编码一个向量，我们应用编码函数:
$$
f(x)=\rm\boldsymbol{D}^T\boldsymbol{x}
$$
进一步使用矩阵乘法，我们也可以定义PCA重构操作：
$$
r(\boldsymbol{x})=g(f(\boldsymbol{x}))=\rm\boldsymbol{DD}^T\boldsymbol{x}
$$
接下里，我们需要选择编码矩阵$\boldsymbol{D}$。要做到这一点，我们需要回顾最小化输入和重构之间$\boldsymbol{L}^2$距离的想法。然而，因为我们使用相同的矩阵来解码所有点，我们就不能孤立考虑每个点。我们必须最小化误差矩阵的Frobenius范数：
$$
\boldsymbol{D^\ast}=\rm {\mathop{argmin}_D}\sqrt{\sum_{i,j}(\boldsymbol{x}_{j}^{(i)}-r(\boldsymbol{x^{(i)}})_j)^2}  其中\boldsymbol{D^\rm{T}D}=\boldsymbol{I_l}\tag{2.3}
$$
为了导出寻找$\boldsymbol{D^\ast}$的算法，我们先考虑$l=1$的情况。在这种情况下，$\boldsymbol{D}$只是一个单一的矢量$\boldsymbol{d}$。将2.2代入2.3，并将$\boldsymbol{D}$化为$\boldsymbol{d}$
$$
\boldsymbol{d^\ast}=\rm \mathop{argmin}_d\sum_{i}||\boldsymbol{x}^{(i)}-\boldsymbol{dd^\rm{T}x^{(i)}}||_2^2  其中||\boldsymbol{d}||_2=1
$$
上面是带入之后最直接的化简方式，但是对于写等式来说风格不悦目。它把标量放在了矢量的右边。而更方便的方式是将标量洗漱放在矢量的左边。因此，我们通常将等式写成下面的形式：
$$
\boldsymbol{d^\ast}=\rm argmin\sum_{i}||\boldsymbol{x^{(i)}}-\boldsymbol{d^\rm{T}x^{\rm{(i)}}d}||_2^2其中||\boldsymbol{d}||_2=1
$$
或者，根据标量的转置等于本身
$$
\boldsymbol{d^\ast}=\rm argmin\sum_{i}||\boldsymbol{x^{(i)}}-\boldsymbol{x^{\rm{(i)}}dd}||_2^2其中||\boldsymbol{d}||_2=1
$$
上面的方式使得我们能够用更紧凑的符号来表示。让$\boldsymbol{X}\in\rm{R^{m\times n}}$表示所有用来描述点的向量所定义的矩阵，这样的话$\boldsymbol{X_{i,:}=x^{(\rm i)}}$。我们现在将问题重写为：
$$
\boldsymbol{d^\ast}=\rm argmin||\boldsymbol{X-Xdd^{\rm T}}||_F^2其中||\boldsymbol{d}||_2=1
$$
暂时忽略限制，我们可以将Frobenius范数化为：
$$
\rm argmin||\boldsymbol{X-Xdd^{\rm T}}
$$

$$
=\rm argmin Tr((\boldsymbol{X-Xdd^{\rm T}})^T(\boldsymbol{X-Xdd^{\rm T}}))
$$
（Frobenius范数的另一种定义）
$$
=\rm argminTr(\boldsymbol{X^{\rm T}X-X^{\rm T}Xdd^{\rm T}-dd^{\rm T}X^{\rm T}X}+\boldsymbol{dd^{\rm T}X^{\rm T}Xdd^{\rm T}})
$$

$$
=\rm argminTr(\boldsymbol{X^{\rm T}}-Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})-Tr(\boldsymbol{dd^{\rm T}X^{\rm T}X}+Tr(\boldsymbol{dd^{\rm T}X^{\rm T}Xdd^{\rm T}})
$$

$$
=\rm argmin-Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})-Tr(\boldsymbol{dd^{\rm T}X^{\rm T}X}+Tr(\boldsymbol{dd^{\rm T}X^{\rm T}Xdd^{\rm T}})
$$
（因为第一项与$\boldsymbol{d}$无关，不会影响最小化）
$$
=\rm argmin-2Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})+Tr(\boldsymbol{dd^{\rm T}X^{\rm T}Xdd^{\rm T}})
$$
（因为在迹中我们可以循环矩阵的顺序）
$$
=\rm argmin-2Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})+Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}dd^{\rm T}})
$$
（同样利用上面的性质）。现在，加上限制：
$$
=\rm argmin-2Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})+Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}dd^{\rm T}})其中||\boldsymbol{d}||_2=1
$$

$$
=\rm argmin-2Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})+Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})其中||\boldsymbol{d}||_2=1
$$
（由于限制条件）
$$
=\rm argmin-Tr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})其中||\boldsymbol{d}||_2=1
$$

$$
=\rm argmaxTr(\boldsymbol{X^{\rm T}Xdd^{\rm T}})其中||\boldsymbol{d}||_2=1
$$

$$
=\rm argmaxTr(\boldsymbol{d^{\rm T}X^{\rm T}Xd})其中||\boldsymbol{d}||_2=1
$$
这个最优化问题可以用特征分解解决。特别地，最优解$\boldsymbol{d}$由$\boldsymbol{X^{\rm T}X}$对应于最大特征值的特征向量给出。
对于一般情况$l>1$，$\boldsymbol{D}$由对应于最大特征值的$l$特征向量给出。这个可以用归纳法证明。

