
# 凸优化系列二:确定步长一维搜索算法 - bitcarmanlee的博客 - CSDN博客


2019年01月21日 23:29:29[bitcarmanlee](https://me.csdn.net/bitcarmanlee)阅读数：131



## 1.精确一维搜索与非精确一维搜索
在上一篇文章中，我们提到第k次的迭代公式为:
$$
x_{k+1} = x_k + \alpha_kd_k
$$
其中，$\alpha_k$表示步长。接下来我们讨论一下怎么确定步长。
我们令
$$
\varphi(\alpha_k) = f(x_k + \alpha_k d_k)
$$
假设我们从点$x_k$出发，沿着方向$d_k$进行搜索，确定$\varphi(\alpha_k)$值最小，这个过程就叫一维搜索。注意我们在这个搜索过程中假设$x_k$与$d_k$都已经确定，只有$\alpha_k$未知。
如果能直接求出这个最优解$\alpha_k$，那么我们这个$\alpha_k$就被称为最优步长，这种方法被称为最优一维搜索，或者说精确一维搜索。
但是实际情况往往是问题比较复杂，数据维度也很高，直接求精确的最优步长$\alpha_k$可能比较困难，这个时候往往会选择不精确一维搜索来进行代替。
不精确的一维搜索也可以成为近似一维搜索。通常的方法是选择合适的$\alpha_k$，使得目标函数有一定的下降量，即$f(x_k + \alpha_kd_k) &lt; f(x_k)$。或者说，只需要找到一个步长，使得目标函数有一定的下降量就可以了。
## 2.精确一维搜索之试探法
精确一维搜索主要包括试探法(区间搜索法)与函数逼近法。
其中，常用的试探法又包括进退法，黄金分割法，二分法等。
### 2.1进退法
算法的步骤如下:
1.确定搜索的起点与初始步长。
2.以起点开始以初始步长向前试探。如果函数值变大，改变步长方向。
3.如果函数值下降，维持原来的试探方向，并将步长加倍。
算法的大致流程如下
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190122225927243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdGNhcm1hbmxlZQ==,size_16,color_FFFFFF,t_70)
## 2.2 黄金分割法
0.618法，又叫黄金分割法，是优选法的一种。它在试验时，把试点安排在黄金分割点上来寻找最佳点。而生产生活中，我们常常取其近似值0.618，因此得名。0.618法是最常用的单因素单峰目标函数优选法之一。(参考文献1)
用0.618法寻找最佳点时，虽然不能保证在有限次内准确找出最佳点，但随着试验次数的增加，最佳点被限定在越来越小的范围内，即存优范围会越来越小。用存优范围与原始范围的比值来衡量一种试验方法的效率，这个比值叫精度。用0.618法确定试点时，每一次实验都把存优范围缩小为原来的0.618.因此，n次试验后的精度为：
$$
\delta_n = 0.618^{n-1}
$$
具体的算法细节可以查阅更为详细的文献与参考资料。
## 2.3 二分法
具体原理与黄金分割类似。
## 3.精确一维搜索之函数逼近法
如果原函数具有比较好的解析性质，那么可以使用函数逼近(插值)的方法。
### 3.1 牛顿法
牛顿法的思路就是利用某一点的函数值，一阶导数值，二阶导数值构造二次插值函数。牛顿法最大的优势就是收敛速度快，具有局部二阶收敛的速度。
将$f(x)$在$x_k$点处泰勒展开
$$
f(x) = f(x_k) + f&\#x27;(x_k)(x-x_k) + \frac{f&\#x27;&\#x27;(x_k)}{2!}(x - x_k)^2 + o(x-x_k)^2
$$
要求上面函数的极值，由高等数学的知识，易知$f&\#x27;(x) = 0$，那么有
$$
f&\#x27;(x_k) + f&\#x27;&\#x27;(x_k)(x - x_k) = 0
$$
求解可知
$$
x = x_k - \frac{f&\#x27;(x_k)}{f&\#x27;&\#x27;(x_k)}
$$
对应到一维搜索中，步长$\alpha$的迭代方式为:
$$
\alpha_{k+1} = \alpha_k - \frac{f&\#x27;(\alpha_k)}{f&\#x27;&\#x27;(\alpha_k)}
$$
每次更新该点，然后迭代查找即可。
### 3.2 插值法
可以有相应的二次插值，三次插值方法，具体可以查看参考文献2关于插值方法的描述。
## 4.不精确搜索
由于实际问题的复杂性，使用精确一维搜索往往要付出很高的代价，还不一定能得到比较好的结果。后来慢慢发现，只要遵循一定的规律，算法就很可能达到收敛。
### 4.1 Armijo-Goldstein准则
Armijo-Goldstein准则的核心思想有两个:
1.目标函数值应该有足够的下降
2.一维搜索的步长$\alpha$不应该太小。
这两个思想的意图非常明显。由于最优化问题的目的就是寻找极小值，因此，让目标函数函数值“下降”是我们努力的方向，所以1正是想要保证这一点。
同理，2也类似：如果一维搜索的步长$\alpha$太小了，那么我们的搜索类似于在原地打转，可能也是在浪费时间和精力。(参考文献3)
所以最后Armijo准则的表达式为两个式子:
$$
f(x_k + \alpha_k d_k) \le f(x_k) + \alpha_k \rho g_k^Td_k
$$

$$
f(x_k + \alpha_k d_k) \ge f(x_k) + \alpha_k (1 - \rho) g_k^Td_k
$$
其中，$0 \lt \rho \lt 1/2$
为什么上面两个式子就可以满足我们的要求，可以阅读参考文献3，或者查阅相关最优化理论的教材。
### 4.2 Wolfe-Powell准则
Armijo-Goldstein准则可能会把最优步长因子排除在可接受区间外，因此Wolfe-Powell准则做了相关的改进。
Wolfe-Powell准则也是有两个表达式。第一个表达式与Armijo-Goldstein准则的第一个表达式相同，而第二个表达式为:
$$
\nabla f(x_k + \alpha_k d_k)^Td_k \ge \sigma g_k^T d_k, \quad \sigma \in (\rho, 1)
$$
上面式子的几何解释为：在可接受点处的切线斜率大于等于初始斜率的$\sigma$倍！
参考文献：
1.[https://zh.wikipedia.org/wiki/黄金分割法](https://zh.wikipedia.org/wiki/%E9%BB%84%E9%87%91%E5%88%86%E5%89%B2%E6%B3%95)
2.[https://blog.csdn.net/bitcarmanlee/article/details/86556744](https://blog.csdn.net/bitcarmanlee/article/details/86556744)
3.[https://www.codelast.com/原创用人话解释不精确线搜索中的armijo-goldstein准则及wo/](https://www.codelast.com/%E5%8E%9F%E5%88%9B%E7%94%A8%E4%BA%BA%E8%AF%9D%E8%A7%A3%E9%87%8A%E4%B8%8D%E7%B2%BE%E7%A1%AE%E7%BA%BF%E6%90%9C%E7%B4%A2%E4%B8%AD%E7%9A%84armijo-goldstein%E5%87%86%E5%88%99%E5%8F%8Awo/)

