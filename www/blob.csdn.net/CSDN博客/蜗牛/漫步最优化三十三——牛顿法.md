
# 漫步最优化三十三——牛顿法 - 蜗牛 - CSDN博客


2017年10月27日 22:09:33[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：235



$\textbf{自从有了你，}$
$\textbf{绝望与无奈远走高飞。}$
$\textbf{自从有了你，}$
$\textbf{我的世界不再天灰灰。}$
$\textbf{自从有了你，}$
$\textbf{时间变得越来越值得回味。}$
$\textbf{自从有了你，}$
$\textbf{我不再形单影只无人依偎。}$
$\textbf{因为你的出现，}$
$\textbf{我的世界多了一个词汇：美。}$
$\qquad\qquad\textbf{——畅宝宝的傻逼哥哥}$
最速下降法是一阶法，这是因为它基于泰勒级数的线性近似。而众所周知的牛顿法则是利用了泰勒级数的二阶近似，如果$\mathbf{x}$的变化量是${\delta}$，$f(\mathbf{x}+{\delta})$为

$$
f(\mathbf{x}+{\delta})\approx f(\mathbf{x})+\sum_{i=1}^n\frac{\partial f}{\partial x_i}\delta_i+\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\frac{\partial^2f}{\partial x_i\partial x_j}\delta_i\delta_j
$$
假设这是函数在点$\mathbf{x}+{\delta}$处的精确表示，那么函数$f(\mathbf{x}+{\delta})$对${\delta}_k$($k=1,2,\ldots,n$)求导并令其等于零可以得出最小化$f(\mathbf{x}+{\delta})$的${\delta}_k$值，由此可得

$$
\frac{\partial f}{\partial x_k}+\sum_{i=1}^n\frac{\partial^2 f}{\partial x_i\partial x_k}\delta_i=0\quad\text{for}\ k=1,2,\ldots,n
$$
或者用矩阵形式来表示为

$$
\mathbf{g}=-\mathbf{H}{\delta}
$$
因此$\mathbf{x}$的最优变化量为

$$
{\delta}=-\mathbf{H}^{-1}\mathbf{g}
$$
这个解要想存在，当且仅当下面的两个条件成立：
海森矩阵是非奇异的
泰勒级数的近似是有效的
假设$\mathbf{x}^*$处最小值的二阶条件成立，那么$\mathbf{H}$在点$\mathbf{x}^*$处以及邻域内($\lVert\mathbf{x}-\mathbf{x}^*\rVert<\varepsilon$)都是正定的，这就意味着对于$\lVert\mathbf{x}-\mathbf{x}^*\rVert<\varepsilon,\mathbf{H}$是非奇异的并存在逆。因为任何函数$f(x)\in C^2$都可以用泰勒级数的二次近似来表示$\mathbf{x}^*$的邻域，所以上面的解释存在的，更进一步，对满足$\lVert\mathbf{x}-\mathbf{x}^*\rVert<\varepsilon$的任意点$\mathbf{x}$，一次迭代就能得到$\mathbf{x}\approx\mathbf{x}^*$。
任何二次函数都有海森矩阵，且对任意的$\mathbf{x}\in E^n$都为常数。如果函数有最小值，那么二阶条件满足，故对任意点$\mathbf{x}\in E^n,\mathbf{H}$是正定的，也是非奇异的。因为二次函数可以用泰勒级数的二次近似准确的表示，所以上面的解是存在的，更进一步，对于任意点$\mathbf{x}\in E^n$，一次迭代就能得到解。
如果我们要最小化一般的非二次函数，对于任意点$\mathbf{x}$，两个条件不一定都满足。如果第一个条件不满足，那么会有无数多个解或者没有解。另一方面，如果第二个条件不满足，那么一次迭代后${\delta}$不一定能得到解，如果$\mathbf{H}$不是正定的，甚至${\delta}$不会令目标函数变小。
对于第一个问题，我们可以加入一些操作强行使$\mathbf{H}$是正定的。而对于第二个问题，我们可以加入一个迭代过程解决。这个迭代过程抵消掉了一次迭代无法得到解的事实，并使用线搜索达到最大化减小$f(\mathbf{x})$。这个方法通过选择下一个迭代点$\mathbf{x}_{k+1}$为

$$
\mathbf{x}_{k+1}=\mathbf{x}_k+{\delta}=\mathbf{x}_k+\alpha_k\mathbf{d}_k
$$
就能实现，其中

$$
\mathbf{d}_k=-\mathbf{H}_k^{-1}\mathbf{g}_k
$$
且$\alpha_k$是最小化$f(\mathbf{x}_k+\alpha\mathbf{d}_k)$的$\alpha$，向量$\mathbf{d}_k$称为点$\mathbf{x}_k$处的牛顿方向。当两个条件都满足的情况下，第一次迭代我们令$\alpha_k=1$。
最小化开始的时候，对某些类型的函数该过程会很慢，但不管怎样，$f(\mathbf{x})$一直在减小，然而随着越来越靠近问题的解，两个条件都会满足，因此最终达到收敛，收敛的阶数是二。从效果上看，牛顿法的收敛属性与最速下降法形成互补的关系，即远离解的时候收敛慢，靠近解的时候收敛快。
## 修正海森矩阵
如果海森矩阵不是正定的，那么我们需要将其强行变成正定的，修正$\mathbf{H}_k$的方法有许多，这里不再详细给出，如果需要可以私信或留言。
## 海森矩阵的计算量
牛顿法的缺点是需要知道函数的二阶导，也就是需要计算海森矩阵。如果无法获得准确的形式或者比较难算出来，那么我们可以用下面的数值公式进行计算：

$$
\begin{align*}
\frac{\partial f}{\partial x_1}=\lim_{\delta\to 0}\frac{f(\mathbf{x}+{\delta}_1)-f(\mathbf{x})}{\delta}=f^\prime(\mathbf{x})\quad\text{with}\ {\delta_1}=[\delta\ 0\ 0\ \cdots\ 0]^T\\
\frac{\partial^2f}{\partial x_1\partial x_2}=\lim_{\delta\to0}\frac{f^\prime(\mathbf{x}+{\delta}_2)-f^\prime(\mathbf{x})}{\delta}\quad\text{with}\ {\delta}_2=[0\ \delta\ 0\ \cdots\ 0]^T
\end{align*}
$$


