
# 聚类：对聚类性能的评价 - lpty的博客 - CSDN博客

2018年07月10日 21:06:00[lpty](https://me.csdn.net/sinat_33741547)阅读数：1302



## 一、前言
对于有监督的学习方法，我们可以找到许多评价指标，但是要评价无监督算法的质量，相对来说比较少有提及，正好最近在做一个相关的工作，稍微整理一下。
## 二、方法
下述提及方法均以k-means算法为基础， 不同聚类方法有不同的评价指标，这里说说k-means常用的两种方法
### 1、肘部法则–Elbow Method
我们知道k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。
畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。
基于这个指标，我们可以重复训练多个k-means模型，选取不同的k值，来得到相对合适的聚类类别，
![这里写图片描述](https://img-blog.csdn.net/20180710203233557?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzNzQxNTQ3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180710203233557?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzNzQxNTQ3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如上图所示，在k=2时，畸变程度得到大幅改善，可以考虑选取k=2作为聚类数量，附简单代码：
[
](https://img-blog.csdn.net/20180710203233557?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzNzQxNTQ3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)`from sklearn.cluster import KMeans
model = KMeans(n_clusters=k)
model.fit(vector_points)
md = model.inertia_ / vector_points.shape[0]`[

](https://img-blog.csdn.net/20180710203233557?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzNzQxNTQ3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
### 2、轮廓系数–Silhouette Coefficient
[
](https://img-blog.csdn.net/20180710203233557?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzNzQxNTQ3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)对于一个聚类任务，我们希望得到的簇中，簇内尽量紧密，簇间尽量远离，轮廓系数便是类的密集与分散程度的评价指标，公式表达如下：
s=$s = \frac{b-a}{max(a, b)}$
其中a代表同簇样本到彼此间距离的均值，b代表样本到除自身所在簇外的最近簇的样本的均值，s取值在[-1, 1]之间。
如果s接近1，代表样本所在簇合理，若s接近-1代表s更应该分到其他簇中。
同样，利用上述指标，训练多个模型，对比选取合适的聚类类别：
![这里写图片描述](https://img-blog.csdn.net/20180710204649415?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzNzQxNTQ3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180710204649415?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzNzQxNTQ3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如上图， 当k=3时，轮廓系数最大，代表此时聚类的效果相对合理，简单代码如下：
`from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
model = KMeans(n_clusters=k)
model.fit(vector_points)
s = silhouette_score(vector_points, model.labels_)`
## 三、其他
### 欧式距离与余弦距离
疑问：欧式距离与余弦距离都是常见的距离度量方法，那么这两个距离各有什么特点与应用场景呢？
从两者公式上来看，欧式距离衡量的是空间中各点的绝对距离特征，与各个点所在空间位置直接相关；而余弦距离更多衡量的是一种方向上的差异，根据余弦角度的大小判断。
由上述两者的衡量区别，可以相应的联想其应用场景，欧式距离应该用在注重个体数值特征的差异的场景中，但同时因为其度量当时易受数值量级影响，使用前应考虑标准化；
余弦距离更多体现方向上的差异，对数值不敏感，可以考虑的应用场景比如通过用户评分来区分用户兴趣的相似度与差异度，假设存在A用户对某两个物品评分（3,3），B用户对某个两个物品评价（5,5），我们可以发现用户对物品的认知是一致的，但是欧式距离可能会有很大的差异。

