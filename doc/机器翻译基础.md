# 机器翻译基础



端到端的神经网络机器翻译（End-to-End Neural Machine Translation）是近几年兴起的一种全新的机器翻译方法。本文首先将简要介绍传统的统计机器翻译方法以及神经网络在机器翻译中的应用，然后介绍NMT中基本的“编码-解码”框架（Encoder-Decoder）。


本文中，详细介绍的工作有： 
Kyunghyun Cho, Bart van Merrenboer, Caglar Gulcehre Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine. In Proceedings of the 2014 Conference on EMNLP, 1724-1734.

Ilya Sutskever, Oriol Vinyals, Quoc V. Le. 2014. Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems, 4:3104-3112.





**机器翻译**就是让机器实现自动将一种语言映射到另一种语言，是自然语言处理的重要研究领域之一。

## 1 Statistical Machine Translation

传统的机器翻译方法就是让语言学专家编写两种语言之间的转换规则，再将这些规则录入给计算机。但是这对人类专家的要求非常高，既要能熟知两种语言，还要具备相当的翻译功底，并且，我们不可能知道一门语言究竟有多少隐含的使用规则，更何况是两种或更多的语言，因此我们往往无法得到一个完备的规则集合。这也是传统方法面临的主要挑战。

统计机器翻译（Statistical Machine Translation，SMT）的目标，同样是实现让机器自动将一种语言转化到另一种语言，但是其中的转化规则是机器自动从大规模的语料中学习得到的而非我们人主动提供规则，这就客服了传统方法面临的知识获取瓶颈问题。
我们用$D=（x^1，y^1）…，（x^N, y^N)$表示一个包含N个平行句对的平行语料，$x^n$和$y^n$分别表示第n个源语言句子和对应的目标语言句子。有了平行语料作为训练数据，我们可以开始构建模型，建立从源语言到目标语言之间的映射函数，并对其进行打分。这个映射函数通常表示成条件概率$p(y|x)$的形式，如果翻译越准确，那么这个概率就越高。那么接着就可以用对数似然函数来对整个训练数据的翻译进行打分，如公式（1）所示。
$$
L(\theta,D)=log \prod_{(x^n,y^n) \in D} p(y^n|x^n,\theta) \tag 1
$$




如果对数似然函数L太小，则说明模型没有赋予正确的训练样本足够的概率质量，也就是，损失了过多的概率质量在错误的结果上。我们要找到一个合适的配置参数0使得对数似然最大。但是还有一个更重要的问题，那就是我们该如何定义$p(y|x,\theta)$？
研究者们在20多年前就开始有关于对$p(y|x,\theta)$建模的探讨，例如IBM T.J.Wstson Research的研究者们提出的IBM model1-5五个模型[Brown et al.1993]。现有的统计机器翻译大都以对数线性模型（log-linear model）进行建模，对数线性模型将真实情况的$logp（y|x）$表示成一系列特征的线性组合：
$$
logp(y|x)\approx logp(y|x,\theta)=\sum_i \theta_i f_i(x,y)+C(\theta)  \tag 2
$$


其中C是约束项，$f_i$和$\theta_i$分别表示特征和对应的特征权重。随着该模型的引入，特征选择也变得更加灵活，也就有了非常多关于特征函数$f_i$的研究。定义完各个特征，那么剩下的问题就是如何找到对数线性模型的一组合适的参数{$\theta_i$}能够平衡各个不同的特征，亦或者说，这组参数能够对候选译文进行重排序（re-rank），使得从正确的译文在候选译文中的排名更加靠前。不同于IBM model最大化对数似然的方法，现有的研究大多以BLEU[Papineni et al.2002]作为机器翻译译文自动评价指标，采用最小错误率训练（Minimum Error Rate Training，MERT）[Och and Ney2002]算法来获取较优的特征权重。SMT的更多具体内容可以看[Koehn2009]或者宗成庆老师、张霄军老师翻译的《统计机器翻译》一书。

尽管统计机器翻译较于传统基于规则的方法有诸多优势，但仍存在许多挑战，例如SMT需要人为设计许多特征，这些特征无法覆盖所有的语言规则；统计机器翻译难以利用全局的特征；统计机器翻译依赖于许多预处理工作，比如词语对齐（如果是中文，则要进行分词）、规则抽取，如果定义了句法层面的特征还要先进行句法分析，这种流水线型的架构每个环节都会出现错误，错误会逐步影响后续的处理工作，对翻译的影响也越来越大。
面对上述SMT面临的挑战，一个较好的解决方法就是利用深度学习。利用深度学习的机器翻译大致可以分为两类，一类仍以统计机器翻译系统为框架，利用深度学习来改进其中的关键模块，比如翻译模型、语言模型、调序模型等。以语言模型为例，语言模型衡量了一句话的流利程度，或者说用以评价人使用某个单词序列的可能性，即给定一个n-1个词的序列{$w_{i-n+1},w_{i-n+2},...w_{i-1}$}，第n个词为$w_i$的概率为
$$
p(w_i|w_{i-n+1}，W_{i-n+2},…，W_{i-1}).
$$
传统的语言模型是在大规模语料上统计n元短语出现的次数，再采用极大似然估计法得到这一概率分布。但是训练语料无法涵盖所有词序列的出现情况，很多n元短语在语料上没出现或者出现次数很少，因此传统的n-
gram语言模型存在非常严重的数据稀疏性问题。基于神经网络的语言模型[Bengio et al.2003]最早由Bengio教授在2003年提出，通过将每个词映射成为一个固定长度的实数向量（词向量）有效缓解了数据稀疏性问题。再后来又有Mikolov 等人提出基于循环神经网络的语言模型[Mikolov et al.2010]等。不管是传统的n-gram 语言模型还是NNLM、RNNLM，在加入机器翻译系统中时都只考虑了目标语言端前n-1个词，而Devlin等人认为，源语言端的信息也非常重要，由此，他们提出神经网络联合模型（Neural Network Joint Model，NNJM）[Devlin et al.2014]，利用源语言与目标语言端的上下文信息构建NNJM，使得统计机器翻译效果有了很大的提升。这类方法的示意图如图1右边的图所示。图1中间的部分则表示在用原有的SMT系统得到候选译文之后利用神经网络方法对候选译文进行重排序，也大致归为这类方法。



![](.\asset\1554691560(1).png)



图1：Neural MT，SMT+Reranking-by-NN 以及SMT-NIN的图示，引自Bahdanau在ICLR2015上的slides



图1左边的部分则表示利用深度学习的机器翻译的另一类方法，这类方法不再以统计机器翻译作为框架（不再需要词语对齐等预处理、不再需要人去设计特征），而是用神经网络直接将源语言词序列映射成目标语言序列
（端到端的神经网络机器翻译）。本篇文章主要报告了近两年来关于神经网络机器翻译（NeuralMachine Translation，NMT）的一些研究进展。下面将首先简要介绍RINN、LSTM等背景知识以及NMT中典型的“编码解码”结构。

## 2 Neural Machine Translation

### 2.1 Preliminary:Recurrent Neural Networks

在自然语言处理，乃至人工智能领域的很多问题中，输入序列的长度和输出序列的长度都不是固定的，比如机器翻译、手写字体识别，语音识别等。要处理这种 variable-length的输入和输出，我们通常使用循环神经网络（Recurrent Neural Network，RNN）。对于简单的多层前馈神经网络，每一次输入都要重新计算网络的中间状态（即网络隐层的激活值），它不受之前样本计算得到的中间状态的影响，因为网络不能保存每一次得到的中间
状态信息。而RNN就可以保存这一信息，RNN循环读取一个序列（如词序列），根据当前的输入跟历史状态计算当前的状态信息，因此RNN可以处理任意长度的输入，而中间状态也被保存下来，可以一起用于预测输出序列。

![1554691769(1)](D:\code\git\ywlydd\cstd\doc\asset\1554691769(1).png)

图2：一个unfolded RNN图示

RNN的主要思想就是循环地将输入的序列压缩成一个固定维度的向量，也就是网络的中间状态，这一过程是通过不断将当前时刻的输入和历史状态组合实现。具体而言，如图2所示是一个展开的RNN示意图，假如在t时刻，我们有输入向量x*，和前一时刻的网络中间状态，即前一个隐层的激活值ht-1，那么t时刻的中间状态ht可以按照公式3计算。
$$
h_t=\phi_{\theta}(h_{t-1}，x_t)=\theta_h(Wx_t+Uh_{t-1}+b)\tag 3
$$


其中$\theta_h$表示隐含层的激活函数，常见的有sigmoid、tanh等。得到的$h^t$可以用于输出，也可以当作t+1时刻的历史信息继续压缩后续的输入。模型的参数可以使用反向传播算法求解（Backpropgation Through Time，BPTT），要做的就是根据输出层的误差对各个参数求梯度，然后使用随机梯度下降或者mini-batch的方式对参数进行更新。



![1554691998(1)](D:\code\git\ywlydd\cstd\doc\asset\1554691998(1).png)

图3：LSTM的一个unit 图示[Graves2012]

但是RNN存在很严重的问题：梯度爆炸（blow up）或者梯度消失（van-ish）。因为反向传播得到的梯度结果展开后大致长成…是，$\sum...\sum\prod^T_{t=1}a*b$这样的形式，其最后涉及一系列乘法运算，而且执行的次数与循环的层数有关，层数越多，乘的次数就越多。如果该式子中的运算$|a*b.…|<1.0$就容易导致梯度消失，如果$|a*b.…>1.0$就容易导致梯度爆炸的问题（很显然，小于1的数字越乘越多就越来越接近于0，大于1的数越乘越多就可能越接近正无穷）。所以就有研究者对此进行了思考，能否约束了BPTT的过程使$|a*b...|=1.0$从而解决这两个问题呢？答案当然是有的，这也就有了后来的长短时记忆LSTM（Long short-term memory），详细的推导过程可以看LSTM最初的文章[Hochreiter and Schmidhuber1997]。

LSTM也是一种循环神经网络，但是更为复杂，它在RNN的基础上，使用门（gates）改进了隐层节点的激活过程。LSTM将RNN隐层的一个节点修改成如图3所示的一个大的单元（unit）。其中cell保存了隐层的状态信息，左下角、左上角、右边这三个带有f的空心圆圈分别表示input gate，output gate，forget gate，是用sigmoid函数激活之后的（0，1）之间的概率值，这三个gate分别在实心黑点处作用实现了如下功能：
（1）input gate：表示是否允许当前的输入信息加入到隐层状态中；
（2）output gate：表示是否允许当前隐层节点的输出值传递到下一层；
（3）forget gate：表示是否保留当前节点的历史状态。

此外，LSTM最早的文章[Hochreiter and Schmidhuber1997]中并没有forget gate，上述加入了forget gate也是LSTM的一种实现。关于LSTM的具体计算公式不在此列出，可以参考Alex Graves写的 **Supervised Se-quence Labeling with Recurrent Neural Networks** 一书[Graves2012]。
并且书中说到LSTM隐层的一个unit可以有多个cell（图3中只有一个cell是其中的一种情况）也都是LSTM具体的一种实现。同样的，LSTM可以用BPTT的方法估计参数，只需要从输入开始前馈到输出层，再根据输出层定义的损失函数，对各个参数分别求梯度，就可以用梯度下降法估计参数。虽然LSTM的unit很复杂，但是同多层前馈感知机、RNN等网络一样都可以用Theano、Keras等深度学习框架实现，这些框架提供了自动求梯度的功能，只需要定义好网络结构和前馈过程，框架就可以自动计算梯度，实现起来很简单。

## 2.2 Encoder-Decoder Architecture for MT

从机器学习的角度看，机器翻译其实就是一个有监督学习过程，学习一个任意长度的序列到另一个任意长度的序列的过程。前面提到的RNN有两个好处：（1）可以将一个序列压缩成一个表示（representation）；（2）以概率的方式对一个序列进行建模。NMT或者许多基于RNN的序列化标注问题都利用到RNN这两点性质。那么接下来就首先看一看NMT中较为典型的Encoder-Decoder结构。

以“中国经济发展迅速”这一汉语句子为例，首先，编码（encode）阶段由一个编码器将输入词序列转化为一个固定维度的稠密向量（一个激活状态），解码（decode）阶段将根据这一激活状态生成目标译文，比如“The Chinese economy develops rapidly"。Encoder-Decoder的想法非常直观，也是NMT背后最基本的思想。一个基于Encoder-Decoder的NMT系统如图4所示，其中编码的过程和解码的过程通常都使用RNN实现（由于普通的RNN存在梯度消失和梯度爆炸问题，因此通常引入LSTM）。

![1554692312(1)](D:\code\git\ywlydd\cstd\doc\asset\1554692312(1).png)



图4：基于Encoder-Decoder结构的基本NMT系统示例

### 2.2.1 The Encoder encoder

部分利用了RNN压缩表示的性质。首先，源语言句子$x=\{x_1，x_2……c_T\}$的每个词被表示成一个向量$w_i \in R^{|V|}，i=1，2.……T$（T暂不包括句子结束标记`<EOS>`），这个向量的维度与词汇表大小|V|相同，并且向量只有一个维度上有值1，其余全是0，1的位置就对应该词在词汇表中的位置。这样的向量通常被称为one-hot 向量，或者说1-of-K coding，它与词典里的词一一对应，可以唯一地表示一个词，但是这样的向量不实用，因为：（1）向量维度往往很大，容易造成维数灾难；（2）无法刻画词与词之间的关系（例如语义相似性，也就是无法很好地表达语义）。所以接下来要做的就是将每个词映射到一个低维的语义空间，每个词将由一个固定维度的稠密向量表示（也称分布式表示Distributed Representation），也就是词向量。记映射矩阵为$C\in R^{K\times|V|}$，用$s_i=Cw_i$表示词第i个词的词向量向量维度K通常取100到500之间。词向量在整个翻译系统的训练过程中也会被逐步更新，会变得更“meaningful”。

![1554692582(1)](D:\code\git\ywlydd\cstd\doc\asset\1554692582(1).png)





![1554692651(1)](D:\code\git\ywlydd\cstd\doc\asset\1554692651(1).png)



图6：RNN的隐层表示在二维空间上的展示，引自[Sutskever et al.2014]

接着可以用一个RNN压缩源语言词序列，与公式3相同，这一过程的计算公式如下：
$$
hi=\phi_{\theta}（h_{i-1}，s_i）\tag 4
$$


其中$h_o$是一个全零的向量，最后得到的hr就是整个源语言句子的压缩表示。我们知道，词向量在语言学上有一定的含义，这个含义我们人无法直观地理解，但是向量之间的距离在一定程度上可以衡量词的相似性，那么句子的压缩表示hr又表示什么？是否也有类似的性质呢？图6可以很好地回答这一问题。但是模型中的向量维度至少都在100维以上，无法直接画在纸上或者显示在计算机屏幕上，尽管如此，现如今也已经有许多技术可以处理高维数据的可视化问题。图6中，Sutskever等人[Sutskever et al.2014]利用主成分分析（Principle Component Analysis，PCA）方法找到向量hr最主要的两个成分。然后将其映射到二维空间上。从图中可以看出，压缩向量确实可以保存源语言句子的语义信息，因为语义越相近的句子在空间中的距离越接近。这是传统的词袋模型（bag-of-words）所挖掘不出的信息！（即作为对比，从图6中可以发现，调换了主语和宾语的两个句子被认为是语义不相近的句子，事实确实是如此，这是词袋模型所做不到的）。



### 2.2.2 The Decoder decoder 

部分同样使用RNN实现。先根据encoder，我们可以得到一个源语言句子的压缩表示$c=h_T$（其实，这个压缩表示通常被定义为encoder所有隐层状态的组合，即$c=g（\{h_1，…，h_T\}）$，其中q是一个非线性函数，在这里，我们先简单定义$c=h_T$。接着就开始计算RNN的隐层状态$z_t, t=1…，T’$，其中$T’$表示目标语言句子的单词数，$z_t$的计算公式如下：
$$
z_T=\phi_{\theta'}（c，y_{t-1}，z_{t-1}） \tag 5
$$


其中$\phi_{\theta'}$是一个非线性激活函数，$y_{t-1}$表示前一个单词，$y_0$表示源语言句子的结束标记"`<EOS>`"，用表标示源语言输入结束以及解码开始，$z_{t-1}$表示decoder RNN的前一个隐层状态，0为一个全零的向量。根据这一隐层状态，我们可以估计y是哪个单词的概率：
$$
p（y_t|y<t，x）=softmax（W_s z_t+b_z） \tag 6
$$


若$z_t\in R^{H_z}$，那么$W_s$的维度就为$|V|×H_z$，$W_s z_t+b_z$就是对每个可能的输出单词进行打分，用softmax归一化就可以得到y：为每个词的概率。
现在我们得到了关于第t个目标端单词的概率分布，我们可以简单地从这个概率分布中采样出这个单词$y_t$，接着回到公式（5），根据 $y_t$ 以及 $z_{t}$ 计算下一个隐层状态$z_{x+1}$，计算$y_{t+1}$的概率分布，然后接着采样$y_{t+1}$，直到我们得到了句子结束标记`<EOS>`则解码完成。

### 2.2.3 Training:Maximum Likelihood Estimation

训练一个NMT系统常用的方法就是使用极大似然估计法（Maximum likelihood estimation，MLE）。回顾一下，我们有一个训练语料D包含N

![1554693459(1)](D:\code\git\ywlydd\cstd\doc\asset\1554693459(1).png)



图7：Decoder

个平行句对$（x^1，y^1），…（x^N，y^N）$，那么我们训练过程的最大化目标就是给定一个源语言句子x输出正确目标语言句子y的概率（的对数）：
$$
\begin{align}
L（D，\Theta）&=\frac 1 N \sum_{n=1}^N log p（y^n|x^n，\Theta）\\
&=\frac 1 N\sum_{n=1}^N \sum_{t=1}^{T_n'}log p(y_t^n|y_{<t}^n,x^n,\Theta)  
\end{align}
\tag 7
$$
我们可以用随机梯度下降法（stochastic gradient descent，SGD）来估计参数（包含各个层的连接权重、偏差项等）。
对数似然函数L对各个参数的梯度可以使用误差反向传播算法得到。其实只需要将L右部展开，一直展开到输入，就可以依次对各个参数求偏导。
这个过程不难，但是麻烦，现在有许多很方便的工具可以使用，例如Theano的自动求导功能，只用调用theano.tensor.grad（-loglikelihood，parameters）
就可以得到parameter list里各个参数的梯度。在得到了各个参数的梯度之后，就可以慢慢地更新参数，真的是慢慢地更新参数，因为更新过程还需要合适的学习率、momentum等。不过近两年也有许多自适应调节学习率的算法，比如Adadelta[Zeiler2012]、Adam[Kingma and Ba2014]等等。这些算法都可以被很方便地应用到程序当中。



## 2.3 Related Work

**[Kalchbrenner and Blunsom，2013]**最早提出Encoder-Decoder结构的是Kalchbrenner和Blunsom[Kalchbrenner and Blunsom2013]，他们使用卷积神经网络（Convolutional Neural Network，CNN）将输入源语言句子映射为一个连续、稠密的向量，接着利用RNN将该向量转化为目标语言句子。他们通过实验证明了这种新的框架不仅能学习到翻译模型，还能学到语言模型。但是实验并没有获得理想的翻译性能，一个重要的原因是训练RNN时面临着“梯度消失”和“梯度爆炸”的问题。因此，虽然RNN理论上能捕获长距离依赖关系，但是实际上很难处理。

[Cho，2014a]接着还要提一提Cho等人的RNN Encoder-Decoder[Cho et al.2014a]。不过，他们使用2.2节所述的结构构建网络模型，最后将得到的生成概率p（y|x）作为新的特征加入到统计机器翻译对数线性模型中。Cho等人提出一种新的隐含层节点，相较于普通的RNN能捕捉更长距离的依赖关系，同时较于LSTM，计算代价更小，实现简单，该节点的图示如图8所示。其中r表示 reset gate，即是否要重置当前的隐层节点，计算

![1554693854(1)](D:\code\git\ywlydd\cstd\doc\asset\1554693854(1).png)

图8：一个新的隐层节点示意图（简化版的LSTM）

|Models|BLEU-de |BLEU-test |
|----|----|----|
|Baseline|30.64|33.30|
|Baseline +RNN|31.20|33.87|
|Baseline+CSLM+RNN |31.48|34.64|
|Baseline+CSLM+RNN+Word penalty |31.50 |34.54|

表1：[Cho，2014a]机器翻译实验结果，其中RNN表示加入目标语言句子的生成概率，CSLM表示加入了Continues-Space LM特征[Schwenk2007]，Word penalty 表示加入了UNK个数惩罚

公式如下：
$$
r=\sigma(W_r x+U_rh^{\left \langle t-1 \right \rangle})  \tag 8
$$
其中o表示sigmoid函数，类似地，有update gatez，表示是否将新的隐层状态加入：
$$
z=\sigma(W_zx+U_zh^{\left \langle t-1 \right \rangle})  \tag 9
$$


那么最终的隐层状态可以表示为：
$$
h_j^{\left \langle t \right \rangle}=(1-z_j)\otimes h_j^{\langle t-1 \rangle} +z_j \tilde h_j^{\langle t \rangle} \tag{10}
$$
其中，
$$
\tilde h_j^{\langle t\rangle}=\phi ([Wx]_j+[U(r\otimes h^{\langle t-1 \rangle})]_j)  \tag{11}
$$


Cho等人在WMT’14的English/French任务上进行实验，在训练RNN Encoder-Decoder时，限制英法词典都为最频繁的15，000个词，词典外的词都用[UNK]替代，使用AdaDelta训练。baseline使用Moses系统的短语系统，他们的实验结果如表1所示。
[Sutskever et al.，2014]Sutskever 等人在2014年提出将LSTM引入到基于Encoder-Decoder 结构的端到端机器翻译[Sutskever et al.2014]，并且相较于Kalchbrenner等人的工作，Sutskever的Encoder和Decoder用的都是RNN，他们的结构如图9所示。这里主要记录一下他们的实验部分。他们的实际模型与2.2节所述的稍有不同的是：

- 实际的模型，是实现将词序列“CBA”映射”WXYZ”，即将源语言序列反转输入，在测试集上的BLEU值比不反转涨了4.7个点。或许是因为相比与“ABC”->“WXYZ”，虽然这样做，源语言端词与目标语言端对应的词的平均距离没有改变，但是源语言端句首的几个词离目标语言端对应的词距离更近了，降低了minimal time lag的影响。



|Models|BLEU-test|
|----|----|
|Baseline|33.30|
|Cho et al.[ Cho et al.2014a]|34.54|
|Best WMT'14 result|37.0|
|Single forward LSTM, beam size=12|26.17|
|Single reversed LSTM, beam size=12|30.59|
|Ensemble of 5 reversed LSTMs, beam size=1|33.00|
|Ensemble of 5 reversed LSTMs, beam size=2|34.50|
|Ensemble of 5 reversed LSTMs, beam size=12|34.81|
|Rescoring the baseline 1000-best with a single forward LSTM |35.61|
|Rescoring the baseline 1000-best with a single reversed LSTM |35.85|
|Rescoring the baseline 1000-best..5 reversed LSTMs |36.5|

表2：[Sutskever et al.，2014]机器翻译实验结果



![1554694822(1)](D:\code\git\ywlydd\cstd\doc\asset\1554694822(1).png)



- Encoder和Decoder各用一个4层的LSTM，效果会比浅层的LSTM好，

具体的他们训练细节如下：

- Encoder和Decoder各用一个4层的LSTM，每层1000个cell，词向量维度为1000，源语言端词典大小为160，000，目标语言端词典大小为80，000，整个深度LSTM使用8000个实值表示一个句子（文章里面没有描述具体的网络结构是怎样的，猜想可能是图10所示的结构，但始终不明白如此一来为什么整个句子会用一个8000维的向量表示，一种可能的解释就是用双向的LSTM，但是文中强调将输入词序列反转输入，就没理由再用双向的LSTM了）。输出层维度为80，000维，即一个softmax层。最终，整个网络有384M参数，其中的64M是单纯的循环连接的参数（Encoder和Decoder各32M）。



![1554694896(1)](D:\code\git\ywlydd\cstd\doc\asset\1554694896(1).png)



图10：一个4层LSTM的Encoder-Decoder 结构示例

- LSTM的参数使用均匀分布初始化，范围是[-0.08，0.08]。
- 使用minibatch的随机梯度下降法，batch大小是128，学习率一开始固定为0.7，在5个epoch之后，每半个epoch对学习率减半。总共训练了7.5个epoch。
- 虽然LSTM能避免梯度消失的问题，但还是有可能会有梯度爆炸的问题（exploding gradients），因此引入一个硬约束：对于每个batch，计算$s=||g||_2$，其中g是batch的平均梯度，如果s>5，那么设置$g=\frac {5g}s$。
- 对于长短句问题，在训练集中，通常有很多的短句（长度为20-30），以及少部分的长句（长度大于100）。所以一个batch（从训练集中随机采样）也会包含少部分的长句，这些长句将影响整个batch的时间，造成大量计算能力的浪费。因此他们确保每个batch里的句子长度都相近。

对于解码过程，目标是得到翻译$\hat T=argmaxp_{T} \  p(T|S)$，Sutskever等人用一个简单的left-to-right beam search decoder，用各个可能的词扩展翻译假设。
  他们的实验也同样比较了用LSTM去重排序1000-best候选译文列表，实验基于WMT14的English/French任务，实验结果如表2所示。这是目前第一次，单纯的基于神经网络的机器翻译系统能够超过传统的短语系统。并且Sutskever等人分析了Encoder得到的源语言句子的压缩表示，即图6的分析。



## 参考文献

[Bahdanau et al.2015]Dzmitry Bahdanau，Kyunghyun Cho，and Yoshua Bengio.2015.Neural machine translation by jointly learning to align and translate.Eprint Arxiv.

[Bengio et al.2003]Yoshua Bengio，Rejean Ducharme，Pascal Vincent，and Christian Jauvin.2003.A neural probabilistic language model.Journal of Machine Learning Research，3（6）：1137-1155.

[Brown et al.1993]Peter E.Brown，Stephen A.Della Pietra，VincentJ.Della Pietra，and Robert L.Mercer.1993.The mathematics of sta-
tistical machine translation:Parameter estimation.Computational Lin-
guistics，19（2）.

[Cho et al.2014]Kyunghyun Cho，Bart Van Merrienboer，Dzmitry Bah-danau，and Yoshua Bengio.2014.On the properties of neural machine translation:Encoder-decoder approaches.Eprint Arxiv.

[Cho et al.2014al Kyunghyun Cho，Bart van Merrienboer，Caglar Gulcehre，Dzmitry Bahdanau，Fethi Bougares，Holger Schwenk，and Yoshua Ben-
gio.2014a.Learning phrase representations using rnn encoder-decoder for statistical machine translation.In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Processing(EMNLP), pages 1724-1734, Doha, Qatar, October. Association for Computational Linguistics.

[ Devlin et al.2014] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul.2014. Fast and robust neural network joint models for statistical machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pages 1370-1380, Baltimore, Mary-
land, June. Association for Computational Linguistics.

[ Goodfellow et al.2013] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.2013. Maxout networks.
In Proceedings of The 32nd International Conference on Machine Learn-
ing, pages 1319-1327.

[ Graves2012] Alex Graves.2012. Supervised Sequence Labeling with Recur-
rent Neural Networks. Springer.

[ Hochreiter and Schmidhuber1997] Sepp Hochreiter and Jurgen Schmidhu-
ber.1997. Long short-term memory. Neural Computation,9(8):1735-
1780.

[ Jean et al.2015] Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio.2015. On using very large target vocabulary for neu-ral machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the ith International Joint Conference on Natural Language Processing (Volume 1: Long Pa-
pers), pages 1-10, Beijing, China, July. Association for Computational Linguistics.

[ Kalchbrenner and Blunsom2013] Nal Kalchbrenner and Phil Blunsom.2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700-1709, October.

[ Kingma and Ba2014] Diederik Kingma and Jimmy Ba.2014. Adam:A method for stochastic optimization. Eprint Arxiv.

[ Koehn2009] Philipp Koehn.2009. Statistical Machine Translation. Cam-bridge University Press.

[ Luong et al.2015a] Thang Luong, Hieu Pham, and Christopher D. Man-
ning.2015a. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 1412-1421, Lisbon, Portugal, September. Association for Computational Linguistics.

[ Luong et al.2015b] Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba.2015b. Addressing the rare word problem inneural machine translation. In Proceedings of the 53rd Annual Meeting ofthe Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Pa-pers), pages 11-19, Beijing, China, July. Association for Computational Linguistics.

[ Mikolov et al.2010] Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur.2010. Recurrent neural network based language model. Interspeech, pages 1045-1048.

[ Och and Ney2002] Franz J Och and Hermann Ney.2002. Discriminative training and maximum entropy models for statistical machine transla-
tion. In Proceedings of the 40th Anmual Meeting of the Association for Computational Linguistics, pages 295-302, July.

[ Papineni et al.2002] Kishore Papineni, Salim Rouos, Todd Ward, and Wei-
Jing Zhu.2002. Bleu:a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, July.

[ Schwenk2007] Holger Schwenk.2007. Continuous space language models.
Computer Speech and Language,21(3):492-518.

[ Sutskever et al.2014] ya Sutskever, Oriol Vinyals, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.2014. Sequence to sequence learning with neural networks. Advances in Neural Information Process-ing Systems,4:3104-3112.

[ Xu et al.2015] Kelvin Xu, Jimmy Ba, Kyunghyun Cho, Aaron Courvillem, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio.2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of The 32nd International Conference on Machine Learning, pages 2048-2057.

[ Zaremba et al.2015] Wojciech Zaremba, ya Sutskever, and Oriol Vinyals.2015. Recurrent neural network regularization. In ICLR.

[ Zeiler2012] Matthew D. Zeiler.2012. Adadelta: An adaptive learning rate method. Eprint Arxiv.








