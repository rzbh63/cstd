
# 「AI科技」机器学习算法之K-means算法原理及缺点改进思路 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月07日 08:00:53[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：63


[https://www.toutiao.com/a6641916717624721933/](https://www.toutiao.com/a6641916717624721933/)
2019-01-03 08:00:00
K-means算法是使用得最为广泛的一个算法，本文将介绍K-means 聚类算法、原理、特点及改进思路。
# K-means聚类算法简介
K-means 聚类算法，是基于距离的一种无监督式的学习算法。在1967年首次由MacQueen提出，常用于模式识别和数据挖掘中，其目的是对一组数据进行几何等价划分进行分类。
![「AI科技」机器学习算法之K-means算法原理及缺点改进思路](http://p99.pstatp.com/large/pgc-image/89b9323710f547289813857b72f979d4)
K-means算法是使用得最为广泛的一个算法，其应用场景遍及医学、经济学、行为学、决策科学等领域。算法以样本均值（质心）代表该类，定义简单具有清晰明了的几何和统计意义。
# K-means聚类算法原理
**算法的基本思路：**
从 一组数据对象随机选择 m 个对象作为初始聚类中心；
根据每个聚类数据对象的均值（质心），计算每个数据对象与这些质心的距离；并根据最小距离重新对相应数据对象进行划分；
重新计算每个（有变化）聚类的均值（质心）；
计算标准测度函数，当满足函数收敛时，则算法终止；如果条件不满足则回到步骤2
**算法的工作流程**
![「AI科技」机器学习算法之K-means算法原理及缺点改进思路](http://p99.pstatp.com/large/pgc-image/5472e5b1a8d54b8dbeedba1ff88f23ac)

# 距离算法及准则函数
一般情况下我们都是以欧拉距离公式来计算两个数据对象间的距离，但还有其他的一些方法可以用于计算，算法如下：
①明氏距离（Minkowski Distance）
![「AI科技」机器学习算法之K-means算法原理及缺点改进思路](http://p3.pstatp.com/large/pgc-image/f0bdbeeac8824da6a227aec67736d830)

```python
这里的xi=( i1，xi2，…，xip)和xj=( j1，xj2，…，xjp)是两个p维的数据对象并且 i≠j。
```
②欧式距离（Euclidean Distance）
![「AI科技」机器学习算法之K-means算法原理及缺点改进思路](http://p99.pstatp.com/large/pgc-image/f115de569bfb48ef99f2d8f9f57ba34b)

```python
当明氏距离中q=2时，公式1即欧式距离。
```
③兰式距离（Canberra Distance）：
![「AI科技」机器学习算法之K-means算法原理及缺点改进思路](http://p3.pstatp.com/large/pgc-image/8bbc4245b05a4c0ea9b4bcd7f0b1144e)
**（2）准则函数E**
对于K-means算法，通常使用准则函数E，也就是误差平方和（Sum of Squared Error，SSE）作为度量聚类质量的目标函数。
![「AI科技」机器学习算法之K-means算法原理及缺点改进思路](http://p3.pstatp.com/large/pgc-image/2eed2d92e07f4933a8af77c64184791d)
其中，d( )表示两个对象之间的距离，可以利用明氏、欧式或兰氏距离求得。
对于相同的k值，更小的SSE说明簇中对象越集中。对于不同的k值，越大的k值应该越小的SSE。

# K-means聚类算法特点
**K-means算法优点：**
原理简单、运算快速
处理大数据集，该算法保持可伸缩性及高效性
当数据接近高斯分布时，聚类效果最好。
**K-means算法缺点：**
需要事先给定聚类的数量K 值；
对初始聚类中心敏感，不同的初始值会对结果产生不同效果
若数据中含有异常点和孤立点，将导致分类偏离严重
不适用于非高斯分布的数据。
![「AI科技」机器学习算法之K-means算法原理及缺点改进思路](http://p99.pstatp.com/large/pgc-image/8ad95efc432f4c60be7144fe52b09b3a)
针对以上确定，最后两点属于数据问题，无法解决，但是前两点还是可以进行改进的。针对第一个缺点，可以通过肘部算法来确定K的数量，具体步骤如下：
① 取K的范围1至10
② 分别统计每个K类的 畸变程度（每个质点到其组内的每个元素的距离之和）
③ 画图呈现，看看在第几点出现畸变缓（肘部），该点就是最优的K值
针对第二个缺点，可以对初始聚类中心的选择进行优化。优化思想为：选择批次距离尽可能远的K个点。具体选择步骤如下：
① 随机选择一个点作为第一个初始类簇中心点，
② 选择距离该点最远的那个点作为第二个初始类簇中心点，
③ 然后再选择距离前两个点的最近距离最大的点作为第三个初始类簇的中心点，
④ 以此类推，直至选出K个初始类簇中心点。
后续将通过python代码对K-means聚类算法进行实现。

