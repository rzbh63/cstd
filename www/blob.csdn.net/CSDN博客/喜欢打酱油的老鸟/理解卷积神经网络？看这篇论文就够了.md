
# 理解卷积神经网络？看这篇论文就够了 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月02日 09:03:42[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：163


[https://www.toutiao.com/a6641180491963695629/](https://www.toutiao.com/a6641180491963695629/)
2018-12-31 23:32:08
南洋理工大学的综述论文《Recent Advances in Convolutional Neural Networks》对卷积神经网络的各个组件以及进展情况进行总结和解读，其中涉及到 CNN 中各种重要层的数学原理以及各种激活函数和损失函数。机器之心技术分析师对该论文进行了解读。
论文地址：https://arxiv.org/abs/1512.07108
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/fed9bc400a6544c6beb3d7bf0633ab82)
引言
近段时间来，深度 CNN 已经在多个深度学习应用领域取得了出色的表现，并且也有很好的泛化的能力。图像分类、目标检测、实例分割和场景理解等大量任务都使用 CNN 实现了当前最佳。这篇论文总结了从最初到现在的 CNN 发展情况，并给出了以下指示：
解释卷积层的数学原理
解释池化层的数学原理
介绍某些常用的激活函数
介绍某些常用的损失函数
简要介绍基于 CNN 的应用
卷积层的数学原理
从数学上看，第 l 层的第 k 个特征图的位置 (i,j) 处的特征值可以写成 z_{i,j,k}^l，可这样计算：
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/3c99490a44464a54a1e96b598b884819)
其中 w_k^l 是第 l 层的权重，b_k^l 是第 l 层的偏置。x_{i,j}^l 是第 l 层的 (i,j) 位置的输入图块。一个特征图的权重是共享的。CNN 的最大优势之一就是这种权重共享机制，因为参数更少时，它能显著降低计算复杂度，同时还能让训练和优化更简单。
基于这类基本卷积层的一种最著名的应用是用于 USPS 数字和 MNIST 识别的 LeNet-5，如下图所示：
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/4b29a864071b4ccab8129a3dc98443c6)
LeNet-5 由三类层构成，即卷积层、池化层和全连接层。卷积层的目标是学习更能代表输入的特征，池化层则是为了降低空间维度，全连接层则是用于类别预测。具体而言，卷积层由多个卷积核构成，这使其能够得到多个特征图，而且特征图的每个神经元都会连接到之前一层中的临近神经元区域，我们将这个区域称为之前一层中神经元的感受野（receptive field）。
如上图所示，首先通过求输入与一个学习后的卷积核的卷积，可以得到一个新的特征图，然后在卷积得到的结果上逐元素应用非线性激活。在当时，MNIST 或 USPS 数字的分类结果是很好的，即使现在也仍有很多研究者在使用它们来进行简单实验，以解释说明各种各样的算法。一般而言，任何类型的卷积网络都能在这些数据集上轻松实现 97%-98% 的准确度。
近段时间来，研究者还为卷积运算提出了一些新型的架构。其中最著名的一个是扩张卷积（dilated convolution）。扩张型 CNN 是 CNN 的一项近期进展，为卷积层引入了某些额外的超参数。通过在过滤器之间插入零，可以增大网络的感受野，使得网络能够覆盖更多相关信息。从数学上看，使用扩张方法的 1-D 扩张卷积可将信号 F 与大小为 r 的卷积核 k 进行卷积：(F_{*l} k)_t = sum_t (k_t * F_{t−l * t})，其中 *l 表示 l-扩张卷积。通过这种方式，该公式可以直接扩展成 2-D 扩张卷积。
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/b2b0b80c98994fccae93910d6ab87ae1)
上图展示了三个扩张卷积层，其中扩张因子 l 随每层而指数增大。中间特征图 F2 是由底部特征图 F1 通过应用 1-扩张卷积而得到的，其中 F2 中的每个元素都有一个大小为 3×3 的感受野。F3 是由 F2 通过应用 2-扩张卷积而得到的。特征图 F4 是由 F3 通过应用 4-扩张卷积而得到的。见红色轮廓。
池化层的数学原理
池化层是 CNN 中一个很重要的模块，这类层最重要的目标是降低特征图的大小，具体方式是通过使用某个函数来融合子区域，比如取平均或最大/最小值。池化的工作方式是在输入上滑动一个窗口并将该窗口中的内容送入池化函数。池化的工作方式非常类似于离散卷积，只是用某个其它函数替代了卷积核的线性组合。
描述沿某个方向 j 的池化层的性质的数学公式：
i_j：沿方向 j 的输入大小
k_j：沿方向 j 的池化窗口大小
s_j：沿方向 j 的步幅
所得到的沿方向 j 的输出大小 o_j 可以这样计算：o_j = (i_j - k_j) / s_j + 1，注意池化层通常不会使用零填充。
最大池化和平均池化可以归纳为 Lp 池化，可描述为：
![理解卷积神经网络？看这篇论文就够了](http://p99.pstatp.com/large/pgc-image/df5260fefd134322b955432ecc89565c)
其中 y_{i,j,k} 是池化算子在第 k 个特征图的位置 (i,j) 处的输出，a_{m,n,k} 是第 k 个特征图中的池化区域 R_{ij} 中位置 (m,n) 处的特征值。特别要指出，当 p=1 时，Lp 对应于平均池化，当 p 趋近无穷是，Lp 会变成最大池化。
池化还有一些其它选择，比如混合池化，其公式可写成：
![理解卷积神经网络？看这篇论文就够了](http://p99.pstatp.com/large/pgc-image/fb546b80a01b4cc49216611d3c5c6e16)
这可被视为最大池化和平均池化的混合版本，其中 λ 是一个 0 到 1 之间的随机值，表明了使用平均池化和最大池化的位置。在实践中，这有望降低过拟合问题，且表现也会比单纯的最大池化或平均池化好一点。
某些常用的激活函数
**ReLU**
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/003aba587c1546a1b9ec6c1f6f089c49)
上图是 ReLU 函数。ReLU 激活函数定义如下：
![理解卷积神经网络？看这篇论文就够了](http://p9.pstatp.com/large/pgc-image/213e24c5582045058497533b14a9c71f)
其中 z_{i,j,k} 是第 k 通道中位置 (i,j) 处的激活函数的输入。ReLU 是一个逐元素的函数，会保留正的部分，并将负的部分变成零。相比于 sigmoid 或 tanh，ReLU 中的 max(.) 运算能使其速度更快。ReLU 的缺点是其在 0 处的不连续性，这可能会导致在反向传播中出现梯度消失问题。
**Leaky ReLU**
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/a5120fe7dfac44b4a32fcd1c43dee06d)
为了避免梯度消失问题，研究者提出了一种很重要的 ReLU 变体函数：Leaky ReLU。其数学形式为：
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/f8a3e80595ae492d8096f6e2629e3deb)
其中 λ 的取值范围是 (0,1)。Leaky ReLU 并不强制负的部分为零，相反它允许有一个较小的非零梯度。
**PArametric ReLU**
![理解卷积神经网络？看这篇论文就够了](http://p99.pstatp.com/large/pgc-image/1ea81ba3354e49fabd8afcaa6f48ebb2)
上图是 PReLU 的图示。PReLU 和 Leaky ReLU 的不同之处是参数 λ。首先来看 PReLU 的公式：
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/4a52dda07e7d46deafe60fb27e46a3a9)
其中 λ_k 是第 k 个通道所学习到的参数。这些 λ 不是预定义的，而是通过数据学习到的，而且可学习的参数 λ_k 的数量正是网络中通道的数量。因为用于训练的额外参数很少，所以无需担心过拟合问题。它可以在反向传播过程中与其它参数一起同时优化。另外也还有很多其它有用的非线性激活函数，比如 ELU、Maxout 等。
某些常用的损失函数
**hinge 损失**
hinge 损失通常用于训练 SVM 及其变体。用于多类 SVM 的 hinge 损失定义如下：
![理解卷积神经网络？看这篇论文就够了](http://p99.pstatp.com/large/pgc-image/d119994a26724dbc877b7abb31b5077d)
其中 w 是可训练的权重；δ(y(i), j) 是指示函数，如果 y(i) = j，则其输出为 1，否则输出为 0。N 是样本数量，K 是类别数量。如果 p=1，则称为 L1-hinge 损失；如果 p=2，则称为 L2-hinge 损失。
**softmax 损失**
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/b106df659478422992899dc5c6030a2d)
softmax 损失可以说是分类目标方面最流行的损失，其中 N 是图像数量，K 是类别数量，p_j 是第 j 类的概率，y 是基本真值。1{.} 是一个指示函数，如果 y_i == j，则输出 1，否则输出 0。
softmax 损失是一种泛化的 logistic 损失，能将预测结果转换成范围 (0,1) 之间的非负值并给出在各类别上的概率分布。
**对比损失**
对比损失常被用于训练用于图像检索或人脸验证的孪生网络。其基本思想是增大不相似配对之间的距离，减小相似配对之间的距离。数学公式可写成：
![理解卷积神经网络？看这篇论文就够了](http://p99.pstatp.com/large/pgc-image/00f68f45e88a4f56bc810c4f94fe1430)
如果 y=1，则第二项为 0，总损失就来自第一项，这就意味着目标就成了降低相似配对之间的距离。如果 y=0，则第一项为 0，总损失就来自第二项，这就意味着目标就成了增大不相似配对之间的距离。注意，如果不相似配对之间的距离小于余量，则总损失会增大。
**三重损失**
三重损失是对对比损失的一种聪明的改进。假设我们有三张图像，这个三元组单元 (x_a, x_p, x_n) 包含一个锚图像、正例和负例。
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/0046c64f49714cf7a2eee019c3b7473c)
上述公式是三重损失，其中 N 是图像数量，d_{a,p} 是锚图像与其正例图像之间的距离，d_{a,n} 是锚图像与其负例图像之间的距离，m 是余量。其目标是增大 d_{a,n}，使得 d_{a,p} 和 d_{a,n} 之差大于余量 m。三重损失的目标是最小化锚图像和正例之间的距离，以及最大化负例和锚图像之间的距离。
常用的损失函数有很多，比如 histogram 损失、lda 损失、KL-散度损失等。就不一一介绍了。
CNN 的应用
**图像分类**
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/f84732722cad49a3a36f0aa7b9825377)
2012 年，Krizhevsky et al. [1] 提出了 LeNet-5 的一个扩展版 AlexNet，在 ILSVRC 2012 取得了最佳表现。上图即为其架构。由于计算时间限制，他们使用了一种并行结构来进行训练。一个 GPU 运行图中顶部的层部分，另一个 GPU 运行上图底部的层。
目标检测
![理解卷积神经网络？看这篇论文就够了](http://p1.pstatp.com/large/pgc-image/c2259cef749b4b448c59a28699a37ee5)
2014-2017 年，微软和 Facebook 的研究者开发了多种基于 CNN 的目标检测方法，包括但不限于 R-CNN、SPP-Net、fast RCNN、faster RCNN 和 Mask RCNN。上图是基本的 R-CNN [2]。在从原始图像中提取出区域提议之后，它们会被调整为固定尺寸并会被发送给一个预训练的 CNN。然后输出特征会得到优化，以便边界框回归和分类。
图像分割
![理解卷积神经网络？看这篇论文就够了](http://p3.pstatp.com/large/pgc-image/31e3550d5c474499acf2158a35c63c0e)
图像分类可以粗略地分为两部分：形义分割和实例分割。其目标是预测单张图像中每个像素属于哪一类。CNN 可用于从像素层面或图块层面预测类别可能性。Long et al. [3] 为像素级分割提出了一种全卷积网络，如上图所示，该网络类似于一般的网络，只是最后一层不同，这一层被称为去卷积层。这一层的学习目标是上采样标签图的分辨率。其结果可以是一张输出图像，其中包含了密集的像素级预测。
基于 CNN 的方法可用于很多不同的领域，比如图像检索、人脸识别、文本分类、机器翻译、3D 重建、视觉问答或图像绘制等。这里不可能全面覆盖。
分析师简评
尽管 CNN 表现出色而且泛化能力优异，但仍还有些问题仍待进一步研究。CNN 的训练需要大规模数据集和大量计算能力，但人工收集数据的成本很高而且易出错。因此，研究者们也在探索能利用大量无标注数据的弱监督学习和无监督学习。与此同时，为了加速训练过程，也有必要开发高效且可扩展的并行训练算法。最后且最重要的是如何解释网络以及理解网络的内在部分，因为人们如果不知道 CNN 表现优异的原因，人们不会愿意坐在基于 CNN 的自动驾驶汽车中或接受基于 CNN 的医疗技术的治疗。

