
# 新词发现 - sinat_33731745的博客 - CSDN博客

2018年02月26日 22:47:31[Tao-Tao-Tao](https://me.csdn.net/sinat_33731745)阅读数：1808


挖掘新词的传统方法是，先对文本进行分词，然后猜测未能成功匹配的剩余片段就是新词。这似乎陷入了一个怪圈：分词的准确性本身就依赖于词库的完整性，如果词库中根本没有新词，我们又怎么能信任分词结果呢？此时，一种大胆的想法是，首先不依赖于任何已有的词库，仅仅根据词的共同特征，将一段大规模语料中可能成词的文本片段全部提取出来，不管它是新词还是旧词。然后，再把所有抽出来的词和已有词库进行比较，不就能找出新词了吗？有了抽词算法后，我们还能以词为单位做更多有趣的数据挖掘工作。
## 算法
Matrix67提出了一种基于信息熵的新词发现算法，成词的标准有两个：
内部凝固度
自由运用程度
所谓内部凝固度，用来衡量词搭配（collocation）是否合理。比如，对于“的电影”、“电影院”这两个搭配，直观上讲“电影院”更为合理，即“电影”和“院”凝固得更紧一些。在计算语言学中，PMI (Pointwise mutual information)被用来度量词搭配与关联性，定义如下：
pmi(x,y)=log
$$
pmi(x,y) = log\frac{P(x,y)}{P(x)P(y)}
$$
若PMI高，即两个词共现（co-occurrence）的频率远大于两个词自由拼接的乘积概率，则说明这两个词搭配更为合理一些。针对一个词有多种搭配组合，比如“电影院”可以由“电影”+“院”构成，也可以由“电”+“影院”构成，Matrix67取其所有pmi最小值（去掉log）作为内部凝固度：
solid(c1m)=min=
$$
solid(c_1^m) = min\frac{P(c_1^m)}{\prod P(c_i^j)} = \frac{P(c_1^m)}{max\prod P(c_i^j)}
$$
其中，c1m=c1c2⋯cm$c_1^m=c1c2⋯c_m$表示长度为m的字符串，P(c1m)$P(c_1^m)$表示词c1m$c_1^m$的频率。
光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑“被子”和“辈子”这两个片段。我们可以说“买被子”、“盖被子”、“进被子”、“好被子”、“这被子”等，在“被子”前面加各种字；但“辈子”的用法却非常固定，除了“一辈子”、“这辈子”、“上辈子”、“下辈子”，基本上“辈子”前面不能加别的字了。“辈子”这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，“辈子”并不单独成词，真正成词的其实是“一辈子”、“这辈子”之类的整体。
所以，Matrix67提出了自由运用程度，用以衡量一个词的左邻字与右邻字的丰富程度。正好信息熵可以完美地诠释了这种丰富程度，熵越大则丰富程度越高。“被子”和“辈子”这两个片段的左邻字熵le$le$与右邻字熵re$re$分别如下：
le(被子) = 3.67453
re(被子) = 3.8740
le(辈子) = 1.25963
re(辈子) = 4.11644
可以看出，“被子”的左邻字熵与右邻字熵都较高，而“辈子”的左邻字熵较小，即左邻字非常贫乏。因此，“被子”较“辈子”更有可能成词。自由运用程度的定义如下：
free(c1m)=min{le(c1m),re(c1m)}
$$
free(c_1^m)=\min\{le(c_1^m), re(c_1^m)\}
$$
给频数、内部凝固度与自由运用程度哥设定一个阈值，提取出来符合阈值的候选词，去掉词典中存在的词即为新词了。
### 参考资料
[http://www.matrix67.com/blog/archives/5044](http://www.matrix67.com/blog/archives/5044)
[http://www.cnblogs.com/en-heng/p/6699531.html](http://www.cnblogs.com/en-heng/p/6699531.html)
代码：[https://github.com/Moonshile/ChineseWordSegmentation](https://github.com/Moonshile/ChineseWordSegmentation)

