
# 深入浅出LSTM神经网络 - 喜欢打酱油的老鸟 - CSDN博客


2018年08月17日 13:54:24[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：466


**摘要：**根据深度学习三大牛的介绍，LSTM网络已被证明比传统的RNNs更加有效。本文由UCSD研究机器学习理论和应用的博士生Zachary Chase Lipton撰写，用浅显的语言解释了卷积网络的基本知识，并介绍长短期记忆（LSTM）模型。
**【编者按】**使用前馈卷积神经网络（convnets）来解决计算机视觉问题，是深度学习最广为人知的成果，但少数公众的注意力已经投入到使用递归神经网络来对时间关系进行建模。而根据深度学习三大牛的阐述，LSTM网络已被证明比传统的RNNs更加有效。本文由加州大学圣迭戈分校（UCSD）研究机器学习理论和应用的博士生Zachary Chase Lipton撰写，用浅显的语言解释了卷积网络的基本知识，并介绍长短期记忆（LSTM）模型。
鉴于深度学习在现实任务中的广泛适用性，它已经吸引了众多技术专家、投资者和非专业人员的关注。尽管深度学习最著名的成果是使用前馈卷积神经网络（convnets）来解决计算机视觉问题，少数公众的注意力已经投入到使用递归神经网络来对时间关系进行建模。
（注：为了帮助你开始体验LSTM递归网络，我附上了一个简单的微实例，预装了numpy、theano和一个Jonathan Raiman的LSTM样例Git克隆）
在最近的文章《学习阅读递归神经网络》中，我解释了为什么尽管前馈网络有难以置信的成功，它们受制于无法明确模拟时间关系，以及所有数据点都是由固定长度的向量组成的假设。在那篇文章的结论部分，我承诺写一篇的文章，解释卷积网络的基本知识，并介绍长短期记忆（LSTM）模型。
![](http://img.ptcms.csdn.net/article/201506/05/55713db332cfe.jpg)

首先，介绍一下神经网络的基本知识。一个神经网络可以表示为一个人工神经元的图，或者说节点和有向边，用来对突触建模。每个神经元是一个处理单元，它将连接到它的节点的输出作为输入。在发出输出之前，每个神经元会先应用一个非线性激活函数。正是由于这个激活函数，神经网络具有对非线性关系进行建模的能力。
现在，考虑这个最近的著名论文Playing Atari with Deep Reinforcement Learning，结合convnets和强化学习来训练电脑玩视频游戏。该系统在某些游戏上有超越人类的表现，比如Breakout!，这种游戏在任意时候的合适的策略，都可以通过查看屏幕推断出来。但是，当优化策略是需要在长时间跨度规划时，系统就和人的表现相差甚远，例如太空侵略者（Space Invaders）。
因此， 我们引入递归神经网络（RNN），一个赋予神经网络对时间进行显式建模的能力，通过添加跨越时间点的自连接隐藏层。换句话说，隐藏层的反馈，不仅仅进入输出端，而且还进入了下一时间步骤隐藏层。在本文中，我将使用递归网络的一些示意图，从我即将审查的这一主题的文献中摘录。
![](http://img.ptcms.csdn.net/article/201506/05/55713deeab1c8.jpg)

现在，我们可以通过通过两个时间步来展开这个网络，将连接以无环的形式可视化。注意权重（从输入到隐藏和隐藏到输出）在每个时间步是相同的。递归网络有时被描述为深度网络，其深度不仅仅发生在输入和输出之间，而且还发生在跨时间步，每个时间步可以被认为是一个层。
![](http://img.ptcms.csdn.net/article/201506/05/55713e03949e5.jpg)
一旦被展开，这些网络可以使用反向传播，进行端到端的训练。这种跨时间步的反向传播扩展，被称为沿时间反向传播（Backpropagation Through Time）。
然而有一个问题，在Yoshua Bengio经常被引用的论文（Learning Long-Term Dependencies with Gradient Descent is Difficult）中提到，那就是消失的梯度。换句话说，后面时间步的错误信号，往往并不能回到足够远的过去，像更早的时间步一样，去影响网络。这使它很难以学习远距离的影响，比如放过的那只小卒会在12步后回来将你。
补救这一问题的措施是1997年首先被Sepp Hochreiter和Jurgen Schmidhuber提出的长短期记忆（LSTM）模型。在这个模型中，常规的神经元，即一个将S型激活应用于其输入线性组合的单位，被存储单元所代替。每个存储单元是与一个输入门，一个输出门和一个跨越时间步骤无干扰送入自身的内部状态相关联。
![](http://img.ptcms.csdn.net/article/201506/05/55713e20b35ee.jpg)
在该模型中，对于每个存储单元，三套权重从输入训练而得，包括先前时间步中完整的隐藏状态。一个馈送到输入节点，在上图的底部。一个馈送到输入门，在最右侧的单元格底部显示。另一个馈送到输出门，在顶部最右侧的显示。每个蓝色节点与一个激活函数相关联，典型情况是S型函数，以及表示乘法的Pi节点。单元中最中央的节点称为内部状态，并且以1的权重跨越时间步，反馈回本身。内部状态的自连接边，被称为恒定误差传送带或CEC。
就前传递而言，输入门学习来决定何时让激活传入存储单元，而输出门学习何时让激活传出存储单元。相应的，关于后传递，输出门是在学习何时让错误流入存储单元，而输入门学习何时让它流出存储单元，并传到网络的其余部分。这些模型已被证明在多种多样的手写识别和图像加字幕任务上非常成功。也许得到多一些关爱，它们能在太空侵略者上获胜。
作者介绍：Zachary Chase Lipton is a PhD student in the Computer Science department at UCSD. He researches machine learning theory and applications, and is a contributing editor to KDnuggets.
**原文链接**：Demystifying LSTM Neural Networks（翻译/王玮 责编/周建丁）

[https://www.csdn.net/article/2015-06-05/2824880](https://www.csdn.net/article/2015-06-05/2824880)

