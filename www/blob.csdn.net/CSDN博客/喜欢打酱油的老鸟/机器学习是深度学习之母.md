
# 机器学习是深度学习之母 - 喜欢打酱油的老鸟 - CSDN博客


2018年09月22日 11:19:28[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：95标签：[机器学习																](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)[深度学习																](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)


[https://blog.csdn.net/u013139259/article/details/52856466](https://blog.csdn.net/u013139259/article/details/52856466)
博主不才，研究深度学习有一段时间，主要专注CV领域。本科期间学习了机器学习的一些算法，用SVM做的人脸识别的工作，之后又去中科院实习，用深度学习来做人脸识别。所以，从传统算法，到深度，这个过程，我是深深体会到的。单从CV领域，尤其是二维方面的工作，卷积神经网络取得的成果，让我深深感到传统算法的无力。总的来说，深度学习算法是近几年机器学习革命般的成果。举个例子，很明显的对于传统算法很困难的LFW人脸数据集，现在已经被各家公司刷Acc都接近99.9了[1],基本宣布该数据集的死刑。深度学习的火热也引起国内外众多CV领域的人进行创业。国内的商汤科技，图森，face++，腾讯优图，中科院下属公司，等等哪一个没有利用深度技术呢？Caffe,Mxnet , Tensorflow , Torch7,等深度学习框架都是近几年火热起来的,而前几年热门的svm以及框架libsvm最近的确暗淡了不少。那Paper方面更不用说了，2016 CVPR最佳论文就是何凯明的深度残差网络[2]。
那么先来回答这两个问题。
“深度学习会替代传统的机器学习算法吗？”
我认为会。是因为火吗？肯定不是，是因为我相信这门技术，更接近人类脑思维。当然，这只是我的期待。很明显的事实，虽然深度学习在监督学习领域，取得了卓越的发展，但是它并不是万能的，比如百度翻译对专业领域，以及长句的翻译依然不如人意——百度的深度学习技术也算是顶尖的了。当然并不是很多领域都适合深度，一个是，不适合，比如大材小用，我只需要根据极少的样本来解决的分类问题，比如，周志华《机器学习》上一直提到的西瓜判断好坏瓜的问题，总共的属性，大概10个，传统的决策树，和SVM都能做到很好，的确没必要去用深度来做(当然，你可以尝试下）。另外的是，做不了，举个例子，就拿最近很火的 AlphaGo 来说，由于快速走子（fast rollout）要达到微秒级的响应速度，神经网络目前还做不到，因此 Google DeepMind 采用了逻辑斯谛回归（logistic regression）算法。有机器学习背景的同学都知道，逻辑斯谛回归作为一种线性模型[3]。当然，我依然认为会。深度学习不是一成不变的，需要我们很多科研人员不断的去改进和优化。虽然我一直都认为深度学习不太适合无监督，就如，Andrew Ng说:深度学习依然在unsupervised learning 上效果不如supervised。最近的关于无监督也有不少有影响力的paper,Lecun在Quora上提到的对抗神经网络(adversarial network)，将其认为是最近几年深度的一个突破，GAN[4],以及卷积网络的改进版DCGAN[5]，通过深度网络以及对抗过程，无监督学习生成模型，产生的图片基本接近自然图片。而且，DCGAN将网络中的判别网路(discriminative model)用于图像的分类工作，在CIFAR-10几乎取得了state of art 的效果。所以，深度学习也不是一成不变的，它也在成长，改变。所以，我还是说:相信它。
“传统机器学习算法，还有必要去学习吗”
还需要争论吗？肯定是需要的啊。
机器学习是深度之母，你火了，能忘了你的母亲吗？
从两个方面回答这个问题。
(1)深度学习的本质
从起源上说，感知器(Perceptron)算法是deeping learning 的前身，然后从单层，到多层感知器，再到神经网络，最后到深度学习，无非是，神经网络算法+一些trick=dl。所以，从起源上说，深度学习，就是机器学习的一个分支，深度学习的发展，也是机器学习的发展，深度学习的进步，也是机器学习的进步。从要解决的问题，无非是分类，回归问题，无非是生成模型，决策模型，这些都是都逃离不了机器学习的大的范畴，二者都离不开数据统计，优化算法的问题，都会涉及目标函数，损失函数，偏导求解，参数初始化，数据预处理等等，都会遇到数据缺失，数据分布不均，数据不均衡，训练过拟合，欠拟合的问题。总的来说，本质是一样的。
(2)深度学习与传统算法的结合
深度学习能成功得益于tricks的提出,而且很多trick的思想基本上都是源于传统方法。举个例如，CNN中的Pooling层，典型的降维思想，解决维度灾难，利于训练和优化。还有比如经典的dropout[6]，BN[7]，仔细想想，这些方面，思想尤其简单，在传统机器学习算法中都是会想到的。更明显的就是两者结合，比如，强化学习[8]和深度学习的结合成深度强化学习[8]。在我阅读的论文中，将SVM与CNN结合起来的论文相当多，比如，大牛汤晓鸥的论文—大规模的车型识别[9]只是将CNN作为一个特征提取器，然后使用传统的分类算法，比如SVM，或者Bayesian算法(发现，汤很喜欢，将CNN和传统算法结合起来)。DCGAN[5]同样也是使用训练好的判别模型，加上L2-SVM结合，来做测试集的分类的工作。一般情况下，基本上非end-to-end的算法都是进行两者的结合。最近比较火热的detection的问题，R-CNN[10]也是吸取了传统检测算法的“region proposal”的思想，然后将CNN用到检测问题中，其中的候选框使用的“selective search”算法，而且，在fast-rcnn[11]中，依然依赖该算法。最后faster-rcnn[12]版本，才使用神经网络来寻找候选框，但是，其中在feature map中搜索的过程，也是一种滑动窗口的方式。哈哈。所以，综上所述，deep learning的发展，依赖传统算法的基础。
最后，DL学的再好，没有ML的基础，也走不远。做人，不能忘本！
引用
[1]http://vis-www.cs.umass.edu/lfw/results.html
[2]Deep Residual learning for image Recognition
[3]知乎，作者：张驭宇 链接：[https://www.zhihu.com/question/42061396/answer/93433458](https://www.zhihu.com/question/42061396/answer/93433458)
[4]Ian J. Goodfellow .Generative adversarial networks
[5]Unsupervised representation learning with deep convolutional generative adversarial networks.
[6]  Dropout: A Simple Way to Prevent Neural Networks from Overfitting
[7]Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
[8]https://en.wikipedia.org/wiki/Reinforcement_learning
[9]Rich feature hierarchies for accurate object detection and semantic segmentation
[10]Fast R-CNN.ICCV 2015
[11]Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
---------------------
本文来自 张骞晖2 的CSDN 博客 ，
全文地址请点击：[https://blog.csdn.net/u013139259/article/details/52856466?utm_source=copy](https://blog.csdn.net/u013139259/article/details/52856466?utm_source=copy)

