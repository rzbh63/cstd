
# 李航：机器学习新动向 从人机交互中学习 - peghoty - CSDN博客


2013年10月12日 16:48:00[皮果提](https://me.csdn.net/peghoty)阅读数：3370


来源:[和讯科技](http://tech.hexun.com/)作者:
 郑希2012年9月8日12:14
原文地址[http://tech.hexun.com/2012-09-08/145627821.html](http://tech.hexun.com/2012-09-08/145627821.html)

[和讯科技](http://tech.hexun.com/)消息
 9月8日—9日，中国软件开发者大会在北京国家会议中心举行，本届大会邀请了众多国内 IT 技术领军人物针对软件应用与开发等热门话题进行深入探讨。和讯[科技](http://tech.hexun.com/)对本次大会进行了全程报道。[华为](http://gongsi.hexun.com/fc_285.shtml)诺亚方舟实验室首席科学家李航发表主题演讲。
![华为诺亚方舟实验室首席科学家李航](http://i8.hexunimg.cn/2012-09-08/145627822.jpg)
华为诺亚方舟实验室首席科学家李航
李航表示，到目前为止，人工智能其他领域研究，我们发现最有力的手段可能还是要基于数据，通过机器学习这样的办法才能够使得我们的机器更加智能化。
同时，李航认为在机器学习里面到底我们学习多少数据，我们一个结论是需要很多数据的，即使我们仅仅是学一个二类分类器可能都需要成千上万的标注数据。
**以下为华为诺亚方舟实验室首席科学家李航演讲实录：**
李航：大家上午好，非常高兴有这样的机会跟大家一起交流，我是第一次参加这个会议，自己主要是做研究的，研究的方向是自然源处理信息检索信息挖掘，跟孙博士还有一些同仁都是做同一个领域的相关工作。
我今天跟大家一起分享一下我最近观察到的和感触到的机器学习的新动向，从人机交互中获取数据使得我们学习更有效，能够构建起更加智能的系统。我们大家都同意，智能化是计算机科学发展的必然趋势，让我们计算机越来越智能，这个过程当中我们必然希望有一个非常强有力的手段，到目前为止，人工智能其他领域研究，我们发现最有力的手段可能还是要基于数据，通过机器学习这样的办法才能够使得我们的机器更加智能化。我写了一篇博文叫做《机器学习正在改变我们的工作与生活》，里面介绍了为什么智能化必然是基于大规模数据的，必然是统计机器学习驱动的。在这里做一个广告，今年出了一本书是介绍统计学习方法，写这个书目的之一是希望软件开发者很快掌握这些方法，很快构建智能化的系统。
为什么机器学习需要大量数据，到底有多少数据才是足够的数据，我们再看一下机器学习新动向，怎么样通过跟人机交互能够获得更多数据，这个包括日志数据挖掘、重包，还有对进比较热门的**人机协同计算**。最后我想介绍一下有了大量数据以后怎么样能够使用这些数据，构建一个非常智能化的系统，使得我们这个系统更加智能。
大家知道统计机器学习是基于数据的，最重要的一个环节就是收集采集数据，高质量的大规模的数据能够帮助我们构建出一个非常智能化的系统。有一个非常朴素的问题，到底有多少数据才能够足够帮助我们去构建一个智能化的系统，这是一个非常重要的问题。在机器学习里面有很多研究，比如说在统计机器学习理论里面一个重要的研究课题就是所谓的**样本复杂度**，到底我要多少样本要多少训练数据，才能够很好地学一个模型。这个问题是一个非常难的问题，虽然从事的研究很多，但是有的时候并不很容易就能够很好地回答这个问题。比如说我们要构建一个二类分类器，这是机器学习里最基本的模型，判断一个图片是不是含有人脸，就是所谓人脸检测问题，这就需要通过构建二类分类器去完成。对于这个问题有一个理论定理叫**欧卡姆剃须刀**，它的结论是**学一个二类分类器的时候需要样本容量、样本数目是跟要求学习的精度有关**。我们希望学习精度越高需要的样本也就越多，同时跟学习模型复杂度也有关，模型如果非常复杂的话学习的样本也非常多。这个结论是什么呢？样本的数目是用
 S 集合容量来表示的话，它至少要大于这样一个量，这个量其实跟我们要求的模型的精度、和我们做判断时候的确信度，我们希望判断是非常有确信度的。比如说我们要学一个二类分类器，里面模型复杂度是 100，根据定理我们可以看出 5 万以上训练数据才能学好这样一个分类器。这是一个非常大的量，我们需要大量的数据才能完成这个任务。
一般经验上来说，我们学习的时候假设要学习的模型有 K 个参数，**参数个数一般表示模型复杂度**，经验上至少模型参数数百倍训练样本才能够大概把模型学得比较好。我们现在应用往往都是非常的复杂、需要做的事情都是非常智能化，这个时候模型参数个数是非常大的，有时候都是上百万量级的模型参数，如果上面再加上几百倍量的样本，我们要很好地学习一个模型的话，在机器学习里面需要大量的学习样本。这个学习样本并不是我们随便搜集一些简单数据就能够用来学习，通常我们要做什么事情呢？比如说我们在人脸检测里面，看到一个数码相机拍的照片有没有人脸，这个是大家花钱雇佣很多专业人员用人工办法标注照片，在不同光线条件下、不同的像素条件下和不同年龄、种族、性别的人在照片里出现的时候，到底有没有人脸，要标注大量这样的数据，通过大量数据的标注能够采集出真正能够覆盖各种情况的数据，这个时候我们才能够非常有效地学习我们的人脸检测的模型。所以现实当中，我们需要非常多的高质量的数据来帮助我们去构建智能化的系统。这时候就给我们一个很大的挑战，我们到底怎么样才能够采集到这么多高质量数据？机器学习里有一个新的趋势，就是通过跟人机的交互，我们希望**交互过程当中采集大量高质量数据**，这变成了机器学习领域里的一个非常值得大家关注的新的动向。
我们希望通过人机交互用各种巧妙的办法采集到数据，有日志数据挖掘、众包、人机协同计算等方法被提出，现在在研究领域也是热门研究话题，就是怎么样能够构建出更好的机制，有效地从用户那里采集到大量高质量机器学习的训练数据。在互联网搜索引擎里边，搜索引擎一定会记录下所有日志，比如一个用户提交了查询，系统会返回一组
 URL，用户点击 URL 的过程被记录在搜索引擎里边作为日志数据。这个数据是非常有用的，对帮助提高搜索引擎相关排序是非常有帮助的，等于说我们每天有上亿用户使用搜索引擎，提交的查询也都是更大数量级的查询，不同用户提交不同查询，之后点击了不同 URL，我们把这个收集起来可以从用户那里得到就是到底用户需要从搜索引擎得到什么样的信息，这样大量的查询对搜索引擎质量和相关排序质量提高非常有关。这种所谓的数据挖掘在搜索引擎里面应用非常普遍，在其他不同应用里边大家也在尝试着采集各种不同日志数据，能够帮助提高相关应用。一般用户在使用过程当中自己不会花更多的代价做什么事，他只是更多地使用应用，但是我们把用户使用过程很好地记录下来，把这些数据拿回来作为用户的反馈，这是一种隐式的反馈，用这种反馈数据帮助我们提高现在的应用，这个思想是非常合理很自然的想法。如果这个系统本身是基于机器学习的话，一般来说能够很好地利用日志数据帮助我们提高应用各方面的性能。当然用户行为数据往往是比较含噪音的，怎么样去除噪音，提高日志数据的质量是我们需要考虑的问题。
再一个例子，大家可能也都知道 Amazon Mechanical Turk，我如果是一个想标注数据的人，我就把这个标注任务需求放在市场上，市场里有很多注册会员，他们就是所谓的员工，如果这些员工可以看到提供的各种各样标注的任务，根据自己的兴趣、喜好、能力选择感兴趣的任务进行标注，通过标注可以得到一些报酬，有些人把这个当成自己业余获得收入的办法。也有一些人把这个当做一种娱乐或者学习的过程，或者消磨时间的过程，这个非常火爆。在
 Amazon Mechanical Turk 上常驻注册工人上百万，每天成交大量标注工作，所谓标注包含各种，比如说图片里边的标注，人脸检测就可以成为 Amazon Mechanical Turk 里边的一个标注任务，给工人大量图片让工人判断图片里是不是含有人脸，这个工作对人来说往往简单，可能几秒钟就标注一个。但是让机器来判断的话往往是非常难的，如果大量工人帮助标注大量图片数据可以帮助我们很快地构建一个智能系统，这个方法就是所谓的**众包**，它确实能够有效地帮助智能系统开发者收集大量的数据，往往以很小的代价就能达到我们的目的。互联网的这样一个环境给我们提供了这样一个可能性，我们能够有这样一个市场，在这个市场上能够很快地募集到合适的工人帮助我们完成这些标注工作，这就是众包的特点，这就是以
 Amazon Mechanical Turk 为代表的平台能够把对标注工作有需求的人联系在一起，给大家提供这样一个平台，大家可以在这上面进行各种各样的标注工作。
还有一种办法是通过游戏的办法来采集数据，有目的地做游戏，一个有名的游戏叫**ESP**，两个玩者，给他们同时展示一个图片，让他俩同时对图片做标注，如果他俩标注的关键词 （或者标记）一致的话他俩都得分，如果不一致就都不得分。这样的话，这两个玩者都希望把图片标记标得非常准确，这样才能得分。根据我们一般的常识，两个人一定会尝试着尽量找合适的常识性的标记把标签标在图片上。在图片上标志的标记就可以用来机器学习的算法，Google
 做图片搜索的时候就是使用了这样的方法。
还一个例子是**ReCAPTCHA**，在登陆网站的时候常常让我们输入验证码，很多网站采用了这个 ReCAPTCHA 系统，给我们提供两段验证码需要输入，用户不一定知道其中一个单词真正是验证码，是计算机故意把它做一些图形变换，让机器识别不出来人能看出来，人敲出这个词使得机器鉴别当前用户是人还是机器。还有一部分是 ReCAPTCHA 这个系统找到一些 OCR 识别不出来的（即
 OCR 处理过程当中比较难处理的），在 ReCAPTCHA 系统里边会当成验证码的另外一部分，也让用户输入进去，用户输入两部分验证码。这时候第二部分等于是帮助我们提高 OCR 系统，用户给我们提供了大量的 OCR 训练数据，整个可以帮助我们把过去的图书变成数字化，大量用户在网上间接地参与了数据化的工作，也帮助我们提高 OCR 的准确率。
注：（来自百度词条） OCR （Optical Character Recognition，光学字符识别）是指电子设备（例如扫描仪或数码相机）检查纸上打印的字符，通过检测暗、亮的模式确定其形状，然后用字符识别方法将形状翻译成计算机文字的过程，即，对文本资料进行扫描，然后对图像文件进行分析处理，获取文字及版面信息的过程。如何除错或利用辅助信息提高识别正确率，是
  OCR 最重要的课题，ICR（Intelligent Character Recognition）的名词也因此而产生。衡量一个 OCR 系统性能好坏的主要指标有：拒识率、误识率、识别速度、用户界面的友好性，产品的稳定性，易用性及可行性等。
刚才说的 ReCAPTCHA 和 ESP 游戏。Luis VON Ahn 是这个时代非常杰出的科学家，提出了很多有意思的方法，他把这些概念进一步升华，提出了 Human Computation，我们可以把人当成一个个计算机，现在世界上两种计算机，一种**机器计算机**一种**人肉计算机**，这两种计算机各有所长，我们应该两者做各自擅长的东西，然后两者协同计算，互相取长补短，使得我们能够更好地完成很多任务，这是所谓**人机协同计算**的主要想法。
有三种方式帮助我们采集数据：数据挖掘、众包、和人机协同计算。日志数据挖掘用户没有感觉到自己奉献了数据，他只是使用，这个过程中给系统提供数据帮助系统提高。在众包和人机协同计算里边用户意识到自己在参与，更多地是得到经济上的报酬，另一个得到其他的一些满足或者是参加游戏或者做一些其它的事情，这能够帮助系统越来越改善，给系统提供更多有用数据。
我们现在有各种各样的方法帮助我们采集大量的高质量的有用的数据，有了这样的数据真的能帮助我们做很多事情吗？答案是真的，如果我们很好地设计我们数据采集的方法，很好地设计我们机器学习的方法，我们真的能够很好地把这两者结合起来很好地构建我们的智能系统。我给大家一个例子，这是我以前在**微软亚洲研究院**做的一个工作，那时候我们在做互联网搜索项目，希望通过采集大量用户搜索中的日志数据点击数据帮助我们学习搜索时候的**排序模型**或者**关联模型**。要解决的问题是在搜索过程当中用户的查询和网页内容上语意上应该匹配，但是词面上往往不一定匹配，这个时候搜索引擎还是基于关键词匹配做网页相关排序。比如说用户查询
 SDCC，网页写中国软件开发者大会，语意上关联，但是词面上一个是英文一个是中文，这之间也有不匹配的问题。搜索中希望解决的问题是自动学一些相关模型，把查询和网页之间匹配关系在语意上匹配多少计算出来，这是我们要做的事情。我们有这个想法就是通过大量用户点击数据来完成学习任务，在搜索引擎里面可以采集到大量的点击数据，我们其实可以认为这个问题是这样一个问题，我们有两个空间，一个是查询的空间，有很多查询数据表示在这个空间里边，还有一个网页文本空间，网页空间和文本空间有相似度，它在各自空间里边能够计算，比如说我们就算两个查询到底单词有多大，这个时候可以用距离来算两个查询相似度。文本也是一样，可以把文本表示两个向量，判断两个文本相似度。现在一个非常有价值的数据是什么？我们通过日志里面收集到的点击数据，我们把两者联系起来，查询
 Q1 和文本 D2，我们看到数据关联起来了，我们知道 Q1 和 D2 之间是关联的，我们通过这个给定的数据希望学习一个模型，把所有这些查询和文本都映射到一个新的空间里。新的空间希望能够自动寻找一个相关度函数，或者距离或者相似度，新空间里给你任何一个查询能找到跟他相似的文本，给我一个文本也能找到相似的查询，本利是一指数据，在新空间变成同样的处理。我们需要学的是这种映射，有了点击数据就应该能够很好地判断查询和文本之间的相关关系。这个方法其实包含了传统的信息检索里面的最基本模型，比如说 VSM，这些模型不需要自动学习这两个映射，它相当于人工定义的两个映射，而且是相当简单的模型，传统信息检索模型都是简单模型，我们想做的是更一般的模型，动作数据驱动的方法自动学习比传统信息模型更加有效更一般的模型。大家可以大概想象到，我们通过这样的学习其实可以学习到一指数据之间的批准，通过这样的学习虽然字面上两者匹配并不是很好，语意匹配，我们通过学习可以了解到两个词是很相关的。结果通过这样的点击数据里面学到这种模型，就比传统的
 BM25 这些模型都做的更好。
这个方法还可以做其他的事情，比如我们今天看到的图形图片标注，假设有很多图片很多人做标记，这也是两种不同数据，一个是文字一个是图片。这个图片被标了 hook，这个被标记了 fishing，在文字空间有文字相似度，图片空间有图片相似度，我们知道它们两个之间的关系，我们可以按照刚才说的学习方法把图片文字都映射到新的空间里，按照语意近的就应该近，跟
 hook 和 fishing 同样的都变得比较近，这样我们真正把一指数据相似度学好。我们可以把这个规模做的非常大，图片是可以做到滨临级的量级(注：由于是速记手稿，这里应该没写对)，我们其实没有学任何的语意或者图片内容，但是就是因为有了这样大量的数据我们就可以真正把图片里面的内容学出来，通过我们这样大量有标记数据把这个东西学出来了。
在机器学习里面到底我们学习多少数据，我们一个结论是需要很多数据的，即使我们仅仅是学一个二类分类器可能都需要成千上万的标注数据，这给了我们一个很大的挑战，怎么样解决这个挑战，在机器学习里面最近有一些新的动向，包括日志数据挖掘、众包、人机协同计算，我们希望能够很巧妙地设计出各式各样的机制，能够从用户那里得到大量高质量的数据。我们看到有了这样大规模高质量的数据我们真的可以用在互联网搜索或者图片标注上。其实这里面最主要解决的问题是什么？首先是我们要想一些非常巧妙的机制，比如说有
 ESP 等等机制，这个机制如果设置的很巧妙，用户就很容易参与到数据采集过程当中来。比如说 ESP 就是给用户提供游戏机制，让用户自愿参与进来，同时由于游戏设置非常巧妙，用户被诱导到提供数据当中来，他需要提供很好的标记才能得分，有这样一个巧妙的设计就能够促使用户提供大量高质量数据。怎么样找到很多的用户能够帮助我们去做这种数据描述工作是我们要想的。如果数据质量非常差让机器学的话，机器一定学习不好的，所以怎么样保证数据质量非常高同时对我们又有用，我们希望有一个非常巧妙的设计满足我们这样的条件，使得我们得到大量高质量的数据。有了这些数据以后要考虑怎么样构建积极学习方法处理大量数据，真正构建高性能的模型，这样的话才能够达到我们的目标，使得系统变得更加智能化。
最后感谢我现在的同事诺亚方舟实验室的**杨强**主任，很多方法是跟他讨论当中得到的，还有我们的很多实习生做的搜索工作，再次感谢大会组委会提供这样一个机会跟大家交流，谢谢。


