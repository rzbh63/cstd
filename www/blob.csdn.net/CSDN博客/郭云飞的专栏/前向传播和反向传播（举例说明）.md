
# 前向传播和反向传播（举例说明） - 郭云飞的专栏 - CSDN博客


2017年09月19日 16:25:06[guoyunfei20](https://me.csdn.net/guoyunfei20)阅读数：12076


假设神经网络结构如下图所示：有2个输入单元；隐含层为2个神经元；输出层也是2个神经元，隐含层和输出层各有1个偏置。
![](https://img-blog.csdn.net/20170919162604453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
为了直观，这里初始化权重和偏置量，得到如下效果：
![](https://img-blog.csdn.net/20170919163150250?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
**----前向传播----**
隐含层神经元**h1**的输入：
![](http://www.zhihu.com/equation?tex=net_%7Bh_%7B1%7D+%7D+%3Dw_%7B1%7D%5Cast+i_%7B1%7D++%2Bw_%7B2%7D%5Cast+i_%7B2%7D%2Bb_%7B1%7D%5Cast+1)
代入数据可得：
![](http://www.zhihu.com/equation?tex=net_%7Bh_%7B1%7D+%7D+%3D0.15%5Cast+0.05++%2B0.2%5Cast+0.1%2B0.35%5Cast+1%3D0.3775)
假设激励函数用**logistic**函数，计算得隐含层神经元**h1**的输出：
![](http://www.zhihu.com/equation?tex=out_%7Bh_%7B1%7D+%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bh_%7B1%7D+%7D+%7D+%7D++%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-0.3775%7D++%7D%3D0.593269992)
同样的方法，可以得到隐含层神经元**h2**的输出为：
![](http://www.zhihu.com/equation?tex=out_%7Bh_%7B2%7D+%7D%3D0.596884378)
对输出层神经元重复这个过程，使用隐藏层神经元的输出作为输入。这样输出层神经元**O***1*的输出为：
![](http://www.zhihu.com/equation?tex=net_%7Bo_%7B1%7D+%7D+%3Dw_%7B5%7D%5Cast+out_%7Bh_%7B1%7D+%7D++%2Bw_%7B6%7D%5Cast+out_%7Bh_%7B2%7D+%7D%2Bb_%7B2%7D%5Cast+1)
代入数据：
![](http://www.zhihu.com/equation?tex=net_%7Bo_%7B1%7D+%7D+%3D0.4%5Cast+0.593269992++%2B0.45%5Cast+0.596884378%2B0.6%3D1.105905967)
输出层神经元**O1**的输出：
![](http://www.zhihu.com/equation?tex=out_%7Bo_%7B1%7D+%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bo_%7B1%7D+%7D+%7D+%7D++%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-1.105905967%7D++%7D%3D0.75136507)
同样的方法，可以得到输出层神经元**O2**的输出为：
![](http://www.zhihu.com/equation?tex=out_%7Bo_%7B2%7D+%7D%3D0.772928465)
**----统计误差----**
假如误差公式为：
![](http://www.zhihu.com/equation?tex=E_%7Btotal%7D+%3D%5Csum_%7B%7D%5E%7B%7D%7B%5Cfrac%7B1%7D%7B2%7D%28target+-+output%29%5E%7B2%7D++%7D+)
如上图，**O1**的原始输出为0.01，而神经网络的输出为0.75136507，则其误差为：
![](http://www.zhihu.com/equation?tex=E_%7Bo_%7B1%7D+%7D+%3D%5Csum_%7B%7D%5E%7B%7D%7B%5Cfrac%7B1%7D%7B2%7D%280.01+-+0.75136507%29%5E%7B2%7D++%7D%3D0.298371109+)
同理可得，**O2**的误差为：
![](http://www.zhihu.com/equation?tex=E_%7Bo_%7B2%7D+%7D+%3D0.023560026)
这样，总的误差为：
![](http://www.zhihu.com/equation?tex=E_%7Btotal%7D+%3DE_%7Bo_%7B1%7D+%7D%2B++E_%7Bo_%7B2%7D+%7D%3D0.298371109)
**----反向传播----**
对于**w5**，想知道其改变对总误差有多少影响，得求偏导：
![](http://www.zhihu.com/equation?tex=%5Cfrac%7Bd+E_%7Btotal%7D+%7D%7Bd+w_%7B5%7D+%7D+)
根据链式法则：
![](http://www.zhihu.com/equation?tex=%5Cfrac%7Bd+E_%7Btotal%7D+%7D%7Bd+w_%7B5%7D+%7D%3D%5Cfrac%7Bd+E_%7Btotal%7D+%7D%7Bd+out_%7Bo_%7B1%7D+%7D+%7D%5Cast%5Cfrac%7Bd+out_%7Bo_%7B1%7D+%7D+%7D%7Bd+net_%7Bo_%7B1%7D+%7D+%7D%5Cast+%5Cfrac%7Bd+net_%7Bo_%7B1%7D+%7D+%7D%7Bd+w_%7B5%7D+%7D+++)
在这个过程中，需要弄清楚每一个部分。
首先：
![](https://pic2.zhimg.com/v2-5414eaa222162dfb50acd23cb3de78ed_b.png)
其次：
![](https://pic1.zhimg.com/v2-4b4f1d78ed0cf45eae5176e7d0183718_b.png)
最后：
![](https://pic3.zhimg.com/v2-c9e6e0d82026c81c894aaf8427387636_b.png)
把以上三部分相乘，得到：
![](https://pic4.zhimg.com/v2-efedc9a244680df235402145a9f99dfb_b.png)
根据梯度下降原理，从当前的权重减去这个值（假设学习率为0.5），得：
![](https://pic1.zhimg.com/v2-ddb478df679c272e70a1d040bc32ed7c_b.png)
同理，可以得到：
![](https://pic3.zhimg.com/v2-0d8aecf78d509a20b0ee8ed3dc04b7e2_b.png)
这样，输出层的所以权值就都更新了（先不管偏置），接下来看**隐含层**：
对**w1**求偏导：
![](https://img-blog.csdn.net/20170919165824421?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
用图表来描述上述链式法则求导的路径：
![](https://img-blog.csdn.net/20170919165942656?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
接下来，又是一部分一部分的计算：
>>>>>>>> 1
![](https://pic1.zhimg.com/v2-c741f67ca891f519be8717e137e71b58_b.png)
上式中，第一部分前边已经计算过了：
![](https://pic2.zhimg.com/v2-f25ef621f63f509f800b234a5bb84315_b.png)
第二部分中，因为：
![](https://pic2.zhimg.com/v2-05a9ae954225c0b6c790d3d32f5bd47d_b.png)
所以：
![](https://pic3.zhimg.com/v2-2c04f6b29317d490fd98e8ae5cfc04be_b.png)
两部分相乘，得：
![](https://pic2.zhimg.com/v2-a0d613775e47315d943f7b2561312a39_b.png)
>>>>>>>> 2
![](https://pic2.zhimg.com/v2-fc5e10242608a24810575e6093911b25_b.png)
>>>>>>>> 3
![](https://pic3.zhimg.com/v2-8132aab8bab44dfbc413ec85c1bfe15e_b.png)
![](https://pic2.zhimg.com/v2-447093c12b169f618ef5bfb1ff9332dd_b.png)
>>>>>>>> 4
![](https://pic3.zhimg.com/v2-93ae04497041672d768898e0efe9dfb6_b.png)
![](https://pic3.zhimg.com/v2-d2eb31f3c209c9734e3739a8c27658a6_b.png)
这样对**W1**的偏导就出来了：
![](https://pic1.zhimg.com/v2-b719d41429df7815c89bb6d60f39e2a0_b.png)
更新权值：
![](https://pic2.zhimg.com/v2-cdd296204d065efcff88fb84cacbadad_b.png)
同理得到：
![](https://pic1.zhimg.com/v2-83782c29f468095dc2001afde9c61558_b.png)
最后，更新了所有的权重！ 当最初前馈传播时输入为0.05和0.1，网络上的误差是0.298371109。 在第一轮反向传播之后，总误差现在下降到0.291027924。 它可能看起来不太多，但是在重复此过程10,000次之后。例如，错误倾斜到0.000035085。
在这一点上，当前馈输入为0.05和0.1时，两个输出神经元产生0.015912196（相对于目标为0.01）和0.984065734（相对于目标为0.99）。





