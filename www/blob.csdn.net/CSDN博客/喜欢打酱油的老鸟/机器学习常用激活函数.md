
# 机器学习常用激活函数 - 喜欢打酱油的老鸟 - CSDN博客


2018年09月12日 14:28:55[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：361


**机器学习常用激活函数**
**摘要：**激活函数就是神经网络输出端的一个节点，来连接两个神经网络。本文主要介绍激活函数的功能以及激活函数类型。
**什么是激活函数？**
激活函数就是神经网络输出端的一个节点，来连接两个神经网络。
**为什么要在神经网络中使用激活函数**
激活函数用来确定神经网络的输入，比如“是”或“否”，将结果映射为[0，1]或[-1，1]之间，根据函数的曲线，可分为两种类型：线性激活函数、非线性激活函数。
**1.****线性激活函数（恒等激活函数）**
如下图所示，函数是线性的，函数的输出范围为（-∞，+∞）。
![](https://img-blog.csdn.net/20180912142612682?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
线性激活函数
方程式：*f(x) = x*
取值范围：（-∞，+∞）
**2.****非线性激活函数**
非线性激活函数是最常用的激活函数，其曲线如下图所示：
![](https://img-blog.csdn.net/20180912142625951?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
非线性激活函数
使用非线性激活函数，模型可以更容易进行自我调整，并区分不同的输出。非线性激活函数中的主要术语有：
1.导数或微分：y轴随x轴的变化，称为斜率。
2.单调函数：完全递增或完全递减的函数。
根据取值范围，非线性激活函数可分为以下几种：Sigmoid激活函数、Tanh激活函数、ReLU激活函数、
**1.Sigmoid****激活函数（Logistic激活函数）**
Sigmoid激活函数的曲线呈“S”形。
![](https://img-blog.csdn.net/2018091214264078?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
Sigmoid激活函数
**sigmoid****函数很受大众的欢迎，其主要原因是：它的输出处于[0,1]范围内，特别适用于输出概率的模型。**由于任何概率的取值在0和1范围之间，因此，sigmoid激活函数是最好的选择。
该函数是可微的，也就是说，我们可以得到“S”曲线上任意两点之间的斜率。这个函数是单调的，但是其导数不是，sigmoid 激活函数可能会导致神经网络在训练的时候卡住。
Softmax函数是一种更通用的逻辑激活函数，用于多类分类。
**2.Tanh****激活函数**
tanh激活函数和sigmoid激活函数类似，但是要比sigmoid激活函数好。tanh激活函数的取值范围是（-1,1），曲线也呈“S”形。
![](https://img-blog.csdn.net/20180912142655814?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
sigmoid激活函数和tanh激活函数
tanh激活函数的优点在于，如果输入为负数，则输出也为负数，输入为0，则输出也近似为0。
该函数是可微分、单调的，但其导数不单调。tanh激活函数主要用于分类。
tanh和sigmoid激活函数都可用于前馈网络。
**3.ReLU****激活函数(Rectified Linear Unit)**
在神经网络中，使用最多的激活函数是ReLU激活函数，它几乎可用于所有卷积神经网络或深度学习中。
![](https://img-blog.csdn.net/20180912142712865?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
ReLU激活函数和Sigmoid激活函数
如上图所示，在ReLU激活函数中，当z<0时，f(z)=0；当z>0时，f(z)=z。取值范围为[0,+∞]
ReLU激活函数及其导数都是单调的。
但这存在一个问题：当输入为负值时，输出立刻变为0，这就降低了模型拟合或训练数据的能力。反过来说，为了不影响结果，就不能映射负值输入。

**4. Leaky ReLU****激活函数**
Leaky ReLU激活函数的出现，试图解决ReLU激活函数中出现的死亡问题。
![](https://img-blog.csdn.net/20180912142732734?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
ReLU激活函数和Leaky ReLU激活函数
Leaky ReLU激活函数扩大了ReLU激活函数的取值范围，如上图所示，通常，a的值为0.01左右。取值范围：（-∞，+∞）。
当a不是0.01时，该函数称为Randomized ReLU。
本质上来说，Leaky  ReLU函数和Randomized ReLU函数都是单调的。 而且，它们的导数也单调。
**为什么要使用导数和微分？**
在更新曲线时，我们要知道哪个方向上会发生变化，或者是根据斜率来更新曲线。这就是我们要在机器学习和深度学习的每个部分都使用微分的原因。
![](https://img-blog.csdn.net/20180912142755881?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
激活函数汇总
![](https://img-blog.csdn.net/2018091214281083?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
激活函数的导数曲线图汇总



