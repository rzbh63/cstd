
# 朴素贝叶斯分类（Nave Bayes） - 郭云飞的专栏 - CSDN博客


2017年12月27日 14:18:07[guoyunfei20](https://me.csdn.net/guoyunfei20)阅读数：11751


**一、条件概率（Conditional probability）**
条件概率是指事件A在另外一个事件B已经发生条件下的发生概率，记P（A|B）。举例说明：
![](https://img-blog.csdn.net/20171227115401397)
上图左边为一个布袋，里边装有5个球，其中2个蓝色球，3个红色球。每次随机从布袋里拿一颗球（不放回），求连续2次拿到篮球的概率是多少？
非常浅显的一点是——第一次拿球和第二次拿球是相关事件（是有关系的，即不是相互独立的事件）。第一次随机拿一颗球，拿到篮球的可能性为2/5。但拿掉一颗球后情形就不同啦。如果第一次拿的是红球，则第二次拿蓝球的可能性为2/4。如果第一次拿的是篮球，则第二次拿篮球的可能性为1/4。这从上图能清楚的看出来。
利用决策树来表示这个过程如下：
![](https://img-blog.csdn.net/20171227122542399)
现在我们可以尝试解答像这样的问题了：“拿到2颗蓝球的概率是多少？"，看下图：
![](https://img-blog.csdn.net/20171227123020882)
上图中，拿到2个篮球的概率即P（AB），事件A为第一次拿到篮球的概率，这里为P（A）= 2/5；事件B为第二次拿到篮球的概率，这里为在第一次拿到篮球的条件下，第二次拿到篮球的条件概率P（B|A）。
根据上边等式，可以推导出条件概率的求解公式为：
![](https://img-blog.csdn.net/20171227124251011)
![](https://img-blog.csdn.net/20171227124305299)

**二、贝叶斯定理**
贝叶斯定理是关于随机事件A和B的条件概率（或边缘概率）的一则定理。下面不加证明地直接给出贝叶斯定理：
![](https://img-blog.csdn.net/20171227135019368)
贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。

**三、朴素贝叶斯分类**
朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。
![](https://img-blog.csdn.net/20171227140824671)
那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：
![](https://img-blog.csdn.net/20171227141813067)
根据上述分析，朴素贝叶斯分类的流程可以由下图表示：
![](https://img-blog.csdn.net/20171227142307769)
可以看到，整个朴素贝叶斯分类分为三个阶段：

准备工作阶段，任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。
分类器训练阶段。这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。
应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。
估计类别下特征属性划分的条件概率及Laplace校准：
由上文看出，计算各个划分的条件概率**P(a|y)**是朴素贝叶斯分类的关键性步骤，当特征属性为离散值时，只要很方便的统计训练样本中各个划分在每个类别中出现的频率即可用来估计P(a|y)，下面重点讨论特征属性是连续值的情况。
当特征属性为连续值时，通常假定其值服从高斯分布（也称正态分布）。即：
![](https://img-blog.csdn.net/20171227162420975)
因此只要计算出训练样本中各个类别中此特征项划分的各均值和标准差，代入上述公式即可得到需要的估计值。
另一个需要讨论的问题就是当P(a|y)=0怎么办，当某个类别下某个特征项划分没有出现时，就是产生这种现象，这会令分类器质量大大降低。为了解决这个问题，我们引入Laplace校准，它的思想非常简单，就是对没类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。
**四、举例**
下面讨论一个使用朴素贝叶斯分类解决实际问题的例子，为了简单起见，对例子中的数据做了适当的简化。这个问题是这样的，对于SNS社区来说，不真实账号（使用虚假身份或用户的小号）是一个普遍存在的问题，作为SNS社区的运营商，希望可以检测出这些不真实账号，从而在一些运营分析报告中避免这些账号的干扰，亦可以加强对SNS社区的了解与监管。
如果通过纯人工检测，需要耗费大量的人力，效率也十分低下，如能引入自动检测机制，必将大大提升工作效率。这个问题说白了，就是要将社区中所有账号在真实账号和不真实账号两个类别上进行分类，下面我们一步一步实现这个过程。
首先，设：
C = 0 表示真实账号
C = 1 表示不真实账号
1、确定特征属性及划分
这一步要找出可以帮助我们区分真实账号与不真实账号的特征属性，在实际应用中，特征属性的数量是很多的，划分也会比较细致，但这里为了简单起见，我们用少量的特征属性以及较粗的划分，并对数据做了修改。
我们选择三个特征属性：
a1：日志数量 / 注册天数（比例）
a2：好友数量 / 注册天数（比例）
a3：是否使用真实头像。
在SNS社区中这三项都是可以直接从数据库里得到或计算出来的。由于特征a1、a2都是连续变量，我们进行如下划分：
a1：{a<=0.05,  0.05<a<0.20, a>=0.20}
a2：{a<=0.10,  0.10<a<0.80, a>=0.80}
a3：{a=0（不是真是头像）,a=1（是真是头像）}
2、获取训练样本
这里使用运维人员曾经人工检测过的1万个账号作为训练样本。
3、训练
计算训练样本中每个类别的频率，用训练样本中真实账号和不真实账号数量分别除以一万，得到：
![](https://img-blog.csdn.net/20171227153837819)
计算每个类别条件下各个特征属性划分的频率：
![](https://img-blog.csdn.net/20171227154748547)
所谓的训练就是计算样本集中各特征取值的条件概率的值。
4、预测
下面我们使用上面训练得到的分类器鉴别一个账号，这个账号使用非真实头像，日志数量与注册天数的比率为0.1，好友数与注册天数的比率为0.2。即：

a1 = 0.1
a2 = 0.2
a3 = 0
![](https://img-blog.csdn.net/20171227161556589)
可以看到，虽然这个用户没有使用真实头像，但是通过分类器的鉴别，更倾向于将此账号归入C = 0，即真实账号类别。这个例子也展示了当特征属性充分多时，朴素贝叶斯分类对个别属性的抗干扰性。





















