# 自然语言处理-BM25相关度打分

#  

(注：文中大写Query、Document等代表集合，小写query、document等代表集合中的个体)

## 一、优缺点

**适用于**：在文档包含查询词的情况下，或者说查询词精确命中文档的前提下，如何计算相似度，如何对内容进行排序。

**不适用于：**基于传统检索模型的方法会存在一个固有缺陷，就是检索模型只能处理 Query 与 Document 有重合词的情况，传统检索模型无法处理词语的语义相关性。

**白话举例：**提出一个**query**：当下最火的女网红是谁？ 
在**Document集合**中**document1**的内容为：[当下最火的男明星为鹿晗]； 
**document2**的内容为：[女网红能火的只是一小部分]。 
显然document1和document2中都包含[火]、[当下]、[网红]等词语。但是**document3**的内容可能是：[如今最众所周知的网络女主播是周二柯]。很显然与当前Query能最好匹配的应该是document3，可是document3中却没有一个词是与query中的词相同的（即上文所说的没有“精确命中”），此时就无法应用BM25检索模型。

## 二、算法核心

BM25算法是一种常见用来做相关度打分的公式，思路比较简单，主要就是计算一个query里面所有词q1,q2...qnq1,q2...qn和文档的相关度，然后再把分数做累加操作。公式如下： 
$$
Score(Q,d)=\sum_i^n{W_i}\cdot{R(q_i,d)}
$$
其中$R(q_i,d)$是查询语句query中每个词$q_i$和文档d的相关度值，$W_i$是该词的权重，最后将所有词的$W_i*R(q_i,d)$相加。

$W_i$一般情况下为IDF(InverseDocumentFrequency)值，即逆向文档频率，公式如下： 
$$
IDF(q_i)=log\frac{N+0.5}{n(q_i)+0.5}
$$
其中N是文档总数，$n(q_i)$是包含该词的文档数，0.5是调教系数，避免$n(q_i)=0$的情况。

log函数是为了让IDF的值受N和$n(q_i)$的影响更加平滑。

 从公式中显然能看出IDF值的含义：即总文档数越大，包含词$q_i$的文档数越小，则$q_i$的IDF值越大。

 

**白话举例就是：**比如我们有1万篇文档，而单词basketball,Kobe Bryant几乎只在和体育运动有关的文档中出现，说明这两个词的IDF值比较大，而单词is, are, what几乎在所有文档中都有出现，那么这几个单词的IDF值就非常小。



解决了$W_i$，现在再来解决$R(q_i,d)$。$R(q_i,d)$公式如下： 
$$
R(q_i,d)=\frac{{f_i}\cdot{(k_1+1)}}{f_i+K}\cdot{\frac{{qf_i}\cdot{(k_2+1)}}{qf_i+k_2}}
$$
其中k1,k2,b都是调节因子，一般k1=1,k2=1,b=0.75。



式中$qf_i$为词$q_i$在查询语句query中的出现频率，$f_i$为$q_i$在文档d中的出现频率。由于绝大多数情况下一条简短的查询语句query中，词$q_i$只会出现一次，即$qf_i=1$，因此公式可化简为：
$$
R(q_i,d)=\frac{{f_i}\cdot{(k_1+1)}}{f_i+K}
$$
其中
$$
K={k_1}\cdot{(1-b+b\cdot{\frac{dl}{avgdl}})}
$$
dl为文档d的长度，avgdl为所有文档的平均长度。意即该文档d的长度和平均长度比越大，则K越大，则相关度R(qi,d)越小,b为调节因子，b越大，则文档长度所占的影响因素越大，反之则越小。

 

**白话举例就是：**一个query为：诸葛亮在哪里去世的？

 document1的内容为：诸葛亮在五丈原积劳成疾，最终去世；

 document2的内容为：司马懿与诸葛亮多次在五丈原交锋；

 而document3为一整本中国历史书的内容。

 

显然document3中肯定包含了大量[诸葛亮]、[哪里]、[去世]这些词语，可是由于document3文档长度太大，所以

K非常大，所以和query中每个词$q_i$的相关度$R(q_i,d)$非常小。

综上所述，可将BM25相关度打分算法的公式整理为： 
$$
Score(Q,d)=\sum_i^nIDF(q_i)\cdot\frac{{f_i}\cdot{(k_1+1)}}{f_i+k_1\cdot{(1-b+b\cdot{\frac{dl}{avgdl}})}}
$$




```python
import math
import jieba
from utils import utils
 
# 测试文本
text = '''
自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。
它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。
自然语言处理是一门融语言学、计算机科学、数学于一体的科学。
因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，
所以它与语言学的研究有着密切的联系，但又有重要的区别。
自然语言处理并不是一般地研究自然语言，
而在于研制能有效地实现自然语言通信的计算机系统，
特别是其中的软件系统。因而它是计算机科学的一部分。
'''
 
class BM25(object):
 
    def __init__(self, docs):
        self.D = len(docs)
        self.avgdl = sum([len(doc)+0.0 for doc in docs]) / self.D
        self.docs = docs
        self.f = []  # 列表的每一个元素是一个dict，dict存储着一个文档中每个词的出现次数
        self.df = {} # 存储每个词及出现了该词的文档数量
        self.idf = {} # 存储每个词的idf值
        self.k1 = 1.5
        self.b = 0.75
        self.init()
 
    def init(self):
        for doc in self.docs:
            tmp = {}
            for word in doc:
                tmp[word] = tmp.get(word, 0) + 1  # 存储每个文档中每个词的出现次数
            self.f.append(tmp)
            for k in tmp.keys():
                self.df[k] = self.df.get(k, 0) + 1
        for k, v in self.df.items():
            self.idf[k] = math.log(self.D-v+0.5)-math.log(v+0.5)
 
    def sim(self, doc, index):
        score = 0
        for word in doc:
            if word not in self.f[index]:
                continue
            d = len(self.docs[index])
            score += (self.idf[word]*self.f[index][word]*(self.k1+1)
                      / (self.f[index][word]+self.k1*(1-self.b+self.b*d
                                                      / self.avgdl)))
        return score
 
    def simall(self, doc):
        scores = []
        for index in range(self.D):
            score = self.sim(doc, index)
            scores.append(score)
        return scores
 
if __name__ == '__main__':
    sents = utils.get_sentences(text)
    doc = []
    for sent in sents:
        words = list(jieba.cut(sent))
        words = utils.filter_stop(words)
        doc.append(words)
    print(doc)
    s = BM25(doc)
    print(s.f)
    print(s.idf)
    print(s.simall(['自然语言', '计算机科学', '领域', '人工智能', '领域']))
```

