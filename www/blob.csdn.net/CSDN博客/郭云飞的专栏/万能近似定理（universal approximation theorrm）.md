
# 万能近似定理（universal approximation theorrm） - 郭云飞的专栏 - CSDN博客


2017年10月19日 19:07:17[guoyunfei20](https://me.csdn.net/guoyunfei20)阅读数：6506个人分类：[深度学习基础理论																](https://blog.csdn.net/guoyunfei20/article/category/7223550)


神经网络的**架构**（**architecture**）指网络的整体结构。大多数神经网络被组织成称为**层**的单元组，然后将这些层布置成链式结构，其中每一层都是前一层的函数。在这种结构中，第一层由下式给出：
![](https://img-blog.csdn.net/20171019191009681?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
第二层：
![](https://img-blog.csdn.net/20171019191100925?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
第三层，以此类推！
可以看出，每一层的主体都是线性模型。线性模型，通过矩阵乘法将特征映射到输出，顾名思义，仅能表示线性函数。它具有易于训练的优点，因为当使用线性模型时，许多损失函数会导出**凸优化**问题。不幸的是，我们经常希望我们的系统学习非线性函数。
乍一看，我们可能认为学习非线性函数需要为我们想要学习的那种非线性专门设计一类模型族。幸运的是，具有隐藏层的前馈网络提供了一种万能近似框架。
具体来说，**万能近似定理（universal approximation theorem）(Hornik et al., 1989;Cybenko, 1989)**表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何**从一个有限维空间到另一个有限维空间**的Borel 可测函数。
万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的MLP 一定能够表示这个函数。然而，我们不能保证训练算法能够学得这个函数。即使MLP能够表示该函数，学习也可能因两个不同的原因而失败。
用于训练的优化算法可能找不到用于期望函数的参数值。
训练算法可能由于过拟合而选择了错误的函数。
根据‘**‘没有免费的午餐’’ 定理**，说明了没有普遍优越的机器学习算法。前馈网络提供了表示函数的万能系统，在这种意义上，给定一个函数，存在一个前馈网络能够近似该函数。但不存在万能的过程既能够验证训练集上的特殊样本，又能够选择一个函数来扩展到训练集上没有的点。
总之，具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。




