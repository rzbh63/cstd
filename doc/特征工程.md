# 特征工程

目录
1 特征工程是什么？

2 特征清洗

3 数据预处理 
　　3.1 无量纲化 
　　　　3.1.1 标准化 
　　　　3.1.2 归一化（区间缩放） 
　　　　3.1.3 正则化

　　3.2 对定量特征二值化 
　　3.3 对定性特征哑编码 
　　3.4 缺失值计算 
　　3.5 生成多项式特征和自定义转换 
4 特征选择 
　　4.1 Filter 
　　　　4.1.1 方差选择法 
　　　　4.1.2 相关系数法 
　　　　4.1.3 卡方检验 
　　　　4.1.4 互信息法 
　　4.2 Wrapper 
　　　　4.2.1 递归特征消除法 
　　4.3 Embedded 
　　　　4.3.1 基于惩罚项的特征选择法 
　　　　4.3.2 基于树模型的特征选择法 
5 降维 
　　5.1 主成分分析法（PCA）

 5.2 线性判别分析法（LDA）

1 关于特征工程：
在数据预处理之前，先来了解下特征工程，特征工程指的是在原始数据之中提取，构造，选择数据特征的过程。有句话是：“数据和特征工程决定了你能到达的上限，机器学习模型决定了你能多么逼近这个上限”。特征工程是数据分析过程中最重要的一步，当然也很能简单的理解，毕竟特征工程是建模步骤的基础和准备。

特征工程详细分解为特征使用、特征获取、特征处理、特征监控。

特征使用是指基于业务理解，尽可能找出对因变量有影响的所有自变量（特征）。找出特征后需要评价其特征的获取难度、覆盖率和准确率。

特征获取是指如何获取这些特征、如何存储这些特征。

接下来是特征处理，特征处理分为特征清洗、对单个特征的预处理、对多个特征的预处理和衍生变量。

最后是特征监控，指验证特征的有效性分析，防止特征质量下降、影响模型效果。

2 特征清洗
3 数据预处理（数据变换）
通过特征提取，得到未经处理的特征存在以下问题：

不属于同一量纲：即特征的单位不同，不能相互比较。

信息冗余：这里主要争对定量特征，我们只关心某变量属于哪一类区间而不是具体等于具体值，这个时候需要对其值进行区间划分，比如二值化。

定性特征不能直接使用：对于某些模型不能直接使用定性特征，通常使用哑编码（又叫独热编码）的方式将定性特征转化为定量特征，假设某个定类特征有N个定类值，则处理方式是原始特征值为第i种定性值时，将第i个扩展特征赋值为1，其余扩展特征赋值为0。这样原来一个特征从形式上来看就被扩展为N个特征，这种方式不用增加调参的工作。

存在缺失值：要么剔除要么填充。

信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。

我们使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。

3.1 无量纲化
无量纲化使不同量纲的数据转换到同一标准上。目前数据标准化方法有多种，归结起来可以分为直线型方法(如极值法、标准差法)、折线型方法(如三折线法)、曲线型方法(如半正态性分布)。不同的标准化方法，对系统的评价结果会产生不同的影响，然而不幸的是，在数据标准化方法的选择上，还没有通用的法则可以遵循。

常见的无量纲化方法有：min-max标准化（Min-max normalization），log函数转换，atan函数转换，z-score标准化（zero-mena normalization，此方法最为常用），模糊量化法。本文只介绍min-max法，z-score法（标准化），正则化。

3.1.1 标准化

标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。标准化需要计算特征的均值和标准差，对特征进行以下处理：
x′=x−X¯¯¯¯S
x′=x−X¯S
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
iris = load_iris()
# 载入鸢尾花数据
a = StandardScaler()
# a选择StandardScaler作为数据的estimator.
b = a.fit_transform(iris.data)
# iris.data 为特征矩阵 iris.target为目标向量
# fit_transform为拟合并转换数据
print(b)
1
2
3
4
5
6
7
8
9
10
3.1.2 归一化（区间缩放）

区间缩放利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。区间缩放需要找特征中的最大最小值， 
x′=x−MinMax−Min
x′=x−MinMax−Min
from sklearn.preprocessing import MinMaxScaler
#区间缩放，返回值为缩放到[0, 1]区间的数
a = MinMaxScaler()
b = a.fit_transform(iris.data)
print(b)
1
2
3
4
5
3.1.3 正则化（还没有想到实际的数据转化应用）

标准化和正则化的区别，简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。正则化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。（其他相似解释：正则化数据和之前的标准化数据不同，之前标准化数据是针对特征来说的，而现在正则化是对样本来做的，是用样本数据除以他的范式）

Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数（l1-norm,l2-norm）等于1。在sklearn中有三种正则化方法，l1范数、l2范数、max范数。

 p-范数的计算公式：
||X||p=(|x1|p+|x2|p+...+|xn|p)1/p
||X||p=(|x1|p+|x2|p+...+|xn|p)1/p
该方法主要应用于文本分类和聚类中。例如，对于两个TF-IDF向量的l2-norm进行点积，就可以得到这两个向量的余弦相似性。

规则为l2（2-范数）的正则化公式如下： 
x′=xi∑x2i−−−−√
x′=xi∑xi2
from sklearn.preprocessing import Normalizer 
#正则化，返回值为正则化后的数据
Normalizer(norm='l2').fit_transform(iris.data)
1
2
3
其实这里的翻译有点儿问题，正确的应该是normalization(归一化)，regurlarization(正则化)，但是这里的normalizer方法是正则化方法。

3.2 对定量特征二值化
定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下： 
x′′={10当x≥threshold时;当x<threshold时.
x"={1当x≥threshold时;0当x<threshold时.
from sklearn.preprocessing import Binarizer
#二值化，阈值设置为3，返回值为二值化后的数据
Binarizer(threshold=3).fit_transform(iris.data)
1
2
3
3.3 对定性特征哑编码
除了使用sklearn中的OneHotEncoder类得到哑特征，还推荐使用pandas中的 
get_dummies方法来创建哑特征，get_dummies默认会对DataFrame中所有字符串类型的列进行独热编码，使用preproccessing库的OneHotEncoder类对数据进行哑编码的代码如下：

from sklearn.preprocessing import OneHotEncoder
#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据
OneHotEncoder().fit_transform(iris.target.reshape((-1,1)))
1
2
3
3.4 缺失值计算
对值缺失严重的可以采用dropna方法直接消除该特征或样本。

而不想丢失数据信息，保留那些非缺失值数据的同时处理缺失值，要进行处理的数据集中包含缺失值一般步骤如下：

1、使用字符串’nan’来代替数据集中的缺失值；

2、将该数据集转换为浮点型便可以得到包含np.nan的数据集；

3、使用sklearn.preprocessing.Imputer类来处理使用np.nan对缺失值进行编码过的数据集。

代码如下：

import numpy as np
from sklearn.preprocessing import Imputer
imp=Imputer(missing_values='NaN',strategy='mean',axis=0)
x=np.array([[1, 2],
           [np.nan, 3],
           [7, 6]])
imp.fit(x)
print(x)
print(imp.transform(x))
1
2
3
4
5
6
7
8
9
fit方法用于从训练集中学习模型参数，transform用学习到的参数转换数据。关于fit函数和Transfrom函数的具体作用参考：

http://blog.csdn.net/wang1127248268/article/details/53264041

3.5 生成多项式特征和自定义转换
常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下：



使用preproccessing库的PolynomialFeatures类对数据进行多项式转换的代码如下：

from sklearn.preprocessing import PolynomialFeatures
#多项式转换
#参数degree为度，默认值为2
PolynomialFeatures().fit_transform(iris.data)
1
2
3
4
基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下：

from numpy import log1p
from sklearn.preprocessing import FunctionTransformer
#自定义转换函数为对数函数的数据变换
#第一个参数是单变元函数
FunctionTransformer(log1p).fit_transform(iris.data)
1
2
3
4
5
4 特征选择
当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：

特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。

特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。

根据特征选择的形式又可以将特征选择方法分为3种：

Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。

Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。

Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

我们使用sklearn中的feature_selection库来进行特征选择。

4.1 Filter
4.1.1 方差选择法

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：

from sklearn.feature_selection import VarianceThreshold
#方差选择法，返回值为特征选择后的数据
#参数threshold为方差的阈值
VarianceThreshold(threshold=3).fit_transform(iris.data)
1
2
3
4
单变量特征选择的原理是分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标。

对于分类问题(y离散)，可采用： 
　　卡方检验，*f_classif, mutual_info_classif，互信息 
对于回归问题(y连续)，可采用： 
　　皮尔森相关系数，*f_regression, mutual_info_regression，最大信息系数

4.1.2 相关系数法

使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：

from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr
#选择K个最好的特征，返回选择特征后的数据
#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
#参数k为选择的特征个数
SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T,k=2).fit_transform(iris.data, iris.target)
1
2
3
4
5
6
4.1.3 卡方检验

卡方检验的样本量要求：卡方分布本身是连续型分布，但是在分类资料的统计分析中，显然频数只能以整数形式出现，因此计算出的统计量是非连续的。只有当样本量比较充足时，才可以忽略两者问的差异，否则将可能导致较大的偏差具体而言，一般认为对于卡方检验中的每一个单元格，要求其最小期望频数均大于1，且至少有4／5的单元格期望频数大于5，此时使用卡方分布计算出的概率值才是准确的。如果数据不符合要求，可以采用确切概率法进行概率的计算。经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有NN种取值，因变量有MM种取值，考虑自变量等于ii且因变量等于jj的样本频数的观察值与期望的差距，构建统计量： 
χ2=∑(A−E)2E
χ2=∑(A−E)2E

这个统计量的含义简而言之就是自变量对因变量的相关性。(χ2=χ2i+χ2jχ2=χi2+χj2)用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
#选择K个最好的特征，返回选择特征后的数据
SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
1
2
3
4
4.1.4 互信息法

互信息指的是两个随机变量之间的关联程度，即给定一个随机变量后，另一个随机变量不确定性的削弱程度，因而互信息取值最小为0，意味着给定一个随机变量对确定一另一个随机变量没有关系，最大取值为随机变量的熵，意味着给定一个随机变量，能完全消除另一个随机变量的不确定性。通俗理解为：原来我对XX有些不确定(不确定性为H(X))H(X))，告诉我YY后我对XX不确定性变为H(X|Y)H(X|Y), 这个不确定性的减少量就是X,YX,Y之间的互信息I(X;Y)=H(X)−H(X|Y)I(X;Y)=H(X)−H(X|Y)。

根据熵农的信息理论，熵（entropy）是随机变量不确定性的度量，一个离散随机变量XX，其可能取值集合记为SxSx,则XX的熵定义为

H(X)=−∑x属于Sxp(x)logp(x)
H(X)=−∑x属于Sxp(x)logp(x)
当变量YY已知，变量XX中剩下的不确定性用条件熵来度量：

H(X|Y)=∑x属于Sxp(y)H(X|Y=y)=−∑x属于Sxp(y)∑y属于Syp(x|y)logp(x|y)=−∑x属于Sx∑y属于Syp(x,y)logp(x|y)
H(X|Y)=∑x属于Sxp(y)H(X|Y=y)=−∑x属于Sxp(y)∑y属于Syp(x|y)logp(x|y)=−∑x属于Sx∑y属于Syp(x,y)logp(x|y)
注意：
这个条件熵，不是指在给定某个数（某个变量为某个值）的情况下，另一个变量的熵是多少，变量的不确定性是多少？而是期望！因为条件熵中X也是一个变量，意思是在一个变量X的条件下（变量X的每个值都会取），另一个变量Y熵对X的期望。这是最容易错的！

条件熵与熵的关系：

H(X|Y)=H(XY)−H(Y)
H(X|Y)=H(XY)−H(Y)
两个随机变量X和Y的统计依存关系用互信息来度量：

I(X;Y)=−∑x属于Sx∑y属于Syp(x,y)logp(x,y)p(x)p(y)
I(X;Y)=−∑x属于Sx∑y属于Syp(x,y)logp(x,y)p(x)p(y)
如果两个随机变量的互信息较大，则这两个随机变量相关性较大，互信息和熵有如下关系：

I(X;Y)=H(X)−H(X|Y)=H(Y)−H(Y|X)=H(X)+H(Y)−H(XY)
I(X;Y)=H(X)−H(X|Y)=H(Y)−H(Y|X)=H(X)+H(Y)−H(XY)
为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下：

from sklearn.feature_selection import SelectKBest
from minepy import MINE
#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
def mic(x, y):
    m = MINE()
    m.compute_score(x, y)
    return (m.mic(), 0.5)
#选择K个最好的特征，返回特征选择后的数据
SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
1
2
3
4
5
6
7
8
9
互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 minepy 提供了MIC功能。

4.2 Wrapper
Wrapper方法寻找所有特征子集中能使后续学习算法达到较高性能的子集，在特征选择阶段，wrapper可以看做：搜索方法+学习算法。通俗理解就是Wrapper方法将特征集的选择视为一个搜索问题，会先准备若干种特征的组合方案，然后评估，相互比较。评估所使用的标准通常是模型的准确率。

4.2.1 递归特征消除法

递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，再基于新的特征集进行下一轮训练。

sklearn官方解释：对特征含有权重的预测模型(例如，线性模型对应参数coefficients)，RFE通过递归减少考察的特征集规模来选择特征。首先，预测模型在原始特征上训练，每个特征指定一个权重。之后，那些拥有最小绝对值权重的特征被踢出特征集。如此往复递归，直至剩余的特征数量达到所需的特征数量。

RFECV 通过交叉验证的方式执行RFE，以此来选择最佳数量的特征：对于一个数量为d的feature的集合，他的所有的子集的个数是2的d次方减1(包含空集)。指定一个外部的学习算法，比如SVM之类的。通过该算法计算所有子集的validation error。选择error最小的那个子集作为所挑选的特征。

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
1
2
3
4
5
6
使用LogisticRegression作为基模型主要是能直接得到特征的权重。

4.3 Embedded
4.3.1 基于惩罚项的特征选择法

使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
#带L1惩罚项的逻辑回归作为基模型的特征选择
SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target)
1
2
3
4
通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验；具体方法参照：http://www.cnblogs.com/jasonfreak/p/5448385.html

关于正则化能起到特征选择和防止过拟合的作用公式推导：http://blog.csdn.net/u012162613/article/details/44261657

关于正则化的的解释可参考：https://www.zhihu.com/question/20924039

4.3.2 基于树模型的特征选择法

树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import GradientBoostingClassifier
#GBDT作为基模型的特征选择
SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)
1
2
3
4
关于GBDT算法的原理参考：https://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html

http://www.cnblogs.com/LeftNotEasy/archive/2011/01/02/machine-learning-boosting-and-gradient-boosting.html

5 降维
当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。【LDA是给定了已分类的数据，然后在该数据下训练出判别函数（该判别函数就是降维功能），可以用该函数得到新数据集的分类，而PCA是直接对一个没有分类的数据集进行降维。】

PCA与LDA有着非常近似的意思，LDA的输入数据是带标签的，而PCA的输入数据是不带标签的，所以PCA是一种unsupervised learning。LDA通常来说是作为一个独立的算法存在，给定了训练数据后，将会得到一系列的判别函数（discriminate function），之后对于新的输入，就可以进行预测（分类）了。而PCA更像是一个预处理的方法，它可以将原本的数据降低维度（PCA分类能力很差），而使得降低了维度的数据之间的方差最大。



协方差矩阵对角线上就是各个特征的方差，求其特征值矩阵D（D的值从大到小排列），那么取前K个特征值对应的特征就是方差较大的特征。符合PCA的投影后特征间的方差最大化的原则。

这里解释了为什么协方差矩阵C能代替R作为变换矩阵, R=UAUTR=UAUT, A中的元素(R的特征值)为D中元素开根号。（这里举例的R矩阵为对称正定阵）：https://www.zhihu.com/question/36348219

当R为m∗nm∗n的非对称矩阵时，D开根号仍然为R的奇异值（特征值），只是奇异值矩阵剩下的部分补零，R=UAVTR=UAVT。如还需进一步理解参考邹博线性代数那章节关于SVD的PPT。

5.1 主成分分析法（PCA）

使用decomposition库的PCA类选择特征的代码如下：

from sklearn.decomposition import PCA
#主成分分析法，返回降维后的数据
#参数n_components为主成分数目
PCA(n_components=2).fit_transform(iris.data)
1
2
3
4
5.2 线性判别分析法（LDA）

使用lda库的LDA类选择特征的代码如下：

from sklearn.lda import LDA
#线性判别分析法，返回降维后的数据
#参数n_components为降维后的维数
LDA(n_components=2).fit_transform(iris.data, iris.target)
1
2
3
4
总结
本文除了知道数据的预处理方法和特征选择算法外，还学习到了关于fit、transform、fit_transform函数的区别、关于特征值的理解对降维的重要性、一些机器学习算法的基本原理（比如熵、互信息、正则化）、还有后面要学习的机器学习算法运用到特征选择中（比如逻辑回归、决策树、随机森林、GBDT等）。
--------------------- 
作者：Pylady 
来源：CSDN 
原文：https://blog.csdn.net/Pylady/article/details/78855636 
版权声明：本文为博主原创文章，转载请附上博文链接！