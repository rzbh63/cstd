# 人脸活体检测

[![LucasX](https://pic1.zhimg.com/v2-83161661800852d63461b457f2c1de37_xs.jpg)](https://www.zhihu.com/people/xulu-0620)

[LucasX](https://www.zhihu.com/people/xulu-0620)

ML/CV/DS

30 人赞同了该文章

## **介绍**

随着人脸识别、人脸解锁等技术在金融、门禁、移动设备等日常生活中的广泛应用，人脸防伪/活体检测（Face Anti-Spoofing）技术在近年来得到了越来越多的关注。设想一下，假设你的Face Verification算法做的再漂亮，而Face Anti-Spoofing做的很烂，如果这个时候恰恰有某位同学拿着马云脸的视频去刷了支付宝，那............



![img](https://pic3.zhimg.com/80/v2-2381e0d4da7dd143044d85476c411bbe_hd.jpg)Fig 1. 马云演示支付宝刷脸支付



**人脸验证（Face Verification）**：意思就是说，给定两张图，算法要判断出这两张图是不是同一个人，这是近年来一个非常热门的研究方向，也产生了一大批模型和 Loss Function。

**人脸防伪（Face Anti-Spoofing）**：意思就是说，你刷脸的时候，算法要判别这张脸是不是真人活体脸，而对于合成的、或者他人照片来攻击算法的，应该予以拒绝。



PA（Presentation Attacks）是常用的攻击方式，主要包含print attack（即打印出人脸照片）、replay attack（播放视频）、mask attack（带人脸假体面具等）。



Face Anti-Spoofing已经有好些年的研究历史了，传统方法将其视为一个“活体”VS“假体”的二分类问题。人脸活体检测主要有以下三种方案（注：更多方案请参考文章末尾附录2，3，或相关研究综述查阅更多）：

1. **Texture-based Methods** 
   提取人脸的几何特征、纹理特征等再结合SVM、LR等分类器进行分类，为了克服光照等因素的影响，会常常将RGB输入空间变换到HSV，YCbCr或Fourier spectrum空间；AlexNet之后，研究者们纷纷转向设计更有效的深度神经网络结构来做二分类。
2. **Temporal-based Methods**
   通过执行系统发出的“眨眼”、“点头”、“转向”等指令来辨别真假活体。
3. **Remote Photoplethysmography (rPPG)**
   rPPG可以在不接触皮肤的情况下获取生物信号（例如心率等），因此可利用rPPG来进行活体检测，之前也有基于rPPG的研究发表。



尽管这些Deep Models在很多benchmark上均取得了不错的成绩，但是笔者认为**很多模型在现实中的可用性几乎为零**。利用 Softmax Loss + DNN 取得良好成绩的原因极有可能是 DNN 仅仅学习到了 **too young too simple的区分特征**（例如用照片攻击时CNN学习到了照片边框；拍摄Print Attack时的照片反光/过曝；用录制视频攻击时CNN学习到了如何根据Moire Pattern(即“水波纹”)来区分），而并不是学习到了真正如何区分活体与非活体。所以这些模型往往都是在某一个benchmark上**严重过拟合**，在 cross-datasets testing（即将模型用于其他数据集测试） 的时候，泛化结果就会非常非常差。尽管这种现象可以通过搜集更加 diverse 的数据集来缓和，但是依然不能从根本上解决问题。下面介绍一篇CVPR2018上的 Face Anti-Spoofing 相关的文献，大致看了一遍，还是非常不错的工作，有兴趣的读者请去阅读原文。

http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Learning_Deep_Models_CVPR_2018_paper.pdfopenaccess.thecvf.com



------

## **方法**

这篇Paper的idea主要是基于这样一个思想：

充分利用活体与假体人脸的时空信息来形成更为有效的特征；

从**空间信息**角度来看，活体人脸是带有一定的距离信息的，例如正面拍照时，活体人脸的鼻子与摄像头的距离会比脸部其他部位要近一些（因为鼻子是凸出来的嘛~），而打印的照片像素深度上往往都是平的，所以可以用来进行基本的区分。

从**时间信息**角度来看，有研究表明，可以从真实人脸视频中检测出rPPG信息（例如心律信号）。因此可以利用rPPG来辅助分类。

![img](https://pic2.zhimg.com/80/v2-e44cb9664a0bab7752f87f577f2df1c5_hd.jpg)Fig 2. pipeline

本文模型（CNN-RNN）利用CNN提取人脸特征，然后将feature map与depth map（即从2D图像中得到的3D形状表达）输入到作者提出的non-rigid registration layer，再将aligned feature maps与rPPG输入到RNN进行分类。Method pipeline如 Fig 2. 所示。



人脸正面的3D信息 ![S_F\in \mathbb{R}^{3\times Q}](https://www.zhihu.com/equation?tex=S_F%5Cin+%5Cmathbb%7BR%7D%5E%7B3%5Ctimes+Q%7D) 可以表示为identity base与expression base的线性组合：

![S_F=S_0 + \sum_{i=1}^{N_{id}}\alpha_{id}^i S_{id}^i + \sum_{i=1}^{N_{exp}}\alpha_{exp}^i S_{exp}^i](https://www.zhihu.com/equation?tex=S_F%3DS_0+%2B+%5Csum_%7Bi%3D1%7D%5E%7BN_%7Bid%7D%7D%5Calpha_%7Bid%7D%5Ei+S_%7Bid%7D%5Ei+%2B+%5Csum_%7Bi%3D1%7D%5E%7BN_%7Bexp%7D%7D%5Calpha_%7Bexp%7D%5Ei+S_%7Bexp%7D%5Ei)

由于从2D图像中估计3D信息是一件比较困难的事情，因此在这里对其进行归一化到 ![[0, 1]](https://www.zhihu.com/equation?tex=%5B0%2C+1%5D) 区间，然后将 Z-Buffer 算法应用于 ![S](https://www.zhihu.com/equation?tex=S) 来投影到2D平面，这样就得到了人脸图像预估的2D depth map ![D\in \mathbb{R}^{32\times 32}](https://www.zhihu.com/equation?tex=D%5Cin+%5Cmathbb%7BR%7D%5E%7B32%5Ctimes+32%7D) 。



传统rPPG信息往往容易受到光照、人脸表情、面部姿态等因素的影响使得结果不够精确。本文提出利用 RNN 来学习rPPG。

对于跟踪到的人脸区域，首先计算正交色度信号：

![x_f=3r_f-2g_f \\ y_f=1.5r_f+g_f-1.5b_f](https://www.zhihu.com/equation?tex=x_f%3D3r_f-2g_f+%5C%5C+y_f%3D1.5r_f%2Bg_f-1.5b_f)

然后利用色度信号标准差的比例 ![\gamma=\frac{\sigma(x_f)}{\sigma(y_f)}](https://www.zhihu.com/equation?tex=%5Cgamma%3D%5Cfrac%7B%5Csigma%28x_f%29%7D%7B%5Csigma%28y_f%29%7D) 来计算血流信号：

![p=3(1-\frac{\gamma}{2})r_f-2(1+\frac{\gamma}{2})g_f+\frac{3\gamma}{2}b_f](https://www.zhihu.com/equation?tex=p%3D3%281-%5Cfrac%7B%5Cgamma%7D%7B2%7D%29r_f-2%281%2B%5Cfrac%7B%5Cgamma%7D%7B2%7D%29g_f%2B%5Cfrac%7B3%5Cgamma%7D%7B2%7Db_f)

（注：这一块儿的处理超出了我的知识储备，所以我不太明白是否这样能work，还是说先前已有生物、医学领域的成果表明这样能work...）

接下来对 ![p](https://www.zhihu.com/equation?tex=p) 应用快速傅里叶变换，就得到了50维的 rPPG 信号。



网络结构如下图所示：

![img](https://pic4.zhimg.com/80/v2-b9a575e4d15bff5017e9af1416d1770f_hd.jpg)Fig 3. CNN-RNN architecture

用于预测 depth map 以及生成feature map 的 CNN 的 Loss 如下：

![\Theta_D = argmin_{\Theta_D} \sum_{i=1}^{N_d}||CNN_D(I_i;\Theta_D)-D_i||_1^2](https://www.zhihu.com/equation?tex=%5CTheta_D+%3D+argmin_%7B%5CTheta_D%7D+%5Csum_%7Bi%3D1%7D%5E%7BN_d%7D%7C%7CCNN_D%28I_i%3B%5CTheta_D%29-D_i%7C%7C_1%5E2)

用于预测 rPPG 信号的 RNN Loss 如下：

![\Theta_R=argmin_{\Theta_R}\sum_{i=1}^{N_s}||RNN_R([\{F_j\}_{j=1}^{N_f}]_i;\Theta_R)-f_i||_1^2](https://www.zhihu.com/equation?tex=%5CTheta_R%3Dargmin_%7B%5CTheta_R%7D%5Csum_%7Bi%3D1%7D%5E%7BN_s%7D%7C%7CRNN_R%28%5B%5C%7BF_j%5C%7D_%7Bj%3D1%7D%5E%7BN_f%7D%5D_i%3B%5CTheta_R%29-f_i%7C%7C_1%5E2)

Classification score的计算方式如下：

![score=||\hat{f}||_2^2+\lambda||\hat{D}||_2^2](https://www.zhihu.com/equation?tex=score%3D%7C%7C%5Chat%7Bf%7D%7C%7C_2%5E2%2B%5Clambda%7C%7C%5Chat%7BD%7D%7C%7C_2%5E2)

其中， ![\hat{f}](https://www.zhihu.com/equation?tex=%5Chat%7Bf%7D) 代表rPPG信号， ![\hat{D}](https://www.zhihu.com/equation?tex=%5Chat%7BD%7D) 代表最后一帧的depth map。



为了让 RNN学习不同时间和不同人物主体下的相同脸部区域的activation变化信息，作者添加了Non-rigid Registration Layer，个人认为是整个方法里面一个非常亮眼的idea。

首先保留depth map中超过threshold的部分：

![V=\hat{D}\geq threshold](https://www.zhihu.com/equation?tex=V%3D%5Chat%7BD%7D%5Cgeq+threshold)

然后计算mask ![V](https://www.zhihu.com/equation?tex=V) 和 feature map ![T](https://www.zhihu.com/equation?tex=T) 的内积：

![U=T\bigodot V](https://www.zhihu.com/equation?tex=U%3DT%5Cbigodot+V)

接下来再利用3D信息 ![S](https://www.zhihu.com/equation?tex=S) 得到frontalized image ![F](https://www.zhihu.com/equation?tex=F) :

![F(i,j)=U(S(m_{ij},1),S(m_{ij},2))](https://www.zhihu.com/equation?tex=F%28i%2Cj%29%3DU%28S%28m_%7Bij%7D%2C1%29%2CS%28m_%7Bij%7D%2C2%29%29)



Non-rigid Registration Layer的作用在于消除了脸部表情、姿态，背景区域，以及更好地利用depth map信息方面，从而有效地提升了spoofing recognition的性能。

![img](https://pic2.zhimg.com/80/v2-0e81763f694796d469927548e5a31ec1_hd.jpg)Fig 4. Non-rigid Registration Layer

------

## **总结**

个人觉得，这篇文章整体上idea还是不错的，novelty也很够。实验结果也还比较漂亮，个人比较肯定的点在于利用时空信息结合rPPG做特征融合，以及Non-rigid Registration Layer的助攻，来综合提高活体检测的性能，以及可解释性方面的工作。而不像传统活体检测领域的 Paper 单纯只是 train 个 binary classifier，各种花式 part-based + ensemble model 来刷分。

------

## **References**

1. Yaojie Liu, Amin Jourabloo, Xiaoming Liu. Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision[C]. // IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2018
2. [人脸识别中的活体检测](https://zhuanlan.zhihu.com/p/25401788)
3. [人脸活体检测技术简介](https://zhuanlan.zhihu.com/p/26112838)









# 活体检测新文by京东金融：利用多帧人脸来预测更精确的深度

2018年11月21日 12:04:05 [SIGAI_CSDN](https://me.csdn.net/SIGAI_CSDN) 阅读数：424



接着上次的《活体检测Face anti-spoofing综述》，再来讲讲arXiv上新挂的文章：

 

京东金融和中科院联合发表的“Exploiting temporal and depth information for multi-frame face anti-spoofing”[1]

 

它的主要创新和贡献是:利用了多帧的时空信息来更精准地预测深度图，再而进行活体检测

 

 

Related Work

先把提一下之前的state-of-the-art，就是MSU发表在CVPR2018上的工作 [2]。

 

文中的主要思路是：

\1. 通过活体与非活体的单帧人脸图，来预测其深度图（因为先验知识知道 真人图 与 纸张或屏幕攻击载体 的深度很不同）

 

\2. 通过活体与非活体的多帧人脸图，来预测其rPPG信号的频域分布（同理先验知道 真人人脸的rPPG信号 与 无生命的纸张或屏幕 很不同）

上述共享一个 backbone，后接两个分支。分支一直接回归深度图；分支二用来预测rPPG频域分布：即是通过non-rigid registration层来将pose都归一到正脸同姿态，后接RNN来获得temporal信息。这里就列下共享的主干网络，因为京东这文章也是用相同的网络：

![1.jpg](http://www.sigai.cn/upload/image/20181121/1542772330320135.jpg)

图1.主干网络

 

依据与Motivation

作者认为，MSU上述方法有一定drawback:

\1. 因为使用了non-rigid注册层去除脸部表情和姿态的影响，这样忽略了重要的线索：非活体脸部不同表情与姿态的不自然变化（unnatural changes）

 

\2. 只用了单帧图像来预测深度，忽略了多帧间的空间微变化可以帮助重构环境3D信息。

基于上面两点，作者发现可以把该问题看出multi-view的SLAM问题，无论是摄像头在动，还是人脸在动，它们帧间的动态信息都可以用来重构3D空间，即**用多帧信息理论上会比单帧更好地重构深度图**。

 

作者画出下面草图来描述活体与非活体间帧间的微变化，可见在 左边(a)活体场景，明显侧脸时鼻子与耳朵的角度比正脸时大；而对于右侧(b)屏幕攻击，则反之。

![2.jpg](http://www.sigai.cn/upload/image/20181121/1542772509339484.jpg)

图2.活体与非活体的多帧视差

 

算法框架

总框架主要分两部分（单帧部分和多帧部分），如下图所示：

![img](https://img-blog.csdnimg.cn/20181121120340182.gif)

![3.jpg](http://www.sigai.cn/upload/image/20181121/1542772526979055.jpg)

图3.网络总框架图

 

单帧部分和MSU文章的主干网络基本一致（唯一的不同就是MSU用了 RGBHSV 6通道作为输入，本文用的是RGB三通道），就是每帧单独预测深度图：

![img](https://img-blog.csdnimg.cn/20181121120340244.gif)

![4.jpg](http://www.sigai.cn/upload/image/20181121/1542772568730280.jpg)

图4. 单帧网络部分

 

多帧部分主要由 optical flow guided features (OFF) Block 和 ConvGRU Unit 组成，因为OFF Blcok主要构建在相邻两帧间，而 ConvGRU 是构建在整个多帧的clips间，故前者用来获取short-term信息，后者则获得Long-term信息。

 

下图是OFF Block架构图，我们来看看都做了什么：

\1. Fl(t)为当前帧特征经过1x1卷积后降维的特征

 

\2. FlS(t)为当前帧特征经过Sobel算子后的空间XY方向梯度

 

\3. FlT(t)为当前帧特征与相邻后一帧特征的差异（空间梯度）

 

\4. FlS(t+△t)为相邻后一帧特征的Sobel算子后的空间XY方向梯度

 

\5. OFFl-1(t)为上个OFF Blcok输出的特征 （即多个OFF Block是 stacked）

最后把上述5个特征都concatenate在一起，3x3卷积再降维~~

![img](https://img-blog.csdnimg.cn/20181121120340286.gif)

![5.jpg](http://www.sigai.cn/upload/image/20181121/1542772590237085.jpg)

图5. OFF Block的架构

 

至于作者为什么要这样设计呢？OFF全称是optical flow guided features，则作者希望使用相邻帧间feature-level的光流，这样比起传统光流，表征能力更强且计算消耗更小。传统光流公式如下：

![image.png](http://www.sigai.cn/upload/image/20181121/1542772638996240.png)![img](https://img-blog.csdnimg.cn/20181121120340289.gif)

通过泰勒分解和变形后，得到：

![image.png](http://www.sigai.cn/upload/image/20181121/1542772648250373.png)

这里的 (vx,vy) 就是光流，而通过上面公式可得正交关系：

![image.png](http://www.sigai.cn/upload/image/20181121/1542772661556815.png)

 

故![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACnBMGpXl1bdzpLQPCkygM65vbPl1lqRt0v5vx7l7kiciaYhuicgbH3kcUMb4QpTZUpryPBzY9VUY4NvA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)是被光流引导的。而通常传统光流需要通过 Low-level 和 high-level 特征去匹配得到，故我们将上式的输入图像 I 换成特征图来输入，则使用多级特征图的X方向梯度，Y方向梯度和时间梯度，便可类似地表示光流。所以OFF block里的5个元素，就是按照这个来的~~

 

PS：文中光流假设有点问题，文中说光流前后(x,y) 与(x+△t,y+△t)的亮度应不变。不过在人脸的应用中，肯定是会换的，即人脸相同位置的pixel，随着心脏驱动的血液流动，它的亮度值也会周期性地变化，这就是做rPPG的原理~~Whatever, 我们这里先不谈这个，作者开心就好~~

 

最后我们来说说 Loss function，主要由三部分组成:

\1. 二值分类误差（活体or非活体）

 

\2. 每帧深度图的 L1 loss

 

\3. 作者自己提出的每帧深度图的 contrasive depth regression loss:

![image.png](http://www.sigai.cn/upload/image/20181121/1542772671563460.png)

目的是更好学到每个 pixel 的拓扑关系，更强约束其与周边neighbor的对比度。对应的Kernel如下图所示：

![6.jpg](http://www.sigai.cn/upload/image/20181121/1542772681629499.jpg)

图6.对比深度损失的Kernel

 

实验结果

在Oulu-NPU上的结果：

![7.jpg](http://www.sigai.cn/upload/image/20181121/1542772698933516.jpg)

图7. Oulu-NPU结果

 

FAS-BAS 指的是 MSU文章[2] 的方法，可见京东的方法用单纯的Depth，还是要比MSU的 rPPG+Depth 方法性能要好~~

 

接着我们来看看网络里各个模块及Loss的作用：

![9.jpg](http://www.sigai.cn/upload/image/20181121/1542772712327556.jpg)

可见 OFF-Block 和 Contrastive Depth loss 的作用还是蛮大的~~

 

最后来定性可视化下出来的深度图的可判别性如何：

![10.jpg](http://www.sigai.cn/upload/image/20181121/1542772729452955.jpg)

图8. 深度图可视化

 

使用多帧来重构的深度图，对于Replay屏幕攻击有明显的改善。对于Print打印攻击，好像还更糟糕了一点。

 

总结与展望未来

文章给出了很好的思路和结论来使用多帧，这也是继MSU使用多帧来预测rPPG频域后的一大进步，这样未来face anti-spoofing将更多focus在多帧上；而不是单帧深度，单帧color texture~~

 

未来展望的话，可以看看其他图像预测深度图的文章如字节跳动DeepLens[3]等等，来激发灵感用于活体的任务~~另外正如前面综述所说，探索脸部微变化如rPPG等，和结合人脸检测，人脸识别，人脸微表情等任务来找关联性都是值得探索~~

 

Reference:

[1] Zezheng Wang et al. Exploiting temporal and depth information for multi-frame face anti-spoofing, 2018

[2] Yaojie Liu, Amin Jourabloo, Xiaoming Liu, Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision ，CVPR2018

[3]LIJUN WANG et al. DeepLens: Shallow Depth Of Field From A Single Image, 2018





