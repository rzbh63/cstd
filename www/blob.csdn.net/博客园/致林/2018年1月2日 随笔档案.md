
# 2018年1月2日 随笔档案 - 致林 - 博客园






[2018年1月2日](https://www.cnblogs.com/bincoding/archive/2018/01/02.html)
摘要: 梯度下降：值越大，函数变化越快。 梯度下降（Gradient Descent）小结 激活函数：Relu函数是一个单调递增函数，用来解决线形问题。 ReLu(Rectified Linear Units)激活函数 池化和全连接： tensorflow 1.0 学习：池化层（pooling)和全连接层([阅读全文](https://www.cnblogs.com/bincoding/p/8178911.html)

