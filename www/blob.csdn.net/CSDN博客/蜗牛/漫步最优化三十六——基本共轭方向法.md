
# 漫步最优化三十六——基本共轭方向法 - 蜗牛 - CSDN博客


2017年11月02日 21:50:11[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：602



$\textbf{用我的眼神，}$
$\textbf{拍下你的睫毛，}$
$\textbf{你微笑的嘴角。}$
$\textbf{你的微笑像毒药，}$
$\textbf{却洋溢着幸福的味道。}$
$\textbf{我就是戒不掉，}$
$\textbf{你粉嫩清秀的外表，}$
$\textbf{像多汁的水蜜桃。}$
$\textbf{你亮丽的唇膏，}$
$\textbf{充盈着自信的骄傲。}$
$\textbf{就想一直咬着水蜜桃，}$
$\textbf{看着你的骄傲。}$
$\textbf{——畅宝宝的傻逼哥哥}$
上篇文章最后给出的$\mathbf{x}^*$计算公式可以迭代运算，从初始点$\mathbf{x}_0=\mathbf{0}$开始，连续进行$n$次调整$\alpha_k\mathbf{d}_k$。也就是说由关系

$$
\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k\mathbf{d}_k
$$
其中

$$
\alpha_k=-\frac{\mathbf{b}^T\mathbf{d}_k}{\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k}
$$
生成一个序列，$\mathbf{x}_0=\mathbf{0}$，当$k=n-1$时收敛到

$$
\mathbf{x}_n=\mathbf{x}^*
$$
从任意初始点$\mathbf{x}_0$开始都能得到相似的结果，这个事实可以用下面的定理得到证实。
$\textbf{定理1：}$如果$\{\mathbf{d}_0,\mathbf{d}_1,\ldots,\mathbf{d}_{n-1}\}$是非零共轭方向构成的集合，$\mathbf{H}$是$n\times n$的正定矩阵且问题

$$
\text{minimize}_x\ f(\mathbf{x})=a+\mathbf{x}^T\mathbf{b}+\frac{1}{2}\mathbf{x}^T\mathbf{H}\mathbf{x}
$$
是二次的，那么对任意初始点$\mathbf{x}_0$，由关系

$$
\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k\mathbf{d}_k\quad \text{for}\ k\geq 0
$$
其中

$$
\alpha_k=-\frac{\mathbf{g}_k^T\mathbf{d}_k}{\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k}
$$
且

$$
\mathbf{g}_k=\mathbf{b}+\mathbf{Hx}_k
$$
在$n$次迭代后收敛到二次问题的唯一解$\mathbf{x}^*$，即$\mathbf{x}_n=\mathbf{x}^*$。
$\textbf{证明：}$向量$\mathbf{x}^*-\mathbf{x}_0$可以表示成共轭方向的线性组合

$$
\mathbf{x}^*-\mathbf{x}_0=\sum_{i=0}^{n-1}\alpha_i\mathbf{d}_i
$$
由上篇文章末尾处可知

$$
\alpha_k=\frac{\mathbf{d}_k^T\mathbf{H}(\mathbf{x}^*-\mathbf{x}_0)}{\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k}
$$
根据迭代公式$\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k\mathbf{d}_k$可得

$$
\mathbf{x}_{k}-\mathbf{x}_0=\sum_{i=0}^{k-1}\alpha_i\mathbf{d}_i
$$
所以

$$
\mathbf{d}_k^T\mathbf{H}(\mathbf{x}_{k}-\mathbf{x}_0)=\sum_{i=0}^{k-1}\alpha_i\mathbf{d}_k^T\mathbf{H}\mathbf{d}_i=0
$$
显然

$$
\mathbf{d}_k^T\mathbf{H}\mathbf{x}_{k}=\mathbf{d}_k^T\mathbf{H}\mathbf{x}_0
$$
那么

$$
\alpha_k=\frac{\mathbf{d}_k^T(\mathbf{H}\mathbf{x}^*-\mathbf{H}\mathbf{x}_k)}{\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k}
$$
又因为

$$
\mathbf{Hx}_k=\mathbf{g}_k-\mathbf{b}
$$
且在最小点$\mathbf{x}_k$处$\mathbf{g}_k=\mathbf{0}$，所以我们有

$$
\mathbf{Hx}^*=-\mathbf{b}
$$
最后得出

$$
\alpha_k=-\frac{\mathbf{d}_k^T\mathbf{g}_k}{\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k}=-\frac{\mathbf{g}_k^T\mathbf{d}_k}{\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k}
$$
接下来令$k=n$可得

$$
\mathbf{x}_n=\mathbf{x}_0+\sum_{i=0}^{n-1}\alpha_i\mathbf{d}_i=\mathbf{x}^*
$$
得证。$||$
根据定理1，如果使用不同的方法来生成共轭方向，那么我们就能得到不同的共轭方向法。
基于定理1得到的方法存在许多共同的属性，下面的定理就给出了其中的两个属性。
$\textbf{定理2：}$
梯度\mathbf{g}_k与方向\mathbf{d}_i,0\leq i<k正交，即
\mathbf{g}_k^T\mathbf{d}_i=\mathbf{d}_i^T\mathbf{g}_k=0\quad \text{for}\quad 0\leq i<k
定理1中的\alpha=\alpha_k在每条线
\mathbf{x}=\mathbf{x}_{k-1}+\alpha\mathbf{d}_i\quad \text{for}\quad 0\leq i<k
上最小化f(\mathbf{x})
\textbf{证明：}(a)假设
\begin{equation}
\mathbf{g}_k^T\mathbf{d}_i=0\quad \text{for}\quad 0\leq i<k\tag1
\end{equation}
我们需要说明
\mathbf{g}_{k+1}^T\mathbf{d}_i=0\quad \text{for}\quad 0\leq i<k+1
因为
\mathbf{g}_{k+1}-\mathbf{g}_k=\mathbf{H}(\mathbf{x}_{k+1}-\mathbf{x}_k)
并由\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k\mathbf{d}_k\quad \text{for}\ k\geq 0可得
\begin{equation}
\mathbf{g}_{k+1}=\mathbf{g}_k+\alpha_k\mathbf{H}\mathbf{d}_k\tag2
\end{equation}
因此
\begin{equation}
\mathbf{g}_{k+1}^T\mathbf{d}_i=\mathbf{g}_k^T\mathbf{d}_i+\alpha_k\mathbf{d}_k^T\mathbf{H}\mathbf{d}_i\tag3
\end{equation}
对于i=k，我们有
\begin{equation}
\mathbf{g}_{k+1}^T\mathbf{d}_k=\mathbf{g}_k^T\mathbf{d}_k+\alpha_k\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k=0\tag4
\end{equation}
对于0\leq i<k，根据等式1可得
\mathbf{g}_k^T\mathbf{d}_i=0
而且\mathbf{d}_i,\mathbf{d}_k是共轭的，
\mathbf{d}_k^T\mathbf{H}\mathbf{d}_i=0
所以由等式3可得
\begin{equation}
\mathbf{g}_{k+1}^T\mathbf{d}_i=0\quad for\quad 0\leq i<k\tag5
\end{equation}
结合等式4与等式5可得
\begin{equation}
\mathbf{g}_{k+1}^T\mathbf{d}_i=0\quad \text{for}\quad 0\leq i<k+1\tag6
\end{equation}
如果k=0，那么等式6得出\mathbf{g}_1^T\mathbf{d}_i=0\quad \text{for}\quad 0\leq i<1，从等式1与等式6我们可以得出
\begin{align*}
\mathbf{g}_2^T\mathbf{d}_i&=0\quad \text{for}\quad 0\leq i<2\\
\mathbf{g}_3^T\mathbf{d}_i&=0\quad \text{for}\quad 0\leq i<3\\
&\vdots\qquad\qquad\qquad\vdots\\
\mathbf{g}_k^T\mathbf{d}_i&=0\quad \text{for}\quad 0\leq i<k
\end{align*}
(b)因为
\begin{align*}
\mathbf{g}_k^T\mathbf{d}_i
&\equiv\mathbf{g}^T(\mathbf{x})\mathbf{d}_i=\mathbf{g}(\mathbf{x}_{k-1}+\alpha\mathbf{d}_i)^T\mathbf{d}_i\\
&=\frac{df(\mathbf{x}_{k-1}+\alpha\mathbf{d}_i)}{d\alpha}=0
\end{align*}
所以f(\mathbf{x})在每条线
\mathbf{x}=\mathbf{x}_{k-1}+\alpha\mathbf{d}_i\quad \text{for}\quad 0\leq i<k
上都被最小化。
上述定理第二部分的应用就是\mathbf{x}_k在向量\{\mathbf{d}_0,\mathbf{d}_1,\ldots,\mathbf{d}_{k-1}\}生成的子空间上最小化f(\mathbf{x})，那么\mathbf{x}_n就是在向量\{\mathbf{d}_0,\mathbf{d}_1,\ldots,\mathbf{d}_{n-1}\}生成的子空间(即E^n)上最小化f(\mathbf{x})，这是另一种表述\mathbf{x}_n=\mathbf{x}^*的方式。

