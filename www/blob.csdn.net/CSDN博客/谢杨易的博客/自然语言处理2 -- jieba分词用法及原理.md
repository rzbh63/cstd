
# 自然语言处理2 -- jieba分词用法及原理 - 谢杨易的博客 - CSDN博客

2018年08月16日 11:25:49[谢杨易](https://me.csdn.net/u013510838)阅读数：1505


系列文章，请多关注
[Tensorflow源码解析1 – 内核架构和源码结构](https://blog.csdn.net/u013510838/article/details/84103503)
[带你深入AI（1） - 深度学习模型训练痛点及解决方法](https://blog.csdn.net/u013510838/article/details/79835563)
[自然语言处理1 – 分词](https://blog.csdn.net/u013510838/article/details/81673016)
[自然语言处理2 – jieba分词用法及原理](https://blog.csdn.net/u013510838/article/details/81738431)
[自然语言处理3 – 词性标注](https://blog.csdn.net/u013510838/article/details/81907121)
[自然语言处理4 – 句法分析](https://blog.csdn.net/u013510838/article/details/81976427)
[自然语言处理5 – 词向量](https://blog.csdn.net/u013510838/article/details/82108381)
[自然语言处理6 – 情感分析](https://blog.csdn.net/u013510838/article/details/82558797)
# 1 概述
上篇文章我们分析了自然语言处理，特别是中文处理中，分词的几个主要难点。为了解决这些难点，我们提出了基于字符串匹配的算法和基于统计的分词算法。针对当前的几种分词引擎，我们对其分词准确度和速度进行了评估。jieba分词作为一个开源项目，在准确度和速度方面均不错，是我们平时常用的分词工具。本文将对jieba分词的使用方法以及原理进行讲解，便于我们在理解jieba分词原理的同时，加深对前文讲解的分词难点和算法的理解。
# 2 jieba分词用法
jieba分词是一个开源项目，地址为[https://github.com/fxsjy/jieba](https://github.com/fxsjy/jieba)。它在分词准确度和速度方面均表现不错。其功能和用法如下。
#### 2.1 分词
支持三种分词模式
精确分词，试图将句子最精确的切开，适合文本分析
全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义
搜索引擎模式，在精确模式基础上，对长词进行再次切分，提高recall，适合于搜索引擎。
```python
# encoding=utf-8
```
```python
import
```
```python
jieba
seg_list
```
```python
=
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
"我来到北京清华大学"
```
```python
,
```
```python
cut_all
```
```python
=
```
```python
True
```
```python
)
```
```python
print
```
```python
(
```
```python
"Full Mode: "
```
```python
+
```
```python
"/ "
```
```python
.
```
```python
join
```
```python
(
```
```python
seg_list
```
```python
)
```
```python
)
```
```python
# 全模式
```
```python
seg_list
```
```python
=
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
"我来到北京清华大学"
```
```python
,
```
```python
cut_all
```
```python
=
```
```python
False
```
```python
)
```
```python
print
```
```python
(
```
```python
"Default Mode: "
```
```python
+
```
```python
"/ "
```
```python
.
```
```python
join
```
```python
(
```
```python
seg_list
```
```python
)
```
```python
)
```
```python
# 精确模式
```
```python
seg_list
```
```python
=
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
"他来到了网易杭研大厦"
```
```python
)
```
```python
# 默认是精确模式
```
```python
print
```
```python
(
```
```python
", "
```
```python
.
```
```python
join
```
```python
(
```
```python
seg_list
```
```python
)
```
```python
)
```
```python
seg_list
```
```python
=
```
```python
jieba
```
```python
.
```
```python
cut_for_search
```
```python
(
```
```python
"小明硕士毕业于中国科学院计算所，后在日本京都大学深造"
```
```python
)
```
```python
# 搜索引擎模式
```
```python
print
```
```python
(
```
```python
", "
```
```python
.
```
```python
join
```
```python
(
```
```python
seg_list
```
```python
)
```
```python
)
```
输出为
```python
【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
【精确模式】: 我/ 来到/ 北京/ 清华大学
【新词识别】：他, 来到, 了, 网易, 杭研, 大厦    (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)
【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造
```
#### 2.2 添加自定义词典
主要是为了解决新词问题，jieba分词基于HMM算法会自动识别新词，但用户如果能直接给出新词，则准确率会更高。
使用起来很简单，我们先创建一个文件，比如user_dict.txt，其中每一行代表一个新词，分别为词语，词频，词性。如下：
```python
创新办 3 i
云计算 5
凱特琳 nz
台中
```
然后在代码中分词前，加载这个自定义词典即可。
```python
jieba
```
```python
.
```
```python
load_userdict
```
```python
(
```
```python
"user_dict.txt"
```
```python
)
```
加载自定义词典的分词效果：
```python
之前： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 /
加载自定义词库后：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /
```
#### 2.3 调整词典
```python
# 1 使用del_word()使得某个词语不会出现
```
```python
>>
```
```python
>
```
```python
print
```
```python
(
```
```python
'/'
```
```python
.
```
```python
join
```
```python
(
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
'如果放到post中将出错。'
```
```python
,
```
```python
HMM
```
```python
=
```
```python
False
```
```python
)
```
```python
)
```
```python
)
```
```python
如果
```
```python
/
```
```python
放到
```
```python
/
```
```python
post
```
```python
/
```
```python
中将
```
```python
/
```
```python
出错
```
```python
/
```
```python
。
```
```python
>>
```
```python
>
```
```python
jieba
```
```python
.
```
```python
del_word
```
```python
(
```
```python
"中将"
```
```python
)
```
```python
>>
```
```python
>
```
```python
print
```
```python
(
```
```python
'/'
```
```python
.
```
```python
join
```
```python
(
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
'如果放到post中将出错。'
```
```python
,
```
```python
HMM
```
```python
=
```
```python
False
```
```python
)
```
```python
)
```
```python
)
```
```python
如果
```
```python
/
```
```python
放到
```
```python
/
```
```python
post
```
```python
/
```
```python
中
```
```python
/
```
```python
将
```
```python
/
```
```python
出错
```
```python
/
```
```python
。
```
```python
# 2 使用add_word()添加新词到字典中
```
```python
>>
```
```python
>
```
```python
print
```
```python
(
```
```python
'/'
```
```python
.
```
```python
join
```
```python
(
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
'「台中」正确应该不会被切开'
```
```python
,
```
```python
HMM
```
```python
=
```
```python
False
```
```python
)
```
```python
)
```
```python
)
```
```python
「
```
```python
/
```
```python
台
```
```python
/
```
```python
中
```
```python
/
```
```python
」
```
```python
/
```
```python
正确
```
```python
/
```
```python
应该
```
```python
/
```
```python
不会
```
```python
/
```
```python
被
```
```python
/
```
```python
切开
```
```python
>>
```
```python
>
```
```python
jieba
```
```python
.
```
```python
add_word
```
```python
(
```
```python
"台中"
```
```python
)
```
```python
>>
```
```python
>
```
```python
print
```
```python
(
```
```python
'/'
```
```python
.
```
```python
join
```
```python
(
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
'「台中」正确应该不会被切开'
```
```python
,
```
```python
HMM
```
```python
=
```
```python
False
```
```python
)
```
```python
)
```
```python
)
```
```python
「
```
```python
/
```
```python
台中
```
```python
/
```
```python
」
```
```python
/
```
```python
正确
```
```python
/
```
```python
应该
```
```python
/
```
```python
不会
```
```python
/
```
```python
被
```
```python
/
```
```python
切开
```
```python
# 3 使用suggest_freq()调整某个词语的词频，使得其在设置的词频高是能分出，词频低时不能分出
```
```python
>>
```
```python
>
```
```python
jieba
```
```python
.
```
```python
suggest_freq
```
```python
(
```
```python
'台中'
```
```python
,
```
```python
True
```
```python
)
```
```python
69
```
```python
>>
```
```python
>
```
```python
print
```
```python
(
```
```python
'/'
```
```python
.
```
```python
join
```
```python
(
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
'「台中」正确应该不会被切开'
```
```python
,
```
```python
HMM
```
```python
=
```
```python
False
```
```python
)
```
```python
)
```
```python
)
```
```python
「
```
```python
/
```
```python
台中
```
```python
/
```
```python
」
```
```python
/
```
```python
正确
```
```python
/
```
```python
应该
```
```python
/
```
```python
不会
```
```python
/
```
```python
被
```
```python
/
```
```python
切开
```
#### 2.4 关键词提取
关键词提取，将文本中最能表达文本含义的词语抽取出来，有点类似于论文的关键词或者摘要。关键词抽取可以采取：
有监督学习：文本作为输入，关键词作为标注，进行训练得到模型。此方法难点在于需要大量人工标注
无监督学习：先抽取出候选词，对每个候选词打分，取出前K个分值高的作为最后的关键词。jieba分词实现了基于TF-IDF和基于TextRank的关键词抽取算法。
**基于TF-IDF的关键词抽取算法**，目标是获取文本中词频高，也就是TF大的，且语料库其他文本中词频低的，也就是IDF大的。这样的词可以作为文本的标志，用来区分其他文本。
```python
from
```
```python
jieba
```
```python
import
```
```python
analyse
```
```python
# 引入TF-IDF关键词抽取接口
```
```python
tfidf
```
```python
=
```
```python
analyse
```
```python
.
```
```python
extract_tags
```
```python
# 原始文本
```
```python
text
```
```python
=
```
```python
"线程是程序执行时的最小单位，它是进程的一个执行流，\
        是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\
        线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\
        线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\
        同样多线程也可以实现并发操作，每个请求分配一个线程来处理。"
```
```python
# 基于TF-IDF算法进行关键词抽取
```
```python
keywords
```
```python
=
```
```python
tfidf
```
```python
(
```
```python
text
```
```python
)
```
```python
print
```
```python
"keywords by tfidf:"
```
```python
# 输出抽取出的关键词
```
```python
for
```
```python
keyword
```
```python
in
```
```python
keywords
```
```python
:
```
```python
print
```
```python
keyword
```
```python
+
```
```python
"/"
```
```python
,
```
```python
# 输出为：
```
```python
keywords by tfidf
```
```python
:
```
```python
线程
```
```python
/
```
```python
CPU
```
```python
/
```
```python
进程
```
```python
/
```
```python
调度
```
```python
/
```
```python
多线程
```
```python
/
```
```python
程序执行
```
```python
/
```
```python
每个
```
```python
/
```
```python
执行
```
```python
/
```
```python
堆栈
```
```python
/
```
```python
局部变量
```
```python
/
```
```python
单位
```
```python
/
```
```python
并发
```
```python
/
```
```python
分派
```
```python
/
```
```python
一个
```
```python
/
```
```python
共享
```
```python
/
```
```python
请求
```
```python
/
```
```python
最小
```
```python
/
```
```python
可以
```
```python
/
```
```python
允许
```
```python
/
```
```python
分配
```
```python
/
```
__基于TextRank的关键词抽取算法__步骤为，
先将文本进行分词和词性标注，将特定词性的词（比如名词）作为节点添加到图中。
出现在一个窗口中的词语之间形成一条边，窗口大小可设置为2~10之间，它表示一个窗口中有多少个词语。
对节点根据入度节点个数以及入度节点权重进行打分，入度节点越多，且入度节点权重大，则打分高。
然后根据打分进行降序排列，输出指定个数的关键词。
```python
from
```
```python
jieba
```
```python
import
```
```python
analyse
```
```python
# 引入TextRank关键词抽取接口
```
```python
textrank
```
```python
=
```
```python
analyse
```
```python
.
```
```python
textrank
```
```python
# 原始文本
```
```python
text
```
```python
=
```
```python
"线程是程序执行时的最小单位，它是进程的一个执行流，\
        是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\
        线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\
        线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\
        同样多线程也可以实现并发操作，每个请求分配一个线程来处理。"
```
```python
print
```
```python
"\nkeywords by textrank:"
```
```python
# 基于TextRank算法进行关键词抽取
```
```python
keywords
```
```python
=
```
```python
textrank
```
```python
(
```
```python
text
```
```python
)
```
```python
# 输出抽取出的关键词
```
```python
for
```
```python
keyword
```
```python
in
```
```python
keywords
```
```python
:
```
```python
print
```
```python
keyword
```
```python
+
```
```python
"/"
```
```python
,
```
```python
# 输出为：
```
```python
keywords by textrank
```
```python
:
```
```python
线程
```
```python
/
```
```python
进程
```
```python
/
```
```python
调度
```
```python
/
```
```python
单位
```
```python
/
```
```python
操作
```
```python
/
```
```python
请求
```
```python
/
```
```python
分配
```
```python
/
```
```python
允许
```
```python
/
```
```python
基本
```
```python
/
```
```python
共享
```
```python
/
```
```python
并发
```
```python
/
```
```python
堆栈
```
```python
/
```
```python
独立
```
```python
/
```
```python
执行
```
```python
/
```
```python
分派
```
```python
/
```
```python
组成
```
```python
/
```
```python
资源
```
```python
/
```
```python
实现
```
```python
/
```
```python
运行
```
```python
/
```
```python
处理
```
```python
/
```
#### 2.5 词性标注
利用jieba.posseg模块来进行词性标注，会给出分词后每个词的词性。词性标示兼容ICTCLAS 汉语词性标注集，可查阅网站[https://www.cnblogs.com/chenbjin/p/4341930.html](https://www.cnblogs.com/chenbjin/p/4341930.html)
```python
>>
```
```python
>
```
```python
import
```
```python
jieba
```
```python
.
```
```python
posseg
```
```python
as
```
```python
pseg
```
```python
>>
```
```python
>
```
```python
words
```
```python
=
```
```python
pseg
```
```python
.
```
```python
cut
```
```python
(
```
```python
"我爱北京天安门"
```
```python
)
```
```python
>>
```
```python
>
```
```python
for
```
```python
word
```
```python
,
```
```python
flag
```
```python
in
```
```python
words
```
```python
:
```
```python
.
```
```python
.
```
```python
.
```
```python
print
```
```python
(
```
```python
'%s %s'
```
```python
%
```
```python
(
```
```python
word
```
```python
,
```
```python
flag
```
```python
)
```
```python
)
```
```python
.
```
```python
.
```
```python
.
```
```python
我 r
```
```python
# 代词
```
```python
爱 v
```
```python
# 动词
```
```python
北京 ns
```
```python
# 名词
```
```python
天安门 ns
```
```python
# 名词
```
#### 2.6 并行分词
将文本按行分隔后，每行由一个jieba分词进程处理，之后进行归并处理，输出最终结果。这样可以大大提高分词速度。
```python
jieba
```
```python
.
```
```python
enable_parallel
```
```python
(
```
```python
4
```
```python
)
```
```python
# 开启并行分词模式，参数为并行进程数
```
```python
jieba
```
```python
.
```
```python
disable_parallel
```
```python
(
```
```python
)
```
```python
# 关闭并行分词模式
```
#### 2.7 Tokenize：返回词语在原文的起止位置
```python
result
```
```python
=
```
```python
jieba
```
```python
.
```
```python
tokenize
```
```python
(
```
```python
u
```
```python
'永和服装饰品有限公司'
```
```python
)
```
```python
for
```
```python
tk
```
```python
in
```
```python
result
```
```python
:
```
```python
print
```
```python
(
```
```python
"word %s\t\t start: %d \t\t end:%d"
```
```python
%
```
```python
(
```
```python
tk
```
```python
[
```
```python
0
```
```python
]
```
```python
,
```
```python
tk
```
```python
[
```
```python
1
```
```python
]
```
```python
,
```
```python
tk
```
```python
[
```
```python
2
```
```python
]
```
```python
)
```
```python
)
```
```python
# 输出为
```
```python
word 永和                start
```
```python
:
```
```python
0
```
```python
end
```
```python
:
```
```python
2
```
```python
word 服装                start
```
```python
:
```
```python
2
```
```python
end
```
```python
:
```
```python
4
```
```python
word 饰品                start
```
```python
:
```
```python
4
```
```python
end
```
```python
:
```
```python
6
```
```python
word 有限公司            start
```
```python
:
```
```python
6
```
```python
end
```
```python
:
```
```python
10
```
#### 2.8 延迟加载机制
jieba采用延迟加载方式，import jieba 时不会立刻加载jieba词典，使用时才开始加载。如果想提前加载和初始化，可以手动触发
```python
import
```
```python
jieba
jieba
```
```python
.
```
```python
initialize
```
```python
(
```
```python
)
```
```python
# 手动初始化（可选）
```
# 3 jieba分词源码结构
我们分词的jieba源码版本为0.39。代码结构如下
![image.png | left | 827x680](https://cdn.nlark.com/lark/0/2018/png/9304/1534304094132-c349a8e5-0fde-4c8f-92d6-d45914d70354.png)
主要的模块如下
基本API的封装，在Tokenizer类中，相当于一个外观类。如cut del_word  add_word  enable_parallel  initialize 等
基于字符串匹配的分词算法，包含一个很大很全的词典，即dict.txt文件
基于统计的分词算法，实现了HMM隐马尔科夫模型。jieba分词使用了字符串分词和统计分词，结合了二者的优缺点。
关键词提取，实现了TFIDF和TextRank两种无监督学习算法
词性标注，实现了HMM隐马尔科夫模型和viterbi算法
# 4 jieba分词原理分析
jieba分词综合了基于字符串匹配的算法和基于统计的算法，其分词步骤为
初始化。加载词典文件，获取每个词语和它出现的词数
切分短语。利用正则，将文本切分为一个个语句，之后对语句进行分词
构建DAG。通过字符串匹配，构建所有可能的分词情况的有向无环图，也就是DAG
构建节点最大路径概率，以及结束位置。计算每个汉字节点到语句结尾的所有路径中的最大概率，并记下最大概率时在DAG中对应的该汉字成词的结束位置。
构建切分组合。根据节点路径，得到词语切分的结果，也就是分词结果。
HMM新词处理：对于新词，也就是dict.txt中没有的词语，我们通过统计方法来处理，jieba中采用了HMM隐马尔科夫模型来处理。
返回分词结果：通过yield将上面步骤中切分好的词语逐个返回。yield相对于list，可以节约存储空间。
### 4.1 初始化
词典是基于字符串匹配的分词算法的关键所在，决定了最终分词的准确度。jieba词典dict.txt是jieba作者采集了超大规模的语料数据，统计得到的。有5M，包含349,046条词语。每一行对应一个词语，包含词语 词数 词性三部分。如下
`凤凰寺 22 ns
凤凰山 311 ns
凤凰岭 15 ns
凤凰岭村 2 ns
凤凰木 3 ns`初始化时，先加载词典文件dict.txt，遍历每一行，生成词语-词数的键值对和总词数，并将生成结果保存到cache中，下次直接从cache中读取即可。代码如下，删除了无关的log打印。只需要看关键节点代码即可，不提倡逐行逐行阅读代码，最重要的是理解代码执行的主要流程和关键算法。
```python
def
```
```python
initialize
```
```python
(
```
```python
self
```
```python
,
```
```python
dictionary
```
```python
=
```
```python
None
```
```python
)
```
```python
:
```
```python
# 获取词典路径
```
```python
if
```
```python
dictionary
```
```python
:
```
```python
abs_path
```
```python
=
```
```python
_get_abs_path
```
```python
(
```
```python
dictionary
```
```python
)
```
```python
if
```
```python
self
```
```python
.
```
```python
dictionary
```
```python
==
```
```python
abs_path
```
```python
and
```
```python
self
```
```python
.
```
```python
initialized
```
```python
:
```
```python
return
```
```python
else
```
```python
:
```
```python
self
```
```python
.
```
```python
dictionary
```
```python
=
```
```python
abs_path
            self
```
```python
.
```
```python
initialized
```
```python
=
```
```python
False
```
```python
else
```
```python
:
```
```python
abs_path
```
```python
=
```
```python
self
```
```python
.
```
```python
dictionary
```
```python
with
```
```python
self
```
```python
.
```
```python
lock
```
```python
:
```
```python
try
```
```python
:
```
```python
with
```
```python
DICT_WRITING
```
```python
[
```
```python
abs_path
```
```python
]
```
```python
:
```
```python
pass
```
```python
except
```
```python
KeyError
```
```python
:
```
```python
pass
```
```python
if
```
```python
self
```
```python
.
```
```python
initialized
```
```python
:
```
```python
return
```
```python
# 获取cache_file
```
```python
default_logger
```
```python
.
```
```python
debug
```
```python
(
```
```python
"Building prefix dict from %s ..."
```
```python
%
```
```python
(
```
```python
abs_path
```
```python
or
```
```python
'the default dictionary'
```
```python
)
```
```python
)
```
```python
t1
```
```python
=
```
```python
time
```
```python
.
```
```python
time
```
```python
(
```
```python
)
```
```python
if
```
```python
self
```
```python
.
```
```python
cache_file
```
```python
:
```
```python
cache_file
```
```python
=
```
```python
self
```
```python
.
```
```python
cache_file
```
```python
# default dictionary
```
```python
elif
```
```python
abs_path
```
```python
==
```
```python
DEFAULT_DICT
```
```python
:
```
```python
cache_file
```
```python
=
```
```python
"jieba.cache"
```
```python
# custom dictionary
```
```python
else
```
```python
:
```
```python
cache_file
```
```python
=
```
```python
"jieba.u%s.cache"
```
```python
%
```
```python
md5
```
```python
(
```
```python
abs_path
```
```python
.
```
```python
encode
```
```python
(
```
```python
'utf-8'
```
```python
,
```
```python
'replace'
```
```python
)
```
```python
)
```
```python
.
```
```python
hexdigest
```
```python
(
```
```python
)
```
```python
cache_file
```
```python
=
```
```python
os
```
```python
.
```
```python
path
```
```python
.
```
```python
join
```
```python
(
```
```python
self
```
```python
.
```
```python
tmp_dir
```
```python
or
```
```python
tempfile
```
```python
.
```
```python
gettempdir
```
```python
(
```
```python
)
```
```python
,
```
```python
cache_file
```
```python
)
```
```python
# prevent absolute path in self.cache_file
```
```python
tmpdir
```
```python
=
```
```python
os
```
```python
.
```
```python
path
```
```python
.
```
```python
dirname
```
```python
(
```
```python
cache_file
```
```python
)
```
```python
# 加载cache_file
```
```python
load_from_cache_fail
```
```python
=
```
```python
True
```
```python
if
```
```python
os
```
```python
.
```
```python
path
```
```python
.
```
```python
isfile
```
```python
(
```
```python
cache_file
```
```python
)
```
```python
and
```
```python
(
```
```python
abs_path
```
```python
==
```
```python
DEFAULT_DICT
```
```python
or
```
```python
os
```
```python
.
```
```python
path
```
```python
.
```
```python
getmtime
```
```python
(
```
```python
cache_file
```
```python
)
```
```python
>
```
```python
os
```
```python
.
```
```python
path
```
```python
.
```
```python
getmtime
```
```python
(
```
```python
abs_path
```
```python
)
```
```python
)
```
```python
:
```
```python
try
```
```python
:
```
```python
with
```
```python
open
```
```python
(
```
```python
cache_file
```
```python
,
```
```python
'rb'
```
```python
)
```
```python
as
```
```python
cf
```
```python
:
```
```python
self
```
```python
.
```
```python
FREQ
```
```python
,
```
```python
self
```
```python
.
```
```python
total
```
```python
=
```
```python
marshal
```
```python
.
```
```python
load
```
```python
(
```
```python
cf
```
```python
)
```
```python
load_from_cache_fail
```
```python
=
```
```python
False
```
```python
except
```
```python
Exception
```
```python
:
```
```python
load_from_cache_fail
```
```python
=
```
```python
True
```
```python
# cache_file不存在或者加载失败时，加载原始词典
```
```python
if
```
```python
load_from_cache_fail
```
```python
:
```
```python
wlock
```
```python
=
```
```python
DICT_WRITING
```
```python
.
```
```python
get
```
```python
(
```
```python
abs_path
```
```python
,
```
```python
threading
```
```python
.
```
```python
RLock
```
```python
(
```
```python
)
```
```python
)
```
```python
DICT_WRITING
```
```python
[
```
```python
abs_path
```
```python
]
```
```python
=
```
```python
wlock
```
```python
with
```
```python
wlock
```
```python
:
```
```python
# 加载原始词典，得到每个词与其词数的键值对，以及总词数。单个词数除以总词数，即可计算词频
```
```python
self
```
```python
.
```
```python
FREQ
```
```python
,
```
```python
self
```
```python
.
```
```python
total
```
```python
=
```
```python
self
```
```python
.
```
```python
gen_pfdict
```
```python
(
```
```python
self
```
```python
.
```
```python
get_dict_file
```
```python
(
```
```python
)
```
```python
)
```
```python
try
```
```python
:
```
```python
# 保存加载的原始词典到cache_file中
```
```python
fd
```
```python
,
```
```python
fpath
```
```python
=
```
```python
tempfile
```
```python
.
```
```python
mkstemp
```
```python
(
```
```python
dir
```
```python
=
```
```python
tmpdir
```
```python
)
```
```python
with
```
```python
os
```
```python
.
```
```python
fdopen
```
```python
(
```
```python
fd
```
```python
,
```
```python
'wb'
```
```python
)
```
```python
as
```
```python
temp_cache_file
```
```python
:
```
```python
marshal
```
```python
.
```
```python
dump
```
```python
(
```
```python
(
```
```python
self
```
```python
.
```
```python
FREQ
```
```python
,
```
```python
self
```
```python
.
```
```python
total
```
```python
)
```
```python
,
```
```python
temp_cache_file
```
```python
)
```
```python
_replace_file
```
```python
(
```
```python
fpath
```
```python
,
```
```python
cache_file
```
```python
)
```
```python
except
```
```python
Exception
```
```python
:
```
```python
try
```
```python
:
```
```python
del
```
```python
DICT_WRITING
```
```python
[
```
```python
abs_path
```
```python
]
```
```python
except
```
```python
KeyError
```
```python
:
```
```python
pass
```
```python
self
```
```python
.
```
```python
initialized
```
```python
=
```
```python
True
```
```python
# 加载原始词典
```
```python
def
```
```python
gen_pfdict
```
```python
(
```
```python
self
```
```python
,
```
```python
f
```
```python
)
```
```python
:
```
```python
lfreq
```
```python
=
```
```python
{
```
```python
}
```
```python
ltotal
```
```python
=
```
```python
0
```
```python
f_name
```
```python
=
```
```python
resolve_filename
```
```python
(
```
```python
f
```
```python
)
```
```python
# 遍历词典每一行，一行包含一个词，词数，以及词性
```
```python
for
```
```python
lineno
```
```python
,
```
```python
line
```
```python
in
```
```python
enumerate
```
```python
(
```
```python
f
```
```python
,
```
```python
1
```
```python
)
```
```python
:
```
```python
try
```
```python
:
```
```python
line
```
```python
=
```
```python
line
```
```python
.
```
```python
strip
```
```python
(
```
```python
)
```
```python
.
```
```python
decode
```
```python
(
```
```python
'utf-8'
```
```python
)
```
```python
# 取出词语和它的词数
```
```python
word
```
```python
,
```
```python
freq
```
```python
=
```
```python
line
```
```python
.
```
```python
split
```
```python
(
```
```python
' '
```
```python
)
```
```python
[
```
```python
:
```
```python
2
```
```python
]
```
```python
freq
```
```python
=
```
```python
int
```
```python
(
```
```python
freq
```
```python
)
```
```python
# 将词语和它的词数构造成键值对
```
```python
lfreq
```
```python
[
```
```python
word
```
```python
]
```
```python
=
```
```python
freq
```
```python
# 计算总词数，这个是为了以后计算某个词的词频，词频越大，则改词出现的概率越大
```
```python
ltotal
```
```python
+=
```
```python
freq
```
```python
# 遍历词语中的每个字，如果该字没有出现在词典中，则建立其词语-词数键值对，词数设置为0
```
```python
for
```
```python
ch
```
```python
in
```
```python
xrange
```
```python
(
```
```python
len
```
```python
(
```
```python
word
```
```python
)
```
```python
)
```
```python
:
```
```python
wfrag
```
```python
=
```
```python
word
```
```python
[
```
```python
:
```
```python
ch
```
```python
+
```
```python
1
```
```python
]
```
```python
if
```
```python
wfrag
```
```python
not
```
```python
in
```
```python
lfreq
```
```python
:
```
```python
lfreq
```
```python
[
```
```python
wfrag
```
```python
]
```
```python
=
```
```python
0
```
```python
except
```
```python
ValueError
```
```python
:
```
```python
raise
```
```python
ValueError
```
```python
(
```
```python
'invalid dictionary entry in %s at Line %s: %s'
```
```python
%
```
```python
(
```
```python
f_name
```
```python
,
```
```python
lineno
```
```python
,
```
```python
line
```
```python
)
```
```python
)
```
```python
f
```
```python
.
```
```python
close
```
```python
(
```
```python
)
```
```python
# 返回词语-词数的键值对，以及总词数
```
```python
return
```
```python
lfreq
```
```python
,
```
```python
ltotal
```
初始化可以简单理解为，读取词典文件，构建词语-词数键值对，方便后面步骤中查词典，也就是字符串匹配。
### 4.2. 切分短语
使用汉字正则，切分出连续的汉字和英文字符，形成一段段短语。可以理解为以空格 逗号 句号为分隔，将输入文本切分为一个个短语，之后会基于一个个短语来分词。代码如下
```python
def
```
```python
cut
```
```python
(
```
```python
self
```
```python
,
```
```python
sentence
```
```python
,
```
```python
cut_all
```
```python
=
```
```python
False
```
```python
,
```
```python
HMM
```
```python
=
```
```python
True
```
```python
)
```
```python
:
```
```python
# 编码转换，utf-8或gbk
```
```python
sentence
```
```python
=
```
```python
strdecode
```
```python
(
```
```python
sentence
```
```python
)
```
```python
# 根据是否全模式，以及是否采用HMM隐马尔科夫，来设置正则re_han re_skip，以及cut_block
```
```python
if
```
```python
cut_all
```
```python
:
```
```python
re_han
```
```python
=
```
```python
re_han_cut_all
        re_skip
```
```python
=
```
```python
re_skip_cut_all
```
```python
else
```
```python
:
```
```python
re_han
```
```python
=
```
```python
re_han_default
        re_skip
```
```python
=
```
```python
re_skip_default
```
```python
if
```
```python
cut_all
```
```python
:
```
```python
cut_block
```
```python
=
```
```python
self
```
```python
.
```
```python
__cut_all
```
```python
elif
```
```python
HMM
```
```python
:
```
```python
cut_block
```
```python
=
```
```python
self
```
```python
.
```
```python
__cut_DAG
```
```python
else
```
```python
:
```
```python
cut_block
```
```python
=
```
```python
self
```
```python
.
```
```python
__cut_DAG_NO_HMM
```
```python
# 将输入文本按照空格 逗号 句号等字符进行分割，生成一个个语句子串
```
```python
blocks
```
```python
=
```
```python
re_han
```
```python
.
```
```python
split
```
```python
(
```
```python
sentence
```
```python
)
```
```python
# 遍历语句子串
```
```python
for
```
```python
blk
```
```python
in
```
```python
blocks
```
```python
:
```
```python
if
```
```python
not
```
```python
blk
```
```python
:
```
```python
continue
```
```python
if
```
```python
re_han
```
```python
.
```
```python
match
```
```python
(
```
```python
blk
```
```python
)
```
```python
:
```
```python
# 对语句进行分词
```
```python
for
```
```python
word
```
```python
in
```
```python
cut_block
```
```python
(
```
```python
blk
```
```python
)
```
```python
:
```
```python
yield
```
```python
word
```
```python
else
```
```python
:
```
```python
tmp
```
```python
=
```
```python
re_skip
```
```python
.
```
```python
split
```
```python
(
```
```python
blk
```
```python
)
```
```python
for
```
```python
x
```
```python
in
```
```python
tmp
```
```python
:
```
```python
if
```
```python
re_skip
```
```python
.
```
```python
match
```
```python
(
```
```python
x
```
```python
)
```
```python
:
```
```python
yield
```
```python
x
```
```python
elif
```
```python
not
```
```python
cut_all
```
```python
:
```
```python
for
```
```python
xx
```
```python
in
```
```python
x
```
```python
:
```
```python
yield
```
```python
xx
```
```python
else
```
```python
:
```
```python
yield
```
```python
x
```
首先进行将语句转换为UTF-8或者GBK。
然后根据用户指定的模式，设置cut的真正实现。
然后根据正则，将输入文本分为一个个语句。
最后遍历语句，对每个语句单独进行分词。
### 4.3 构建DAG
下面我们来分析默认模式，也就是精确模式下的分词过程。先来看__cut_DAG方法。
```python
def
```
```python
__cut_DAG
```
```python
(
```
```python
self
```
```python
,
```
```python
sentence
```
```python
)
```
```python
:
```
```python
# 得到语句的有向无环图DAG
```
```python
DAG
```
```python
=
```
```python
self
```
```python
.
```
```python
get_DAG
```
```python
(
```
```python
sentence
```
```python
)
```
```python
# 动态规划，计算从语句末尾到语句起始，DAG中每个节点到语句结束位置的最大路径概率，以及概率最大时节点对应词语的结束位置
```
```python
route
```
```python
=
```
```python
{
```
```python
}
```
```python
self
```
```python
.
```
```python
calc
```
```python
(
```
```python
sentence
```
```python
,
```
```python
DAG
```
```python
,
```
```python
route
```
```python
)
```
```python
x
```
```python
=
```
```python
0
```
```python
buf
```
```python
=
```
```python
''
```
```python
N
```
```python
=
```
```python
len
```
```python
(
```
```python
sentence
```
```python
)
```
```python
while
```
```python
x
```
```python
<
```
```python
N
```
```python
:
```
```python
# y表示词语的结束位置，x为词语的起始位置
```
```python
y
```
```python
=
```
```python
route
```
```python
[
```
```python
x
```
```python
]
```
```python
[
```
```python
1
```
```python
]
```
```python
+
```
```python
1
```
```python
# 从起始位置x到结束位置y，取出一个词语
```
```python
l_word
```
```python
=
```
```python
sentence
```
```python
[
```
```python
x
```
```python
:
```
```python
y
```
```python
]
```
```python
if
```
```python
y
```
```python
-
```
```python
x
```
```python
==
```
```python
1
```
```python
:
```
```python
# 单字，一个汉字构成的一个词语
```
```python
buf
```
```python
+=
```
```python
l_word
```
```python
else
```
```python
:
```
```python
# 多汉字词语
```
```python
if
```
```python
buf
```
```python
:
```
```python
if
```
```python
len
```
```python
(
```
```python
buf
```
```python
)
```
```python
==
```
```python
1
```
```python
:
```
```python
yield
```
```python
buf
                    buf
```
```python
=
```
```python
''
```
```python
else
```
```python
:
```
```python
if
```
```python
not
```
```python
self
```
```python
.
```
```python
FREQ
```
```python
.
```
```python
get
```
```python
(
```
```python
buf
```
```python
)
```
```python
:
```
```python
# 词语不在字典中，也就是新词，使用HMM隐马尔科夫模型进行分割
```
```python
recognized
```
```python
=
```
```python
finalseg
```
```python
.
```
```python
cut
```
```python
(
```
```python
buf
```
```python
)
```
```python
for
```
```python
t
```
```python
in
```
```python
recognized
```
```python
:
```
```python
yield
```
```python
t
```
```python
else
```
```python
:
```
```python
for
```
```python
elem
```
```python
in
```
```python
buf
```
```python
:
```
```python
yield
```
```python
elem
                    buf
```
```python
=
```
```python
''
```
```python
yield
```
```python
l_word
```
```python
# 该节点取词完毕，跳到下一个词语的开始位置
```
```python
x
```
```python
=
```
```python
y
```
```python
# 通过yield，逐词返回上一步切分好的词语
```
```python
if
```
```python
buf
```
```python
:
```
```python
if
```
```python
len
```
```python
(
```
```python
buf
```
```python
)
```
```python
==
```
```python
1
```
```python
:
```
```python
yield
```
```python
buf
```
```python
elif
```
```python
not
```
```python
self
```
```python
.
```
```python
FREQ
```
```python
.
```
```python
get
```
```python
(
```
```python
buf
```
```python
)
```
```python
:
```
```python
recognized
```
```python
=
```
```python
finalseg
```
```python
.
```
```python
cut
```
```python
(
```
```python
buf
```
```python
)
```
```python
for
```
```python
t
```
```python
in
```
```python
recognized
```
```python
:
```
```python
yield
```
```python
t
```
```python
else
```
```python
:
```
```python
for
```
```python
elem
```
```python
in
```
```python
buf
```
```python
:
```
```python
yield
```
```python
elem
```
主体步骤如下
得到语句的有向无环图DAG
动态规划构建Route，计算从语句末尾到语句起始，DAG中每个节点到语句结束位置的最大路径概率，以及概率最大时节点对应词语的结束位置
遍历每个节点的Route，组装词语组合。
如果词语不在字典中，也就是新词，使用HMM隐马尔科夫模型进行分割
通过yield将词语逐个返回。
下面我们来看构建DAG的过程。先遍历一个个切分好的短语，对这些短语来进行分词。首先要构建短语的有向无环图DAG。查词典进行字符串匹配的过程中，可能会出现好几种可能的切分方式，将这些组合构成有向无环图，如下图所示
![image.png | left | 799x300](https://cdn.nlark.com/lark/0/2018/png/9304/1534379635245-7eebb000-4914-4e05-b10a-32eda11cc98b.png)
可以看到，构成了两条路径：
有意/见/分歧
有/意见/分歧
DAG中记录了某个词的开始位置和它可能的结束位置。开始位置作为key，结束位置是一个list。比如位置0的DAG表达为
{0: [1, 2]}, 也就是说0位置为词的开始位置时，1，2位置都有可能是词的结束位置。上面语句的完整DAG为
```python
{
```
```python
0
```
```python
:
```
```python
[
```
```python
1
```
```python
,
```
```python
2
```
```python
]
```
```python
,
```
```python
1
```
```python
:
```
```python
[
```
```python
2
```
```python
,
```
```python
3
```
```python
]
```
```python
,
```
```python
2
```
```python
:
```
```python
[
```
```python
3
```
```python
]
```
```python
,
```
```python
3
```
```python
:
```
```python
[
```
```python
4
```
```python
,
```
```python
5
```
```python
]
```
```python
,
```
```python
4
```
```python
:
```
```python
[
```
```python
5
```
```python
]
```
```python
}
```
DAG构建过程的代码如下：
```python
# 获取语句的有向无环图
```
```python
def
```
```python
get_DAG
```
```python
(
```
```python
self
```
```python
,
```
```python
sentence
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
check_initialized
```
```python
(
```
```python
)
```
```python
DAG
```
```python
=
```
```python
{
```
```python
}
```
```python
N
```
```python
=
```
```python
len
```
```python
(
```
```python
sentence
```
```python
)
```
```python
for
```
```python
k
```
```python
in
```
```python
xrange
```
```python
(
```
```python
N
```
```python
)
```
```python
:
```
```python
tmplist
```
```python
=
```
```python
[
```
```python
]
```
```python
i
```
```python
=
```
```python
k
        frag
```
```python
=
```
```python
sentence
```
```python
[
```
```python
k
```
```python
]
```
```python
while
```
```python
i
```
```python
<
```
```python
N
```
```python
and
```
```python
frag
```
```python
in
```
```python
self
```
```python
.
```
```python
FREQ
```
```python
:
```
```python
if
```
```python
self
```
```python
.
```
```python
FREQ
```
```python
[
```
```python
frag
```
```python
]
```
```python
:
```
```python
tmplist
```
```python
.
```
```python
append
```
```python
(
```
```python
i
```
```python
)
```
```python
i
```
```python
+=
```
```python
1
```
```python
frag
```
```python
=
```
```python
sentence
```
```python
[
```
```python
k
```
```python
:
```
```python
i
```
```python
+
```
```python
1
```
```python
]
```
```python
if
```
```python
not
```
```python
tmplist
```
```python
:
```
```python
tmplist
```
```python
.
```
```python
append
```
```python
(
```
```python
k
```
```python
)
```
```python
DAG
```
```python
[
```
```python
k
```
```python
]
```
```python
=
```
```python
tmplist
```
```python
return
```
```python
DAG
```
### 4.4 构建节点最大路径概率，以及结束位置
中文一般形容词在前面，而相对来说更关键的名词和动词在后面。考虑到这一点，jieba中对语句，从右向左反向计算路径的最大概率，这个类似于逆向最大匹配。每个词的概率 = 字典中该词的词数 / 字典总词数。对于上图构建每个节点的最大路径概率的过程如下：
```python
p
```
```python
(
```
```python
5
```
```python
)
```
```python
=
```
```python
1
```
```python
,
```
```python
p
```
```python
(
```
```python
4
```
```python
)
```
```python
=
```
```python
max
```
```python
(
```
```python
p
```
```python
(
```
```python
5
```
```python
)
```
```python
*
```
```python
p
```
```python
(
```
```python
4
```
```python
-
```
```python
>
```
```python
5
```
```python
)
```
```python
)
```
```python
,
```
```python
p
```
```python
(
```
```python
3
```
```python
)
```
```python
=
```
```python
max
```
```python
(
```
```python
p
```
```python
(
```
```python
4
```
```python
)
```
```python
*
```
```python
p
```
```python
(
```
```python
4
```
```python
-
```
```python
>
```
```python
5
```
```python
)
```
```python
,
```
```python
p
```
```python
(
```
```python
5
```
```python
)
```
```python
*
```
```python
p
```
```python
(
```
```python
3
```
```python
-
```
```python
>
```
```python
5
```
```python
)
```
```python
)
```
```python
,
```
```python
# 对于节点3，他有3->4, 3->5两条路径，我们取概率最大的路径作为节点3的路径概率，并记下概率最大时节点3的结束位置
```
```python
p
```
```python
(
```
```python
2
```
```python
)
```
```python
=
```
```python
max
```
```python
(
```
```python
p
```
```python
(
```
```python
3
```
```python
)
```
```python
*
```
```python
p
```
```python
(
```
```python
2
```
```python
-
```
```python
>
```
```python
3
```
```python
)
```
```python
)
```
```python
p
```
```python
(
```
```python
1
```
```python
)
```
```python
=
```
```python
max
```
```python
(
```
```python
p
```
```python
(
```
```python
2
```
```python
)
```
```python
*
```
```python
p
```
```python
(
```
```python
1
```
```python
-
```
```python
>
```
```python
2
```
```python
)
```
```python
,
```
```python
p
```
```python
(
```
```python
3
```
```python
)
```
```python
*
```
```python
p
```
```python
(
```
```python
1
```
```python
-
```
```python
>
```
```python
3
```
```python
)
```
```python
)
```
```python
p
```
```python
(
```
```python
0
```
```python
)
```
```python
=
```
```python
max
```
```python
(
```
```python
p
```
```python
(
```
```python
1
```
```python
)
```
```python
*
```
```python
p
```
```python
(
```
```python
0
```
```python
-
```
```python
>
```
```python
1
```
```python
)
```
```python
,
```
```python
p
```
```python
(
```
```python
2
```
```python
)
```
```python
*
```
```python
p
```
```python
(
```
```python
0
```
```python
-
```
```python
>
```
```python
2
```
```python
)
```
```python
)
```
对应代码如下
```python
def
```
```python
calc
```
```python
(
```
```python
self
```
```python
,
```
```python
sentence
```
```python
,
```
```python
DAG
```
```python
,
```
```python
route
```
```python
)
```
```python
:
```
```python
N
```
```python
=
```
```python
len
```
```python
(
```
```python
sentence
```
```python
)
```
```python
route
```
```python
[
```
```python
N
```
```python
]
```
```python
=
```
```python
(
```
```python
0
```
```python
,
```
```python
0
```
```python
)
```
```python
logtotal
```
```python
=
```
```python
log
```
```python
(
```
```python
self
```
```python
.
```
```python
total
```
```python
)
```
```python
for
```
```python
idx
```
```python
in
```
```python
xrange
```
```python
(
```
```python
N
```
```python
-
```
```python
1
```
```python
,
```
```python
-
```
```python
1
```
```python
,
```
```python
-
```
```python
1
```
```python
)
```
```python
:
```
```python
# route[idx] = (该汉字到最后一个汉字的最大路径概率， 最大路径概率时该汉字对应的词语结束位置)
```
```python
# 遍历DAG中该汉字节点的结束位置，也就是DAG[idx]，计算idx到x之间构成的词语的概率，然后乘以x到语句结束位置的最大概率，即可得到idx到语句结束的路径最大概率
```
```python
route
```
```python
[
```
```python
idx
```
```python
]
```
```python
=
```
```python
max
```
```python
(
```
```python
(
```
```python
log
```
```python
(
```
```python
self
```
```python
.
```
```python
FREQ
```
```python
.
```
```python
get
```
```python
(
```
```python
sentence
```
```python
[
```
```python
idx
```
```python
:
```
```python
x
```
```python
+
```
```python
1
```
```python
]
```
```python
)
```
```python
or
```
```python
1
```
```python
)
```
```python
-
```
```python
logtotal
```
```python
+
```
```python
route
```
```python
[
```
```python
x
```
```python
+
```
```python
1
```
```python
]
```
```python
[
```
```python
0
```
```python
]
```
```python
,
```
```python
x
```
```python
)
```
```python
for
```
```python
x
```
```python
in
```
```python
DAG
```
```python
[
```
```python
idx
```
```python
]
```
```python
)
```
### 4.5 构建切分组合
从节点0开始，按照步骤4中构建的最大路径概率以及结束位置，取出节点0的结束位置，构成词语。如果是单字词语，则直接通过yield返回。如果词语在字典中，也直接通过yield返回。如果词语不在字典中，也就是新词，则需要通过HMM隐马尔科夫模型来分割。节点0处理完毕，则跳到下一个词语的开始处进行处理，直至到达语句末尾。
代码参见__cut_DAG()，也就是主体流程代码。
### 4.6 HMM新词处理
对于新词，也就是dict.txt中没有的词语，我们通过统计方法来处理，jieba中采用了HMM隐马尔科夫模型。回顾下HMM的五要素：观测序列，隐藏序列，发射概率，起始概率，转移概率。由这五大要素可以对我们的短语建模。
观测序列：语句本身，我们能看见的
隐藏序列：由BMES构成的分词标注序列，上篇文章详细讲解了的。每个汉字可以由BMES来进行标注，B表示词语的开始，M词语中间，E词语结束，S单字词语。比如“有意见分歧”对应的标注有两种，为SBEBE和BESBE，分别对应分词序列“有/意见/分歧”和“有意/见/分歧”。
发射概率：隐藏值到观测值的概率，比如S是汉字“有”的概率。
起始概率：隐藏值起始概率，起始只能是B或者S，通过语料大规模训练可以得到B和S作为起始的概率。结果为{‘B’: 0.769, ‘E’: 0, ‘M’: 0, ‘S’: 0.231}，可见起始为B的概率要远大于S，这也符合我们通常情况。
转移概率：隐藏值之间转移的概率，比如B->E, 表示为P(E|B), B->M, 表示为p(M|B)
通过语料大规模训练，可以得到发射概率，起始概率和转移概率。通过viterbi算法，可以得到概率最大的隐藏序列，也就是    BEMS标注序列，通过BEMS就可以对语句进行分词了。我们观察发现，新词被分成二字词语的概率很大。
转移概率在prob_trans.py中，如下
```python
P
```
```python
=
```
```python
{
```
```python
'B'
```
```python
:
```
```python
{
```
```python
'E'
```
```python
:
```
```python
-
```
```python
0.510825623765990
```
```python
,
```
```python
'M'
```
```python
:
```
```python
-
```
```python
0.916290731874155
```
```python
}
```
```python
,
```
```python
# exp后为概率，此处为{'E': 0.6, 'M': 0.4}
```
```python
'E'
```
```python
:
```
```python
{
```
```python
'B'
```
```python
:
```
```python
-
```
```python
0.5897149736854513
```
```python
,
```
```python
'S'
```
```python
:
```
```python
-
```
```python
0.8085250474669937
```
```python
}
```
```python
,
```
```python
'M'
```
```python
:
```
```python
{
```
```python
'E'
```
```python
:
```
```python
-
```
```python
0.33344856811948514
```
```python
,
```
```python
'M'
```
```python
:
```
```python
-
```
```python
1.2603623820268226
```
```python
}
```
```python
,
```
```python
'S'
```
```python
:
```
```python
{
```
```python
'B'
```
```python
:
```
```python
-
```
```python
0.7211965654669841
```
```python
,
```
```python
'S'
```
```python
:
```
```python
-
```
```python
0.6658631448798212
```
```python
}
```
```python
}
```
起始概率在prob_start.py中，如下
```python
P
```
```python
=
```
```python
{
```
```python
'B'
```
```python
:
```
```python
-
```
```python
0.26268660809250016
```
```python
,
```
```python
'E'
```
```python
:
```
```python
-
```
```python
3.14e+100
```
```python
,
```
```python
'M'
```
```python
:
```
```python
-
```
```python
3.14e+100
```
```python
,
```
```python
'S'
```
```python
:
```
```python
-
```
```python
1.4652633398537678
```
```python
}
```
```python
# exp后为概率，此处为{'B': 0.769, 'E': 0, 'M': 0, 'S': 0.231}
```
隐马尔科夫模型处理代码主要为
```python
# 通过HMM隐马尔科夫模型获取语句的BEMS序列标注，并通过它来进行分词
```
```python
def
```
```python
__cut
```
```python
(
```
```python
sentence
```
```python
)
```
```python
:
```
```python
global
```
```python
emit_P
```
```python
# 通过viterbi算法和start_P, trans_P, emit_P三个训练好的概率，得到语句对应的BEMS序列标注
```
```python
prob
```
```python
,
```
```python
pos_list
```
```python
=
```
```python
viterbi
```
```python
(
```
```python
sentence
```
```python
,
```
```python
'BMES'
```
```python
,
```
```python
start_P
```
```python
,
```
```python
trans_P
```
```python
,
```
```python
emit_P
```
```python
)
```
```python
begin
```
```python
,
```
```python
nexti
```
```python
=
```
```python
0
```
```python
,
```
```python
0
```
```python
# 得到分词结果。根据上面得到pos_list, 也就是语句对应的BEMS序列，来对原始语句进行分词。
```
```python
for
```
```python
i
```
```python
,
```
```python
char
```
```python
in
```
```python
enumerate
```
```python
(
```
```python
sentence
```
```python
)
```
```python
:
```
```python
pos
```
```python
=
```
```python
pos_list
```
```python
[
```
```python
i
```
```python
]
```
```python
if
```
```python
pos
```
```python
==
```
```python
'B'
```
```python
:
```
```python
# 词语开始
```
```python
begin
```
```python
=
```
```python
i
```
```python
elif
```
```python
pos
```
```python
==
```
```python
'E'
```
```python
:
```
```python
# 词语结束，可以根据begin开始位置来返回分词词语了
```
```python
yield
```
```python
sentence
```
```python
[
```
```python
begin
```
```python
:
```
```python
i
```
```python
+
```
```python
1
```
```python
]
```
```python
nexti
```
```python
=
```
```python
i
```
```python
+
```
```python
1
```
```python
elif
```
```python
pos
```
```python
==
```
```python
'S'
```
```python
:
```
```python
# 单字词语，直接返回
```
```python
yield
```
```python
char
            nexti
```
```python
=
```
```python
i
```
```python
+
```
```python
1
```
```python
# 理论上不会走到下面这儿，只是以防万一
```
```python
if
```
```python
nexti
```
```python
<
```
```python
len
```
```python
(
```
```python
sentence
```
```python
)
```
```python
:
```
```python
yield
```
```python
sentence
```
```python
[
```
```python
nexti
```
```python
:
```
```python
]
```
viterbi算法的代码如下
```python
# 通过viterbi算法，由观测序列，也就是语句，来得到隐藏序列，也就是BEMS标注序列
```
```python
# obs为语句，states为"BEMS"四种状态，
```
```python
# start_p为起始概率, trans_p为转移概率, emit_p为发射概率，三者通过语料训练得到
```
```python
def
```
```python
viterbi
```
```python
(
```
```python
obs
```
```python
,
```
```python
states
```
```python
,
```
```python
start_p
```
```python
,
```
```python
trans_p
```
```python
,
```
```python
emit_p
```
```python
)
```
```python
:
```
```python
V
```
```python
=
```
```python
[
```
```python
{
```
```python
}
```
```python
]
```
```python
# 每个汉字的每个BEMS状态的最大概率。
```
```python
path
```
```python
=
```
```python
{
```
```python
}
```
```python
# 分词路径
```
```python
# 初始化每个state，states为"BEMS"
```
```python
for
```
```python
y
```
```python
in
```
```python
states
```
```python
:
```
```python
V
```
```python
[
```
```python
0
```
```python
]
```
```python
[
```
```python
y
```
```python
]
```
```python
=
```
```python
start_p
```
```python
[
```
```python
y
```
```python
]
```
```python
+
```
```python
emit_p
```
```python
[
```
```python
y
```
```python
]
```
```python
.
```
```python
get
```
```python
(
```
```python
obs
```
```python
[
```
```python
0
```
```python
]
```
```python
,
```
```python
MIN_FLOAT
```
```python
)
```
```python
path
```
```python
[
```
```python
y
```
```python
]
```
```python
=
```
```python
[
```
```python
y
```
```python
]
```
```python
# 逐字进行处理
```
```python
for
```
```python
t
```
```python
in
```
```python
xrange
```
```python
(
```
```python
1
```
```python
,
```
```python
len
```
```python
(
```
```python
obs
```
```python
)
```
```python
)
```
```python
:
```
```python
V
```
```python
.
```
```python
append
```
```python
(
```
```python
{
```
```python
}
```
```python
)
```
```python
newpath
```
```python
=
```
```python
{
```
```python
}
```
```python
# 遍历每个状态
```
```python
for
```
```python
y
```
```python
in
```
```python
states
```
```python
:
```
```python
# 得到某状态到某个字的发射概率
```
```python
em_p
```
```python
=
```
```python
emit_p
```
```python
[
```
```python
y
```
```python
]
```
```python
.
```
```python
get
```
```python
(
```
```python
obs
```
```python
[
```
```python
t
```
```python
]
```
```python
,
```
```python
MIN_FLOAT
```
```python
)
```
```python
# 计算前一个状态到本状态的最大概率和它的前一个状态
```
```python
(
```
```python
prob
```
```python
,
```
```python
state
```
```python
)
```
```python
=
```
```python
max
```
```python
(
```
```python
[
```
```python
(
```
```python
V
```
```python
[
```
```python
t
```
```python
-
```
```python
1
```
```python
]
```
```python
[
```
```python
y0
```
```python
]
```
```python
+
```
```python
trans_p
```
```python
[
```
```python
y0
```
```python
]
```
```python
.
```
```python
get
```
```python
(
```
```python
y
```
```python
,
```
```python
MIN_FLOAT
```
```python
)
```
```python
+
```
```python
em_p
```
```python
,
```
```python
y0
```
```python
)
```
```python
for
```
```python
y0
```
```python
in
```
```python
PrevStatus
```
```python
[
```
```python
y
```
```python
]
```
```python
]
```
```python
)
```
```python
# 将该汉字下的某状态（BEMS）的最大概率记下来
```
```python
V
```
```python
[
```
```python
t
```
```python
]
```
```python
[
```
```python
y
```
```python
]
```
```python
=
```
```python
prob
```
```python
# 记录状态转换路径
```
```python
newpath
```
```python
[
```
```python
y
```
```python
]
```
```python
=
```
```python
path
```
```python
[
```
```python
state
```
```python
]
```
```python
+
```
```python
[
```
```python
y
```
```python
]
```
```python
path
```
```python
=
```
```python
newpath
```
```python
# 尝试合并ES两种状态，因为ES经常可以组成一个完整词语
```
```python
(
```
```python
prob
```
```python
,
```
```python
state
```
```python
)
```
```python
=
```
```python
max
```
```python
(
```
```python
(
```
```python
V
```
```python
[
```
```python
len
```
```python
(
```
```python
obs
```
```python
)
```
```python
-
```
```python
1
```
```python
]
```
```python
[
```
```python
y
```
```python
]
```
```python
,
```
```python
y
```
```python
)
```
```python
for
```
```python
y
```
```python
in
```
```python
'ES'
```
```python
)
```
```python
# 返回语句的BEMS序列
```
```python
return
```
```python
(
```
```python
prob
```
```python
,
```
```python
path
```
```python
[
```
```python
state
```
```python
]
```
```python
)
```
### 4.7 返回分词结果
通过yield将上面步骤中切分好的词语逐个返回。yield相对于list，可以节约存储空间。
# 5 总结
jiaba分词是一款十分优秀的开源分词引擎，它结合了基于字符串匹配的算法和基于统计的算法。使用最大概率路径动态规划算法，进行字符串匹配，可以在分词速度快的同时，保持较高的分词精度。使用HMM隐马尔科夫模型对新词进行分词，可以有效解决字符串匹配无法识别新词的难点。阅读它的源码有利于我们加深对分词难点和算法的理解，也能加深对HMM隐马尔卡尔模型这种常用的机器学习算法的理解。
系列文章，请多关注
[Tensorflow源码解析1 – 内核架构和源码结构](https://blog.csdn.net/u013510838/article/details/84103503)
[带你深入AI（1） - 深度学习模型训练痛点及解决方法](https://blog.csdn.net/u013510838/article/details/79835563)
[自然语言处理1 – 分词](https://blog.csdn.net/u013510838/article/details/81673016)
[自然语言处理2 – jieba分词用法及原理](https://blog.csdn.net/u013510838/article/details/81738431)
[自然语言处理3 – 词性标注](https://blog.csdn.net/u013510838/article/details/81907121)
[自然语言处理4 – 句法分析](https://blog.csdn.net/u013510838/article/details/81976427)
[自然语言处理5 – 词向量](https://blog.csdn.net/u013510838/article/details/82108381)
[自然语言处理6 – 情感分析](https://blog.csdn.net/u013510838/article/details/82558797)

