
# 学 AI 和机器学习的人必须关注的 6 个领域 - 喜欢打酱油的老鸟 - CSDN博客


2018年09月13日 08:55:21[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：180


[https://mp.weixin.qq.com/s/wbJYoqcrKR3dG7Fo2-kK4g](https://mp.weixin.qq.com/s/wbJYoqcrKR3dG7Fo2-kK4g)
近期热门的话题， 人们开始重新讨论这一基本定义----什么是人工智能（AI）。有些人将 AI 重新命名为「认知计算」或「机器智能」，而其他人则错误地将 AI 与「机器学习」概念进行交换。在某种程度上，这是因为 AI 不是一种技术。它实际上是一个由许多学科组成的广泛领域，从机器人学到机器学习。我们大多数人都认为，人工智能的终极目标是为了建造能够完成任务和认知功能的机器，否则这些机器只能在人类的智能范围内从事相关工作。为了实现这一目标，机器必须能够自主学习这些能力，而不是让每个功能都被端到端地明确编程。
人工智能领域在过去十年中取得了巨大进步，从自动驾驶汽车到语音识别及合成，这一点令人惊讶。在这种背景下，人工智能已经成为越来越多公司和家庭的话题，他们不再将人工智能视为一种需要 20 年时间开发的技术，而是影响他们今天生活的东西。事实上，流行的新闻报道几乎每天都会报道 AI 和技术巨头，阐述他们重要的长期人工智能策略。虽然一些投资者和老牌企业都渴望了解如何在这个新世界中攫取价值，但大多数人仍在摸索着想出这一切意味着什么。与此同时，各国政府正在努力应对自动化在社会中的影响（见奥巴马的告别演说）。
鉴于 AI 将影响整个经济，而这些讨论中的参与者代表了社会上观点的整体分布、理解水平以及构建或使用 AI 系统的经验程度。因此，对人工智能的讨论至关重要—包括由此产生的问题、结论和建议—必须以数据和现实为基础，而不是猜想，这点至关重要。毕竟各种民间大V从公布的研究、科技新闻公告、投机评论和思想实验中大肆推断其中的含义，这太容易了（有时令人兴奋！）。
尤其值得注意人工智能的六个领域在影响数字产品和服务的未来方面产生的作用。我将会阐述它们分别是什么、为什么它们很重要、它们今天如何被使用，并列出了从事这些技术的公司和研究人员的清单（并非详尽无遗）。

## 1、强化学习（RL）
RL 是一种通过试错来学习的范例，这种反复试错受到人类学习新任务的方式启发。在典型的 RL 设置中，智能体的任务是在数字环境中观察其当前状态并采取最大化其已设置的长期奖励的累积的动作。 该智能体接收来自环境的每个动作结果的反馈，以便它知道该动作是否促进或阻碍其进展。因此，RL 的 智能体必须平衡对其环境的探索，以找到获得奖励的最佳策略，并利用其发现的最佳策略来实现预期目标。这种方法在 Google DeepMind 的 Atari 游戏和 Go 中（https://www.youtube.com/watch?v=Ih8EfvOzBOY）非常流行。RL 在现实世界中工作的一个例子是优化能源效率以冷却 Google 数据中心。在此项目中，RL 使得该系统的冷却成本降低了 40％。在可以模拟的环境（例如视频游戏）中使用 RL 智能体的一个重要的原生优势是训练数据可以以非常低的成本生成。这与监督式的深度学习任务形成鲜明对比，后者通常需要昂贵且难以从现实世界中获取的训练数据。
应用程序：多个智能体在他们自己的环境实例中学习共享模型，或者通过在同一环境中相互交互和学习，学习在迷宫或城市街道等 3D 环境中进行自动驾驶，通过学习任务目标（例如学习驾驶或赋予非玩家视频游戏角色以类似人的行为）反向强化学习以概括观察到的行为。
顶尖专业：Pieter Abbeel（OpenAI），David Silver，Nando de Freitas，Raia Hadsell，Marc Bellemare（谷歌 DeepMind），Carl Rasmussen（剑桥），Rich Sutton（阿尔伯塔大学），John Shawe-Taylor（UCL）等。
代表公司：Google DeepMind，Prowler.io，Osaro，MicroPSI，Maluuba / Microsoft，NVIDIA，Mobileye，OpenAI。

## 2、生成模型
与用于分类或回归任务的判别模型不同，生成模型学习训练样本的概率分布。通过从这种高维分布中抽样，生成模型输出与训练数据类似的新例子。这意味着，例如，在面部的真实图像上训练的生成模型可以输出相似面部的新合成图像。有关这些模型如何工作的更多详细信息，请参阅 Ian Goodfellow 的 NIPS 2016 指导手册（https://arxiv.org/abs/1701.00160）。他引入的架构，生成对抗网络（GAN），现在在研究领域特别热门，因为它们为无监督学习提供了一条道路。对于 GAN，有两个神经网络：一个生成器，它将随机噪声作为输入，负责合成内容（例如一个图像），一个鉴别器，它了解了真实图像的样子，并负责识别生成器生成的图像是真实的还是伪造的。对抗训练可以被认为是一种游戏，其中生成器必须迭代地学习如何从噪声创建图像，使得鉴别器不再能够将生成的图像与真实的图像区分开。该框架正在扩展到许多数据模式和任务。
应用范围：模拟时间序列的可能未来（例如，用于强化学习中的规划任务）；超分辨率图像；从 2D 图像重建 3D 结构； 从小标记数据集推广；一个输入可以产生多个正确输出的任务（例如，预测视频 0 中的下一帧；在会话界面中运用自然语言处理（例如机器人）；加密；当不是所有标签都可用时运用半监督学习；艺术风格转移；合成音乐和声音；图像修复。
代表公司：Twitter Cortex，Adobe，Apple，Prisma，Jukedeck*，Creative.ai，Gluru*，Mapillary*，Unbabel。
顶尖专家：Ian Goodfellow (OpenAI)，Yann LeCun and Soumith Chintala (Facebook AI Research)，Shakir Mohamed and Aäron van den Oord (Google DeepMind)，Alyosha Efros  (Berkeley) and 其他的专家。

## 3、记忆网络
为了让 AI 系统像我们一样在不同的环境中都能得到适用，他们必须能够不断学习新任务并记住如何在未来完成所有任务。然而，传统的神经网络通常不能进行这种连续的任务学习。这个缺点被称为灾难性遗忘。之所以出现这种情况，是因为当网络随后经过训练以解决任务 B 时，网络中对于任务 A 来说很重要的权重会发生变化。
然而，有几种强大的架构可以赋予神经网络不同程度的记忆性。这些包括能够处理和预测时间序列的长短期记忆网络（递归神经网络的一种变体），DeepMind 的可微分神经计算机，它结合了神经网络和记忆系统，以便自己学习和导航复杂的数据结构，弹性权重合并算法，根据它们对先前看到的任务的重要程度，减慢对某些权重的学习，以及学习特定任务的模型之间的横向连接的渐进式神经网络，以从先前学习的网络中为新任务提取有用的特征。
应用范围：可以推广到新环境的学习智能体；机器人手臂控制系统；自动驾驶汽车；时间序列预测（例如金融市场、视频、物联网）；      自然语言处理和下一步预测。
代表公司：Google DeepMind，NNaisense，SwiftKey/Microsoft Research，Facebook AI Research。
顶尖专家：Alex Graves，Raia Hadsell，Koray Kavukcuoglu（Google DeepMind），Jürgen Schmidhuber（IDSIA），Geoffrey Hinton（Google Brain/Toronto），James Weston，Sumit Chopra，Antoine Bordes（FAIR）。

## 4、从较少数据学习并构建更小的模型
深度学习模型值得注意的是需要大量的训练数据才能达到最先进的性能。例如，ImageNet 大规模视觉识别挑战赛中，每支队伍需要挑战他们的图像识别模型，包含 120 万个手工标记 1000 个对象类别的训练图像。如果没有大规模的训练数据，深度学习模型将无法收敛于其最佳设置，并且在语音识别或机器翻译等复杂任务上表现不佳。只有当单个神经网络用于端到端解决问题时，此数据要求才会增长；也就是说，将语音的原始录音作为输入并输出语音的文本转录。这与使用多个网络形成对比，每个网络各自提供中间表示（例如，原始语音音频输入→音位→单词→文本转录输出；或来自直接映射到转向命令的相机的原始图像）。如果我们希望 AI 系统能够解决训练数据特别具有挑战性、成本高、敏感或耗时的任务，那么开发能够从较少的样本（即一次或零次学习）学习最佳解决方案的模型非常重要。在对小型数据集进行培训时，难点包括过度拟合，处理异常值的困难，训练和测试之间数据分布的差异。另一种方法是通过使用统称为迁移学习的过程来迁移从先前任务获得的机器学习模型的知识来改进新任务的学习。
一个相关的问题是使用类似数量或明显更少的参数构建具有最先进性能的较小的深度学习架构。优点包括更高效的分布式培训，因为数据需要在服务器之间进行通信，将新模型从云端导出到外围设备的带宽更少，以及部署到内存有限的硬件的可行性得到提高。
应用范围：通过学习模拟最初训练大型标记训练数据的深层网络的性能来训练浅层网络；具有较少参数但与深度模型具有相同性能的架构（例如 SqueezeNet）；机器翻译。
代表企业：Geometric Intelligence/Uber，DeepScale.ai，Microsoft Research，Curious AI Company，Google，Bloomsbury AI。
顶尖专家：Zoubin Ghahramani（Cambridge），Yoshua Bengio（Montreal），Josh Tenenbaum（MIT），Brendan Lake（NYU），Oriol Vinyals（Google DeepMind），Sebastian Riedel（UCL）。

## 5、用于训练和预测的硬件
人工智能进步的主要催化剂是重新利用图形处理单元（GPU）来训练大型神经网络模型。与以顺序方式计算的中央处理单元（CPU）不同，GPU 提供可以同时处理多个任务的大规模并行架构。鉴于神经网络必须处理大量（通常是高维数据），GPU 上的训练要比使用 CPU 快得多。这就是为什么自 2012 年 AlexNet 发布以来 GPU 已经真正成为淘金热的原因 - 这是 GPU 上实现的第一个神经网络。在 2017 年 NVIDIA 继续领先于英特尔，高通，AMD 以及谷歌。
但是，GPU 不是专门用于训练或预测的；它们是为了呈现视频游戏的图形而创建的。 GPU具有高计算精度，但并不总是需要，并且存在内存带宽和数据吞吐量问题。这为像 Google 这样的大公司的新一代初创项目开辟了公平的竞争环境，专门为高维机器学习应用设计和生产芯片。新芯片设计承诺的改进包括更大的内存带宽，图形而非矢量（GPU）或标量（CPU）计算，更高的计算密度，效率和耗能性能。这是令人兴奋的，因为 AI 系统为其所有者和用户提供了明显的加速回报：更快，更高效的模型培训→更好的用户体验→用户更多地参与产品→创建更大的数据集→通过优化提高模型性能。因此，能够更快地训练并部署计算和能量效率的 AI 模型的人会占据显著的优势。
应用范围：加快模型训练（特别是图表）；预测能源和数据效率；在外围运行 AI 系统（物联网设备）；实时活动的 IoT 设备；云基础服务；自动驾驶汽车，无人机和机器人。
代表企业： Graphcore，Cerebras，Isocline Engineering，Google（TPU），NVIDIA（DGX-1），Nervana      Systems（Intel），Movidius（Intel），Scortex。

## 6、仿真环境
如前所述，为 AI 系统生成训练数据通常具有挑战性。 更重要的是，如果 AI 在现实世界中对我们有用，那么它必须可以推广到很多情况。 因此，开发模拟现实世界的物理和行为的数字环境将为我们提供测试和训练 AI 的一般智能的测试平台。 这些环境向 AI 呈现初始情况，然后 AI 采取行动以解决他们已设置（或学习）的目标。 在这些模拟环境中进行训练可以帮助我们了解 AI 系统如何学习，如何改进它们，同时也为我们提供可能转移到实际应用程序的模型。
应用范围：驾驶技能学习；制造业；工业设计；游戏开发；智慧城市。
代表企业： Improbable，Unity 3D，Microsoft（Minecraft），Google DeepMind/Blizzard，OpenAI，Comma.ai，Unreal Engine，Amazon Lumberyard。
专家：Andrea Vedaldi（Oxford）。

原文链接：https://medium.com/@NathanBenaich/6-areas-of-artificial-intelligence-to-watch-closely-673d590aa8aa

