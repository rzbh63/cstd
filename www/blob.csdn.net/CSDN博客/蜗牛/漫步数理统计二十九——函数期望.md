
# 漫步数理统计二十九——函数期望 - 蜗牛 - CSDN博客


2017年05月07日 21:46:40[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：470


令$\mathbf{X}=(X_1,\ldots,X_n)^\prime$表示某试验的随机变量，我们一般对$\mathbf{X}$的函数感兴趣，表示为$T=T(\mathbf{X})$。例如如果$\mathbf{X}$是一个样本，$T$可能是我们感兴趣的统计量。我们先从$\mathbf{X}$的线性函数开始；例如对某个特定的向量$\mathbf{a}=(a_1,\ldots,a_n)^\prime$，

$$
T=\mathbf{a^\prime X}=\sum_{i=1}^na_iX_i
$$
然后我们会得到这种随机变量的均值与方差。
$T$的均值根据期望运算的线性性质可以立刻得出，如下定理所示：
$\textbf{定理1：}$令$T=\sum_{i=1}^na_iX_i$，假设对$i=1,\ldots,n,E[|X_i|]<\infty$，那么

$$
E(T)=\sum_{i=1}^na_iE(X_i)
$$
对于$T$的方差，我们先给出涉及到协方差的一个结论。令$\mathbf{Y}=(Y_1,\ldots,Y_m)^\prime$表示另一个随机向量，对某个特定的向量$\mathbf{b}=(b_1,\ldots,b_m)^\prime,W=\mathbf{b^\prime Y}$。
$\textbf{定理2：}$令$T=\sum_{i=1}^na_iX_i,W=\sum_{i-1}^mb_iY_i$，如果对$i=1,\ldots,n,j=1,\ldots,m,E[X_i^2]<\infty,E[Y_j^2]<\infty$，那么

$$
cov(T,W)=\sum_{i=1}^n\sum_{j=1}^ma_ib_jcov(X_i,Y_j)
$$
$\textbf{证明：}$根据协方差的定义以及定理1，我们可得

$$
\begin{align*}
cov(T,W)
&=E[\sum_{i=1}^n\sum_{j=1}^m(a_iX_i-a_iE(X_i))(b_jY_j-b_jE(Y_j))]\\
&=\sum_{i=1}^n\sum_{j=1}^ma_ib_jE[(x_i-E(X_i))(Y_j-E(Y_j))]
\end{align*}
$$
得证。$||$
为了求出$T$的方差，我们用$T$替换定理2中的$W$，从而得到下面的推论：
$\textbf{推论1：}$令$T=\sum_{i=1^n}a_iX_i$，假设对于$i=1,\ldots,n,E[X_i^2]<\infty$，

$$
\begin{equation}
var(T)=cov(T,T)=\sum_{i=1}^na_i^2var(X_i)+2\sum_{i<j}a_ia_jcov(X_i,X_j)\tag1
\end{equation}
$$
注意如果$X_1,\ldots,X_n$是独立的随机变量，那么$cov(X_i,X_j)=0$，从而$(1)$得到进一步简化，如下面的推论：
$\textbf{推论2：}$如果$X_1,\ldots,X_n$是拥有有限个变量的独立随机变量，那么

$$
\begin{equation}
var(T)=\sum_{i=1}^na_i^2var(X_i)\tag2
\end{equation}
$$
注意只需要对所有的$i\neq j,X_i,X_j$不相干即可得到这个结论；例如当$X_1,\ldots,X_n$是独立的，那么$cov(X_i,X_j)=0,i\neq j$。
考虑我们有一个感兴趣的随机变量$X$，它的密度为$f(x:\theta)$，其中$\theta\in\Omega$，参数$\theta$是未知的且我们需要基于样本估计它，关于估计的第一个性质就是它的期望。
$\textbf{定义1：}$令$X$是随机变量，pdf为$f(x:\theta)$或者pmf为$p(x:\theta)$，$\theta\in\Omega$。令$X_1,\ldots,X_n$是来自$X$分布的随机样本并令$T$表示一个统计量。我们称$T$为$\theta$的无偏估计，如果

$$
\begin{equation}
E(T)=\theta,\ for\ all\ \theta\in\Omega\tag3
\end{equation}
$$
如果$T$不是无偏的(即，$E(T)=\neq\theta$) ，我们称$T$是$\theta$的有偏估计。
$\textbf{例1：}$令$X_1,\ldots,X_n$是均值为$\mu$，方差为$\sigma^2$的随机变量$X$的分布中随机得到的样本，回忆一下样本均值为$\bar{X}=n^{-1}\sum_{i=1}^nX_i$，它是样本观测值的线性组合，系数为$a_i=n^{-1}$；因此根据定理1与推论2我们有

$$
E(\bar{X})=\mu,\quad var(\bar{X})=\frac{\sigma^2}{n}
$$
因此$\bar{X}$是$\mu$的无偏估计。进一步，$\bar{X}$的方差在$n$很大时非常小。从极限角度来说就是当$n$无限大时，样本均值$\bar{X}$收敛到$\mu$。
$\textbf{例2：}$$X_1,\ldots,X_n$如上例所示，样本方差定义为

$$
S^2=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X})^2=(n-1)^{-1}\left(\sum_{i=1}^nX_i^2-n\bar{X}^2\right)
$$
利用上例的结论以及$E(X^2)=\sigma^2+\mu^2$可得

$$
\begin{align*}
E(S^2)
&=(n-1)^{-1}\left(\sum_{i=1}^nE(X_i^2)-nE(\bar{X}^2)\right)\\
&=(n-1)^{-1}\{n\sigma^2+n\mu^2-n[(\sigma^2/n+\mu^2)]\}\\
&=\sigma^2
\end{align*}
$$
因此样本方差是$\sigma^2$的无偏估计。如果$V=n^{-1}\sum_{i-1}^n(X_i-\bar{X})^2$，那么$E(V)=((n-1)/n)sigma^2$，也就是说$V$是$\sigma^2$的无偏估计，这也就是为何我们用$n-1$而不是$n$。
$\textbf{例3：}$令$X_1,\ldots,X_n$是均匀分布$(0,\theta)$的随机样本，假设$\theta$未知，$\theta$的直观估计为样本的最大值。令$Y_n=\max\{X_1,\ldots,X_n\}$，那么$Y_n$的cdf为

$$
F_{Y_n}(t)=
\begin{cases}
1&t>\theta\\
(\frac{t}{\theta})^n&0<t\leq\theta\\
0&t\leq 0
\end{cases}
$$
因此$Y_n$的pdf为

$$
f_{Y_n}(t)=
\begin{cases}
\frac{n}{\theta^n}t^{n-1}&0<t\leq\theta\\
0&elsewhere
\end{cases}
$$
基于这个pdd可得$E(Y_n)=(n/(n+1))\theta$，所以$Y_n$是$\theta$的有偏估计，注意$((n+1)/n)Y_n$是$\theta$的无偏估计。
$\textbf{例4：}$$X_1,\ldots,X_n$随机变量$X$分布的随机样本，该变量的pdf为$f(x)$。假设$\mu=E(X)$存在，进一步假设pdf关于$\mu$对称，例1已经说明样本均值是$\mu$的无偏估计，那么样本中值$T=T(X_1,X_2,\ldots,X_n)=med\{X_1,X_2,\ldots,X_n\}$呢？样本中值满足两个性质：(1)如果样本增加(或减少)$b$，那么中值也增加(或减少)$b$。(2)如果样本均乘以-1，那么中值也乘以-1。我们将这两个性质简写成：

$$
\begin{align*}
T(X_1+b,X_2+b,\ldots,X_n+b)&=T(X_1,X_2,\ldots,X_n)+b\\
T(-X_1,-X_2,\ldots,-X_n)&=-T(X_1,X_2,\ldots,X_n)
\end{align*}
$$
如果$X_i$关于$\mu$对称，那么随机向量$(X_1-\mu,\ldots,X_n-\mu)$与随机向量$(-(X_1-\mu),\ldots,-(X_n-\mu))$的分布是一样的，特别的他们的期望是一样的。由上面的结论可得：

$$
\begin{align*}
E[T]-\mu
&=E[T(X_1,\ldots,X_n)]-\mu=E[T(X_1-\mu,\ldots,X_n-\mu)]\\
&=E[T(-(X_1-\mu),\ldots,-(X_n-\mu))]\\
&=-E[T(X_1-\mu_1,\ldots,X_n-\mu)]\\
&-E[T(X_1,\ldots,X_n)]+\mu=-E[T]+\mu
\end{align*}
$$
即$2E(T)=2\mu$，所以$E[T]=\mu$。在上面两个性质的条件下，样本中值是$\theta$的无偏估计。那么样本均值与样本中值那个更好呢？后面的文章会详细介绍。

