
# 探索强化学习算法背后的思想起源！ - 喜欢打酱油的老鸟 - CSDN博客


2019年01月09日 08:07:13[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：65标签：[强化学习																](https://so.csdn.net/so/search/s.do?q=强化学习&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)


[https://www.toutiao.com/a6630657888442384909/](https://www.toutiao.com/a6630657888442384909/)
![探索强化学习算法背后的思想起源！](http://p9.pstatp.com/large/pgc-image/00c80eb3367b4653a141e75a8b46355c)
接受生物大脑的混乱和电子大脑的秩序
人们对人工智能的追求总是与另一场斗争交织在一起，更富有哲理、更浪漫、更不切实际。因此需要对人类智能有着更好的理解。
虽然目前在监督学习方面的突破似乎是基于优化的硬件、复杂的训练算法和过于复杂的神经网络架构，但强化学习仍然是比较传统陈旧。
这个想法很简单：如果你是一个环境中的学习代理。我们假设你的目标是满足自己的需求（不是吗？），那么你就会采取行动。基于这些行为，环境会以奖励来回应，你可以根据奖励调整行为，以最大限度地提高自己的满意度。
![探索强化学习算法背后的思想起源！](http://p99.pstatp.com/large/pgc-image/cabbdb58bee4496cb4a1513c2e97a595)
RL有限制吗？已故的日本将棋（shogi）选手村上佐治在面对AlphaGo Zero时发表声明， "计算机打败职业游戏玩家的日子永远不会到来"，但这个声明已经遭到现实的打击。
我们花了很长时间才把生物体通过强化和人工智能学习的能力联系起来。早在1948年，图灵就描述了一种享乐-痛苦系统（pleasure-pain system），该系统遵循几十年后建立的强化学习规则。
> 智力是适应变化的能力——斯蒂芬·霍金斯
由于其简单性，社区的第一次尝试针对西洋双陆棋(Backgammon)游戏，提供少量离散状态和简单规则。如今我们有人工智能代理使用强化学习来玩雅达利（Atari）、我的世界（Minecraft）和翻转煎饼（flip pancakes）游戏。那么，我们是如何做到这一切的呢？简短的回答是深度学习。
本文将探讨更多的答案。它将探索我们几十年来一直使用的强化学习算法背后的思想的起源。我们最近的成功不仅仅是深度神经网络的产物，而且是深层观察历史、结论和理解学习机制的尝试。
强化学习是一个难以追溯的起源领域。它的大部分理论基础都是控制理论家的。马尔可夫决策过程是最优控制问题的离散随机版本，因此几乎所有的强化学习算法都是基于控制理论中推导出的解决方案，这不足为奇。
然而，控制理论提供的背景不足以创建强化学习。我们如今仍然使用的算法需要诸如经典条件学和时间差异学习之类的思想来形成学习的过程。
如果不是少数好奇的生物学家、心理学家和不守规矩的计算机科学家的努力，人工智能社区可能不会拥有实施学习的工具。
我们如何在不可预见的情况下采取行动？如何采纳我们的行为？环境如何影响我们的行为？我们如何改进？如何学习技能？
**这是一个反复试验的世界**
桑迪克（Thorndike）在1898年做了一个实验，也许对他的猫感到非常生气，或者可能对动物的行为非常好奇。他把猫锁在一个笼子里，并在笼面放了一盘美味的鱼，猫只能通过拉动杠杆逃离笼子，才能吃到鱼。
**猫会怎么反应？**
没有推理，也没有推理或比较的过程，没有思考事物，没有这两个事物放在一起。并且没有想法，动物也不会想到笼子、食物或者将要实施的行为。
桑迪克观察到的是他的猫看起来并不聪明：它刚开始在笼子中到处走动，并不急于出笼，只有当它通过随机机会拉动杠杆并自行释放时，才会开始提高其逃脱技能。
根据这一观察结果，桑迪克提出了一个效果定律，该定律规定任何可能带来愉快后果的行为都可能会重复出现，并且任何可能导致不愉快后果的行为都可能会被制止。
这项法则引起了操作性条件反射领域，由斯金纳（Skinner）于1938年正式定义。对于强化学习社区，它提供了制定代理的理由，这些代理基于奖励及其与环境的互动来学习政策。
它还为我们提供了关于动物学习的新见解，因为效果法则可疑地类似于当时众所周知的另一种法则：自然选择。我们的理智是否能成为适者生存的理念？
然而，有两个特点使强化学习成为一个独特的过程：
它是选择性的。这与监督学习不同，因为代理会尝试各种选择，并通过比较它们的结果来从中进行选择。
它是联想的。这意味着通过选择找到的替代方案与特定情况或状态相关联，以形成代理的策略。自然选择是选择过程的一个主要例子，但它不是关联的。
"我们就是要反复做。因此，卓越不是一种行为而是一种习惯。"- 亚里士多德
**享乐主义者的学习指南**
![探索强化学习算法背后的思想起源！](http://p9.pstatp.com/large/pgc-image/8ad5e4567da6491bbed625bb28e65627)
在分析人类思维方面，克洛普夫（Klopf）的总结非常简洁："人的基本本质是什么？是享乐主义者。"
在他颇具争议的名为《享乐主义神经元——记忆、学习和智能理论》著作中，克洛普夫利用神经科学、生物学、心理学，以及解除他的推理的简单性和好奇心来说服我们，我们的神经元是享乐主义者。是的，神经元和你一样快乐。
当面对他那个时代的主导神经元模型，罗森布拉特（Rosenblatt）的感知器（Perceptron）（它是当今神经网络的构建块）时，克洛普夫对此感到奇怪："如果假设神经元被认为是非追求目标的组成部分，那么追求目标的大脑功能必须被视为一种新兴现象。这样的观点是否能够对记忆，学习以及更普遍的情报进行解释？"
他提出了一个名为基本异质稳定器的新构建模块，作为未来人工智能研究的基础。克洛普夫还认为，维持体内平衡，追求一种良好稳定的状态并不是复杂系统的目的，例如人类和动物。解释植物的目标可能已经足够了，但是我们可以假设人类在确保了休内平衡之后，追求最大限度的愉悦，而不是稳定它。为什么我们的神经元会有所不同？
这些想法可能听起来难以置信，它们可以归因于震动人工智能的世界。克洛普夫认识到，随着学习研究人员几乎专注于监督学习，适应行为的基本方面正在丧失。根据克洛普夫的说法，缺少的是行为的享乐方面，从环境中获得某些结果的驱动力，控制环境朝向期望的目的前进，远离不受欢迎的目的。
在一篇批评当前控制论原理的广泛章节中，正如机器学习在当时被称为的那样，可以强调三种攻击方式：
**我们应该使用深度神经网络吗？**
需要明确的是，两层足以满足上世纪50年代的网络需求。克洛普夫似乎对感知器（Perceptron）模型感到满意，但他质疑它在深度网络中的学习能力。克洛普夫提出了一个问题，即使在今天，也不能让机器学习科学家置身事外：
"但是，该算法仅适用于单层自适应网络。许多后续研究未能为多层网络的一般情况产生真正可行的确定性自适应机制。一般情况下的核心问题是，在系统行为不合适时，确定任何给定网络元素应该做什么。事实证明，这非常困难，因为深层网络中各个元素的大多数输出​​与系统的最终输出具有非常间接的关系。 "
**人工智能的目的是什么？**
克洛普夫还质疑人工智能研究的追求。在他试图接近正确的学习目标的过程中，他采用了一种论点，我在后来的增强学习研究者中也发现了这个论点：
https://www.sciencedirect.com/science/article/pii/S0921889005800259
"生命在这个星球上已经进化了大约30亿年。在那段时间里，90％用于改进我们与爬行动物共享的神经基质。从爬行动物的时代开始，到人类出现之前，它只有相对较短的3亿年。关于智力进化的过程出现了一个问题。如果进化过程花费90％的时间来开发神经基质，剩下的10％用于制定有效的更高水平的机制，那么为什么人工智能研究人员试图以其他方式去做呢？"
**智力是否聪明？**
在下面的摘录中，感觉好像桑迪克和克洛普夫一直是强化学习的伙伴："人工智能研究人员对智力的感知似乎与生命系统中这种现象的本质不符，还有另外一种方式。在生命系统中，智力往往不是智能的，至少不是研究人员有时看到的智力现象。与其相反，生命系统中的智能通常是有效的。如果一种'强力'性质可以用于智能生物的日常信息处理，那么似乎会有很多。即使对于最聪明的人来说，开展更加聪明的活动也是困难的。因此，人们想知道，智力与更高层次的信息处理之间的联系是否可能使人工智能研究人员对这一现象的看法过于狭隘。在短期内，更温和的观点会产生更有成效的理论吗？"
**巴甫洛夫的狗玩西洋双陆棋**
到目前为止，我们可能一直在讨论强化学习，但事实是，这个术语最初是由巴甫洛夫在1927年关于条件反射的专著的英译本中使用的。
https://academic.oup.com/brain/article-abstract/51/1/129/268769?redirectedFrom=PDF
巴甫洛夫在他著名的实验中观察到的是，当一只狗被提供食物，并且在非常接近喂食时间时发出声音，狗因此学会了将喂食与声音联系起来，甚至在没有食物的情况下，当听到声音时，狗也会流口水。
https://www.simplypsychology.org/pavlov.html
![探索强化学习算法背后的思想起源！](http://p1.pstatp.com/large/pgc-image/2082130ce9dd484f970e8b12ce669f02)
通过这一观察，巴甫洛夫为经典条件反射奠定了基础，这是第一个将时间纳入学习过程的理论。如今，RL算法主要采用时差学习，这意味着在计算动作的"质量"以做出决策时，我们也会考虑未来的奖励。
1989年，克里斯·沃特金斯（Chris Watkins）开发了Q-learning，这是最著名的强化学习算法之一，它将时间差异和最佳控制线程完全结合在一起。
1992年，Tesauro在玩西洋双陆棋的代理身上采用了时差学习的概念。这是说服研究界相信这种机器学习有潜力的时刻和应用。
虽然目前的研究主题集中在深度学习和游戏，但我们如今不会有强化学习的领域，而不是一群人谈论猫、神经元和狗。
可以说，我们从解决西洋双陆棋获得的奖励，直到那一点难以想象的艰巨任务，促使我们进一步探索强化学习的潜力。这是一个强化学习的例子吗？

