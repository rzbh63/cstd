
# spark生成DataFrame - bitcarmanlee的博客 - CSDN博客


2017年09月07日 18:28:17[bitcarmanlee](https://me.csdn.net/bitcarmanlee)阅读数：989



## 1.为什么要有DataFrame
Spark中的RDD叫做分布式弹性数据集。RDD是一个粗粒度的分布式计算，用函数声明式的api就能完成分布式的计算，比如wordcount，在mapreduce要写比较冗长的代码，而在Spark中可以用一行代码搞定。
既然RDD这么简单方便，为什么还要搞出一个DataFrame来呢？DataFrame是借鉴了R与pandas众DataFrame的思想，是业界处理标准化数据集的事实标准。DataFrame跟RDD相比，多了一些限制条件，但同时也有了更多的优化。比如基于Spark Catalyst优化器，提供如列裁剪，谓词下推，map join等优化。同时，采用code generation ，动态编译表达式，提升性能，比用rdd的自定义函数性能高5倍左右。
Example：
```python
rdd.map{x => x.
```
```python
split
```
```python
(
```
```python
"\t"
```
```python
); (
```
```python
lines
```
```python
(
```
```python
0
```
```python
),
```
```python
lines
```
```python
(
```
```python
1
```
```python
))}
    .
```
```python
filter
```
```python
(x => x.
```
```python
_2
```
```python
>=
```
```python
18
```
```python
)
    .select(x => (x.
```
```python
_1
```
```python
, x.
```
```python
_2
```
```python
))
```
而如果采用类似DataFrame的结构：
```python
sqlContext
```
```python
.table
```
```python
(
```
```python
"people"
```
```python
)
```
```python
.filter
```
```python
(col(
```
```python
"age"
```
```python
) >=
```
```python
19
```
```python
)
```
```python
.select
```
```python
(
```
```python
"id"
```
```python
,
```
```python
"name"
```
```python
)
```
用rdd读结构化文本要用map函数，需要按位置获取数据，没有schema，性能和可读性都不好。
而用dataframe可以直接通过sede读取结构化数据，性能比RDD高2到3倍左右，比MR高5倍左右，同时，具有结构化的数据，可读性更好。
Spark2.0以后的版本推出了DataSet，是更加强类型的API，用了scala的泛型，能在编译是发现更多的编译问题DataFrame是DataSet〈Row〉类型，DS在接口上和DataFrame很相似。
## 2.通过toDF方法创建DataFrame
sparkSQL中implicits里面有toDF方法，toDF方法可以将本地的Seq，Array或者RDD转化为DataFrame
```python
import
```
```python
sqlContext
```
```python
.
```
```python
implicits
```
```python
.
```
```python
_
val wordDataFrame
```
```python
=
```
```python
Seq((
```
```python
0
```
```python
,
```
```python
Array
```
```python
(
```
```python
"Hi"
```
```python
,
```
```python
"I"
```
```python
,
```
```python
"heard"
```
```python
,
```
```python
"about"
```
```python
,
```
```python
"Spark"
```
```python
)),(
```
```python
1
```
```python
,
```
```python
Array
```
```python
(
```
```python
"I"
```
```python
,
```
```python
"wish"
```
```python
,
```
```python
"Java"
```
```python
,
```
```python
"could"
```
```python
,
```
```python
"use"
```
```python
,
```
```python
"case"
```
```python
,
```
```python
"classes"
```
```python
)),(
```
```python
2
```
```python
,
```
```python
Array
```
```python
(
```
```python
"Logistic"
```
```python
,
```
```python
"regression"
```
```python
,
```
```python
"models"
```
```python
,
```
```python
"are"
```
```python
,
```
```python
"neat"
```
```python
)))
```
```python
.
```
```python
toDF(
```
```python
"label"
```
```python
,
```
```python
"words"
```
```python
)
```
在spark shell中运行上面代码以后，结果如下：
```python
scala> val wordDataFrame = Seq((
```
```python
0
```
```python
, Array(
```
```python
"Hi"
```
```python
,
```
```python
"I"
```
```python
,
```
```python
"heard"
```
```python
,
```
```python
"about"
```
```python
,
```
```python
"Spark"
```
```python
)),(
```
```python
1
```
```python
, Array(
```
```python
"I"
```
```python
,
```
```python
"wish"
```
```python
,
```
```python
"Java"
```
```python
,
```
```python
"could"
```
```python
,
```
```python
"use"
```
```python
,
```
```python
"case"
```
```python
,
```
```python
"classes"
```
```python
)),(
```
```python
2
```
```python
, Array(
```
```python
"Logistic"
```
```python
,
```
```python
"regression"
```
```python
,
```
```python
"models"
```
```python
,
```
```python
"are"
```
```python
,
```
```python
"neat"
```
```python
))).toDF(
```
```python
"label"
```
```python
,
```
```python
"words"
```
```python
)
wordDataFrame: org.apache.spark.sql.DataFrame = [label:
```
```python
int
```
```python
, words:
```
```python
array
```
```python
<
```
```python
string
```
```python
>
```
```python
]
```
可以看到我们已经得到了一个DataFrame。
如果toDF方法不指定字段名称，那么默认列名为”_1”, “_2”, …
```python
scala> val wordDataFrame = Seq((
```
```python
0
```
```python
, Array(
```
```python
"Hi"
```
```python
,
```
```python
"I"
```
```python
,
```
```python
"heard"
```
```python
,
```
```python
"about"
```
```python
,
```
```python
"Spark"
```
```python
)),(
```
```python
1
```
```python
, Array(
```
```python
"I"
```
```python
,
```
```python
"wish"
```
```python
,
```
```python
"Java"
```
```python
,
```
```python
"could"
```
```python
,
```
```python
"use"
```
```python
,
```
```python
"case"
```
```python
,
```
```python
"classes"
```
```python
)),(
```
```python
2
```
```python
, Array(
```
```python
"Logistic"
```
```python
,
```
```python
"regression"
```
```python
,
```
```python
"models"
```
```python
,
```
```python
"are"
```
```python
,
```
```python
"neat"
```
```python
))).toDF()
wordDataFrame: org.apache.spark.sql.DataFrame = [_1:
```
```python
int
```
```python
, _2:
```
```python
array
```
```python
<
```
```python
string
```
```python
>
```
```python
]
```
我们还可以通过case class 与toDF的方式得到DataFrame
```python
scala> import sqlContext
```
```python
.implicits
```
```python
._
import sqlContext
```
```python
.implicits
```
```python
._
scala> import scala
```
```python
.collection
```
```python
.JavaConversions
```
```python
._
import scala
```
```python
.collection
```
```python
.JavaConversions
```
```python
._
scala> case class word(num: Int, wordlist:Array[String])
defined class word
sc
```
```python
.parallelize
```
```python
(Seq((
```
```python
0
```
```python
, Array(
```
```python
"Hi"
```
```python
,
```
```python
"I"
```
```python
,
```
```python
"heard"
```
```python
,
```
```python
"about"
```
```python
,
```
```python
"Spark"
```
```python
)),(
```
```python
1
```
```python
, Array(
```
```python
"I"
```
```python
,
```
```python
"wish"
```
```python
,
```
```python
"Java"
```
```python
,
```
```python
"could"
```
```python
,
```
```python
"use"
```
```python
,
```
```python
"case"
```
```python
,
```
```python
"classes"
```
```python
)),(
```
```python
2
```
```python
, Array(
```
```python
"Logistic"
```
```python
,
```
```python
"regression"
```
```python
,
```
```python
"models"
```
```python
,
```
```python
"are"
```
```python
,
```
```python
"neat"
```
```python
))))
```
```python
.map
```
```python
(
```
```python
x
```
```python
=> word(
```
```python
x
```
```python
._1
```
```python
.toInt
```
```python
,
```
```python
x
```
```python
._2))
```
```python
.foreach
```
```python
(
```
```python
x
```
```python
=> println(
```
```python
x
```
```python
.num
```
```python
,
```
```python
x
```
```python
.wordlist
```
```python
.mkString
```
```python
(
```
```python
"-"
```
```python
)))
(
```
```python
1
```
```python
,I-wish-Java-could-use-case-classes)
(
```
```python
2
```
```python
,Logistic-regression-models-are-neat)
(
```
```python
0
```
```python
,Hi-I-heard-about-Spark)
```
## 3.通过createDataFrame创建DataFrame
在SqlContext中使用createDataFrame也可以创建DataFrame。跟toDF一样，这里创建DataFrame的数据形态也可以是本地数组或者RDD。这种方法在由于数据的结构以字符串的形式编码而无法提前定义定制类的情况下非常实用。
```python
import org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
._
import org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.types
```
```python
._
scala> val schema = StructType(List(StructField(
```
```python
"integer_column"
```
```python
, IntegerType, nullable = false),StructField(
```
```python
"string_column"
```
```python
, StringType, nullable = true),StructField(
```
```python
"date_column"
```
```python
, DateType, nullable = true)))
```
```python
schema:
```
```python
org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.types
```
```python
.StructType
```
```python
= StructType(StructField(integer_column,IntegerType,false), StructField(string_column,StringType,true), StructField(date_column,DateType,true))
scala> val rdd = sc
```
```python
.parallelize
```
```python
(Seq(Row(
```
```python
1
```
```python
,
```
```python
"First Value"
```
```python
, java
```
```python
.sql
```
```python
.Date
```
```python
.valueOf
```
```python
(
```
```python
"2010-01-01"
```
```python
)),Row(
```
```python
2
```
```python
,
```
```python
"Second Value"
```
```python
, java
```
```python
.sql
```
```python
.Date
```
```python
.valueOf
```
```python
(
```
```python
"2010-02-01"
```
```python
))))
```
```python
rdd:
```
```python
org
```
```python
.apache
```
```python
.spark
```
```python
.rdd
```
```python
.RDD
```
```python
[org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.Row
```
```python
] = ParallelCollectionRDD[
```
```python
4
```
```python
] at parallelize at <console>:
```
```python
39
```
```python
scala> val df = sqlContext
```
```python
.createDataFrame
```
```python
(rdd, schema)
```
```python
df:
```
```python
org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.DataFrame
```
```python
= [integer_column: int, string_column: string, date_column: date]
```
## 4.通过parquet文件或者json文件创建DataFrame
```python
val df = sqlContext
```
```python
.read
```
```python
.parquet
```
```python
(
```
```python
"xxx/file"
```
```python
)
val df = spark
```
```python
.read
```
```python
.json
```
```python
(
```
```python
"xxx/file.json"
```
```python
)
```

