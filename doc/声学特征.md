# 声学特征





# PLP

## PLP的由来

[Linear prediction](https://en.wikipedia.org/wiki/Linear_prediction)可以用来获得语音功率谱P(ω)的全极点模型A(ω)A(ω)，也可以把LP看做获取P(ω)P(ω)的频谱包络的手段，参考[前面的文章](http://blog.csdn.net/xmdxcsj/article/details/72419948) 
由于LP对待所有频率一视同仁，它不符合人耳的听觉机理，比如人耳对于高于800Hz的感知会下降，对于中间频段更敏感。 
为了解决这个问题，Hermansky在进行LP之前修改语音的功率谱P(ω)P(ω)，称为perceptual linear predictive。可以理解为更符合听觉感知的LP。

## PLP流程

在LP之前修改P(ω)，需要的步骤见下图 
![这里写图片描述](https://img-blog.csdn.net/20171112161746973?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### critical-band spectral resolution

首先在频率轴上将Hz转化为Bark，参考[bark scale](https://en.wikipedia.org/wiki/Bark_scale),500Hz以下跟频率近似线性关系，500Hz以上近似log关系 
![这里写图片描述](https://img-blog.csdn.net/20171112161807742?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 


$$
\Omega(\omega)=6ln\{\omega/1200\pi+[(\omega/1200\pi)^2+1]^{0.5}\}
$$


然后和critial-band masking curve $\Psi(\Omega)$进行卷积
$$
\Theta(\Omega_i)=\sum_{\Omega=-1.3}^{2.5}P(\Omega-\Omega_i)\Psi(\Omega)
$$


### equal-loudness preemphasis

为了模拟听觉对于不同频带敏感度的不同，使用equal-loudness curve对频谱进行预加重 
$$
\Xi[\Omega(\omega)]=E(\omega)\Theta[\Omega(\omega)]
$$
其中E(ω)如下：
$$
E(\omega)=[(\omega^2+56.8*10^6)\omega^4]/[(\omega^2+6.3*10^6)^2*(\omega^2+0.38*10^9)]
$$


### intensity-loudness power law

为了描述声音强度和感知响度之间的关系 
$$
\Phi(\Omega)=\Xi(\Omega)^{0.33}
$$


## 参考

Perceptual linear predictive (PLP) analysis of speech











# PNCC

2017年11月12日 16:25:02 [xmucas](https://me.csdn.net/xmdxcsj) 阅读数：2190



------

## 特点

power-normalized cepstral coefficients相比于MFCC特征： 
\- 在噪声和混响场景下提升识别效果，尤其在训练语料是clean语音的时候 
\- 相比于MFCC，计算量提升34.6%

使用pncc相比mfcc，噪声和口音测试集可以得到10-15%的相对提升

## 细节

![这里写图片描述](https://img-blog.csdn.net/20171112162400318?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
和MFCC/PLP特征的整体对比如上图

### filter bank

相比于MFCC的triangular filters，PNCC使用gammatone filters，40维，截止频率分别是200/8000。 
![这里写图片描述](https://img-blog.csdn.net/20171112162420061?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 
获得$P[m,l]$,m表示frame，l表示channel

### medium-time power calculation

由于噪声的能量变化相比语音慢很多，所以更大的时间窗口可以得到更好的性能，所以对每一帧进行了平滑处理(左右2帧做平均)。 
$$
\bar Q[m,l]=\frac{1}{2M+1}\sum_{m'=m-M}^{m+M}P[m',l]
$$
得到的$\bar Q[m,l]$用于后面的噪声估计和补偿



### asymmetric noise suppression

因为语音的能量相比噪声变化快，所以使用谱减法来过滤掉低频部分以达到抑制噪声的目的。 
![这里写图片描述](https://img-blog.csdn.net/20171112162435460?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 
其中的temperal masking的引入可以减弱混响的影响，首先获得每个channel的moving peak，如果某一帧的能量低于这个peak曲线，缩小对应的能量。 
获得$\bar R[m,l]$

### spectral weight smoothing

在不同的channel之间做平滑。 
$$
\bar s[m,l]=(\frac{1}{l_2-L_1+1}\sum_{l'=l_1}^{l_2}\frac{\bar R[m,l']}{\bar Q[m,l']})
$$
其中$l_2=min(l+N,L)$，L表示channel的个数，$l_1=max(l-N,1)$，N设为4

 

$\bar S[m,l]$通过medium-time power calculation实现了在时间维度上的平滑，通过spectral weight smoothing则实现了在频率维度上的平滑，时域是左右2帧，频域是左右4个channel最后调制$P[m,l]$得到
$$
T[m,l]=P[m,l]\bar S[m,l]
$$

### mean power normalization

均值的获取使用online的形式 
$$
\mu[m]=\lambda_{\mu}\mu [m-1]+\frac{1-\lambda_{\mu}}{L}\sum_{l=0}^{L-1}T[m,l]
$$
归一化以后：
$$
U[m,l]=k\frac{T[m,l]}{\mu [m]}
$$

### rate-level nonlinearity

相比于MFCC使用的log非线性函数，PNCC使用指数函数，更符合人耳听觉神经的压缩感知 
$$
V[m,l]=U[m,l]^{1/15}
$$

## 参考

Power-Normalized Cepstral Coefficients (PNCC) for Robust Speech Recognition 
<http://www.cs.cmu.edu/~robust/archive/algorithms/PNCC_C>







# ivector

2017年11月12日 16:26:38 [xmucas](https://me.csdn.net/xmdxcsj) 阅读数：12021



------

## 提取流程

### 1.UBM

universal background model[1] 
使用GMM建模，UBM的训练通过EM算法完成，有两种方法： 
\- 所有的数据训练出来一个UBM，需要保证训练数据的均衡 
\- 训练多个UBM，然后合在一起，比如根据性别分成两个，这样的话可以更有效的利用非均衡数据以及控制最后的UBM。

### 2.supervector

![这里写图片描述](https://img-blog.csdn.net/20171112162625889?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
使用MAP adaptation对UBM的高斯进行线性插值，获得speaker相关的GMM模型，该模型的均值作为supervector[2]。详细的训练过程参考[1]. 
假设UBM有C个分量，特征维度为F，那么最后得到的supervector的维度为C*F

### 3.ivector

identity vector 
$$
s=m+Tw
$$
ss : supervector

m: ubm’s mean supervector

T: total-vavriability matrix

w: i-vector



s和m前两部分已经获得，为了获得最后的w，只剩下获得T。

使用EM算法可以获得最后的T[3].



### 4.LDA PLDA

ivector同时包含speaker和channel的信息，使用LDA和WCCN来减弱channel影响。

## kaldi实现

### 1.UBM

universal background model 使用gmm来刻画 
UBM训练流程，最后得到**final.dubm**：

```powershell
steps/online/nnet2/train_diag_ubm.sh
#gmm-global-init-from-feats 根据所有特征训练gmm
#gmm-gselect gmm-global-acc-stats 获取gmm训练的统计量
#gmm-global-est 根据统计量重新训练gmm
#gmm-global-copy 转化final.dubm为文本形式12345
```

假设特征40维，高斯个数为512

### 2.extractor

ivector模型用来提取100维ivector特征，和mfcc特征合在一起当做dnn的输入，最后生成的模型是**final.ie**，训练流程如下

```powershell
steps/online/nnet2/train_ivector_extractor.sh
#ivector-extractor-init 使用final.dubm初始化最开始的ivector
#gmm-global-get-post 根据final.dubm获取cmvn后的特征的后验概率
#ivector-extractor-sum-accs 获取统计量
#ivector-extractor-est 根据统计量获得最后ivector模型final.ie

ivector-extractor-init --binary=false --ivector-dim=100 --use-weights=false "gmm-global-to-fgmm final.dubm -|" txt #查看文本形式的ie1234567
```

由于ss的维度是512*40，mm的维度也是512*40，ww的维度是100，所以最后得到的TT的维度为512*100*40

### 3.提取ivector

ivector可以每一句一个，online的形式可以设成10帧一个，需要的文件包括：

```powershell
--cmvn-config=run/run_chain_1000h_pitch/exp/ivectors/train_max2/conf/online_cmvn.conf
--ivector-period=10
--splice-config=run/run_chain_1000h_pitch/exp/ivectors/train_max2/conf/splice.conf
--lda-matrix=run/run_chain_1000h_pitch/exp/extractor/final.mat
--global-cmvn-stats=run/run_chain_1000h_pitch/exp/extractor/global_cmvn.stats
--diag-ubm=run/run_chain_1000h_pitch/exp/extractor/final.dubm
--ivector-extractor=run/run_chain_1000h_pitch/exp/extractor/final.ie
--num-gselect=5
--min-post=0.025
--posterior-scale=0.1
--max-remembered-frames=1000
--max-count=0123456789101112
```

ivector提取流程如下：

```powershell
steps/online/nnet2/extract_ivectors_online.sh
#1.特征处理：cmvn+splice+lda
#2.根据特征和m(final.dubm)获得每个speaker对应的s
#3.根据s、m(final.dubm)、T(final.ie)得到w

#查看ivector特征
copy-feats --binary=false --compress=false ark:ivector_online.1.ark ark,t:ivector_online.1.ark.txt1234567
```

训练和解码的文件需要保持一致，不然结果会差距比较大。

## 参考文献

[1].Speaker Verification Using Adapted Gaussian Mixture Models 
[2].Support Vector Machines using GMM Supervectors for Speaker Verification 
[3].Implementation of the Standard I-vector System for the Kaldi Speech Recognition Toolkit

