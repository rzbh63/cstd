
# 大数据集群搭建之hadoop、tomcat、jdk等工具的安装（三） - Magician的博客 - CSDN博客


2017年09月18日 22:51:12[春雨里de太阳](https://me.csdn.net/qq_16633405)阅读数：1723所属专栏：[自学大数据之路](https://blog.csdn.net/column/details/18514.html)



[目录](#目录)[一、准备的资源：](#一准备的资源)
[二、安装配置过程](#二安装配置过程)


# 目录
本章就说下各种软件的安装和配置。
## 一、准备的资源：
1、tomcat(如用于在网页上查看HDFS的存储等)
地址：[http://pan.baidu.com/s/1miC93ny](http://pan.baidu.com/s/1miC93ny)
密码：52dd
2、jdk
地址：[http://pan.baidu.com/s/1mhVWiM0](http://pan.baidu.com/s/1mhVWiM0)
密码：fmn1
3、Hadoop（最好用centos6.5系统，否则配置时会出现各种BUG）
地址：[http://pan.baidu.com/s/1qXXPUdm](http://pan.baidu.com/s/1qXXPUdm)
密码：5zqe
## 二、安装配置过程
在安装这些软件之前，再介绍个比较好用的软件给大家
SecureCRT7.3.3 x64(SSH工具)：[http://pan.baidu.com/s/1gfzm7Fd](http://pan.baidu.com/s/1gfzm7Fd)
密码：5zy0
先说下这个软件，这个软件是安装在windows环境下，用于在Windows环境下通过SSH协议来控制Linux系统。
简单介绍下使用的过程：
打开软件按照下图进行操作
![这里写图片描述](https://img-blog.csdn.net/20170918225046915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTY2MzM0MDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
按照以上操作完毕后，你就可以在右边的命令行下来控制Linux，该软件支持命令行的命令粘贴等功能,详细的使可以自行百度。
接下来介绍各个软件的安装：
**1、安装JDK**
1.上传jdk-7u45-linux-x64.tar.gz到Linux上
2.解压jdk到/usr/local目录
tar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local/
3.设置环境变量，在/etc/profile文件最后追加相关内容
vi /etc/profile
export JAVA_HOME=/usr/local/jdk1.7.0_45
export PATH=$PATH:$JAVA_HOME/bin
4.刷新环境变量
source /etc/profile
5.测试java命令是否可用
java -version
**2、安装Tomcat**
1.上传apache-tomcat-7.0.68.tar.gz到Linux上
2.解压tomcat
tar -zxvf apache-tomcat-7.0.68.tar.gz -C /usr/local/
3.启动tomcat
/usr/local/apache-tomcat-7.0.68/bin/startup.sh
./startup.sh
4.查看tomcat进程是否启动
jps
5.查看tomcat进程端口
netstat -anpt | grep 2465
6.通过浏览器访问tomcat
[http://192.168.0.101:8080/](http://192.168.0.101:8080/)
![这里写图片描述](https://img-blog.csdn.net/20170919204542909?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTY2MzM0MDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
**3、安装hadoop**
先上传hadoop的安装包到服务器上去/home/hadoop/
注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop
**伪分布式需要修改5个配置文件**
```python
3.1
```
```python
配置hadoop

第一个：hadoop-env
```
```python
.sh
```
```python
vim hadoop-env
```
```python
.sh
```
```python
#第27行
```
```python
export JAVA_HOME=/usr/java/jdk1
```
```python
.7
```
```python
.0
```
```python
_65
```
```python
第二个：core-site.xml
```
```python
<!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 -->
```
```python
<
```
```python
property
```
```python
>
```
```python
<
```
```python
name
```
```python
>
```
```python
fs.defaultFS
```
```python
</
```
```python
name
```
```python
>
```
```python
<
```
```python
value
```
```python
>
```
```python
hdfs://weekend-1206-01:9000
```
```python
</
```
```python
value
```
```python
>
```
```python
</
```
```python
property
```
```python
>
```
```python
<!-- 指定hadoop运行时产生文件的存储目录 -->
```
```python
<
```
```python
property
```
```python
>
```
```python
<
```
```python
name
```
```python
>
```
```python
hadoop.tmp.dir
```
```python
</
```
```python
name
```
```python
>
```
```python
<
```
```python
value
```
```python
>
```
```python
/home/hadoop/hadoop-2.4.1/tmp
```
```python
</
```
```python
value
```
```python
>
```
```python
</
```
```python
property
```
```python
>
```
```python
第三个：hdfs-site.xml
```
```python
<!-- 指定HDFS副本的数量 -->
```
```python
<
```
```python
property
```
```python
>
```
```python
<
```
```python
name
```
```python
>
```
```python
dfs.replication
```
```python
</
```
```python
name
```
```python
>
```
```python
<
```
```python
value
```
```python
>
```
```python
1
```
```python
</
```
```python
value
```
```python
>
```
```python
</
```
```python
property
```
```python
>
```
```python
<
```
```python
property
```
```python
>
```
```python
<
```
```python
name
```
```python
>
```
```python
dfs.secondary.http.address
```
```python
</
```
```python
name
```
```python
>
```
```python
<
```
```python
value
```
```python
>
```
```python
192.168.1.152:50090
```
```python
</
```
```python
value
```
```python
>
```
```python
</
```
```python
property
```
```python
>
```
```python
第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml)
        mv mapred-site.xml.template mapred-site.xml
        vim mapred-site.xml
```
```python
<!-- 指定mr运行在yarn上 -->
```
```python
<
```
```python
property
```
```python
>
```
```python
<
```
```python
name
```
```python
>
```
```python
mapreduce.framework.name
```
```python
</
```
```python
name
```
```python
>
```
```python
<
```
```python
value
```
```python
>
```
```python
yarn
```
```python
</
```
```python
value
```
```python
>
```
```python
</
```
```python
property
```
```python
>
```
```python
第五个：yarn-site.xml
```
```python
<!-- 指定YARN的老大（ResourceManager）的地址 -->
```
```python
<
```
```python
property
```
```python
>
```
```python
<
```
```python
name
```
```python
>
```
```python
yarn.resourcemanager.hostname
```
```python
</
```
```python
name
```
```python
>
```
```python
<
```
```python
value
```
```python
>
```
```python
weekend-1206-01
```
```python
</
```
```python
value
```
```python
>
```
```python
</
```
```python
property
```
```python
>
```
```python
<!-- reducer获取数据的方式 -->
```
```python
<
```
```python
property
```
```python
>
```
```python
<
```
```python
name
```
```python
>
```
```python
yarn.nodemanager.aux-services
```
```python
</
```
```python
name
```
```python
>
```
```python
<
```
```python
value
```
```python
>
```
```python
mapreduce_shuffle
```
```python
</
```
```python
value
```
```python
>
```
```python
</
```
```python
property
```
```python
>
```
```python
3.2将hadoop添加到环境变量
    vim /etc/proflie
        export JAVA_HOME=/usr/java/jdk1.7.0_65
        export HADOOP_HOME=/itcast/hadoop-2.4.1
        export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
    source /etc/profile
    3.3格式化namenode（是对namenode进行初始化）
        hdfs namenode -format (hadoop namenode -format)
    3.4启动hadoop
        先启动HDFS
        sbin/start-dfs.sh
        再启动YARN
        sbin/start-yarn.sh
    3.5验证是否启动成功
        使用jps命令验证
        27408 NameNode
        28218 Jps
        27643 SecondaryNameNode
        28066 NodeManager
        27803 ResourceManager
        27512 DataNode
        http://192.168.1.101:50070 （HDFS管理界面）
        http://192.168.1.101:8088 （MR管理界面）
```
![这里写图片描述](https://img-blog.csdn.net/20170919211827010?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTY2MzM0MDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
在下面的章节中会介绍下HDFS的原理和基本操作等知识

