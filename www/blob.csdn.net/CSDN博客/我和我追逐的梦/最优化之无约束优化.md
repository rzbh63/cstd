
# 最优化之无约束优化 - 我和我追逐的梦~~~ - CSDN博客


2014年09月02日 13:58:51[一只鸟的天空](https://me.csdn.net/heyongluoyao8)阅读数：14811


转自：http://blog.csdn.net/mytestmy/article/details/16903537

## 1.1问题定义
## 1.1.1优化问题定义

最优化问题数学上定义，最优化问题的一般形式为
![\begin{array}{l}\min f(x)\\s.t.{\rm{ }}x \in X\end{array}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmin%20f%28x%29%5C%5C%0As.t.%7B%5Crm%7B%20%7D%7Dx%20%5Cin%20X%0A%5Cend%7Barray%7D)（1）[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmin%20f%28x%29%5C%5C%0As.t.%7B%5Crm%7B%20%7D%7Dx%20%5Cin%20X%0A%5Cend%7Barray%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmin%20f%28x%29%5C%5C%0As.t.%7B%5Crm%7B%20%7D%7Dx%20%5Cin%20X%0A%5Cend%7Barray%7D)其中的![{\rm{x}} \in {R^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)[是自变量，f(x)是目标函数，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)![{\rm{X}} \subset {R^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BX%7D%7D%20%5Csubset%20%7BR%5En%7D)[为约束集或者说可行域。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BX%7D%7D%20%5Csubset%20%7BR%5En%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)可行域这个东西，有等式约束，也有不等式约束，或者没有约束，这次的精力主要还是集中在无约束的优化问题上，如果有精力，再讨论有约束的。
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)
### 1.1.2最优化方法定义
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)优化问题的解法叫最优化方法。
最优化方法通常采用迭代的方法求它的最优解，其基本思想是：给定一个初始点![{{\rm{x}}_0} \in {R^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bx%7D%7D_0%7D%20%5Cin%20%7BR%5En%7D)。按照某一迭代规则产生一个点列![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)，使得当![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)是有穷点列时，其最后一个点是最优化模型问题的最优解，当是![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)无穷点列时，它有极限点，且其极限点是最优化模型问题的最优解。一个好的算法应具备的典型特征为：迭代点![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)能稳定地接近局部极小值点![{{\rm{x}}^*}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bx%7D%7D%5E%2A%7D)的邻域，然后迅速收敛于![{{\rm{x}}^*}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bx%7D%7D%5E%2A%7D)。当给定的某个收敛准则满足时，迭代即终止。[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bx%7D%7D%5E%2A%7D)好吧，上面是一堆描述，来点定义，设![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)[为第k次迭代点，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)[第k次搜索方向，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)[为第k次步长因子，则第k次迭代为](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{x_{k + 1}} = {x_k} + {\alpha _k}{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D)[   
             （2）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D)
然后后面的工作几乎都在调整![{\alpha _k}{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D%7Bd_k%7D)[这个项了，不同的步长](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D%7Bd_k%7D)![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)[和不同的搜索方向](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)[分别构成了不同的方法。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)当然步长![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)[和搜索方向](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)[是得满足一些条件，毕竟是求最小值，不能越迭代越大了，下面是一些条件](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![\nabla {\rm{f}}{\left( {{x_k}} \right)^T}{d_k} < 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cnabla%20%7B%5Crm%7Bf%7D%7D%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5ET%7D%7Bd_k%7D%20%3C%200)[                          
 （3）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cnabla%20%7B%5Crm%7Bf%7D%7D%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5ET%7D%7Bd_k%7D%20%3C%200)
![{\rm{f}}\left( {{x_k} + {\alpha _k}{d_k}} \right) < {\rm{f}}\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3C%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)[            
 （4）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3C%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
式子（3）的意义是搜索方向必须跟梯度方向（梯度也就是![\nabla {\rm{f}}\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)[）夹角大于90度，也就是基本保证是向梯度方向的另外一边搜索，至于为什么要向梯度方向的另外一边搜索，就是那样才能保证是向目标函数值更小的方向搜索的，因为梯度方向一般是使目标函数增大的（不增大的情况是目标函数已经到达极值）。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
式子（4）的意义就很明显了，迭代的下一步的目标函数比上一步小。
最优化方法的基本结构为：
给定初始点![{x_0}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_0%7D)[，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_0%7D)
（a）      确定搜索方向![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)[，即按照一定规则，构造目标函数f在](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)[点处的下降方向作为搜索方向。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)（b）      确定步长因子![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)[，使目标函数值有某种意义的下降。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
（c）      令![{x_{k + 1}} = {x_k} + {\alpha _k}{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D)若![{x_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)满足某种终止条件，则停止迭代，得到近似最优解![{x_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)，否则，重复以上步骤。[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)能不能收敛到最优解是衡量最优化算法的有效性的一个重要方面。
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)

### 1.1.3收敛速度简介
收敛速度也是衡量最优化方法有效性的一个重要方面。
设算法产生的迭代点列![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)[在某种范数意义下收敛，即](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
![ {\lim }\limits_{{\rm{k}} \to {\rm{\infty }}} {x_k} - {x^*} = 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%20%3D%200)[  
     （5）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%20%3D%200)
若存在实数![{\rm{\alpha }} > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3E%200)[（这个](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3E%200)![{\rm{\alpha }}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D)[不是步长）以及一个与迭代次数k无关的常数](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D)![{\rm{q}} > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)[，使得](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D)![ {\lim }\limits_{{\rm{k}} \to {\rm{\infty }}} \frac{{{x_{k + 1}} - {x^*}}}{{{x_k} - {x^*}^\alpha }} = {\rm{q}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%5Cfrac%7B%7B%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx%5E%2A%7D%7D%7D%7B%7B%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%5E%5Calpha%20%7D%7D%20%3D%20%7B%5Crm%7Bq%7D%7D)[  
 （6）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%5Cfrac%7B%7B%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx%5E%2A%7D%7D%7D%7B%7B%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%5E%5Calpha%20%7D%7D%20%3D%20%7B%5Crm%7Bq%7D%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%5Cfrac%7B%7B%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx%5E%2A%7D%7D%7D%7B%7B%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%5E%5Calpha%20%7D%7D%20%3D%20%7B%5Crm%7Bq%7D%7D)则称算法产生的迭代点列![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)具有![{\rm{Q}} - {\rm{\alpha }}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BQ%7D%7D%20-%20%7B%5Crm%7B%5Calpha%20%7D%7D)阶收敛速度。特别地，常用的说法有以下几种。[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BQ%7D%7D%20-%20%7B%5Crm%7B%5Calpha%20%7D%7D)（a）      当![{\rm{\alpha }} = 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%200)[，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%200)![{\rm{q}} > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)[时，迭代点列](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)[叫做具有Q-线性收敛速度；](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)（b）      当![1 < {\rm{\alpha }} < 2](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=1%20%3C%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%3C%202)[，q>0时或](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=1%20%3C%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%3C%202)![{\rm{\alpha }} = 1](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%201)[，q=0时，迭代点列](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%201)![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)[叫做具有Q-超线性收敛速度；](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%201)（c）      当![{\rm{\alpha }} = 2](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%202)[时，迭代点列](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%202)![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)[叫做具有Q-二阶收敛速度；](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)还有一种叫做R-收敛速度，不讨论了，有兴趣可以查阅相关资料。[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)一般认为，具有超线性收敛速度和二阶收敛速度的方法是比较快速的。但是对于一个算法，收敛性和收敛速度的理论结果并不保证算法在实际执行时一定有好的实际计算特性。一方面是由于这些结果本身并不能保证方法一定有好的特性，另一方面是由于算法忽略了计算过程中十分重要的舍入误差的影响。
[
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)

## 1.2步长确定

### 1.2.1一维搜索

### 如前面的讨论，优化方法的关键是构造搜索方向![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### 和步长因子![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### ，这一节就讨论如何确定步长。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### 设
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{\rm{\varphi }}\left( {\rm{\alpha }} \right) = {\rm{f}}\left( {{x_k} + \alpha {d_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%5Cright%29%20%3D%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%5Calpha%20%7Bd_k%7D%7D%20%5Cright%29)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%5Cright%29%20%3D%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%5Calpha%20%7Bd_k%7D%7D%20%5Cright%29)
### 这样，从x_k出发，沿搜索方向![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [，确定步长因子](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，使得](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{\rm{\varphi }}\left( {{\alpha _k}} \right) < {\rm{\varphi }}\left( 0 \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%20%7B%7B%5Calpha%20_k%7D%7D%20%5Cright%29%20%3C%20%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%200%20%5Cright%29)
### 的问题就是关于α的一维搜索问题。注意这里是假设其他的![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
### [和](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [都确定了的情况下做的搜索，要搜索的变量只有α。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### 如果求得的![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，使得目标函数沿搜索方向](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [达到最小，即达到下面的情况](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{\rm{f}}\left( {{x_k} + {\alpha _k}{d_k}} \right) =  {\min }\limits_{\alpha  > 0} f\left( {{x_k} + \alpha {d_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3D%20%20%7B%5Cmin%20%7D%5Climits_%7B%5Calpha%20%20%3E%200%7D%20f%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%5Calpha%20%7Bd_k%7D%7D%20%5Cright%29)
### 或者说
![{\rm{\varphi }}\left( {{\alpha _k}} \right) =  {\min }\limits_{\alpha  > 0} \varphi \left( \alpha  \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%20%7B%7B%5Calpha%20_k%7D%7D%20%5Cright%29%20%3D%20%20%7B%5Cmin%20%7D%5Climits_%7B%5Calpha%20%20%3E%200%7D%20%5Cvarphi%20%5Cleft%28%20%5Calpha%20%20%5Cright%29)
### 如果能求导这个最优的![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，那么这个](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [就称为最优步长因子，这样的搜索方法就称为最优一维搜索，或者精确一维搜索。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### 但是现实情况往往不是这样，实际计算中精确的最优步长因子一般比较难求，工作量也大，所以往往会折中用不精确的一维搜索。
### 不精确的一维搜索，也叫近似一维搜索。方法是选择![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，使得目标函数f得到可接受的下降量，即使得下降量](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{\rm{f}}\left( {{x_k}} \right) - {\rm{f}}\left( {{x_k} + {\alpha _k}{d_k}} \right) > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20-%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3E%200)
### [是用户可接受的。也就是，只要找到一个步长，使得目标函数下降了一个比较满意的量就可以了。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20-%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3E%200)
### 为啥要选步长？看下图，步长选不好，方向哪怕是对的，也是跑来跑去，不往下走，二维的情况简单点，高维的可能会弄出一直原地不动的情况来。

![](https://img-blog.csdn.net/20131123183207484)

### 一维搜索的主要结构如下：1）首先确定包含问题最优解的搜索区间，再采用某种分割技术或插值方法缩小这个区间，进行搜索求解。
### 当然这个搜索方法主要是适应单峰区间的，就是类似上面的那种，只有一个谷底的。
### 1.2.1.1确定搜索区间
### 确定搜索区间一般用进退法，思想是从某一点出发，按某步长，确定函数值呈现“高-低-高”的三个点，一个方向不成功，就退回来，沿相反方向寻找。
### 下面是步骤。
![](https://img-blog.csdn.net/20131123183922312)
### [
](https://img-blog.csdn.net/20131123183922312)
### [
](https://img-blog.csdn.net/20131123183922312)
### 1.2.1.2搜索求解
### [
](https://img-blog.csdn.net/20131123183922312)
### [
](https://img-blog.csdn.net/20131123183922312)
### 搜索求解的话，0.618法简单实用。虽然Fibonacci法，插值法等等虽然好，复杂，就不多说了。下面是0.618法的步骤。
### [
](https://img-blog.csdn.net/20131123183922312)![](https://img-blog.csdn.net/20131123184054890)
### [
](https://img-blog.csdn.net/20131123184054890)
### [
](https://img-blog.csdn.net/20131123184054890)
### 普通的0.618法要求一维搜索的函数是单峰函数，实际上遇到的函数不一定是单峰函数，这时，可能产生搜索得到的函数值反而大于初始区间端点出函数值的情况。有人建议每次缩小区间是，不要只比较两个内点处的函数值，而是比较两内点和两端点处的函数值。当左边第一个或第二个点是这四个点中函数值最小的点是，丢弃右端点，构成新的搜索区间；否则，丢弃左端点，构成新的搜索区间，经过这样的修改，算法会变得可靠些。步骤就不列了。
### [
](https://img-blog.csdn.net/20131123184054890)
### 1.2.2不精确一维搜索方法
### [
](https://img-blog.csdn.net/20131123184054890)
### 一维搜索过程是最优化方法的基本组成部分，精确的一维搜索方法往往需要花费很大的工作量。特别是迭代点远离问题的解时，精确地求解一个一维子问题通常不是十分有效的。另外，在实际上，很多最优化方法，例如牛顿法和拟牛顿法，其收敛速度并不依赖于精确一维搜索过程。因此，只要保证目标函数f(x)在每一步都有满意的下降，这样就可以大大节省工作量。
### [
](https://img-blog.csdn.net/20131123184054890)
### 有几位科学家Armijo(1966)和Goldstein(1965)分别提出了不精确一维搜索过程。设
### [
](https://img-blog.csdn.net/20131123184054890)![{\rm{J}} = \{ {\rm{\alpha }} > 0|{\rm{f}}\left( {{x_k} + \alpha {d_k}} \right) < {\rm{f}}\left( {{x_k}} \right)\} ](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BJ%7D%7D%20%3D%20%5C%7B%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%3E%200%7C%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%5Calpha%20%7Bd_k%7D%7D%20%5Cright%29%20%3C%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5C%7D%20)
### （2.5.1）
### 是一个区间。看下图
![](https://img-blog.csdn.net/20131123184345125)
### [
](https://img-blog.csdn.net/20131123184345125)
### 在图中，区间J=(0,a)。为了保证目标函数单调下降，同时要求f的下降不是太小（如果f的下降太小，可能导致序列![\left\{ {{\rm{f}}\left( {{x_k}} \right)} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%7D%20%5Cright%5C%7D)
### [的极限值不是极小值），必须避免所选择的α太靠近区间j的短短。一个合理的要求是](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%7D%20%5Cright%5C%7D)
![{\rm{f}}\left( {{x_k} + {s_k}} \right) \le {\rm{f}}\left( {{x_k}} \right) + \rho g_k^T{s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cle%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### [                       
 （2.5.2）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cle%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cle%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)![{\rm{f}}\left( {{x_k} + {s_k}} \right) \ge {\rm{f}}\left( {{x_k}} \right) + \left( {1 - \rho } \right)g_k^T{s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cge%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Cleft%28%20%7B1%20-%20%5Crho%20%7D%20%5Cright%29g_k%5ET%7Bs_k%7D)
### [             
 （2.5.3）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cge%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Cleft%28%20%7B1%20-%20%5Crho%20%7D%20%5Cright%29g_k%5ET%7Bs_k%7D)
### 其中0<ρ<1/2，![{s_k} = {\alpha _k}{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bs_k%7D%20%3D%20%7B%5Calpha%20_k%7D%7Bd_k%7D)
### [。满足(2.5.2)要求的α_k构成区间J_1=(0,c]，这就扔掉了区间J右端附件的点。但是为了避免α太小的情况，又加上了另一个要求(2.5.3)，这个要求扔掉了区间J的左端点附件的点。看图中](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bs_k%7D%20%3D%20%7B%5Calpha%20_k%7D%7Bd_k%7D)![{\rm{f}}\left( {{x_k}} \right) + \rho g_k^T{s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### [和](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)![{\rm{f}}\left( {{x_k}} \right) + \left( {1 - \rho } \right)g_k^T{s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Cleft%28%20%7B1%20-%20%5Crho%20%7D%20%5Cright%29g_k%5ET%7Bs_k%7D)
### [两条虚线夹出来的区间J_2=[b,c]就是满足条件(2.5.2)和(2.5.3)的搜索区间，称为可接受区间。条件(2.5.2)和(2.5.3)称为Armijo-Goldstein准则。无论用什么办法得到步长因子α，只要满足条件(2.5.2)和(2.5.3)，就可以称它为可接受步长因子。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Cleft%28%20%7B1%20-%20%5Crho%20%7D%20%5Cright%29g_k%5ET%7Bs_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### 其中这个要求是必须的，因为不用这个条件，可能会影响牛顿法和拟牛顿法的超线性收敛。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### 在图中可以看到一种情况，极小值e也被扔掉了，为了解决这种情况，Wolfe-Powell准则给出了一个更简单的条件代替
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)![g_{k + 1}^T{d_k} \ge \sigma g_k^T{d_k},{\rm{\sigma }} \in \left( {{\rm{\rho }},1} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g_%7Bk%20%2B%201%7D%5ET%7Bd_k%7D%20%5Cge%20%5Csigma%20g_k%5ET%7Bd_k%7D%2C%7B%5Crm%7B%5Csigma%20%7D%7D%20%5Cin%20%5Cleft%28%20%7B%7B%5Crm%7B%5Crho%20%7D%7D%2C1%7D%20%5Cright%29)
### [                              
 （2.5.4）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g_%7Bk%20%2B%201%7D%5ET%7Bd_k%7D%20%5Cge%20%5Csigma%20g_k%5ET%7Bd_k%7D%2C%7B%5Crm%7B%5Csigma%20%7D%7D%20%5Cin%20%5Cleft%28%20%7B%7B%5Crm%7B%5Crho%20%7D%7D%2C1%7D%20%5Cright%29)

### 其几何解释是在可接受点处切线的斜率![\varphi '\left( {{\alpha _k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cvarphi%20%27%5Cleft%28%20%7B%7B%5Calpha%20_k%7D%7D%20%5Cright%29)
### [大于或等于初始斜率的σ倍。准则(2.5.2)和(2.5.4)称为Wolfe-Powell准则，其可接受区间为J_3=[e,c]。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cvarphi%20%27%5Cleft%28%20%7B%7B%5Calpha%20_k%7D%7D%20%5Cright%29)

### 要求ρ<σ<1是必要的，它保证了满足不精确线性搜索准则的步长因子![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [的存在，不这么弄，可能](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![\sigma g_k^T{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### [这个虚线会往下压，没有交点，就搞不出一个区间来了。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### 一般地，σ值越小，线性搜索越精确。取σ=0.1，就得到一个相当精确的线性搜索，而取σ=0.9，则得到一个相当弱的线性搜索。不过σ值越小，工作量越大。所以不精确线性搜索不要求过小的σ，通常可取ρ=0.1，σ=0.4。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### 下面就给出Armijo-Goldstein准则和Wolfe-Powell准则的框图。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### 从算法框图中可以看出，两种方法是类似的，只是在准则不成立，需要计算新的时，一个利用了简单的求区间中点的方法，另一个采用了二次插值方法。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)![](https://img-blog.csdn.net/20131123185511171)
### [
](https://img-blog.csdn.net/20131123185511171)![](https://img-blog.csdn.net/20131123185516390)

### 算法步骤只给出Armijo-Goldstein不精确一维搜索方法的，下面就是
![](https://img-blog.csdn.net/20131123185520562)
### [
](https://img-blog.csdn.net/20131123185520562)
### 好了，说到这，确定步长的方法也说完了，其实方法不少，实际用到的肯定是最简单的几种，就把简单的几种提了一下，至于为什么这样，收敛如何，证明的东西大家可以去书中慢慢看。


### 1.3方向确定
### 1.3.1最速下降法
### 最速下降法以负梯度方向作为最优化方法的下降方向，又称为梯度下降法，是最简单实用的方法。


### 1.3.1.1算法步骤
### 下面是步骤。
![](https://img-blog.csdn.net/20131123193429984)

### 看个例子。
![](https://img-blog.csdn.net/20131123193544125)
### [
](https://img-blog.csdn.net/20131123193544125)
### [
](https://img-blog.csdn.net/20131123193544125)
### 这个选步长的方法是对二次函数下的特殊情况，是比较快而且好的显式形式，说明步长选得好，收敛很快。
### [
](https://img-blog.csdn.net/20131123193544125)
### 1.3.1.2缺点
### [
](https://img-blog.csdn.net/20131123193544125)
### 数值实验表明，当目标函数的等值线接近一个圆（球）时，最速下降法下降较快，当目标函数的等值线是一个扁长的椭球是，最速下降法开始几步下降较快，后来就出现锯齿线性，下降就十分缓慢。原因是一维搜索满足![g_{k + 1}^T{d_k} = 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g_%7Bk%20%2B%201%7D%5ET%7Bd_k%7D%20%3D%200)
### ，即![g_{k + 1}^T{g_k} = d_{k + 1}^T{d_k} = 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g_%7Bk%20%2B%201%7D%5ET%7Bg_k%7D%20%3D%20d_%7Bk%20%2B%201%7D%5ET%7Bd_k%7D%20%3D%200)
### ，这表明在相邻两个迭代点上函数f(x)的两个梯度繁星是互相直交（正交）的。即，两个搜索方向互相直交，就产生了锯齿性质。当接近极小点时，步长越小，前进越慢。
### 下图是锯齿的一个图。
![](https://img-blog.csdn.net/20131123193816453)

### 1.3.2牛顿法
### 1.3.2.1算法思想和步骤
### 牛顿法的基本思想是利用目标函数的二次Taylor展开，并将其极小化。
### 设f(x)是二次可微实函数，![{x_k} \in {{\rm{R}}^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D%20%5Cin%20%7B%7B%5Crm%7BR%7D%7D%5En%7D)
### ，Hesse矩阵![{\nabla ^2}f\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnabla%20%5E2%7Df%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### 正定。在![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
### 附件用二次Taylor展开近似f，
![](https://img-blog.csdn.net/20131123194251968)
### [
](https://img-blog.csdn.net/20131123194251968)
### [
](https://img-blog.csdn.net/20131123194251968)
### 其中，![s = {\rm{x}} - {x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=s%20%3D%20%7B%5Crm%7Bx%7D%7D%20-%20%7Bx_k%7D)
### [，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=s%20%3D%20%7B%5Crm%7Bx%7D%7D%20-%20%7Bx_k%7D)![{q^{\left( k \right)}}\left( s \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bq%5E%7B%5Cleft%28%20k%20%5Cright%29%7D%7D%5Cleft%28%20s%20%5Cright%29)
### [为f(x)的二次近似。将上式右边极小化，便得](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bq%5E%7B%5Cleft%28%20k%20%5Cright%29%7D%7D%5Cleft%28%20s%20%5Cright%29)
![{x_{k + 1}} = {x_k} - {\left[ {{\nabla ^2}f\left( {{x_k}} \right)} \right]^{ - 1}}\nabla {\rm{f}}\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20-%20%7B%5Cleft%5B%20%7B%7B%5Cnabla%20%5E2%7Df%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%7D%20%5Cright%5D%5E%7B%20-%201%7D%7D%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### 这就是牛顿迭代公式。在这个公式中，步长因子![{\alpha _k} = 1](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D%20%3D%201)
### [。令](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D%20%3D%201)![{G_k} = {\nabla ^2}f\left( {{x_k}} \right) = H(f)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D%20%3D%20%7B%5Cnabla%20%5E2%7Df%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%3D%20H%28f%29)
### [,](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D%20%3D%20%7B%5Cnabla%20%5E2%7Df%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%3D%20H%28f%29)![{g_k} = \nabla {\rm{f}}\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### [，则上面的迭代式也可以写成](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)![{x_{k + 1}} = {x_k} - {\rm{G}}_k^{ - 1}{g_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20-%20%7B%5Crm%7BG%7D%7D_k%5E%7B%20-%201%7D%7Bg_k%7D)
### [。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20-%20%7B%5Crm%7BG%7D%7D_k%5E%7B%20-%201%7D%7Bg_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### 其中的Hesse矩阵的形式如下。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)![](https://img-blog.csdn.net/20131123202137125)

### 一个例子如下。
![](https://img-blog.csdn.net/20131123202052484)
### 对于正定二次函数，牛顿法一步就可以得到最优解。
### 对于非二次函数，牛顿法并不能保证经过有限次迭代求得最优解，但由于目标函数在极小点附近近似于二次函数，所以当初始点靠近极小点时，牛顿法的收敛速度一般是快的。
### 当初始点远离最优解，![{G_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)
### [不一定正定，牛顿方向不一定是下降方向，其收敛性不能保证，这说明步长一直是1是不合适的，应该在牛顿法中采用某种一维搜索来确定步长因子。但是要强调一下，仅当步长因子](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)![\{ {\alpha _k}\} ](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5C%7B%20%7B%5Calpha%20_k%7D%5C%7D%20)
### [收敛到1时，牛顿法才是二阶收敛的。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5C%7B%20%7B%5Calpha%20_k%7D%5C%7D%20)
### 这时的牛顿法称为阻尼牛顿法，步骤如下。
![](https://img-blog.csdn.net/20131123202407078)

### 下面看个例子。
![](https://img-blog.csdn.net/20131123202514343)
![](https://img-blog.csdn.net/20131123202520296)

### [
](https://img-blog.csdn.net/20131123202520296)
### [
](https://img-blog.csdn.net/20131123202520296)
### 这样的牛顿法是总体收敛的。
### [
](https://img-blog.csdn.net/20131123202520296)
### 1.3.2.2缺点
### [
](https://img-blog.csdn.net/20131123202520296)
### 牛顿法面临的主要困难是Hesse矩阵![{G_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)
### 不正定。这时候二次模型不一定有极小点，甚至没有平稳点。当![{G_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)
### 不正定时，二次模型函数是无界的。
### 为了克服这种困难，有多种方法，常用的方法是使牛顿方向偏向最速下降方向![ - {g_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20-%20%7Bg_k%7D)
### 。具体做法是将Hesse矩阵![{G_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)
### 改变成![{{\rm{G}}_k} + {\nu _k}I](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BG%7D%7D_k%7D%20%2B%20%7B%5Cnu%20_k%7DI)
### ，其中![{\nu _k} > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnu%20_k%7D%20%3E%200)
### ，使得![{{\rm{G}}_k} + {\nu _k}I](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BG%7D%7D_k%7D%20%2B%20%7B%5Cnu%20_k%7DI)
### 正定。![{\nu _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnu%20_k%7D)
### 一般希望是比较小，最好是刚刚好能使![{{\rm{G}}_k} + {\nu _k}I](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BG%7D%7D_k%7D%20%2B%20%7B%5Cnu%20_k%7DI)
### 正定。
### 1.3.3拟牛顿法
### 牛顿法在实际应用中需要存储二阶导数信息和计算一个矩阵的逆，这对计算机的时间和空间要求都比较高，也容易遇到不正定的Hesse矩阵和病态的Hesse矩阵，导致求出来的逆很古怪，从而算法会沿一个不理想的方向去迭代。
### 有人提出了介于最速下降法与牛顿法之间的方法。一类是共轭方向法，典型的是共轭梯度法，还有拟牛顿法。
### 其中拟牛顿法简单实用，这里就特地介绍，其他方法感兴趣的读者可以去看相关资料。
### 1.3.3.1算法思想和步骤
### 牛顿法的成功的关键是利用了Hesse矩阵提供的曲率信息。但是计算Hesse矩阵工作量大，并且有些目标函数的Hesse矩阵很难计算，甚至不好求出，这就使得仅利用目标函数的一阶导数的方法更受欢迎。拟牛顿法就是利用目标函数值f和一阶导数g（梯度）的信息，构造出目标函数的曲率近似，而不需要明显形成Hesse矩阵，同时具有收敛速度快的特点。
### 设![{\rm{f}}:{R^n} \to R](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%3A%7BR%5En%7D%20%5Cto%20R)
### 在开集![{\cal D} \subset {{\rm{R}}^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ccal%20D%7D%20%5Csubset%20%7B%7B%5Crm%7BR%7D%7D%5En%7D)
### 上二次可微，f在![{x_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)
### 附近的二次近似为
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)![](https://img-blog.csdn.net/20131123203458406)

### 两边求导，得
![{\rm{g}}\left( {\rm{x}} \right) \approx {g_{k + 1}} + {G_{k + 1}}\left( {x - {x_{k + 1}}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bg%7D%7D%5Cleft%28%20%7B%5Crm%7Bx%7D%7D%20%5Cright%29%20%5Capprox%20%7Bg_%7Bk%20%2B%201%7D%7D%20%2B%20%7BG_%7Bk%20%2B%201%7D%7D%5Cleft%28%20%7Bx%20-%20%7Bx_%7Bk%20%2B%201%7D%7D%7D%20%5Cright%29)
### 令![x = {x_k},{s_k} = {x_{k + 1}} - {x_k},{y_k} = {g_{k + 1}} - {g_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=x%20%3D%20%7Bx_k%7D%2C%7Bs_k%7D%20%3D%20%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx_k%7D%2C%7By_k%7D%20%3D%20%7Bg_%7Bk%20%2B%201%7D%7D%20-%20%7Bg_k%7D)
### [，得](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=x%20%3D%20%7Bx_k%7D%2C%7Bs_k%7D%20%3D%20%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx_k%7D%2C%7By_k%7D%20%3D%20%7Bg_%7Bk%20%2B%201%7D%7D%20-%20%7Bg_k%7D)![G_{k + 1}^{ - 1}{y_k} \approx {s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=G_%7Bk%20%2B%201%7D%5E%7B%20-%201%7D%7By_k%7D%20%5Capprox%20%7Bs_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=x%20%3D%20%7Bx_k%7D%2C%7Bs_k%7D%20%3D%20%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx_k%7D%2C%7By_k%7D%20%3D%20%7Bg_%7Bk%20%2B%201%7D%7D%20-%20%7Bg_k%7D)
### 其中![{g_k} = \nabla f\left( {{x_k}} \right) \ne 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20f%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%5Cne%200)
### [，是梯度。那么，只要构造出Hesse矩阵的逆近似](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20f%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%5Cne%200)![{{\rm{H}}_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D)
### [满足这种上式就可以，即满足关系](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20f%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%5Cne%200)![{{\rm{H}}_{k + 1}}{y_k} = {s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%7By_k%7D%20%3D%20%7Bs_k%7D)
### 这个关系就是拟牛顿条件或拟牛顿方程。
### 拟牛顿法的思想就是——用一个矩阵![{{\rm{H}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### [去近似Hesse矩阵的逆矩阵，这样就避免了计算矩阵的逆。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### 当然![{{\rm{H}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### [需要满足一些条件：](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)

### (a)![{{\rm{H}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### [是一个正定的矩阵](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### (b)    如果![{\nabla ^2}f{\left( {{x_k}} \right)^{ - 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)
### [存在，则](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)![{{\rm{H}}_k} \approx {\nabla ^2}f{\left( {{x_k}} \right)^{ - 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D%20%5Capprox%20%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)
### [。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D%20%5Capprox%20%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)
### (c)    初始正定矩阵![{{\rm{H}}_0}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)
### [取定后，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)![{{\rm{H}}_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D)
### [应该由](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D)![{{\rm{H}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### [递推给出，即](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)![{{\rm{H}}_{k + 1}} = {{\rm{H}}_k} + {{\rm{E}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%20%3D%20%7B%7B%5Crm%7BH%7D%7D_k%7D%20%2B%20%7B%7B%5Crm%7BE%7D%7D_k%7D)
### [；其中](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%20%3D%20%7B%7B%5Crm%7BH%7D%7D_k%7D%20%2B%20%7B%7B%5Crm%7BE%7D%7D_k%7D)![{{\rm{E}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BE%7D%7D_k%7D)
### [是修正矩阵，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BE%7D%7D_k%7D)![{{\rm{H}}_{k + 1}} = {{\rm{H}}_k} + {{\rm{E}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%20%3D%20%7B%7B%5Crm%7BH%7D%7D_k%7D%20%2B%20%7B%7B%5Crm%7BE%7D%7D_k%7D)
### [是修正公式。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%20%3D%20%7B%7B%5Crm%7BH%7D%7D_k%7D%20%2B%20%7B%7B%5Crm%7BE%7D%7D_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BE%7D%7D_k%7D)
### 常用而且有效的修正公式是BFGS公式，如下
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BE%7D%7D_k%7D)![](https://img-blog.csdn.net/20131123204604171)
### [
](https://img-blog.csdn.net/20131123204604171)
### [
](https://img-blog.csdn.net/20131123204604171)
### 下面给出BFGS公式下的拟牛顿法
### [
](https://img-blog.csdn.net/20131123204604171)![](https://img-blog.csdn.net/20131123204718859)
### 在上述步骤中，初始正定矩阵![{{\rm{H}}_0}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)
### [通常取为单位矩阵，即](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)![{{\rm{H}}_0} = {\rm{I}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### [。这样，拟牛顿法的第一次迭代相当于一个最速下降迭代。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 1.3.3.2优缺点
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 与牛顿法相比，有两个优点：
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### (a)    仅需一阶导数
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### (b)    校正保持正定性，因而下降性质成立
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### (c)    无需计算逆矩阵，但具有超线性收敛速度
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### (d)    每次迭代仅需要次乘法计算
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 缺点是初始点距离最优解远时速度还是慢。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 解决方法是，迭代前期用最速下降法进行迭代，得到一定解以后用拟牛顿法迭代。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 1.1问题定义
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 1.1.1优化问题定义
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 最优化问题数学上定义，最优化问题的一般形式为
![\begin{array}{l}\min f(x)\\s.t.{\rm{ }}x \in X\end{array}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmin%20f%28x%29%5C%5C%0As.t.%7B%5Crm%7B%20%7D%7Dx%20%5Cin%20X%0A%5Cend%7Barray%7D)
### （1）
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmin%20f%28x%29%5C%5C%0As.t.%7B%5Crm%7B%20%7D%7Dx%20%5Cin%20X%0A%5Cend%7Barray%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmin%20f%28x%29%5C%5C%0As.t.%7B%5Crm%7B%20%7D%7Dx%20%5Cin%20X%0A%5Cend%7Barray%7D)
### 其中的![{\rm{x}} \in {R^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)
### [是自变量，f(x)是目标函数，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)![{\rm{X}} \subset {R^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BX%7D%7D%20%5Csubset%20%7BR%5En%7D)
### [为约束集或者说可行域。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BX%7D%7D%20%5Csubset%20%7BR%5En%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)
### 可行域这个东西，有等式约束，也有不等式约束，或者没有约束，这次的精力主要还是集中在无约束的优化问题上，如果有精力，再讨论有约束的。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)
### 1.1.2最优化方法定义
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bx%7D%7D%20%5Cin%20%7BR%5En%7D)
### 优化问题的解法叫最优化方法。
### 最优化方法通常采用迭代的方法求它的最优解，其基本思想是：给定一个初始点![{{\rm{x}}_0} \in {R^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bx%7D%7D_0%7D%20%5Cin%20%7BR%5En%7D)
### 。按照某一迭代规则产生一个点列![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### ，使得当![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### 是有穷点列时，其最后一个点是最优化模型问题的最优解，当是![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### 无穷点列时，它有极限点，且其极限点是最优化模型问题的最优解。一个好的算法应具备的典型特征为：迭代点![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
### 能稳定地接近局部极小值点![{{\rm{x}}^*}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bx%7D%7D%5E%2A%7D)
### 的邻域，然后迅速收敛于![{{\rm{x}}^*}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bx%7D%7D%5E%2A%7D)
### 。当给定的某个收敛准则满足时，迭代即终止。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bx%7D%7D%5E%2A%7D)
### 好吧，上面是一堆描述，来点定义，设![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
### [为第k次迭代点，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [第k次搜索方向，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [为第k次步长因子，则第k次迭代为](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{x_{k + 1}} = {x_k} + {\alpha _k}{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D)
### [   
             （2）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D)

### 然后后面的工作几乎都在调整![{\alpha _k}{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D%7Bd_k%7D)
### [这个项了，不同的步长](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D%7Bd_k%7D)![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [和不同的搜索方向](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [分别构成了不同的方法。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### 当然步长![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [和搜索方向](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [是得满足一些条件，毕竟是求最小值，不能越迭代越大了，下面是一些条件](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![\nabla {\rm{f}}{\left( {{x_k}} \right)^T}{d_k} < 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cnabla%20%7B%5Crm%7Bf%7D%7D%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5ET%7D%7Bd_k%7D%20%3C%200)
### [                          
 （3）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cnabla%20%7B%5Crm%7Bf%7D%7D%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5ET%7D%7Bd_k%7D%20%3C%200)
![{\rm{f}}\left( {{x_k} + {\alpha _k}{d_k}} \right) < {\rm{f}}\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3C%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### [            
 （4）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3C%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)

### 式子（3）的意义是搜索方向必须跟梯度方向（梯度也就是![\nabla {\rm{f}}\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### [）夹角大于90度，也就是基本保证是向梯度方向的另外一边搜索，至于为什么要向梯度方向的另外一边搜索，就是那样才能保证是向目标函数值更小的方向搜索的，因为梯度方向一般是使目标函数增大的（不增大的情况是目标函数已经到达极值）。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### 式子（4）的意义就很明显了，迭代的下一步的目标函数比上一步小。
### 最优化方法的基本结构为：
### 给定初始点![{x_0}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_0%7D)
### [，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_0%7D)
### （a）      确定搜索方向![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [，即按照一定规则，构造目标函数f在](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
### [点处的下降方向作为搜索方向。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### （b）      确定步长因子![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，使目标函数值有某种意义的下降。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### （c）      令![{x_{k + 1}} = {x_k} + {\alpha _k}{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D)
### 若![{x_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)
### 满足某种终止条件，则停止迭代，得到近似最优解![{x_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)
### ，否则，重复以上步骤。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)
### 能不能收敛到最优解是衡量最优化算法的有效性的一个重要方面。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)

### 1.1.3收敛速度简介

### 收敛速度也是衡量最优化方法有效性的一个重要方面。
### 设算法产生的迭代点列![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### [在某种范数意义下收敛，即](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
![ {\lim }\limits_{{\rm{k}} \to {\rm{\infty }}} {x_k} - {x^*} = 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%20%3D%200)
### [  
     （5）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%20%3D%200)

### 若存在实数![{\rm{\alpha }} > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3E%200)
### [（这个](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3E%200)![{\rm{\alpha }}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D)
### [不是步长）以及一个与迭代次数k无关的常数](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D)![{\rm{q}} > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)
### [，使得](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D)![ {\lim }\limits_{{\rm{k}} \to {\rm{\infty }}} \frac{{{x_{k + 1}} - {x^*}}}{{{x_k} - {x^*}^\alpha }} = {\rm{q}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%5Cfrac%7B%7B%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx%5E%2A%7D%7D%7D%7B%7B%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%5E%5Calpha%20%7D%7D%20%3D%20%7B%5Crm%7Bq%7D%7D)
### [  
 （6）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%5Cfrac%7B%7B%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx%5E%2A%7D%7D%7D%7B%7B%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%5E%5Calpha%20%7D%7D%20%3D%20%7B%5Crm%7Bq%7D%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20%7B%5Clim%20%7D%5Climits_%7B%7B%5Crm%7Bk%7D%7D%20%5Cto%20%7B%5Crm%7B%5Cinfty%20%7D%7D%7D%20%5Cfrac%7B%7B%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx%5E%2A%7D%7D%7D%7B%7B%7Bx_k%7D%20-%20%7Bx%5E%2A%7D%5E%5Calpha%20%7D%7D%20%3D%20%7B%5Crm%7Bq%7D%7D)
### 则称算法产生的迭代点列![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### 具有![{\rm{Q}} - {\rm{\alpha }}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BQ%7D%7D%20-%20%7B%5Crm%7B%5Calpha%20%7D%7D)
### 阶收敛速度。特别地，常用的说法有以下几种。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BQ%7D%7D%20-%20%7B%5Crm%7B%5Calpha%20%7D%7D)
### （a）      当![{\rm{\alpha }} = 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%200)
### [，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%200)![{\rm{q}} > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)
### [时，迭代点列](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### [叫做具有Q-线性收敛速度；](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bq%7D%7D%20%3E%200)
### （b）      当![1 < {\rm{\alpha }} < 2](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=1%20%3C%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%3C%202)
### [，q>0时或](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=1%20%3C%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%3C%202)![{\rm{\alpha }} = 1](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%201)
### [，q=0时，迭代点列](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%201)![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### [叫做具有Q-超线性收敛速度；](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%201)
### （c）      当![{\rm{\alpha }} = 2](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%202)
### [时，迭代点列](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Calpha%20%7D%7D%20%3D%202)![\left\{ {{x_k}} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### [叫做具有Q-二阶收敛速度；](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### 还有一种叫做R-收敛速度，不讨论了，有兴趣可以查阅相关资料。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)
### 一般认为，具有超线性收敛速度和二阶收敛速度的方法是比较快速的。但是对于一个算法，收敛性和收敛速度的理论结果并不保证算法在实际执行时一定有好的实际计算特性。一方面是由于这些结果本身并不能保证方法一定有好的特性，另一方面是由于算法忽略了计算过程中十分重要的舍入误差的影响。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7Bx_k%7D%7D%20%5Cright%5C%7D)

### 1.2步长确定

### 1.2.1一维搜索

### 如前面的讨论，优化方法的关键是构造搜索方向![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### 和步长因子![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### ，这一节就讨论如何确定步长。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### 设
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{\rm{\varphi }}\left( {\rm{\alpha }} \right) = {\rm{f}}\left( {{x_k} + \alpha {d_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%5Cright%29%20%3D%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%5Calpha%20%7Bd_k%7D%7D%20%5Cright%29)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%5Cright%29%20%3D%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%5Calpha%20%7Bd_k%7D%7D%20%5Cright%29)
### 这样，从x_k出发，沿搜索方向![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [，确定步长因子](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，使得](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{\rm{\varphi }}\left( {{\alpha _k}} \right) < {\rm{\varphi }}\left( 0 \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%20%7B%7B%5Calpha%20_k%7D%7D%20%5Cright%29%20%3C%20%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%200%20%5Cright%29)
### 的问题就是关于α的一维搜索问题。注意这里是假设其他的![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
### [和](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [都确定了的情况下做的搜索，要搜索的变量只有α。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### 如果求得的![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，使得目标函数沿搜索方向](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{{\rm{d}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)
### [达到最小，即达到下面的情况](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7Bd%7D%7D_k%7D)![{\rm{f}}\left( {{x_k} + {\alpha _k}{d_k}} \right) =  {\min }\limits_{\alpha  > 0} f\left( {{x_k} + \alpha {d_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3D%20%20%7B%5Cmin%20%7D%5Climits_%7B%5Calpha%20%20%3E%200%7D%20f%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%5Calpha%20%7Bd_k%7D%7D%20%5Cright%29)
### 或者说
![{\rm{\varphi }}\left( {{\alpha _k}} \right) =  {\min }\limits_{\alpha  > 0} \varphi \left( \alpha  \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7B%5Cvarphi%20%7D%7D%5Cleft%28%20%7B%7B%5Calpha%20_k%7D%7D%20%5Cright%29%20%3D%20%20%7B%5Cmin%20%7D%5Climits_%7B%5Calpha%20%20%3E%200%7D%20%5Cvarphi%20%5Cleft%28%20%5Calpha%20%20%5Cright%29)
### 如果能求导这个最优的![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，那么这个](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [就称为最优步长因子，这样的搜索方法就称为最优一维搜索，或者精确一维搜索。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### 但是现实情况往往不是这样，实际计算中精确的最优步长因子一般比较难求，工作量也大，所以往往会折中用不精确的一维搜索。
### 不精确的一维搜索，也叫近似一维搜索。方法是选择![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [，使得目标函数f得到可接受的下降量，即使得下降量](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![{\rm{f}}\left( {{x_k}} \right) - {\rm{f}}\left( {{x_k} + {\alpha _k}{d_k}} \right) > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20-%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3E%200)
### [是用户可接受的。也就是，只要找到一个步长，使得目标函数下降了一个比较满意的量就可以了。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20-%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7B%5Calpha%20_k%7D%7Bd_k%7D%7D%20%5Cright%29%20%3E%200)
### 为啥要选步长？看下图，步长选不好，方向哪怕是对的，也是跑来跑去，不往下走，二维的情况简单点，高维的可能会弄出一直原地不动的情况来。

![](https://img-blog.csdn.net/20131123183207484)

### 一维搜索的主要结构如下：1）首先确定包含问题最优解的搜索区间，再采用某种分割技术或插值方法缩小这个区间，进行搜索求解。
### 当然这个搜索方法主要是适应单峰区间的，就是类似上面的那种，只有一个谷底的。
### 1.2.1.1确定搜索区间
### 确定搜索区间一般用进退法，思想是从某一点出发，按某步长，确定函数值呈现“高-低-高”的三个点，一个方向不成功，就退回来，沿相反方向寻找。
### 下面是步骤。
![](https://img-blog.csdn.net/20131123183922312)
### [
](https://img-blog.csdn.net/20131123183922312)
### [
](https://img-blog.csdn.net/20131123183922312)
### 1.2.1.2搜索求解
### [
](https://img-blog.csdn.net/20131123183922312)
### [
](https://img-blog.csdn.net/20131123183922312)
### 搜索求解的话，0.618法简单实用。虽然Fibonacci法，插值法等等虽然好，复杂，就不多说了。下面是0.618法的步骤。
### [
](https://img-blog.csdn.net/20131123183922312)![](https://img-blog.csdn.net/20131123184054890)
### [
](https://img-blog.csdn.net/20131123184054890)
### [
](https://img-blog.csdn.net/20131123184054890)
### 普通的0.618法要求一维搜索的函数是单峰函数，实际上遇到的函数不一定是单峰函数，这时，可能产生搜索得到的函数值反而大于初始区间端点出函数值的情况。有人建议每次缩小区间是，不要只比较两个内点处的函数值，而是比较两内点和两端点处的函数值。当左边第一个或第二个点是这四个点中函数值最小的点是，丢弃右端点，构成新的搜索区间；否则，丢弃左端点，构成新的搜索区间，经过这样的修改，算法会变得可靠些。步骤就不列了。
### [
](https://img-blog.csdn.net/20131123184054890)
### 1.2.2不精确一维搜索方法
### [
](https://img-blog.csdn.net/20131123184054890)
### 一维搜索过程是最优化方法的基本组成部分，精确的一维搜索方法往往需要花费很大的工作量。特别是迭代点远离问题的解时，精确地求解一个一维子问题通常不是十分有效的。另外，在实际上，很多最优化方法，例如牛顿法和拟牛顿法，其收敛速度并不依赖于精确一维搜索过程。因此，只要保证目标函数f(x)在每一步都有满意的下降，这样就可以大大节省工作量。
### [
](https://img-blog.csdn.net/20131123184054890)
### 有几位科学家Armijo(1966)和Goldstein(1965)分别提出了不精确一维搜索过程。设
### [
](https://img-blog.csdn.net/20131123184054890)![{\rm{J}} = \{ {\rm{\alpha }} > 0|{\rm{f}}\left( {{x_k} + \alpha {d_k}} \right) < {\rm{f}}\left( {{x_k}} \right)\} ](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7BJ%7D%7D%20%3D%20%5C%7B%20%7B%5Crm%7B%5Calpha%20%7D%7D%20%3E%200%7C%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%5Calpha%20%7Bd_k%7D%7D%20%5Cright%29%20%3C%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5C%7D%20)
### （2.5.1）
### 是一个区间。看下图
![](https://img-blog.csdn.net/20131123184345125)
### [
](https://img-blog.csdn.net/20131123184345125)
### 在图中，区间J=(0,a)。为了保证目标函数单调下降，同时要求f的下降不是太小（如果f的下降太小，可能导致序列![\left\{ {{\rm{f}}\left( {{x_k}} \right)} \right\}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%7D%20%5Cright%5C%7D)
### [的极限值不是极小值），必须避免所选择的α太靠近区间j的短短。一个合理的要求是](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cleft%5C%7B%20%7B%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%7D%20%5Cright%5C%7D)
![{\rm{f}}\left( {{x_k} + {s_k}} \right) \le {\rm{f}}\left( {{x_k}} \right) + \rho g_k^T{s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cle%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### [                       
 （2.5.2）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cle%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cle%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)![{\rm{f}}\left( {{x_k} + {s_k}} \right) \ge {\rm{f}}\left( {{x_k}} \right) + \left( {1 - \rho } \right)g_k^T{s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cge%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Cleft%28%20%7B1%20-%20%5Crho%20%7D%20%5Cright%29g_k%5ET%7Bs_k%7D)
### [             
 （2.5.3）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%20%2B%20%7Bs_k%7D%7D%20%5Cright%29%20%5Cge%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Cleft%28%20%7B1%20-%20%5Crho%20%7D%20%5Cright%29g_k%5ET%7Bs_k%7D)
### 其中0<ρ<1/2，![{s_k} = {\alpha _k}{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bs_k%7D%20%3D%20%7B%5Calpha%20_k%7D%7Bd_k%7D)
### [。满足(2.5.2)要求的α_k构成区间J_1=(0,c]，这就扔掉了区间J右端附件的点。但是为了避免α太小的情况，又加上了另一个要求(2.5.3)，这个要求扔掉了区间J的左端点附件的点。看图中](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bs_k%7D%20%3D%20%7B%5Calpha%20_k%7D%7Bd_k%7D)![{\rm{f}}\left( {{x_k}} \right) + \rho g_k^T{s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### [和](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)![{\rm{f}}\left( {{x_k}} \right) + \left( {1 - \rho } \right)g_k^T{s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Cleft%28%20%7B1%20-%20%5Crho%20%7D%20%5Cright%29g_k%5ET%7Bs_k%7D)
### [两条虚线夹出来的区间J_2=[b,c]就是满足条件(2.5.2)和(2.5.3)的搜索区间，称为可接受区间。条件(2.5.2)和(2.5.3)称为Armijo-Goldstein准则。无论用什么办法得到步长因子α，只要满足条件(2.5.2)和(2.5.3)，就可以称它为可接受步长因子。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Cleft%28%20%7B1%20-%20%5Crho%20%7D%20%5Cright%29g_k%5ET%7Bs_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### 其中这个要求是必须的，因为不用这个条件，可能会影响牛顿法和拟牛顿法的超线性收敛。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)
### 在图中可以看到一种情况，极小值e也被扔掉了，为了解决这种情况，Wolfe-Powell准则给出了一个更简单的条件代替
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%2B%20%5Crho%20g_k%5ET%7Bs_k%7D)![g_{k + 1}^T{d_k} \ge \sigma g_k^T{d_k},{\rm{\sigma }} \in \left( {{\rm{\rho }},1} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g_%7Bk%20%2B%201%7D%5ET%7Bd_k%7D%20%5Cge%20%5Csigma%20g_k%5ET%7Bd_k%7D%2C%7B%5Crm%7B%5Csigma%20%7D%7D%20%5Cin%20%5Cleft%28%20%7B%7B%5Crm%7B%5Crho%20%7D%7D%2C1%7D%20%5Cright%29)
### [                              
 （2.5.4）](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g_%7Bk%20%2B%201%7D%5ET%7Bd_k%7D%20%5Cge%20%5Csigma%20g_k%5ET%7Bd_k%7D%2C%7B%5Crm%7B%5Csigma%20%7D%7D%20%5Cin%20%5Cleft%28%20%7B%7B%5Crm%7B%5Crho%20%7D%7D%2C1%7D%20%5Cright%29)

### 其几何解释是在可接受点处切线的斜率![\varphi '\left( {{\alpha _k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cvarphi%20%27%5Cleft%28%20%7B%7B%5Calpha%20_k%7D%7D%20%5Cright%29)
### [大于或等于初始斜率的σ倍。准则(2.5.2)和(2.5.4)称为Wolfe-Powell准则，其可接受区间为J_3=[e,c]。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cvarphi%20%27%5Cleft%28%20%7B%7B%5Calpha%20_k%7D%7D%20%5Cright%29)

### 要求ρ<σ<1是必要的，它保证了满足不精确线性搜索准则的步长因子![{\alpha _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)
### [的存在，不这么弄，可能](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D)![\sigma g_k^T{d_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### [这个虚线会往下压，没有交点，就搞不出一个区间来了。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### 一般地，σ值越小，线性搜索越精确。取σ=0.1，就得到一个相当精确的线性搜索，而取σ=0.9，则得到一个相当弱的线性搜索。不过σ值越小，工作量越大。所以不精确线性搜索不要求过小的σ，通常可取ρ=0.1，σ=0.4。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### 下面就给出Armijo-Goldstein准则和Wolfe-Powell准则的框图。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)
### 从算法框图中可以看出，两种方法是类似的，只是在准则不成立，需要计算新的时，一个利用了简单的求区间中点的方法，另一个采用了二次插值方法。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Csigma%20g_k%5ET%7Bd_k%7D)![](https://img-blog.csdn.net/20131123185511171)
### [
](https://img-blog.csdn.net/20131123185511171)![](https://img-blog.csdn.net/20131123185516390)

### 算法步骤只给出Armijo-Goldstein不精确一维搜索方法的，下面就是
![](https://img-blog.csdn.net/20131123185520562)
### [
](https://img-blog.csdn.net/20131123185520562)
### 好了，说到这，确定步长的方法也说完了，其实方法不少，实际用到的肯定是最简单的几种，就把简单的几种提了一下，至于为什么这样，收敛如何，证明的东西大家可以去书中慢慢看。


### 1.3方向确定
### 1.3.1最速下降法
### 最速下降法以负梯度方向作为最优化方法的下降方向，又称为梯度下降法，是最简单实用的方法。


### 1.3.1.1算法步骤
### 下面是步骤。
![](https://img-blog.csdn.net/20131123193429984)

### 看个例子。
![](https://img-blog.csdn.net/20131123193544125)
### [
](https://img-blog.csdn.net/20131123193544125)
### [
](https://img-blog.csdn.net/20131123193544125)
### 这个选步长的方法是对二次函数下的特殊情况，是比较快而且好的显式形式，说明步长选得好，收敛很快。
### [
](https://img-blog.csdn.net/20131123193544125)
### 1.3.1.2缺点
### [
](https://img-blog.csdn.net/20131123193544125)
### 数值实验表明，当目标函数的等值线接近一个圆（球）时，最速下降法下降较快，当目标函数的等值线是一个扁长的椭球是，最速下降法开始几步下降较快，后来就出现锯齿线性，下降就十分缓慢。原因是一维搜索满足![g_{k + 1}^T{d_k} = 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g_%7Bk%20%2B%201%7D%5ET%7Bd_k%7D%20%3D%200)
### ，即![g_{k + 1}^T{g_k} = d_{k + 1}^T{d_k} = 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=g_%7Bk%20%2B%201%7D%5ET%7Bg_k%7D%20%3D%20d_%7Bk%20%2B%201%7D%5ET%7Bd_k%7D%20%3D%200)
### ，这表明在相邻两个迭代点上函数f(x)的两个梯度繁星是互相直交（正交）的。即，两个搜索方向互相直交，就产生了锯齿性质。当接近极小点时，步长越小，前进越慢。
### 下图是锯齿的一个图。
![](https://img-blog.csdn.net/20131123193816453)

### 1.3.2牛顿法
### 1.3.2.1算法思想和步骤
### 牛顿法的基本思想是利用目标函数的二次Taylor展开，并将其极小化。
### 设f(x)是二次可微实函数，![{x_k} \in {{\rm{R}}^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D%20%5Cin%20%7B%7B%5Crm%7BR%7D%7D%5En%7D)
### ，Hesse矩阵![{\nabla ^2}f\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnabla%20%5E2%7Df%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### 正定。在![{x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_k%7D)
### 附件用二次Taylor展开近似f，
![](https://img-blog.csdn.net/20131123194251968)
### [
](https://img-blog.csdn.net/20131123194251968)
### [
](https://img-blog.csdn.net/20131123194251968)
### 其中，![s = {\rm{x}} - {x_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=s%20%3D%20%7B%5Crm%7Bx%7D%7D%20-%20%7Bx_k%7D)
### [，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=s%20%3D%20%7B%5Crm%7Bx%7D%7D%20-%20%7Bx_k%7D)![{q^{\left( k \right)}}\left( s \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bq%5E%7B%5Cleft%28%20k%20%5Cright%29%7D%7D%5Cleft%28%20s%20%5Cright%29)
### [为f(x)的二次近似。将上式右边极小化，便得](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bq%5E%7B%5Cleft%28%20k%20%5Cright%29%7D%7D%5Cleft%28%20s%20%5Cright%29)
![{x_{k + 1}} = {x_k} - {\left[ {{\nabla ^2}f\left( {{x_k}} \right)} \right]^{ - 1}}\nabla {\rm{f}}\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20-%20%7B%5Cleft%5B%20%7B%7B%5Cnabla%20%5E2%7Df%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%7D%20%5Cright%5D%5E%7B%20-%201%7D%7D%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### 这就是牛顿迭代公式。在这个公式中，步长因子![{\alpha _k} = 1](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D%20%3D%201)
### [。令](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Calpha%20_k%7D%20%3D%201)![{G_k} = {\nabla ^2}f\left( {{x_k}} \right) = H(f)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D%20%3D%20%7B%5Cnabla%20%5E2%7Df%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%3D%20H%28f%29)
### [,](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D%20%3D%20%7B%5Cnabla%20%5E2%7Df%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%3D%20H%28f%29)![{g_k} = \nabla {\rm{f}}\left( {{x_k}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### [，则上面的迭代式也可以写成](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)![{x_{k + 1}} = {x_k} - {\rm{G}}_k^{ - 1}{g_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20-%20%7B%5Crm%7BG%7D%7D_k%5E%7B%20-%201%7D%7Bg_k%7D)
### [。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D%20%3D%20%7Bx_k%7D%20-%20%7B%5Crm%7BG%7D%7D_k%5E%7B%20-%201%7D%7Bg_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)
### 其中的Hesse矩阵的形式如下。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20%7B%5Crm%7Bf%7D%7D%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29)![](https://img-blog.csdn.net/20131123202137125)

### 一个例子如下。
![](https://img-blog.csdn.net/20131123202052484)
### 对于正定二次函数，牛顿法一步就可以得到最优解。
### 对于非二次函数，牛顿法并不能保证经过有限次迭代求得最优解，但由于目标函数在极小点附近近似于二次函数，所以当初始点靠近极小点时，牛顿法的收敛速度一般是快的。
### 当初始点远离最优解，![{G_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)
### [不一定正定，牛顿方向不一定是下降方向，其收敛性不能保证，这说明步长一直是1是不合适的，应该在牛顿法中采用某种一维搜索来确定步长因子。但是要强调一下，仅当步长因子](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)![\{ {\alpha _k}\} ](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5C%7B%20%7B%5Calpha%20_k%7D%5C%7D%20)
### [收敛到1时，牛顿法才是二阶收敛的。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5C%7B%20%7B%5Calpha%20_k%7D%5C%7D%20)
### 这时的牛顿法称为阻尼牛顿法，步骤如下。
![](https://img-blog.csdn.net/20131123202407078)

### 下面看个例子。
![](https://img-blog.csdn.net/20131123202514343)
![](https://img-blog.csdn.net/20131123202520296)

### [
](https://img-blog.csdn.net/20131123202520296)
### [
](https://img-blog.csdn.net/20131123202520296)
### 这样的牛顿法是总体收敛的。
### [
](https://img-blog.csdn.net/20131123202520296)
### 1.3.2.2缺点
### [
](https://img-blog.csdn.net/20131123202520296)
### 牛顿法面临的主要困难是Hesse矩阵![{G_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)
### 不正定。这时候二次模型不一定有极小点，甚至没有平稳点。当![{G_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)
### 不正定时，二次模型函数是无界的。
### 为了克服这种困难，有多种方法，常用的方法是使牛顿方向偏向最速下降方向![ - {g_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%20-%20%7Bg_k%7D)
### 。具体做法是将Hesse矩阵![{G_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7BG_k%7D)
### 改变成![{{\rm{G}}_k} + {\nu _k}I](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BG%7D%7D_k%7D%20%2B%20%7B%5Cnu%20_k%7DI)
### ，其中![{\nu _k} > 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnu%20_k%7D%20%3E%200)
### ，使得![{{\rm{G}}_k} + {\nu _k}I](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BG%7D%7D_k%7D%20%2B%20%7B%5Cnu%20_k%7DI)
### 正定。![{\nu _k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnu%20_k%7D)
### 一般希望是比较小，最好是刚刚好能使![{{\rm{G}}_k} + {\nu _k}I](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BG%7D%7D_k%7D%20%2B%20%7B%5Cnu%20_k%7DI)
### 正定。
### 1.3.3拟牛顿法
### 牛顿法在实际应用中需要存储二阶导数信息和计算一个矩阵的逆，这对计算机的时间和空间要求都比较高，也容易遇到不正定的Hesse矩阵和病态的Hesse矩阵，导致求出来的逆很古怪，从而算法会沿一个不理想的方向去迭代。
### 有人提出了介于最速下降法与牛顿法之间的方法。一类是共轭方向法，典型的是共轭梯度法，还有拟牛顿法。
### 其中拟牛顿法简单实用，这里就特地介绍，其他方法感兴趣的读者可以去看相关资料。
### 1.3.3.1算法思想和步骤
### 牛顿法的成功的关键是利用了Hesse矩阵提供的曲率信息。但是计算Hesse矩阵工作量大，并且有些目标函数的Hesse矩阵很难计算，甚至不好求出，这就使得仅利用目标函数的一阶导数的方法更受欢迎。拟牛顿法就是利用目标函数值f和一阶导数g（梯度）的信息，构造出目标函数的曲率近似，而不需要明显形成Hesse矩阵，同时具有收敛速度快的特点。
### 设![{\rm{f}}:{R^n} \to R](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bf%7D%7D%3A%7BR%5En%7D%20%5Cto%20R)
### 在开集![{\cal D} \subset {{\rm{R}}^n}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ccal%20D%7D%20%5Csubset%20%7B%7B%5Crm%7BR%7D%7D%5En%7D)
### 上二次可微，f在![{x_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)
### 附近的二次近似为
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_%7Bk%20%2B%201%7D%7D)![](https://img-blog.csdn.net/20131123203458406)

### 两边求导，得
![{\rm{g}}\left( {\rm{x}} \right) \approx {g_{k + 1}} + {G_{k + 1}}\left( {x - {x_{k + 1}}} \right)](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Crm%7Bg%7D%7D%5Cleft%28%20%7B%5Crm%7Bx%7D%7D%20%5Cright%29%20%5Capprox%20%7Bg_%7Bk%20%2B%201%7D%7D%20%2B%20%7BG_%7Bk%20%2B%201%7D%7D%5Cleft%28%20%7Bx%20-%20%7Bx_%7Bk%20%2B%201%7D%7D%7D%20%5Cright%29)
### 令![x = {x_k},{s_k} = {x_{k + 1}} - {x_k},{y_k} = {g_{k + 1}} - {g_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=x%20%3D%20%7Bx_k%7D%2C%7Bs_k%7D%20%3D%20%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx_k%7D%2C%7By_k%7D%20%3D%20%7Bg_%7Bk%20%2B%201%7D%7D%20-%20%7Bg_k%7D)
### [，得](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=x%20%3D%20%7Bx_k%7D%2C%7Bs_k%7D%20%3D%20%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx_k%7D%2C%7By_k%7D%20%3D%20%7Bg_%7Bk%20%2B%201%7D%7D%20-%20%7Bg_k%7D)![G_{k + 1}^{ - 1}{y_k} \approx {s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=G_%7Bk%20%2B%201%7D%5E%7B%20-%201%7D%7By_k%7D%20%5Capprox%20%7Bs_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=x%20%3D%20%7Bx_k%7D%2C%7Bs_k%7D%20%3D%20%7Bx_%7Bk%20%2B%201%7D%7D%20-%20%7Bx_k%7D%2C%7By_k%7D%20%3D%20%7Bg_%7Bk%20%2B%201%7D%7D%20-%20%7Bg_k%7D)
### 其中![{g_k} = \nabla f\left( {{x_k}} \right) \ne 0](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20f%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%5Cne%200)
### [，是梯度。那么，只要构造出Hesse矩阵的逆近似](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20f%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%5Cne%200)![{{\rm{H}}_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D)
### [满足这种上式就可以，即满足关系](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bg_k%7D%20%3D%20%5Cnabla%20f%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%20%5Cne%200)![{{\rm{H}}_{k + 1}}{y_k} = {s_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%7By_k%7D%20%3D%20%7Bs_k%7D)
### 这个关系就是拟牛顿条件或拟牛顿方程。
### 拟牛顿法的思想就是——用一个矩阵![{{\rm{H}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### [去近似Hesse矩阵的逆矩阵，这样就避免了计算矩阵的逆。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### 当然![{{\rm{H}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### [需要满足一些条件：](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)

### (a)![{{\rm{H}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### [是一个正定的矩阵](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### (b)    如果![{\nabla ^2}f{\left( {{x_k}} \right)^{ - 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)
### [存在，则](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)![{{\rm{H}}_k} \approx {\nabla ^2}f{\left( {{x_k}} \right)^{ - 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D%20%5Capprox%20%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)
### [。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D%20%5Capprox%20%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Cnabla%20%5E2%7Df%7B%5Cleft%28%20%7B%7Bx_k%7D%7D%20%5Cright%29%5E%7B%20-%201%7D%7D)
### (c)    初始正定矩阵![{{\rm{H}}_0}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)
### [取定后，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)![{{\rm{H}}_{k + 1}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D)
### [应该由](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D)![{{\rm{H}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)
### [递推给出，即](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_k%7D)![{{\rm{H}}_{k + 1}} = {{\rm{H}}_k} + {{\rm{E}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%20%3D%20%7B%7B%5Crm%7BH%7D%7D_k%7D%20%2B%20%7B%7B%5Crm%7BE%7D%7D_k%7D)
### [；其中](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%20%3D%20%7B%7B%5Crm%7BH%7D%7D_k%7D%20%2B%20%7B%7B%5Crm%7BE%7D%7D_k%7D)![{{\rm{E}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BE%7D%7D_k%7D)
### [是修正矩阵，](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BE%7D%7D_k%7D)![{{\rm{H}}_{k + 1}} = {{\rm{H}}_k} + {{\rm{E}}_k}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%20%3D%20%7B%7B%5Crm%7BH%7D%7D_k%7D%20%2B%20%7B%7B%5Crm%7BE%7D%7D_k%7D)
### [是修正公式。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_%7Bk%20%2B%201%7D%7D%20%3D%20%7B%7B%5Crm%7BH%7D%7D_k%7D%20%2B%20%7B%7B%5Crm%7BE%7D%7D_k%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BE%7D%7D_k%7D)
### 常用而且有效的修正公式是BFGS公式，如下
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BE%7D%7D_k%7D)![](https://img-blog.csdn.net/20131123204604171)
### [
](https://img-blog.csdn.net/20131123204604171)
### [
](https://img-blog.csdn.net/20131123204604171)
### 下面给出BFGS公式下的拟牛顿法
### [
](https://img-blog.csdn.net/20131123204604171)![](https://img-blog.csdn.net/20131123204718859)
### 在上述步骤中，初始正定矩阵![{{\rm{H}}_0}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)
### [通常取为单位矩阵，即](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)![{{\rm{H}}_0} = {\rm{I}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### [。这样，拟牛顿法的第一次迭代相当于一个最速下降迭代。](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)

### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 1.3.3.2优缺点
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 与牛顿法相比，有两个优点：
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### (a)    仅需一阶导数
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### (b)    校正保持正定性，因而下降性质成立
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### (c)    无需计算逆矩阵，但具有超线性收敛速度
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### (d)    每次迭代仅需要次乘法计算
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 缺点是初始点距离最优解远时速度还是慢。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### 解决方法是，迭代前期用最速下降法进行迭代，得到一定解以后用拟牛顿法迭代。
### [
](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)

### [
                      ](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D%20%3D%20%7B%5Crm%7BI%7D%7D)
### [
	](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Crm%7BH%7D%7D_0%7D)
