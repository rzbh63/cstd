# 计算机原理学习



# （1）-- 冯诺依曼体系和CPU工作原理

2017年01月18日 21:09:57

 

wangpengk7788

 

阅读数：311

更多

个人分类： [计算机原理](https://blog.csdn.net/wangpengk7788/article/category/6687371)



# 前言

转载自http://blog.csdn.net/cc_net/article/details/10419645



对于我们80后来说，最早接触计算机应该是在95年左右，那个时候最流行的一个词语是多媒体。 依旧记得当时在同学家看同学输入几个DOS命令就成功的打开了一个游戏，当时实在是佩服的五体投地。因为对我来说，屏幕上的东西简直就是天书。有了计算机我们生活发生了巨大的变化，打游戏，上网，聊天，甚至到现在以此为业。有时无不感叹计算机的强大。



人类总是聪明的而又懒惰的。即便是1+1这种简单的计算都不想自己做，1623年Wilhelm Schickard 制作了一个能进行六位以内数加减法，并能通过铃声输出答案的"计算钟"。通过转动齿轮来进行操作。 这已经相当高端了，说起计算器，我们5000年文明古国在东汉末年就有记载了**---**算盘。

 

计算机的发展也是随着科技的发展经历了机械计算机、电子计算机、晶体管计算机、小规模集成电路和超大规模集成电路计算机。我们无意讨论整个计算机的发展过程，主要还是介绍基于冯诺依曼体系结构的现代计算机。

 

 

# 1. 计算机的发展





计算机的发展包括了硬件和软件的发展，硬件的发展为计算机提供了更快的处理速度，而软件的发展为用户提供了更好的体验。两者相辅相成，密不可分。

 

- **第一阶段**： 60年代中期以前，是计算机系统发展的早期时代。在这个时期通用硬件已经相当普遍，软件却是为每个具体应用而专门编写的，大多数人认为软件开发是无需预先计划的事情。这时的软件实际上就是规模较小的程序，程序的编写者和使用者往往是同一个(或同一组)人；
- **第二阶段**：从60年代中期到70年代中期，是计算机系统发展的第二代。在这10年中计算机技术有了很大进步。多道程序、多用户系统引入了人机交互的新概念，开创了计算机应用的新境界，使硬件和软件的配合上了一个新的层次；
- **第三阶段**：计算机系统发展的第三代从20世纪70年代中期开始，并且跨越了整整10年。在这10年中计算机技术又有了很大进步。分布式系统极大地增加亍计算机系统的复杂性，局域网、广域网、宽带数字通信以及对“即时”数据访问需求的增加，都对软件开发者提出了更高的要求；
- **第四阶段**：在计算机系统发展的第四代已经不再看重单台计算机和程序，人们感受到的是硬件和软件的综合效果。由复杂[操作系统](http://lib.csdn.net/base/operatingsystem)控制的强大的桌面机及局域网和广域网，与先进的应用软件相配合，已经成为当前的主流。计算机体系结构已迅速地从集中的主机环境转变成分布的客户机/服务器。







# 2. 计算机基本原理



 

Copy了点计算机的发展历史，现在可以来看看计算机的基本工作原理了。现代计算机，大部分都是基于冯诺依曼体系结构，而我们这里谈论的也是此问前提。**冯诺依曼的核心是：存储程序，顺序执行**。所以不管计算机如何发展，基本原理是相同的。计算机程序实际上是告诉计算机做什么。



 

## 2.1 冯诺依曼体系结构





冯诺依曼体系结构有以下特点：

1. 计算机处理的数据和指令一律用二进制数表示；
2. 指令和数据不加区别混合存储在同一个存储器中；
3. 顺序执行程序的每一条指令；
4. 计算机硬件由运算器、控制器、存储器、输入设备和输出设备五大部分组成。



冯诺依曼体系结构的计算机必须具有如下功能：

- 把需要的程序和数据送至计算机中；
- 必须具有长期记忆程序、数据、中间结果及最终运算结果的能力；
- 能够完成各种算术、逻辑运算和数据传送等数据加工处理的能力；
- 能够根据需要控制程序走向，并能根据指令控制机器的各部件协调操作；
- 能够按照要求将处理结果输出给用户。



![img](https://img-blog.csdn.net/20130828001944015)



 

## 2.2 计算机工作原理





对于我们现代计算机来说，最关键的2个部件就是CPU和内存。内存存储了要执行的程序指令，而CPU就是用来执行这些指令。CPU首先要知道这些指定存放在存储器的那个区域，然后才能执行，并且把执行的结果写入到执行区域。

 



### 2.2.1 CPU指令和编程语言



在了解CPU和存储器工作原理之前，先来了解一下CPU指令和我们编程语言之间的一些关系。

 



#### 2.2.1.1 CPU指令

因为在计算机中指令和数据都用二进制来表示，也就是说它只认识0和1这样的数字。最早期的计算机程序通过在纸带上打洞来人工操操作的方式来模拟0和1，根据不同的组合来完成一些操作。后来直接通过直0和1编程程序，这种称之为机器语言。这里就会有一个疑问，**计算机怎么知道你这些组合的意思**？

![img](https://img-blog.csdn.net/20130828003345578)

于是就出现了CPU指令，我们现在买CPU都会听到指令集这一说。CPU指令其实就对应了我们这里说的0和1的一些组合。每款CPU在设计时就规定了一系列与其硬件电路相配合的指令系统。有了CPU指令集的文档你就可以通过这个编写CPU认识的机器代码了。所以对于不同CPU来说可能会有不同的机器码。比如下面我们就定义了一套我们CPU硬件电路可以完成的CPU指令。



| **指令** | **格式**                   | **说明**                   |
| -------- | -------------------------- | -------------------------- |
| 0001     | **[address][register]**    | **读取存储取值到寄存器**   |
| 0010     | **[register][address]**    | **写入寄存器的值到存储器** |
| 0011     | **[register1][register2]** | **加法操作**               |



随着计算机的发展，CPU支持的指令也越来越多，功能也越来越强，上图就是现在Core I5处理器支持的指令集。

 



#### 2.2.1.2 汇编语言



使用0和1这样的机器语言好处是CPU认识，可以直接执行，但是对于程序本身来说，没有可读性，难以维护，容易出错。所以就出现了汇编语言，它用助记符(代替操作码指令，用地址符号代替地址码。实际是对机器语言的一种映射，可读性高。



| 指令 | 汇编指令 | 格式                   | 说明                       |
| :--- | :------- | :--------------------- | :------------------------- |
| 0001 | READ     | **[addLable][regLab]** | **读取存储取值到寄存器**   |
| 0010 | WRITE    | **[addLable][regLab]** | **写入寄存器的值到存储器** |
| 0011 | ADD      | **[var1][var2]**       | **加法操作**               |

 

 

 

 



把汇编语言转换为机器语言需要一个叫做**汇编器**的工具。对于目前的CPU厂商，在推出的CPU指令时都会同时退出新的汇编器。如果你还在使用老版本的汇编器那么只能使用机器码来使用新的指令了。

 



#### 2.2.1.3 高级语言



汇编语言的出现大大提高了编程效率，但是有一个问题就是不同CPU的指令集可能不同，这样就需要为不同的CPU编写不同的汇编程序。于是又出现了高级语言比如C，或者是后来的C++,[Java](http://lib.csdn.net/base/javase),C#。 高级语言把多条汇编指令合成成为了一个表达式，并且去除了许多操作细节（比如堆栈操作，寄存器操作），而是以一种更直观的方式来编写程序，而面向对象的语言的出现使得程序编写更加符合我们的思维方式。我们不必把尽力放到低层的细节上，而更多的关注程序的本身的逻辑的实现。



对于高级语言来说需要一个**编译器**来完成高级语言到汇编语言的转换。所以对比不同的CPU结构，只需要有不同编译器和汇编器就能使得我们的程序在不同CPU上都能运行了。如下图在VS2010中，我们可以选择程序编译的目标平台，X86，X64，ARM等。当然除了这些编译类的语言之外还有解释类型的语言如JS，就不在此讨论范围内。

![img](https://img-blog.csdn.net/20130828013619937)

到这里有一个疑问：**当CPU的指令集更新后高级语言会有什么影响和变化？**对于目前来说，一般出现了新的指令，会有对应的新的汇编器和编译器。所以编译器可以把一些高级语言的表达式编译成新的汇编指令，这样对于高级来说不会有任何变化； 当然还有一种情况就是高级语言会增加新的语法来对应一些新的汇编语言和指令。但是这种情况出现的几率很小。所以如果编译器不支持新的指令，那么只有只用汇编会来实现了。

 



#### 2.2.1.4 小结



从上面的我们可以看出，我们写的程序最终都将变成机器认识的二进制可执行程序，然后加载到内存顺序的执行。 从机器码到汇编到高级语言，我们可以看到计算机中无处不在的分层，抽象的思想。不光光是软件，硬件同样适用。**最后留下一个问题在这里： C#和JAVA程序编译出来的文件不是二进制的机器码，而是中间语言，那么他们又是怎么运行的呢？**

 



### 2.2.2 CPU工作原理



前面已经了解了现代计算机的大致结构，也知道CPU是按照CPU指令来执行操作，那么就看看CPU的结构和他是如何执行顺序操作的。





#### 2.2.2.1 CPU功能

1. **指令控制：** 指令控制也称为程序的顺序控制，控制程序严格按照规定的顺序执行。
2. **操作控制：** 将取出的指令的产生一系列的控制信号(微指令)，分别送往相应的部件，从而控制这些部件按指令的要求进行工作。
3. **时间控制：** 有些控制信号在时间上有严格的先后顺序，如读取存储器的数据，只有当地址线信号稳定以后，才能通过数据线将所需的数据读出，否则读出的数据是不正确的数据，这样计算机才能有条不紊地工作。
4. **数据加工：** 所谓数据加工，就是对数据进行算术运算和逻辑运算处理。 所谓数据加工，就是对数据进行算术运算和逻辑运算处理





#### 2.2.2.2 CPU基本组成



以前CPU主要由**运算器**和**控制器**两大部分组成，随着集成电路的发展，目前CPU芯片集成了一些其它逻辑功能部件来扩充CPU的功能，如浮点运算器、内存管理单元、cache和MMX等。下面2张图分别是8086和Pentium CPU的结构图。

![img](https://img-blog.csdn.net/20130828020834562)

 .

![img](https://img-blog.csdn.net/20130828020824437)

 

对于一个通用的CPU来说，我们只需要关注他的核心部**件算数逻辑单元和操作控制单元**。

 ![img](https://img-blog.csdn.net/20130828020554078)



> \1. 控制器的组成和功能： 控制器由程序计数器、指令寄存器、指令译码器、时序产生器和操作控制器组成。它是计算机指挥系统，完成计算机的指挥工作。尽管不同计算机的控制器结构上有很大的区别，当就其基本功能而言，具有如下功能： 

- - 取指令 从内存中取出当前指令，并生成下一条指令在内存中的地址。 
  - 分析指令 指令取出后，控制器还必须具有两种分析的功能。一是对指令进行译码或测试，并产生相应的操作控制信号，以便启动规定的动作。比如一次内存读/写操作，一个算术逻辑运算操作，或一个输入/输出操作。二是分析参与这次操作的各操作数所在的地址，即操作数的有效地址。 
  - 执行指令 控制器还必须具备执行指令的功能，指挥并控制CPU、内存和输入/输出设备之间数据流动的方向，完成指令的各种功能。 
  - 发出各种微操作命令 在指令执行过程中，要求控制器按照操作性质要求，发出各种相应的微操作命令，使相应的部件完成各种功能。 
  - 改变指令的执行顺序 在编程过程中，分支结构、循环结构等非顺序结构的引用可以大大提供编程的工作效率。控制器的这种功能可以根据指令执行后的结果，确定下一步是继续按原程序的顺序执行，还是改变原来的执行顺序，而转去执行其它的指令。 
  - 控制程序和数据的输入与结果输出 这实际也是一个人机对话的设计，通过编写程序，在适当的时候输入数据和输出程序的结果。 
  - 对异常情况和某些请求的处理 当计算机正在执行程序的过程中，发生了一些异常的情况，例如除法出错、溢出中断、键盘中断等。

> \2. 运算器的组成和功能： 运算器由算术逻辑单元（ALU）、累加寄存器、数据缓冲寄存器和状态条件寄存器组成，它是数据加工处理部件，完成计算机的各种算术和逻辑运算。相对控制器而言，运算器接受控制器的命令而进行动作，即运算器所进行的全部操作都是由控制器发出的控制信号来指挥的，所以它是执行部件。运算器有两个主要功能： 
>
> - 执行所有的算术运算，如加、减、乘、除等基本运算及附加运算；
> - 执行所有的逻辑运算，并进行逻辑测试，如与、或、非、零值测试或两个值的比较等。
>
> 

#### 2.2.2.3 CPU工作流程

CPU的基本工作是执行存储的指令序列，即程序。程序的执行过程实际上是不断地取出指令、分析指令、执行指令的过程。几乎所有的冯•诺伊曼型计算机的CPU，其工作都可以分为5个阶段：取指令、指令译码、执行指令、访存取数和结果写回。

![img](https://img-blog.csdn.net/20130828023242328)  ![img](https://img-blog.csdn.net/20130828023418859)





#### 2.2.2.4 指令周期

1. 指令周期： CPU取出一条指令并执行该指令所需的时间称为指令周期。指令周期的长短与指令的复杂程度有关。
2. CPU周期：从主存读取一条指令的最短时间来规定CPU周期。指令周期常常用若干个CPU周期数来表示。
3. 时钟周期：时钟周期是处理操作的最基本时间单位，由机器的主频决定。一个CPU周期包含有若干个时钟周期。



![img](https://img-blog.csdn.net/20130828025742156)

从上面的定义可以知道，对于CPU来说取出和执行任何一条指令所需的最短时间为两个CPU周期。所以频率越高，那么时钟周期越短，这样CPU周期和指令周期也就越短，理论上程序执行的速度也越快。但是频率不能无限的提高，而且频率的提高也带来了功耗，发热等问题，所以目前也有超线程，流水线等技术来提高CPU执行的速度。

 



#### 2.2.2.5 时序发生器

1. 时序信号： 在计算机高速运行的过程中，计算机内各部件的每一个动作都必须严格遵守时间规定，不能有任何差错。计算机内各部件的协调动作需要时间标志，而时间标志则是用时序信号来体现的。计算机各部分工作所需的时序信号，在CPU中统一由时序发生器来产生。
2. 时序发生器： 时序信号发生器是产生指令周期控制时序信号的部件，当CPU开始取指令并执行指令时，操作控制器利用时序信号发生器产生的定时脉冲的顺序和不同的脉冲间隔，提供计算机各部分工作时所需的各种微操作定时控制信号，有条理、有节奏地指挥机器各个部件按规定时间动作。



在这里有一个疑问：**指令和数据都存放在内存中，那么CPU怎么区分是指令还是数据呢？**







从时间上来说，取指令事件发生在指令周期的第一个CPU周期中，即发生在“取指令”阶段，而取数据事件发生在指令周期的后面几个CPU周期中，即发生在“执行指令”阶段。从空间上来说，如果取出的代码是指令，那么一定送往指令寄存器，如果取出的代码是数据，那么一定送往运算器。

 



### 2.2.3 小结

###  

###  

通过以上我们了解了CPU的工作过程。简单来说就是CPU要顺序执行一个程序的指令，首先是控制器获得第一条指令的地址，当CPU取得这个指令并执行后，控制器需要生成下一条要执行的指令的地址。ALU单元负责一些运算操作。下面的FLASH演示了CPU执行一个加法操作的流程。

 

CPU工作流程FLASH: <http://218.5.241.24:8018/C35/Course/ZCYL-HB/WLKJ/jy/Chap05/flash-htm/5.6.swf>

 





# 3 总结

#  



本文主要是简单介绍了计算机的一些发展历史和通用CPU的结构以及工作流程。加深了我们对冯诺依曼体系的储存程序，顺序执行的理解。无论硬件是什么样子，冯诺依曼计算机的基本原理就是这样。



当然了解了基本原理之后，会产生更多的问题，比如可执行文件又是如何被装载到内存的？CPU和内存之间是如何通信的呢？是如何根据地址找到指令的呢？内存结构又是什么样子？ CPU如何和内存之外的设备通信呢？这些问题都会在后面给出答案。

 





# 参考：





<http://software.intel.com/zh-cn/articles/book-Processor-Architecture_CPU_work_process>



<http://blog.sina.com.cn/s/blog_4d126a24010173ru.html>



<http://218.5.241.24:8018/C35/Course/ZCYL-HB/WLKJ/jy/Chap05/5-2-1.HTM>









# （2）-- 存储器和I/O设备和总线

2017年01月18日 21:18:53

 

wangpengk7788

 

阅读数：421

更多

个人分类： [计算机原理](https://blog.csdn.net/wangpengk7788/article/category/6687371)



# 前言

 

转载自http://blog.csdn.net/cc_net/article/details/10439665



前一篇文章介绍了冯诺依曼体系结构的计算机的基本工作原理，其中主要介绍了CPU的结构和工作原理。这一篇主要来介绍存储区，总线，以及IO设备等其他几大组件，来了解整个计算机是如何工作的。 这些东西都是看得见摸得着的硬件，平时我们买电脑时最关注的就是CPU的速度，内存的大小，主板芯片等等的参数。





 

# 1. 存储器

 

前面我们以一个简单通用的计算机模型来介绍了CPU的工作方式，CPU执行指令，而存储器为CPU提供指令和数据。 在这个简单的模型中，存储器是一个线性的字节数组。CPU可以在一个常数的时间内访问每个存储器的位置，虽然这个模型是有效的，但是并不能完全反应现代计算机实际的工作方式。

 



## 1.1 存储器系统层次结构



在前面介绍中，我们一直把存储器等同于了内存，但是实际上在现代计算机中，存储器系统是一个具有不同容量，不同访问速度的存储设备的层次结构。整个存储器系统中包括了寄存器、Cache、内部存储器、外部存储。下图展示了一个计算机存储系统的层次图。层次越高速度越快，但是价格越高，而层次越低，速度越慢，价格越低。

![img](https://img-blog.csdn.net/20130828214855828)



相对于CPU来说，存储器的速度是相对比较慢的。无论CPU如何发展，速度多块，对于计算机来说CPU总是一个稀缺的资源，所以我们应该最大程度的去利用CPU。其面我们提到过CPU周期，一个CPU周期是取1条指令的最短的时间。由此可见，CPU周期在很大程度上决定了计算机的整体性能。你想想如果当CPU去取一条指令需要2s，而执行一个指令只需要2ms，对于计算机来说性能是多么大的损失。所以存储器的速度对于计算机的速度影响是很大的。



对于我们来说，总是希望存储器的速度能和CPU一样或尽量的块，这样一个CPU周期需要的时钟周期就越少。但是现实是，这样的计算机可能相当的昂贵。所以在计算机的存储系统中，采用了一种分层的结构。速度越快的存储器容量越小，这样就能做到在性能和价格之间的一个很好的平衡。

 



## 1.2 存储技术





计算机的发展离不开存储器的发展，早起的计算机没用硬盘，只有几千字节的RAM可用。而我们现在4G,8G的内存已经随处可见，1T的大硬盘以及上百G的固态硬盘，而价格也比10年，20年前便宜的很多很多。所以我先大概了解下各种存储技术。目前存储技术大致分为SRAM存储器、DRAM存储器、ROM存储器和磁盘。

 



### 1.2.1 寄存器



在上一篇文章的图中我们有看得CPU内部有很多寄存器，而上一张图也显示，寄存器在存储层次结构的顶端。它也叫触发器，它往往和CPU同时钟频率，所以速度非常快。但是一个寄存器需要20多个晶体管，所以如果大量使用，CPU的体积会非常大。所以在CPU中只有少量的寄存器。而每个寄存器的大小都是8-64字节。





 

### 1.2.2 RAM随机访问存储



RAM（Read-Access Memory）分为两类，静态（SRAM）和动态（DRAM）。SDRAM比DRAM要快的多，但是价格也要贵的多。

- **静态RAM**： SRAM将每个位存储在一个双稳态的存储单元中，每个存储单元是用一个六晶体管电路实现的。它的特点是可以无限期（只要有电）的保持在两个稳定状态中的一个（正好可以存放0或1），而其他任何状态都是不稳定的会马上切换到这两个状态中的一个；
- **动态RAM**： DRAM是利用电容内储存电荷的多寡来代表一个二进制位元（bit）是1还是0，每一bit由一个晶体管和电容组成。由于在现实中电容会有漏电的现象，导致电位差不足而使记忆消失，因此除非电容经常周期性地充电，否则无法确保记忆长存。由于这种需要定时刷新的特性，因此被称为“动态”记忆体。



SRAM相比DRAM速度更快功耗更低，而由于结构相对复杂占用面积较大，所以一般少量在CPU内部用作Cache，而不适合大规模的集成使用，如内存。而DRAM主要用来作为计算机的内部主存。

- **Cache**： 目前我们CPU中一般集成了2到3级的Cache，容量从128K到4M。对于CPU总的Cache来说，它们的也是和CPU同频率的，所以理论上执行速度和寄存器应该是相同的，但是Cache往往用来存储一些指令和数据，这样就存在一个命中的问题。当没有命中的时候，需要向下一集的存储器获取新的数据，这时Cache会被lock，所以导致实际的执行速度要比寄存器慢。同样对于L1，L2，L3来说，速度也是越来越慢的；
- **主存**： 也就是我们说的内存，使用DRAM来实现。但是我们目前听的内存一般叫DDR SDRAM，还有早期的SDRAM。这是一种同步的DRAM技术，我们不需要了解他的详情，只需要知道它能有效的提高DRAM的传输带宽。而DDR表示双倍的速率，而现在又有了DDR2，DDR3，DDR4，他们的带宽也是越来越大。



 

### 1.2.3 ROM只读存储



前面的RAM在断电后都会丢失数据，所以他们是易失的。另一方面非易失的存储器即便在断点后也能保存数据。一般我们称之为ROM（Read-Only Memory）。虽然这么说，但是ROM在特殊的情况下还是可以写入数据的，否则就不能叫存储器了。

- **PROM**: 可编程ROM，只能被编程一次，PROM包含一种熔丝，每个存储单元只能用高电流烧断一次；
- **EPROM**：可擦写可编程ROM，有一个透明的石英窗口，紫外线通过窗口照射到存储单元就被清除为0，而对它编程是使用一种特殊的设备来写入1。写入次数1K次；
- **EEPROM**:: 电子可擦除可编程ROM，不需要特殊设备而可以直接在印制的电路板上编程。写入次数10万次；
- **Flash Memory**： 这是我们见到最多的闪存，有NOR Flash、NAND Flash、V-NAND Flash、SLC、MLC、TLC。虽然是基于EEPROM，但是速度上却要快很多。其中NOR 、NANA Flash大量的使用在U盘，SD卡、手机存储上。



ROM在计算机中应用也比较多，比如我们的BIOS芯片，最开始采用PROM，后来使用EPROM，如果损坏计算机就无法启动了。而目前手机中也采用ROM来烧入系统，而RAM作为内存，使用Flash Memory作为机身存储。



 

### 1.2.4 磁盘存储



也就是我们最常见的硬盘。目前硬盘主流已经是500G,1T。转速也在7200转左右。相对于8G的内存，一个500G的硬盘可以说是相当的便宜。但是问题在于他的速度非常的慢，从磁盘读取数据需要几个毫秒，而CPU时钟周期是以纳秒计算。磁盘读取操作要比DRAM慢10万倍，比SRAM慢百万倍。

![img](https://img-blog.csdn.net/20130828233132296)![img](https://img-blog.csdn.net/20130828233907015)

相对于CPU，内部存储的电子结构，磁盘存储是一种机械结构。数据都通过电磁流来改变极性的方式被电磁流写到磁盘上，而通过相反的方式读回。一个硬盘由多个盘片组成，每个盘片被划分为磁道，扇区和最小的单位簇。而每个盘面都有一个磁头用来读取和写入数据。而硬盘的马达装置则控制了磁头的运动。

 



### 1.2.5 虚拟硬盘(VHD)和固态硬盘(SSD)



随着计算机的发展，缓慢的磁盘速度已经成为计算机速度的障碍了。大多数情况下，你的CPU够快，内存够大，可是打开一个程序或游戏时，加载的速度总还是很慢。(关于程序加载的过程后面的文章会讲到)。原因就是磁盘读写速度太慢，所以一度出现了虚拟硬盘。就是把一部分内存虚拟成硬盘，这样一些缓存文件直接放到内存中，这样就加快了程序访问这些数据的速度。但是他的问题是易失的。当然你可以保存到磁盘，但是加载和回写的速度会随着数据量加大而加大。所以这个适用于一些临时数据的情况，比如浏览器缓存文件。



而固态硬盘是最近几年出来的，而且随着技术的发展，价格也越来越便宜，越来越多的人采用SSD+HHD的方式来搭建系统，提高系统的速度。其实SSD在上世纪80年代就有基于DRAM的产品，但是因为易失性和价格而无法推广开来。而现在的SSD则是使用Flash Memory。目前市面上最常见的是SLC,MLC,TLC存储介质的固态硬盘。我们知道Flash都是与写入次数限制的。而SLC>MLC>TLC。目前主流的SSD都是使用MLC，比如Intel 520，三星830系列。当然目前三星也退出了基于TLC的固态硬盘，价格相对要便宜一些。

 



### 1.2.6 远程存储



简单可以理解为是将数据指令存储在其他机器上，比如分布式系统，WebService Server，HTTP Server以及现在炒的火热的云端存储。计算机通过网络相互连接。比较起磁盘，远程存储的速度是以秒来计算。

 



### 1.3 局部性



通过上面介绍我们对计算机存储器有了一个了解，并且知道了存储器层次越高速度越快。**那么为什么我们要对存储器分层呢**？ 分成是为了弥补CPU和存储器直接速度的差距。这种方式之所有有效，是因为应用程序的一个特性：局部性。



我们知道计算机的体系是存储程序，顺序执行。所以在执行一个程序的指令时，它后面的指令有很大的可能在下一个指令周期被执行。而一个存储区被访问后，也可能在接下来的操作中再次被访问。这就是局部性的两种形式：

- 时间局部性
- 空间局部性



对于现代计算机来说，无论是应用程序，[操作系统](http://lib.csdn.net/base/operatingsystem)，硬件的各个层次我们都是用了局部性。

1. 硬件：通过引入Cache存储器来保存最近访问的指令数据来提高对主存的访问速度。
2. 操作系统： 允许是用主存作为虚拟地址空间被引用块的高速缓存以及从盘文件的块的高速缓存。
3. 应用程序：将一些远程服务比如HTTP Server的HTML页面缓存在本度的磁盘中。



```cpp
int sumarraycols(int a[m][n])  
{  
  int i, j , sum = 0;  
  for(j = 0; j < n; j++)  
    for(i = 0; i < m; i++)  
      sum += a[i][j];  
    return sum;
}

int sumarraycols(int a[m][n])  
{  
  int i, j , sum = 0;  
  for(i = 0; i < m; i++)  
    for(j = 0; j < n; j++)  
      sum += a[i][j];  
    return sum;  
}  
```



 

以上2段代码差别只有for循环的顺序，但是局部性却相差了很多。我们知道数组在内存中是按照行的顺序来存储的。但是CODE1确实按列去访问，这可能就导致缓存不命中（需要的数据并不在Cache中，因为Cache存储的是连续的内存数据，而CODE1访问的是不联系的），也就降低了程序运行的速度。

 





# 2 存储器访问和总线

 

前面介绍了存储器的存储技术和分层，也一直提到CPU从存储器中获取数据和指令，这一节就介绍一下CPU和存储器之间是如何通信的。

##  



## 2.1 总线



所谓总线是各种功能部件之间传送信息的公共通信干线，它是由导线组成的传输线束。我们知道计算机有运算器，控制器，存储器，输入输出设备这五大组件，所以总线就是用来连接这些组件的导线。



按照计算机所传输的信息种类，计算机的总线可以划分为

- 数据总线： 数据总线DB是双向三态形式的总线，即它既可以把CPU的数据传送到存储器或输入输出接口等其它部件，也可以将其它部件的数据传送到CPU。数据总线的位数是微型计算机的一个重要指标，通常与微处理的字长相一致。我们说的32位，64位计算机指的就是数据总线。
- 地址总线： 地址总线AB是专门用来传送地址的，由于地址只能从CPU传向外部存储器或I/O端口，所以地址总线总是单向三态的，这与数据总线不同。地址总线的位数决定了CPU可直接寻址的内存空间大小。
- 控制总线：控制总线主要用来传送控制信号和时序信号。控制总线的传送方向由具体控制信号而定，一般是双向的，控制总线的位数要根据系统的实际控制需要而定。其实数据总线和控制总线可以共用。

![img](https://img-blog.csdn.net/20130829005115125)

总线也可以按照CPU内外来分类：

- 内部总线：在CPU内部，寄存器之间和算术逻辑部件ALU与控制部件之间传输数据所用的总线称为片内部总线。
- 外部总线：通常所说的总线指片外部总线，是CPU与内存RAM、ROM和输入/输出设备接口之间进行通讯的通路,也称系统总线。

![img](https://img-blog.csdn.net/20130829012000046)

 



## 2.2 控制芯片



前面我面介绍了总线的分类，在我们的简单模型中。CPU通过总线和存储器之间直接进行通信。实际上在现代的计算机中，存在一个控制芯片的模块。CPU需要和存储器，I/O设备等进行交互，会有多种不同功能的控制芯片，我们称之为控制芯片组（Chipset）。



对于目前的计算机结构来说，控制芯片集成在主板上，典型的有南北桥结构和单芯片结构。与芯片相连接的总线可以分为前端总线（FSB）、存储总线、IQ总线，扩展总线等。

- 南北桥芯片结构

  ：

  - 北桥芯片

    ，它控制着CPU的类型，主板的总线频率，内存控制器，显示核心等。它直接与CPU、内存、显卡、南桥相连，所以它数据量非常大；

    - **前端总线**：是将CPU连接到北桥芯片的总线。FSB的频率是指CPU和北桥之间的数据交换速度。速度越快，数据带宽越高，计算机性能越好；
    - **内存总线**：是将内存连接到北桥芯片的总线。用于和北桥之间的通信；
    - **显卡总线**：是将显卡连接到北桥芯片的总新。目前有AGP,PCI-E等接口。其实并没有显卡总线一说，一般认为属于I/O总线；

  - 南桥芯片

    ，它主要负责外部接口和内部CPU的联系；

    - **I/O总线**：连接外部I/O设备连接到南桥的总线, 比如USB设备，ATA,SATA设备，以及一些扩展接口；
    - **扩展总线**：主要是主板上提供的一些PCI，ISA等插槽；

- **单芯片结构**： 单芯片组主要是是取消了北桥，因为现在CPU中内置了内存控制器，不需要再通过北桥来控制，这样就能提高内存控制器的频率，减少延迟。而现在一些CPU还集成了显示单元。也使得显示芯片的频率更高，延迟更低。

![img](https://img-blog.csdn.net/20130829010809750)



## 2.3 运行频率

 

**数据带宽 = （总线频率\*数据位宽）/ 8**



 

### 2.3.1 外频



外频是建立在数字脉冲信号震动速度基础上的。它是CPU与系统总线以及其他外部设备共同运行的速度。我们知道计算机中有一个时序发生器来保证各个部件协同工作，而这里说的外频率就是这个时序发生器的频率。外频也是系统总线的工作频率。

 



### 2.3.2 频率和控制芯片

- 在计算机刚开始的时候，CPU和内存还有I/O设置是直接通过总线连接的而没有控制芯片。所有设备都同步的工作在同一个总线频率下。
- 但是随着CPU的发展，CPU速度越来越块。但受限于I/O设备。于是就出现了芯片。他使得I/O总线不在直接和CPU的系统总线相连。这样就有了2个不同频率的总线，这个芯片实际起到了一个降频的作用，也就相对于系统总线的分频技术。
- 但CPU速度发展相当快，CPU的速度已经高于内存运行的速度，于是引入了倍频的概念。CPU在不改变外频和系统总线频率的情况下运行在更高的频率。
- 发展到后来，就出现了北桥芯片，而CPU和北桥之前的总线称为了FSB总线，而内存与北桥之前称为内存总线。

 

### 2.3.2 分频和倍频

- 分频：使得I/O设备可以和较高的外频协同工作。比如AGP,PCI总线，运行频率在66MHZ和33MHZ，所以对于一个100MHZ的外频来说，采用2/3或1/3分频的方式就能使得CPU和外设同步的工作了。否则设备可能无法正常工作。
- 倍频： 为了提高CPU频率又正常的和内存进行工作，所以产生了倍频。所以对于CPU来说他实际的频率是外频*倍频。

 

### 2.3.3 FSB频率



前面我们现在已经知道CPU和北桥芯片连接是通过FSB。而FSB频率表示CPU和北桥芯片之间的工作速度。但是从前面我们就知道FSB的实际频率是和外频一样的。但是随着技术的发展，Intel的QDR技术和AMD的HT技术，使得CPU在一个时钟周期可以传送4次数据，所以对于FSB涞说虽然工作早外频的频率下，但是等效的频率是外频的4倍。所以我们说的FSB频率是等效频率，而不是实际的工作频率。随着技术的发展，Intel芯片的FSB有800MHz，1600HMz等等。但随着北桥芯片的消失，FSB的概率也慢慢远去。

 



### 2.3.4 内存频率



对于内存频率我们可以看到，一般包括了**核心频率，总线频率和传输频率：**

- 核心频率和外频类似，是建立在脉冲震荡信号上的。
- 总线频率就是指内存总线的工作频率。也就是内存和北桥芯片之间的工作频率。
- 而传输频率类似FSB，是指实际传输数据的频率。



对于SDR来说，它的3个频率是一致的。而DDR在一个时钟周期可以传送2次数据，所以它的传输频率是核心和总线频率的2倍。DDR2在DDR的基础上，采用了4bit预读，所以总线频率是核心频率的2倍，而DDR3采用了8bit预读，总线频率是核心频率的4倍。



| DDR SDRAM Standard | Bus clock (MHz) | Internal rate (MHz) | Prefetch (min burst) | Transfer Rate (MT/s) | Voltage | DIMM pins | SO-DIMM pins | MicroDIMM pins |
| :----------------- | :-------------- | :------------------ | :------------------- | :------------------- | :------ | :-------- | :----------- | :------------- |
| **DDR**            | 100–200         | 100–200             | 2n                   | 200–400              | 2.5/2.6 | 184       | 200          | 172            |
| DDR2               | 200–533         | 100–266             | 4n                   | 400–1066             | 1.8     | 240       | 200          | 214            |
| DDR3               | 400–1066        | 100–266             | 8n                   | 800–2133             | 1.5     | 240       | 204          | 214            |





从下表我们就能看出。所以我们常说的DDR3 1600，DDR2 800指的是内存的传输频率。相同的技术还有显卡的AGP4X,8X，PCIE-8X,16X等技术。

![img](https://img-blog.csdn.net/20130829113906250)

而随着FSB速度不断加快，内存的总线频率组建成为了瓶颈，于是出现了DDR双通道，双通道是指芯片拥有2个内存控制器，所以可以使得传输速率翻倍。

 



### 2.3.5 内存总线工作方式



因为内存总线频率不同，所以内存和CPU之间存在同步和异步两种工作方式。

- **同步方式**：内存总线频率和CPU外频相同。比如以前的PC133和P3处理器，他们以同步的方式工作在133MHZ下。而当你超频时就需要拥有更高总线频率的内存。当然也需要北桥芯片的支持。
- **异步方式**：内存总线频率和CPU外频不同。睡着CPU外频的提高，内存也必须更新，所以出现了异步的方式。比如P4 CPU外频为200MHz，而内存却可以使用DDR333来运行。同时异步方式也在超频时经常使用。一般来说会有一个内存异步比率。在BIOS中会有相应的选项。

从性能上来讲，同步方式的延迟要好于异步方式，这也是为什么以前会说P4 200外频的CPU要使用DDR400才能发挥最大功效。但这也不是绝对的。比如我的I5处理器CPU外频工作在100MHz，而我使用的DDR3-1600的总线频率在200MHz，虽然不同步，但是拥有更高的传输速率。所以不能一概而论。

 



### 2.3.6 QPI和HT总线技术



从前面我们知道了FSB对整个系统的性能影响很大，1600MHZ的FSB能提供的数据带宽也只有12.8GB/s，所以随着技术的发展，现在最新的计算机基本都采用了单芯片设计，北桥的功能被集成到了CPU内部。于是我们前面说的FSB也就不存在了。对于Intel和AMD这2大芯片厂商，分别有自己的技术来提高CPU和存储器以及其他设备之间的传输速率，满足更高的计算要求。

- **QPI**: Intel的QuickPath Interconnect技术缩写为QPI，译为快速通道互联。用来实现芯片之间的直接互联，而不是在通过FSB连接到北桥。早期20位宽的QPI连接其带宽可达惊人的每秒25.6GB，远非FSB可比。而随着技术发展，在高端安腾处理中峰值可以达到96GB/s。
- **HT**：HyperTransport本质是一种为主板上的集成电路互连而设计的端到端总线技术，目的是加快芯片间的数据传输速度。HyperTransport技术在AMD平台上使用后，是指AMD CPU到主板芯片之间的连接总线（如果主板芯片组是南北桥架构，则指CPU到北桥）即HT总线。HT3.1理论上可以达到51.2GB/s。

除此之外，但芯片中的QPI和HT传输不需要经过北桥新片，在CPU内存除了集成内存控制器意外还可以集成PCI-E2.0的图形核心，使得集成显卡的核心频率和数据吞吐量大幅提高。



![img](https://img-blog.csdn.net/20130829131347421)  ![img](https://img-blog.csdn.net/20130829131357500)

如图，Core I7处理器外频只有133MHz, 使用QPI技术后总线频率达到2.4GMhz，而使用DDR3-1600的内存，内存总线频率在800MHz。

 



### 2.3.7 小结



这一结介绍了计算机总线系统以及CPU和各个设备之间的交互。我们可以看到除了CPU自身的速度之外，总线的速度也影响这计算机的整体性能。从发展的过程来看，总线也是一个分分合合的过程。从最初的一条总线，到后来的单独出来的I/O总线，内存总线，就是为了提高CPU的效率。而当CPU和内存速度都发展到一定阶段后，又出现了DDR，双通道等技术，在不提高核心频率的情况下提高了传输率。于是又出现了CPU和内存间直接总线通信降低延迟的情况。 （从2000年开始接触电脑DIY，一直到07年毕业，都对DIY很有兴趣，但是随着电脑越来越快，目前以及弄不太清楚了，复习这些知识也费了我好多时间。）

 

 

# 3. I/O设备

 

前面主要介绍了系统总线和CPU与内存之间的通信，最后一部分简单介绍一下CPU和I/O设备是如何通信的。对于计算机来说输入输出设备也是五大组件。我们知道相对于CPU，I/O设备的工作频率要慢的很多。比如早期的PCI接口工作频率只有33MHz，硬盘的IDE-ATA6的传输速率也只有133MB/s。而现在的 SATA3接口速率能达到600MB/s。

 



## 3.1 I/O设备原理



对于硬件工程师来说，I/O设备是电子芯片、导线、电源、电子控制设备、电机等组成的物理设备。而对于程序员来说，关注的只是I/O设备的编程接口。

 



### 3.1.1 I/O设备分类

- 块设备： 块设备把信息存放在固定大小的块中，每个块都有自己的地址，独立于其他块，可寻址。例如磁盘，USB闪存，CD-ROM等。
- 符号设备：字符设备以字符为单位接收或发送一个字符流，字符设备不可以寻址。列入打印机、网卡、鼠标键盘等。



### 3.1.2 设备控制器



I/O设备一般由机械部件和电子部件两部分组成。电子设备一般称为设备控制器，在计算机上一般以芯片的形式出现，比如我们前面介绍的南桥芯片。不同的控制器可以控制不同的设备。所以南桥芯片中包含了多种设备的控制器，比如硬盘控制器，USB控制器，网卡、声卡控制器等等。而通过总线以及卡槽提供和设备本身的连接。比如PCI，PCI-E，SATA，USB等。

 



### 3.1.3 驱动程序



对于不同的设备控制器，进行的操作控制也是不同的。所以需要专门的软件对他进行控制。这个软件的作用就是用来专门和设备控制器对话，这种软件称为**驱动程序**。一般来说驱动程序由硬件设别厂商提供。所以我们有时会碰到一些设备因为没有安装驱动程序而无法使用的情况。 而目前的OS总都包含了大量的通用驱动程序，使得我们在安装完系统后不需要在额外的安装驱动。但是通用的驱动只能使用设备的基本功能。



驱动程序因为是非操作系统厂商开发，并且需要被安装到操作系统并调用，所以需要有一个统一的模型来开发驱动程序。否则操作系统是无法操作各式各样的设备的。前面我们知道设备非为两大类，所以一般操作系统都定义了这两类设备的标准接口。



 

### 3.1.4 内存映射I/O



每个控制器都有几个寄存器和CPU进行通信。通过写入这些寄存器，可以命令设备发送或接受数据，开启或关闭。而通过读这些寄存器就能知道设备的状态。因为寄存器数量和大小是有限的，所以设备一般会有一个RAM的缓冲区，来存放一些数据。比如硬盘的读写缓存，显卡的显存等。一方面提供数据存放，一方面也是提高I/O操作的速度。



现在的问题是CPU如何和这些设备的寄存器或数据缓冲区进行通信呢？存在两个可选方案：

1. 为每个控制器分配一个I/O端口号，所有的控制器可以形成一个I/O端口空间。存放在内存中。一般程序不能访问，而OS通过特殊的指令和端口号来从设备读取或是写入数据。早期计算机基本都是这种方式。
2. 将所有控制器的寄存器映射到内存空间，于是每个设备的寄存器都有一个唯一的地址。这种称为**内存映射I/O。**



另一种方式是两种的结合，寄存器拥有I/O端口，而数据缓冲区则映射到内存空间。Pentinum就是使用这种方式，所以在IBM-PC兼容机中，内存的0-640K是I/O端口地址，640K-1M的地址是保留给设备数据缓冲区的。(关于内存分布后面文章会介绍)



对于我们程序员来说这两种方案有所不同

1. 对于第一种方式需要使用汇编语言来操作，而第2种方式则可以使用C语言来编程，因为他不需要特殊的指令控制，对待I/O设备和其他普通数据访问方式是相同的。
2. 对于I/O映射方式，不需要特殊的保护机制来组织对I/O的访问，因为OS已经完成了这部分工作，不会把这一段内存地址分配给其他程序。
3. 对于内存可用的指令，也能使用在设备的寄存器上。



任何技术有有点就会有缺点，I/O内存映射也一样：

1. 前面提到过Cache可以对内存进行缓存，但是如果对I/O映射的地址空间进行缓存就会有问题。所以必须有机制来禁用I/O映射空间缓存，这就增大了OS的复杂性。
2. 另一个问题是，因为发送指令后需要判断是内存还是I/O操作，所以它们需要能够检查全部的内存空间。以前CPU,内存和I/O设备在同一个总线上，所以检查很方便。但是后来为了提高CPU和内存效率，CPU和内存之间有一条高速的总线（比如QPI）。这样I/O设备就无法查看内存地址，因为内存地址总线旁落到了内存和CPU的高速总线上，所以需要一个额外的芯片来处理（北桥芯片，内存控制器的作用），增大了系统的复杂度。

 



## 3.2 CPU和I/O设备数据交换方式



前面已经知道CPU通过内存映射的方式和I/O设备交换数据，但是对于CPU来说，无论是从内存还是I/O设备读取数据，都需要把地址放到地址总线上，然后在向控制总线传递一个READ信号，还要用一条信号线来表示是从内存还是I/O读取数据。因为I/O映射的内存区域是特定的，所以不存在无法区分是内存还是I/O操作。目前一共有3种方式进行操作：

1. 程序控制I/O： CPU在向I/O设备发出指令后，通过程序查询方式检查I/O设备是否完成工作，如果完成就读取数据，这种方式缺点是CPU在I/O设备工作时被占用。
2. 中断驱动I/O： CPU是稀缺资源，所以为了提高利用率，减少I/O等待。在I/O设备工作时CPU不再等待，而是进行其他的操作，当I/O设备完成后，通过一个硬件中断信号通知CPU。CPU在来处理接下来的工作，比如读取数据存放到内存。但是每次只能请求一个字节，效率很低。
3. DMA： Direct Memory Access利用一种特性的芯片存在于CPU和I/O设备之间。CPU需要操作I/O设备时只需要发送消息给DMA芯片，后面的事情全部内又DMA来完成，当把所需要数据放入内存后在通知CPU进行操作，整个过程DMA直接和内存总线打交道，而CPU也只需要和DMA芯片和内存交互，大大提高了速度。

![img](https://img-blog.csdn.net/20130829142408906)

![img](https://img-blog.csdn.net/20130829142217531)

 



# 总结



 

这一篇文章介绍了计算机组件中的存储器的分类和工作原理，以及I/O设别的工作方式。通过总线将各个部件连接起来。我们可以看到计算机的发展不光是CPU，存储器以及I/O设备的发展，总线也是起了非常关键的作用。通过前2章的介绍，应该对计算机硬件的工作原理有了大概的了解。后面开始将主要偏向计算机操作系统软件的工作方式。当然这些也是和一些硬件的特性分不开的。

 

 

 

# 参考



《深入理解计算机系统》

《现代操作系统(原书第3版)》

[动态随机存取存储器](http://blog.csdn.net/cc_net/article/details/zh.wikipedia.org/zh-cn/SRAM)

[动态随机存取存储器:](http://zh.wikipedia.org/zh-cn/DRAM)

[寄存器的速度为何比内存更快？](http://www.zhihu.com/question/20075426)

[PC架构系列：CPU/RAM/IO总线的发展历史](http://blog.csdn.net/xport/article/details/1387928)

[内存核心频率、工作频率、预读技术详解](http://www.360doc.com/content/12/0428/16/9523427_207303272.shtml)









# （3）-- 内存工作原理

2017年01月18日 22:40:23 [wangpengk7788](https://me.csdn.net/wangpengk7788) 阅读数：1799



# 前言

 

转载自http://blog.csdn.net/cc_net/article/details/11097267



前面两篇文章介绍了计算机硬件是如何工作的。而从这一章开始将逐渐的转到软件上面来。我们还有内存这一个很重要的部分没有介绍。这一章不仅仅介绍内存的工作原理，还会介绍内存的编址、内存数据存放。逐渐从硬件过渡到软件上来。为后面介绍程序运行打下基础。





 

# 1. 内存工作原理



 

CPU和内存是计算机中最重要的两个组件，前面已经知道了CPU是如何工作的，上一篇也介绍了内存采用的DRAM的存储原理。CPU工作需要知道指令或数据的内存地址，那么这样一个地址是如何和内存这样一个硬件联系起来的呢？现在就看看内存到的是怎么工作的。



 

## 1.1 DRAM芯片结构

![img](https://img-blog.csdn.net/20130904214441234)

上图是DRAM芯片一个单元的结构图。一个单元被分为了N个超单元（可以叫做cell），每个单元由M个DRAM单元组成。我们知道一个DRAM单元可以存放1bit数据， 所以描述一个DRAM芯片可以存储N*M位数据。上图就是一个有16个超单元，每个单元8位的存储模块，我们可以称为16*8bit 的DRAM芯片。而超单元（2,1）我们可以通过如矩阵的方式访问，比如 data = DRAM[2.1] 。这样每个超单元都能有唯一的地址，这也是内存地址的基础。

 

每个超单元的信息通过地址线和数据线传输查找和传输数据。如上图有2根地址线和8根数据线连接到存储控制器（注意这里的存储控制器和前面讲的北桥的内存控制器不是一回事），存储控制器电路一次可以传送M位数据到DRAM芯片或从DRAM传出M位数据。为了读取或写入【i,j】超单元的数据，存储控制器需要通过地址线传入行地址i 和列地址j。这里我们把**行地址称为RAS(Row Access Strobe)请求, 列地址称为(Column Access Strobe)请求。**



但是我们发现地址线只有2为，也就是寻址空间是0-3。而确定一个超单元至少需要4位地址线，那么是怎么实现的呢？

![img](https://img-blog.csdn.net/20130904220940000)

解决这个问题采用的是分时传送地址码的方法。看上图我们可以发现在DRAM芯片内部有一个行缓冲区，实际上获取一个cell的数据，是传送了2次数据，第一次发送RAS，将一行的数据放入行缓冲区，第二期发送CAS，从行缓冲区中取得数据并通过数据线传出。这些地址线和数据线在芯片上是以管脚（PIN）与控制电路相连的。将DRAM电路设计成二维矩阵而不是一位线性数组是为了降低芯片上的管脚数量。入上图如果使用线性数组，需要4根地址管脚，而采用二维矩阵并使用RAS\CAS两次请求的方式只需要2个地址管脚。但这样的缺点是增加了访问时间。

 

 

## 1.2 内存模块

 

内存模块也就是我们常说的内存条。我们在购买内存是经常会听到我这个内存采用的是什么颗粒，如下左图，我们看到内存PCB上的一块块的就是内存颗粒。也就是我们DRAM芯片。通过管脚和PCB连接。不同厂商，不同类型的内存可以的大小，管脚，性能，封装都不一样，但是原理都是一样。这里我们就不展开介绍了。而下有图展示了一个1M*4bit的DRAM芯片的管脚图。

![img](https://img-blog.csdn.net/20130904224030359)  ![img](https://img-blog.csdn.net/20130904225355703)

对于一个内存颗粒来说，它的容量和字长是有限的，所以我们使用内存是会把多个颗粒组成内存模块来对内存进行字长和容量的扩展。目前的内存一般内存条上面会有多颗内存颗粒，比如一条64M的内存可能是由8个8M*8bit 的SDRAM内存颗粒组成。

 



### 1.2.1 字长位数扩展 

![img](https://img-blog.csdn.net/20130904230851218)

位扩展的方法很简单，只需将多片RAM的相应地址端、读/写控制端 和片选信号CS并接在一起，而各片RAM的I/O端并行输出即可。 如上图，我们采用了8个DRAM芯片分，别编号为0-7，每个超单元中存储8位数据。在获取add（row=i，col=j）地址的数据的时候，从每个DRAM芯片的【i, j】单元取出一个字节的数据，这样传送到CPU的一共是8*8b = 64b的数据。我们通过8个8M*8b的内存颗粒扩展为了8M*64b的内存模块。

 

 

### 1.2.2 字存储容量扩展

 

RAM的字扩展是利用译码器输出控制各片RAM的片选信号CS来实现的。RAM进行字扩展时必须增加地址线，而增加的地址线作为高位地址与译码器的输入相连。同时各片RAM的相应地址端、读/写控制端 、相应I/O端应并接在一起使用。下图是我们通过4个2M*8b的内存颗粒，将内存容量扩展到了8M，字长为8位。

![img](https://img-blog.csdn.net/20130904232658406)

 

最后，内存通过主板上的内存插槽DIMM和内存总线相连接。对于不同内存比如SDRAM和DDR他们内存金手指的定义是不同的。这里就不需要详细介绍了。

 



# 2. 内存编址

 

前面我们知道了DRAM颗粒以及内存模块是如何扩展字长和容量的。一个内存可能是8位，也可能是64位，容量可能是1M，也可能是1G。那么内存是如何编地的呢？和地址总线，计算机字长之间又有什么关系呢？

 

 

## 2.1 字长

 

计算机在同一时间内处理的一组二进制数称为一个计算机的“字”，而这组二进制数的位数就是“字长”。。通常称处理字长为8位数据的CPU叫8位CPU，32位CPU就是在同一时间内处理字长为32位的二进制数据。 所以这里的字并不是我们理解的双字节（Word）而是和硬件相关的一个概念。一般来说计算机的数据线的位数和字长是相同的。这样从内存获取数据后，只需要一次就能把数据全部传送给CPU。

 



## 2.2 地址总线

 

前面我们已经介绍过地址总线的功能。地址总线的数量决定了他最大的寻址范围。就目前来说一般地址总线先字长相同。比如32位计算机拥有32为数据线和32为地线，最大寻址范围是4G（0x00000000 ~ 0xFFFFFFFF）。当然也有例外，Intel的8086是16为字长的CPU，采用了16位数据线和20位数据线。

 



## 2.3 内存编址

 

从前面我们知道一个内存的大小和它芯片扩展方式有关。比如我们内存模块是采用 16M*8bit的内存颗粒，那么我们使用4个颗粒进行位扩展，成为16M*32bit，使用4个颗粒进行字容量扩展变为64M*32bit。那么我们内存模块使用了16个内存颗粒，实际大小是256MB。

 

我们需要对这个256M的内存进行编址以便CPU能够使用它，通常我们多种编址方式：

1. 按字编址：    对于这个256M内存来说，它的寻址范围是64M，而每个内存地址可以存储32bit数据。
2. 按半字编址：对于这个256M内存来说，它的寻址范围是128M，而每个内存地址可以存储16bit数据。
3. 按字节编址：对于这个256M内存来说，它的寻址范围是256M，而每个内存地址可以存储8bit数据。



对于我们现在的计算机来说，**主要都是采用按字节编址的方式。**所以我们可以把内存简单的看成一个线性数组，数组每个元素的大小为8bit，我们称为一个存储单元。这一点很重要，因为后面讨论的所有问题内存都是以按字节编址的方式。 这也是为什么对于32位计算机来说，能使用的最多容量的内存为4GB。如果我们按字编地址，能使用的最大内存容量就是16GB了。

 

于是很容易想到一个问题，为什么我们要采用字节编址的方式呢？关于这个问题，我在网上基本没有找到答案，甚至都找不到问这个问题的。所以这里没法给出答案，为什么为什么呢？ 麻烦知道的朋友告诉我哈。

 

另一方面的问题是，内存编址方式和DRAM芯片是否有关呢？ 我认为还是有一定关系。比如我DRAM的芯片是8M*8bit，那么芯片最小的存储单位就是8bit，那么我们内存编址就不能按照半个字节来编址。否则内存取出8bit，根本不知道你要那4bit传给CPU。也有一种说法是现在的DRAM芯片cell都是8bit，所以采用按字节编址。另一方面应该也和数据总线位宽有关。

 

 

# 3. 内存数据

 

前面我们知道了，内存是按字节编址，每个地址的存储单元可以存放8bit的数据。我们也知道CPU通过内存地址获取一条指令和数据，而他们存在存储单元中。现在就有一个问题。我们的数据和指令不可能刚好是8bit，如果小于8位，没什么问题，顶多是浪费几位（或许按字节编址是为了节省内存空间考虑）。但是当数据或指令的长度大于8bit呢？因为这种情况是很容易出现的，比如一个16bit的Int数据在内存是如何存储的呢？

 



## 3.1 内存数据存放

 

其实一个简单的办法就是使用多个存储单元来存放数据或指令。比如Int16使用2个内存单元，而Int32使用4个内存单元。当读取数据时，一次读取多个内存单元。于是这里又出现2个问题：

1. 多个存储单元存储的顺序？
2. 如何确定要读几个内存单元？

 

### 3.1.1 大端和小端存储

1. Little-Endian 就是低位字节排放在内存的低地址端，高位字节排放在内存的高地址端。
2. Big-Endian 就是高位字节排放在内存的低地址端，低位字节排放在内存的高地址端。

![img](https://img-blog.csdn.net/20130905013446437)

需要说明的是，计算机采用大端还是小端存储是CPU来决定的， 我们常用的X86体系的CPU采用小端，一下ARM体系的CPU也是用小端，但有一些CPU却采用大端比如PowerPC、Sun。判断CPU采用哪种方式很简单：





```cpp
bool IsBigEndian()    
{    
    int vlaue = 0x1234;    
    char lowAdd =  *(char *)&value;     
    if( lowAdd == 0x12)    
    {    
        return true;    
    }    
    return false;    
}  
```




既然不同计算机存储的方式不同，那么在不同计算机之间交互就可能需要进行大小端的转换。这一点我们在Socket编程中可以看到。这里就不介绍了，对以我们单一CPU来说我们可以不需要管这个转换的问题，另外我们目前个人PC都是采用小端方式，所以我们后面默认都是这种方式。

 



### 3.1.2 CPU指令



前面我们多次提到了指令的概念，也知道指令是0和1组成的，而汇编代码提高了机器码的可读性。为什么突然在这里介绍CPU指令呢？ 主要是解释上面的第二个问题，当我读取一个数据或指令时，我怎么知道需要读取多少个内存单元。

 



#### 3.1.2.1 CPU指令格式



首先我们来看看CPU指令的格式，我们知道CPU质量主要就是告诉CPU做什么事情，所以一条CPU指令一般包含操作码（OP）和操作



| **操作码字段** | **地址码字段** |
| -------------- | -------------- |
|                |                |



 

 

根据一条指令中有几个操作数地址，可将该指令称为几操作数指令或几地址指令。



| **操作码** | **Ａ1** | **Ａ2** | **Ａ3** |
| ---------- | ------- | ------- | ------- |
|            |         |         |         |



 

**三地址指令: (A1)　OP　(A2)　-->　A3**



| **操作码** | **Ａ1** | **Ａ2** |
| ---------- | ------- | ------- |
|            |         |         |



 

**二地址指令: (A1)　OP　(A2)　-->　A1**



| **操作码** | **Ａ1** |
| ---------- | ------- |
|            |         |



 

**一地址指令: (AC)　OP　(A)　-->　AC**　　　



| **操作码** |      |
| ---------- | ---- |
|            |      |



 

　　　　**零地址指令**

A1为被操作数地址，也称源操作数地址； A2为操作数地址，也称终点操作数地址； A3为存放结果的地址。 同样，A1,A2,A3以是内存中的单元地址，也可以是运算器中通用寄存器的地址。所以就有一个寻址的问题。关于指令寻址后面会介绍。

CPU指令设计是十分复杂的，因为在计算机中都是0和1保存，那计算机如何区分一条指令中的操作数和操作码呢？如何保证指令不会重复呢？这个不是我们讨论的重点，有兴趣的可以看看计算机体系结构的书，里面都会有介绍。从上图来看我们知道CPU的指令长度是变长的。所以CPU并不能确定一条指令需要占用几个内存单元，那么CPU又是如何确定一条指令是否读取完了呢？

 



#### 3.1.2.2 指令的获取

####  

现在的CPU多数采用可变长指令系统。关键是指令的第一字节。 当CPU读指令时，并不是一下把整个指令读近来，而是先读入指令的第一个字节。指令译码器分析这个字节，就知道这是几字节指令。接着顺序读入后面的字节。每读一个字节，程序计数器PC加一。整个指令读入后，PC就指向下一指令（等于为读下一指令做好了准备）。

Sample1:



```
MOV AL,00  机器码是1011 0000 0000 0000  
```


机器码是16位在内存中占用2个字节：

【00000000】 <- 0x0002

【10110000】 <- 0x0001



比如上面这条MOV汇编指令，把立即数00存入AL寄存器。而CPU获取指令过程如下：

1. 从程序计数器获取当前指令的地址0x0001。
2. 存储控制器从0x0001中读出整个字节，发送给CPU。PC+1 = 0X0002.
3. CPU识别出【10110000】表示：操作是MOV AL，并且A2是一个立即数长度为一个字节，所以整个指令的字长为2字节。
4. CPU从地址0x0002取出指令的最后一个字节
5. CPU将立即数00存入AL寄存器。



这里的疑问应该是在第3步，CPU是怎么知道是MOV AL 立即数的操作呢？我们在看下面一个列子。

 

Sample2:

```
MOV AL,[0000] 机器码是1010 0000 0000 0000 0000 0000
```

  

这里同样是一条MOV的汇编指令，整个指令需要占用3个字节。

【00000000】 <-0x0003

【00000000】 <- 0x0002

【10100000】 <- 0x0001

 

我们可以比较一下2条指令第一个字节的区别，发现这里的MOV  AL是1010 0000，而不是Sample1中的1011 000。CPU读取了第一个字节后识别出，操作是MOV AL [D16]，表示是一个寄存器间接寻址，A3操作是存放的是一个16位就是地址偏移量（为什么是16位，后面文章会介绍），CPU就判定这条指令长度3个字节。于是从内存0x0002~0x0003读出指令的后2个字节，进行寻址找到真正的数据内存地址，再次通过CPU读入，并完成操作。

 

从上面我们可以看出一个指令会根据不同的寻址格式，有不同的机器码与之对应。而每个机器码对应的指令的长度都是在CPU设计时就规定好了。8086采用变长指令，指令长度是1-6个字节，后面可以添加8位或16位的偏移量或立即数。 下面的指令格式相比上面2个就更加复杂。

 ![img](https://img-blog.csdn.net/20130905130905375)

- 第一个字节的高6位是操作码，W表示传说的数据是字(W=1)还是字节(W=0)，D表示数据传输方向D=0数据从寄存器传出，D=1数据传入寄存器。
- 第二个字节中REG表示寄存器号，3位可以表示8种寄存器，根据第一字节的W，可以表示是8位还是16位寄存器。表3-1中列出了8086寄存器编码表
- 第二个字节中的MOD和R/M指定了操作数的寻址方式，表3-2列出了8086的编码

![img](https://img-blog.csdn.net/20130905131946328)![img](https://img-blog.csdn.net/20130905131950250)

这里没必要也无法更详细介绍CPU指令的，只需要知道，CPU指令中已经定义了指令的长度，不会出现混乱读取内存单元的现象。有兴趣的可以查看引用中的连接。

####  



## 3.1.3  内存数据

 

#### 3.1.3.1 内存数据的操作

 

从上面我们可以知道，操作数可以是立即数，可以存放在寄存器，也可以存放在内存。对于第一个例子，指令已经说明，操作时是一个字节，于是CPU可以从下一个内存地址读取操作时，而对于第二个列子，操作数只是地址偏移，所以当CPU获得这个数据后，需要转换成实际的内存地址，在进行一次内存访问，把数据读入到寄存器中。这里就出现我们前面提到的问题，这个数据我们要读几个存储单元呢？



```asm
    MyClass cla;  
008C3EC9  lea         ecx,[cla]    
008C3ECC  call        MyClass::MyClass (08C1050h)    
008C3ED1  mov         dword ptr [ebp-4],0    
    cla.num5 = 500;  
008C3ED8  mov         dword ptr [ebp-6Ch],1F4h    
    int b1 = MyClass::num1;  
008C3EDF  mov         dword ptr [b1],64h    
    int b2 = MyClass::num2;  
008C3EE6  mov         dword ptr [b2],0C8h    
    int b3 = MyClass::num3;  
008C3EF0  mov         eax,dword ptr ds:[008C9008h]    
008C3EF5  mov         dword ptr [b3],eax    
    int b4 = cla.num4;  
008C3EFB  mov         eax,dword ptr [cla]    
008C3EFE  mov         dword ptr [b4],eax    
    int b5 = cla.num5;  
008C3F04  mov         eax,dword ptr [ebp-6Ch]    
008C3F07  mov         dword ptr [b5],eax    

```




让我们看一段C++代码和对应的汇编代码，操作很简单，创建一个Myclass对象后，对成员变量赋值。而赋值都是试用Mov操作符。对于这些变量我们有赋值操作和取值操作，那么是如何确定要读取或写入数据的大小呢？



```asm
cla.num5 = 500;  
08C3ED8  mov         dword ptr [ebp-6Ch],1F4h    
```


我看先看看赋值操作，往dword ptr [ebp-6Ch]内存存入一个立即数， [ebp-6Ch]是num5的内存地址，而前面的dword ptr 表示这是进行一个双子操作。还记得上面指令格式中第一个字节的W字段吗？ 在8086中只能进行字节或字操作，而现在CPU都可以进行双字操作。



```cpp
int b5 = cla.num5;  
08C3F04  mov         eax,dword ptr [ebp-6Ch]    
```



同样，当我们要从一个内存读取数据的时候，也要指定读取数据的操作类型，这里也是双字操作。这样以来，就能从内存中正确的读出需要的长度了。就这么一个简单的赋值操作，获取你从来没想过在内存中怎么存放，又是怎么读取的。这一切都是编译器和CPU在背后为我们完成了。

 



#### 3.1.3.2 内存对齐

 前面我们清楚了CPU是如何正确读取数大小不同的数据的，最后一部分来看看有关内存对齐的问题。对于大部分程序员来说，内存对齐应该是透明的。内存对齐是编译器的管辖范围。编译器为程序中的每个数据单元安排在适当的位置上。

 



##### 3.1.3.2.1 对齐原因

从前面我们知道，目前计算机内存按照字节编址，每个地址的内存大小为1个字节。而读取数据的大小和数据线有关。比如数据线为8位那么一次读取一个字节，而如果数据线为32位，那么一次需要读取32个字节，这样是为了一次更多的获取数据提高效率。否则读取一个int变量就需要进行4次内存操作。对于内存访问一般有以下两个条件：

1. CPU进行一次内存访问读取的数据和字长相同。
2. 有些CPU只能对字长倍数的内存地址进行访问。

对于第一个条件一般来说，目前存储器一个cell是8bit，进行位扩展使他和字长还有数据线位数是相同，那么一次就能传送CPU可以处理最多的数据。而前面我们说过目前是按字节编址可能是因为一个cell是8bit，所以一次内存操作读取的数据就是和字长相同。

也正是因为和存储器扩展有关（参考1.2.1的图），每个DRAM位扩展芯片使用相同RAS。如果需要跨行访问，那么需要传递2次RAS。所以以32位CPU为例，CPU只能对0,4,8,16这样的地址进行寻址。而很多32位CPU禁掉了地址线中的低2位A0，A1，这样他们的地址必须是4的倍数，否则会发送错误。

![img](https://img-blog.csdn.net/20130905150702640)

如上图，当计算机数据线为32位时，一次读入4个地址范围的数据。当一个int变量存放在0-3的地址中时，CPU一次就内存操作就可以取得int变量的值。但是如果int变量存放在1-4的地址中呢？ 根据上面条件2的解释，这个时候CPU需要进行2次内存访问，第一次读取0-4的数据，并且只保存1-3的内容，第二次访问读取4-7的数据并且只保存4的数据，然后将1-4组合起来。如下图：

![img](https://img-blog.csdn.net/20130905154217437)

所以内存对齐不但可以解决不同CPU的兼容性问题，还能减少内存访问次数，提高效率。当然目前关于这个原因争论很多，可以看看CSDN上的讨论：<http://bbs.csdn.net/topics/30388330>

 

##### 3.1.3.2.2 如何对齐内存

内存对齐有一个对齐系数，一般是2,4,8,16字节这样。而不同平台上的对齐方式不同，这个主要是编译器来决定的。

具体的规则可以参考之前转的一篇文章，这里就不详细写了： <http://blog.csdn.net/cc_net/article/details/2908600>

 





# 总结





通过这一篇对内存工作的介绍，我们从内存的硬件结构，存储方式过渡到了内存的编址方式，然后又探讨了按字节编址带来的问题和解决的办法。这里就涉及到了CPU的指令格式，编译器的支持。最后我们也是从硬件和软件方面讨论了内存对齐的问题。



我自己感觉，内存的访问管理是计算机中最重要的部分，也是计算机硬件和软件之间交互的过渡的一个地方。所以理解了内存的工作原理，对于后面理解不同的内存模型很有帮助。







# （4）-- 操作系统发展和程序编译

2017年01月19日 15:44:00 [wangpengk7788](https://me.csdn.net/wangpengk7788) 阅读数：222



# 前言

 



转载自http://blog.csdn.net/cc_net/article/details/11396081



前面的文章主要都是计算机硬件相关的一些工作原理。而前一篇文章介绍了内存的工作原理，编址方式，逐步过渡到软件上面来了。前面也说过，内存是一个非常重要的部件，因为CPU所需的指令和数据都在内存中。所以从这一篇开始我们主要看看程序运行时在内存中的布局。

 

我们知道对于计算机系统来说，最底层的是硬件，硬件之上是[操作系统](http://lib.csdn.net/base/operatingsystem)，而我们的程序都是基于操作系统来运行的，而不是基于硬件，这样操作系统为我们提供了一层抽象，所以对于程序员来说，不需要特别的关注计算机硬件。所以正常来说介绍完硬件后，应该来介绍操作系统。但是我们知道在计算机在不断的发展，硬件，操作系统都在不断发展。而无论怎么发展，程序运行都离不开内存，所以我决定以内存和出发点，来看硬件，操作系统以及应用程序的发展过程。

 

 



# 1. 操作系统和内存布局

在第一代计算机时期，构成计算机的主要元器件是电子管，计算机运行速度慢(只有几千次/秒)，当时没有操作系统，甚至没有任何软件。程序员首先编写程序，然后直接在计算机的控制台上操纵程序运行。

 



## 1.1 人工操作方式



当时的计算机有一个控制面板，上面有一些开关，当要启动计算机的使用，就是要把启动程序用手工输入到机器里去，其方法就是利用机柜面板上的一排开关，用二进制代码把指令一条一条拨进去。但是指令有限，干不了太多事情。



所以当时程序员使用机器码编写程序，然后通过打孔机，将程序转换到打孔的纸袋上。纸袋每排有八个孔的位置，穿了孔的代表1，没穿孔的代表0。然后通过纸带机等设备手工将程序装入计算机的内存，按动控制台开关和按扭确定程序的起始地址并启动程序执行。程序员只能通过控制台上的显示灯来观察程序执行情况。当程序运行出错时，程序员直接通过控制台开关来停止程序运行，检查内存及寄存器内容并调试程序。程序运行结果可以通过打印机或穿孔机输出。

![img](https://img-blog.csdn.net/20130908215844906) ![img](https://img-blog.csdn.net/20130908215932140)

上图就是纸袋和纸袋读入机器。但是存储程序的介质是纸袋，而加载程序是通过纸带机。其实这里我也有个疑问：

1. 纸带机是如何把纸袋上的程序加载到内存的呢？
2. 程序被加载到内存的地址是如何确定的呢？



我没有找到答案，不过这个并不影响我们对早期程序内存模型运行的理解。在当时，整个计算机资源（CPU，内存）是由当前运行才程序独占的。所以我们可以想象当时内存的模型。程序的第一条指令被加载到内存的最低地址，比如0x00000000。通过控制面板指定程序首地址，这样CPU就能够按照我们前面介绍的方式来一条条运行。当一个程序运行完成后，要运行下一个程序，需要在使用更换纸袋。清空当前内存，在加载新的程序。



![img](https://img-blog.csdn.net/20130908224907796)

这种方式对于程序来说：

1. 内存范围，可以读写可用的所有内存空间。
2. 独占资源，在更换纸袋时CPU和内存资源是空闲的。

 



## 1.2 脱机输入/输出方式



人工操作方式的主要问题在于，从纸袋读取或写入时浪费了CPU时间，于是就出现了脱机方式。就是首先把要执行的程序通过外围机控制纸带机输入到磁带上。相对于纸带机，从磁带上加载数据到内存要快的多。这个也符合我们前面介绍的存储器分层。脱机方式的有点在于：

1. 减少了CPU空闲的时间，解决人机速度差距的矛盾
2. 通过磁带，提高了I/O速度，进一步减少了CPU的空闲时间

![img](https://img-blog.csdn.net/20130908224121515)

 



## 1.3 单道批处理系统



我们知道那个时候的CPU是非常宝贵的资源。所以必须充分利用，尽量使多个程序能连续的运行。所以一般都会把一批程序以脱机方式输入到磁带上。然后当一个程序执行完成之后，马上运行下一个程序。但是这个过程不是由人来调度的，所以我们需要一个监控程序来控制这些程序一个一个的执行。于是除了我们要运行的程序之外，在内存中还存在这样一个监控程序，我们可以把他看看做早期操作系统的雏形，也就是单道批处理系统。

![img](https://img-blog.csdn.net/20130908224126046)

上图是单道批处理系统的流程图，看起来很简单。这个小程序需要常驻内存，也就是在开始执行其他程序之前，需要把他加载到内存中。当然加载的方式和其他程序相同。然后CPU开始执行这个监控程序。然后这个程序就开始进行批处理操作。于是在内存中，同一时间需要存放两个程序。下图是此时的内存布局.



![img](https://img-blog.csdn.net/20130908225650765) ![img](https://img-blog.csdn.net/20130908232237687)



从此图我们可以发现，这个程序或许并不如我们想象的简单：

1. 控制卡解释程序： 对于监控程序来说，他必须知道当前是那个程序在运行，这就需要每个程序能唯一的标识自己，所以需要在作业程序中加入一段控制卡代码，来唯一标识自己。所以监控程序也需要一个控制卡解释程序来识别这些标识。
2. 设备驱动程序： 我们知道驱动程序是用来控制硬件控制芯片的，而批处理系统在一个程序运行完成时需要通过I/O设备加载下一个程序，所以需要驱动程序完成这些操作。
3. 作业队列：监控程序能够根据控制卡提供的信息自动形成作业队列，所以当程序解释时监控程序可以加载下一个程序。
4. 中端自陷向量表： 关于中端我们前面有介绍过，这里当系统执行I/O操作后，会以中端的方式通知，而这里的向量表则定义了不同中端对应的处理方式。



在这里我们可以发现，对于单道批处理程序来说，内存不再是独享，而是作业程序和监控程序共享，共享就会有一些问题：

1. 作业程序加载的地址如何确定？
2. 如何切换程序的控制权？



所以，我个人觉得，决定程序加载的位置应该是监控程序来确定。比如0x0000~0x050的内存区域是监控程序的区域，这样作业程序的起始地址就是0x0051，而监控程序通过修改PC计数器，来修改CPU要执行的下一条代码来切换程序控制权。

 

 

## 1.4 多道批处理程序



对于单道操作系统来说，他可以连续的运行多个程序，减少了程序切换时的CPU等待时间。但是它的问题在于，当执行I/O操作加时，CPU是空闲的。于是出现了多道批处理程序。多道批处理程序，把多个程序同时加载到内存中，当其中正在运行的程序执行I/O操作时，CPU可以继续执行其他的程序，而当I/O操作结束后，程序可以继续被执行。

![img](https://img-blog.csdn.net/20130908232639656)

上图是多道批处中会存在2个以上的程序，所以需要一个内存分区表来标识每个程序占用的内存范围，以保证每个程序内存空间的独立。

1. 后备队列： 虽然内存中存在多个程序，但是一个时间内只有一个程序会被执行，而其他没有被执行的程序如果单道程序的作业一样也有一个队列，我们叫后备队列。
2. 调度程序：对于多道批处理程序来说，它除了要监控之外，还需要一个调度程序，来决定在I/O操作时，后备队列中的那一个程序获得CPU时间。

![img](https://img-blog.csdn.net/20130908233916984)

多道批处理系统的优点在于资源利用率高、系统吞吐量大、平均运行时间长。同样也存在物交互能力，而且需要内存管理，作业调度，设备管理等功能，这就增加了程序的负责的度。

 



## 1.5 现代操作系统



可以说批处理操作系统是现代操作系统的雏形。从DOS到Windows3.1，到Win95，WinXP，Win8，Unix，[Linux](http://lib.csdn.net/base/linux)。现代操作系统可以说是一个庞大的程序，多CPU，多进程，多线程，虚拟内存等等，相比多道批处理应用程序复杂了千万倍。而内存布局也随着硬件和操作系统的发展而发生了变化。但是主要的目的都是提高系统效率，提高系统稳定性安全性。提供更好的交互体验。

 

 



# 2 程序的编译和链接

到目前为止，我们都只是一直在谈论，程序被加载到内存然后执行的过程，而没有提及到程序是如何从我们编写的代码变为可执行文件的。在我们开始介绍现代操作系统中程序内存布局之前，先看看程序是如何被编译成可执行文件的。因为计算机的发展是和硬件，操作系统，编译器等共同发展分不开的。

 

 

## 2.1 编译过程

 

现在我们基本都是在可视环境下进行开发，比如Eclipse，VS等开发工具。这些工具功能相当的强大，我们只需专注代码的编写，点几下鼠标，一个可执行文件就被生成出来了。但是在这背后，开发工具到底做了什么呢？ 下面一个简单的C程序是如何被编译成可执行文件的呢？

 

```c
#include <stdio.h>  
int main()  
{  
    printf("Hello, world.\n");  
    return 0;  
}  
```



一般来说，一个程序从源代码到可执行文件是通过编译器来完成的，简单的说，编译器的工作就是把高级语言转换为机器码，一个现代的编译器工作流程是：**（源代码）--预处理--编译---汇编---链接--（可执行文件）。**在Linux下一般使用**GCC**来编译[C语言](http://lib.csdn.net/base/c)程序， 而VS中使用**cl.exe**。下图就是上面的代码在GCC中编译的过程。我们后面讨论的都以C语言为例。编译器



![img](https://img-blog.csdn.net/20130909000837484)

 

### 2.1.1 预处理

 

预处理是程序编译的第一步，以C语言为例， 预编译会把源文件预编译成一个 .I 文件。而C++则是编译成 .ii。 GCC中预编译命令如下



```shell
gcc -E hello.c -o hello. I  
```



 

当我们打开hello.i 文件是会发现这个文件变的好大，因为其中包含的<stdio.h> 文件被插入到了hello.i 文件中，一下是截取的一部分内容



```c
# 1 "hello.c"  
# 1 "<built-in>"  
# 1 "<命令行>"  
# 1 "hello.c"  
# 1 "/usr/include/stdio.h" 1 3 4  
# 28 "/usr/include/stdio.h" 3 4  
# 1 "/usr/include/features.h" 1 3 4  
# 324 "/usr/include/features.h" 3 4  
# 1 "/usr/include/i386-linux-gnu/bits/predefs.h" 1 3 4  
# 325 "/usr/include/features.h" 2 3 4  
# 357 "/usr/include/features.h" 3 4  
# 1 "/usr/include/i386-linux-gnu/sys/cdefs.h" 1 3 4  
# 378 "/usr/include/i386-linux-gnu/sys/cdefs.h" 3 4  
# 1 "/usr/include/i386-linux-gnu/bits/wordsize.h" 1 3 4  
# 379 "/usr/include/i386-linux-gnu/sys/cdefs.h" 2 3 4  
# 358 "/usr/include/features.h" 2 3 4  
# 389 "/usr/include/features.h" 3 4  
# 1 "/usr/include/i386-linux-gnu/gnu/stubs.h" 1 3 4# 940 "/usr/include/stdio.h" 3 4  
  
  
  
# 2 "hello.c" 2  
int main()  
{  
    printf("Hello, world.\n");  
    return 0;  
}  
```



 

总结下来预处理有一下作用：

- 所有的#define删除，并且展开所有的宏定义
- 处理所有的条件预编译指令，比如我们经常使用#if #ifdef #elif #else #endif等来控制程序
- 处理#include 预编译指令，将被包含的文件插入到该预编译指令的位置。这也就是为什么我们要防止头文件被多次包含。
- 删除所有注释 “//”和”/* */”.
- 添加行号和文件标识，以便编译时产生调试用的行号及编译错误警告行号。比如上面的 # 2 "hello.c" 2
- 保留所有的#pragma编译器指令，因为编译器需要使用它们。

 

### 2.1.2 编译

 

编译是一个比较复杂的过程。编译后产生的是汇编文件，其中经过了**词法分析、语法分析、语义分析、中间代码生成、目标代码生成、目标代码优化**等六个步骤。大学时有一门《编译原理》的课程就是讲这个的，只可惜当时学的并不好，感觉太枯燥太难懂了。所以当我们语法有错误、变量没有定义等问题是，就会出现编译错误。



```
gcc -S hello.i -o hello.s  
```



通过上面的命令，可以从预编译文件生成汇编文件，当然也可以之际从源文件编译成汇编文件。实际上是通过一个叫做ccl的编译程序来完成的。





```asm
    .file   "hello.c"  
    .section    .rodata  
.LC0:  
    .string "Hello, world."  
    .text  
    .globl  main  
    .type   main, @function  
main:  
.LFB0:  
    .cfi_startproc  
    pushl   %ebp  
    .cfi_def_cfa_offset 8  
    .cfi_offset 5, -8  
    movl    %esp, %ebp  
    .cfi_def_cfa_register 5  
    andl    $-16, %esp  
    subl    $16, %esp  
    movl    $.LC0, (%esp)  
    call    puts  
    movl    $0, %eax  
    leave  
    .cfi_restore 5  
    .cfi_def_cfa 4, 4  
    ret  
    .cfi_endproc  
.LFE0:  
    .size   main, .-main  
    .ident  "GCC: (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3"  
    .section    .note.GNU-stack,"",@progbits  
```





上面就是生成的汇编文件。我们看出其中分了好几个部分。我们只需要关注，LFB0这个段中保存的就是C语言的代码对于的汇编代码.。

 



### 2.1.3 汇编

 

汇编的过程比较简单，就是把汇编代码转换为机器可执行的机器码，每一个汇编语句机会都对应一条机器指令。它只需要根据汇编指令和机器指令的对照表进行翻译就可以了。汇编实际是通过**汇编器as**来完成的，gcc只不过是这些命令的包装。



```shell
#gcc -c hello.s -o hello.o  
//或者  
#as hello.s -o hello.o  
```



汇编之后生成的文件是二进制文件，所以用文本打开是无法查看准确的内容的，用二进制文件查看器打开里面也全是二进制，我们可以用objdump工具来查看：



```asm
cc@cheng_chao-nb-vm:~$ objdump -S hello.o  
  
hello.o:     file format elf32-i386  
  
  
Disassembly of section .text:  
  
00000000 <main>:  
   0:   55                      push   %ebp  
   1:   89 e5                   mov    %esp,%ebp  
   3:   83 e4 f0                and    $0xfffffff0,%esp  
   6:   83 ec 10                sub    $0x10,%esp  
   9:   c7 04 24 00 00 00 00    movl   $0x0,(%esp)  
  10:   e8 fc ff ff ff          call   11 <main+0x11>  
  15:   b8 00 00 00 00          mov    $0x0,%eax  
  1a:   c9                      leave    
  1b:   c3                      ret      
```





上面我们看到了Main函数汇编代码和机器码对应的关系。关于objdump工具后面会介绍。这里生成的.o文件我们一般称为目标文件，此时它已经和目标机器相关了。

 

 

### 2.1.4 链接

链接是一个比较复杂的过程，其实链接的存在是因为库文件的存在。我们知道为了代码的复用，我们可以把一些常用的代码放到一个库文件中提供给其他人使用。而我们在使用C，C++等高级语言编程时，这些高级语言也提供了一些列这样的功能库，比如我们这里调用的printf 函数就是C标准库提供的。 为了让我们程序正常运行，我们就需要把我们的程序和库文件链接起来，这样在运行时就知道printf函数到底要执行什么样的机器码。



```shell
ld -static crt1.o crti.o crtbeginT.o hello.o -start-group -lgcc -lgcc_eh -lc-end-group crtend.o crtn.o  
```





我们看到我们使用了**链接器ld**程序来操作，但是为了得到最终的a.out可执行文件(默认生成a.out)，我们加入了很多目标文件，而这些就是一个printf正常运行所需要依赖的库文件。

 

 

### 2.1.5 托管代码的编译过程

 

对于C#和[Java](http://lib.csdn.net/base/javase)这种运行在虚拟机上的语言，编译过程有所不同。 对于C,C++的程序，生成的可执行文件，可以在兼容的计算机上直接运行。但是C#和JAVA这些语言则不同。他们编译过程是相似的，但是他们最终生成的并不是机器码，而是中间代码，对于C#而言叫IL代码，对于JAVA是字节码。所以C#,JAVA编译出来的文件并不能被执行。



我们在使用.NET或JAVA时都需要安装.NET CLR或者JAVA虚拟机，以.NET为例，CLR实际是一个COM组件，当你点击一个.NET的EXE文件时，它和C++等不一样，不能直接被执行，而是有一个垫片程序来启动一个进程，并且初始化CLR组件。当CLR运行后，一个叫做JIT的编译器会吧EXE中的IL代码编译成对应平台的机器码，然后如同其他C++程序一样被执行。



有关C#程序的编译和运行可以参考之前写的： [.Net学习笔记（一） ------ .NET平台结构](http://blog.csdn.net/cc_net/article/details/1890461)





# 参考



《程序员的自我修养》

《计算机操作系统》











# （5）-- x86-16 CPU和内存管理







# 前言







前面我们已经了解了计算机硬件的工作原理，以及[操作系统](http://lib.csdn.net/base/operatingsystem)的发展。我们知道是内存把计算机硬件和软件联系了起来。不夸张的说，了解了软件在内存中的结构，就基本了解了程序最底层的运行原理。所以从这一篇开始，将深入的讨论计算机中内存管理和布局。内存的管理同计算机硬件以及擦做系统是分不开的。这一篇我们主要讨论早期x86 CPU和DOS系统对于内存的管理。





# 1. 8086 CPU



说到CPU，我们第一个想到的应该就是Intel。 1971年11月15号，Intel发布了全球第一款微处理器Intel 4004，这是一个主频只有108KHz的4bit处理器。而后又发布了8bit的8008处理器。而我们最熟悉的应该就是8086，为什么？因为随便找一本汇编的书籍看看，都会有8086四个大字。因为8086标志着Intel x86体系结构的CPU的开始。而且8086/8088开始用于便携电脑，所以我们就从8086开始介绍。80186除8086内核，另外包括了中断控制器、定时器、DMA、I/O、UART、片选电路等外设。

![img](https://img-blog.csdn.net/20131020012630328)





## 1.1 8086/8088内存访问



8086是x86体系结构的开始，他采用了16bit，但是地址线却用了20位。前面介绍CPU工作原理的时候哦我们知道，CPU内部有一个PC计数器，用来存储下一个要执行的物理地址。但是16位的寄存器如何存储20位的地址呢？



不仅仅是8086，我们发现之前的CPU的位宽和可寻址范围都不是对应的关系，而且4004和8008也找不到地址线位宽。对于8080来说，地址有16位，而它内部有1个主累加器和5个次累加器，所以它使用2个寄存器组合来访问16位地址。而对于8086，并没有采用相同的方式，而是参考了PDP-11小型机，设计出了**分段寻址技术**。

![img](https://img-blog.csdn.net/20131020015240765)

因为CPU一次能送出的地址是16位，要访问20位地址的存储器就需要使用2个16位的地址计算表示一个20位的地址。这里采用的办法是，将内存分为不同逻辑段，每段段有自己的段地址（16位），而段内数据地址则是相对于段首地址的偏移地址（16位）。而且段之间是可以相邻或者重叠的。



因为偏移地址是16位，所以最大的范围是64K，而对于1M内存来说，最少有16个逻辑段。而因为段寄存器也是16位，所以段的物理地址需要是16的倍数，表示为0xXXXX0。这样的地址可以压缩为0xXXXX，所以段首地址的高16位表示段值。所以段首的物理地址 = 段值 * 0x10。那么偏移地址是相对于段首地址来说的，那么要访问的物理地址公式为：**物理地址 = 段值\*0x10 + 偏移地址**

 

这里介绍一下**逻辑地址**的概念，逻辑地址指的是机器语言指令中，用来指定一个操作数或者是一条指令的地址。Intel中段式管理中，对逻辑地址要求，“一个逻辑地址，是由一个段标识符加上一个指定段内相对地址的偏移量“，表示为 [段标识符：段内偏移量]。

![img](https://img-blog.csdn.net/20131024013601390)

 





## 1.2 8086寄存器



![img](https://img-blog.csdn.net/20131020022920421)

 

关于8086CPU的[架构](http://lib.csdn.net/base/architecture)图，我们在前面的文章中出现了多次。这里我们只看看8086的寄存器。我们知道寄存器对于CPU来时是非常的重要，无论是取指令还是做运算都需要寄存器来存放数据。我们在多线程编程中经常听到一个词是切换上下文，这里所指的上下文就包含CPU寄存器的值，当然这个是后话，后面会介绍。



8086一共有14个16位寄存器，具体分类如上图：

- AX,BX,CX,DX 是主寄存器，又叫通用寄存器， 为了兼容8位CPU，采用2个8位寄存器组合而成，这4个寄存器主要用作数据寄存器，节省从寄存器存取操作的时间。
- SI, DI是变址寄存器，主要用于操作字符串，当然也可以作为通用寄存器；而BP,SP是指针寄存器，主要用于堆栈操作。
- CS,DS,ES,SS是段寄存器，是为了内存分段而设置的，用来存放逻辑段的段值。
- IP 是指令指针寄存器，类似前面提到的PC计数器，这里用作表示下一条指令相对于CS段的偏移
- Flags标志寄存器，主要用于表示CPU计算的运算结果和状态。



关于这些寄存器具体作用可以参考：[8086 CPU 寄存器简介](http://www.cnblogs.com/BoyXiao/archive/2010/11/20/1882716.html)





## 1.3 8086段寄存器的引用



从8086开始，采用了分段式的内存管理，于是在访问内存时不在像以前那样，拿到地址直接送到总线进行访问，而是需要通过计算得到的。这就涉及到使用那个段寄存器和偏移量。一般来说代码不需要指定要访问的段，总线可以自行判断，当然也可以显示的指定。

![img](https://img-blog.csdn.net/20131021000402281)

![img](https://img-blog.csdn.net/20131021000406500)

比如我们在获取下一条指令地址时，就是用CS*16+IP，而当执行一条取数据指令时，就是用DS*16+有效地址来访问，上表就定义了不同操作时是用到的寄存器和偏移。



这里可能有一个疑问：程序是如何被加载到不同内存段呢？这个其实和程序的编译，可执行文件的结构以及操作系统有关，有此可见一项新技术的使用是需要硬件和软件相互配合的。这些会在后面介绍。





## 1.4 8086寻址方式



表示指令中操作数所在的方法称为寻址方式。





### 1.4.1 立即寻址

操作数直接包含在指令中，比如MOV AX, 1234H, 一般用于给存储器或寄存器赋值。

![img](https://img-blog.csdn.net/20131021002352078)



### 1.4.2 寄存器寻址



操作数存放在寄存器中，而指令中存放的是寄存器号，比如MOV SI, AX。这里可以用到的寄存器有AX,BX,CX,DX,SI,DI,SP,BP.





### 1.4.3 直接寻址



操作数在存储器中，指令中直接包含存储器的有效地址。比如MOV AX, [1234H]。从前面知道，这里的有效地址并不是真正的物理地址而是偏移地址，因为这是一条取数据操作指令，所以在没指定段的时候，默认访问的是DS数据段。最后得到真正的物理地址。

![img](https://img-blog.csdn.net/20131021003041234)

当然也可以指定要访问的段比如： MOV AX ES:[1234H]，这种寻址方式只适用于段小于64K的情况，在程序中存储器的有效地址一般用变量表示。





### 1.4.4 寄存器间接寻址



操作数存放在寄存器中，操作数的有效地址存放在SI, DI, BX, BP这4个寄存器中。在一般情况下如果有效地址在SI,DI,BX中则访问DS段，而当在BP中时则访问SS段。比如MOV AX,[SI]

![img](https://img-blog.csdn.net/20131021005431734)

同样，也可以指定有效地址要访问的段，比如MOV AX, CS:[BX]。





### 1.4.5 寄存器相对寻址



操作数在存储器中，操作时的有效地址存放在SI, DI, BX, BP寄存器并加上一个8位或16位的位移量中。比如MOV AX, [DI+1234H]，计算方法和寄存器间接寻址相同，只是多加上一个偏移量。

![img](https://img-blog.csdn.net/20131021010127671)

这种寻址方式有利于实现高级语言中对结构类型数据所实施的操作。





### 1.4.6 基址加变址寻址



操作数在存储器中，操作时的有效地址是由基址寄存器BX或BP和变址寄存器SI或DI相加得到的。比如MOV AX, [BX+DI]

![img](https://img-blog.csdn.net/20131021010915328)

这种寻址方式一般用来处理数组的访问，基址寄存器存放数组首地址，而变址寄存器来定位数组的每个元素。也可以写作MOV AX,[BX][DI]。





### 1.4.7 相对基址加变址寻址



操作数存放在存储器，操作数有效地址由基址寄存器BX或BP，变址寄存器SI或DI， 以及一个8位或16位位移量相加得到的。比如MOV AX, [BX+DI-2]

![img](https://img-blog.csdn.net/20131021011740609)







# 2. 程序的内存结构





前面我们介绍过程序编译的过程，当代码被编译成可执行文件后，当运行程序时，通过装入程序把我们的可执行文件从磁盘装载到内存中，然后指定程序入口地址，CPU变开始顺序的执行。这里就涉及到CPU如何定位程序的地址的问题。其实对于早期程序编写，装入我也不是很了解，资料也挺少的。





## 2.1 早期程序的装入



我们知道，早期的计算机中，CPU并没有对内存分段，所以程序运行时，获取的下一条指令或数据的地址就是真实的内存地址。于是早期的程序在编译时就需要确定装载后在了在内存中的绝对位置。比如装载到内存的0x00000010处，那么在编译时生成的指定和数据，就是基于0x00000010向上扩展。

![img](https://img-blog.csdn.net/20131023223642156)

当程序被装入内存后，装载程序把PC计数器设置为0x00000010，然以CPU开始执行我们的程序。这种方式需要对内存使用情况非常熟悉。





## 2.2 8086程序的装入和运行



8086CPU将内存分割成了不同的段，于是指令和数据的有效地址并不是真正的物理地址而是相对于段首地址的偏移地址。CPU在取地址时会进行计算，所以我们在编译程序时无法确定程序在内存中绝对的位置。而且前面介绍了8086的寻址方式，我们知道在不指定段寄存器的时候，如果是取指定会使用CS，而如果是取操作数则是使用DS。所以我们程序的数据和指令必须装载到不同的段中。



我们知道，内存并没有被真正的分段，而是通过CPU中段寄存器来存放不同段的首地址。所以最好的办法是我们对程序也进行分段，把数据放在程序的数据段中，而代码则放在代码段中。这样在编译的时候，每个数据会每条指定的地址都是相对于段的偏移量，我们只需要设置CPU的CS,DS端寄存的地址。8086汇编语言中就有段定义语句，就是为了和存储器结构对应。

![img](https://img-blog.csdn.net/20131023234519375)



上图显示了汇编程序编译后背加载到内存的情况：

1. 8086的汇编支持定义逻辑段，这里定义了逻辑段和代码段。（我们知道汇编语言是CPU指令的一种翻译形式，和硬件密切相关，所以对于8086来说有特定的汇编语言）
2. 在程序编译时，给每条指令和数据分配一个偏移地址。
3. 在加载程序之前，载入程序会在内存中找到可以存放每个段的地址，并且把段值写入到执行文件中，可以理解为给CSEG和DSEG赋值。
4. 正式载入程序时，会把CSEG，DSEG加载到之前找到的物理地址，并且会去更新CS寄存器为CSEG的值。
5. 载入程序把IP寄存器设置为第一条指令的偏移地址。
6. 把控制权交给我们的程序，此时如图CS = 0x0002（开始执行CS*16 + IP）
7. 执行第一条语句，把DSEG的段值写入到AX，然后执行第二条，把AX的值写入到DS， 也就我们设置了DS寄存器的段值。
8. 当执行到MOV AX, VAR1时，因为是取数据操作，所以从DS段取数据，此时DS=0x0001。 最终VAR1在内存中的地址 = 0x0001*0x10 + 0x0000 = 0x00010。



这里CS和DS的值不是编译时确定的，而是在分配段内存时获得的。但是DS的值是从DSEG写入的，那么DSEG的值是不是加载程序写入到执行文件中的呢？不是很确定。另外或许的是物理内存地址，而CS中并不是物理内存，是不是要用物理内存/0x10来计算出CS和DS的值呢？





## 2.3 8086的多种模式



前面说过了，段的大小最大为64位，那么如果我们的程序的段大于64K或者是对于之前的8位程序，又是如何运行的呢？实际上8086根据分段的内存结构，有六种运行方式。对于8位机上的程序可以不考虑段地址直接以.com可执行文件以“微模式”在8086上运行。这是当时8086与MS-DOS作为新平台取得市场成功的关键原因——大量已存的CP/M应用程序能很快得到利用。而对于大于64k的段则运行在大模式中。这块内容完全不懂，有兴趣就自己研究吧。


 



# 3. MS-DOS内存结构

 

前面讨论完了CPU的内存访问方式，最后讨论一下操作系统的内存管理。这里选用了MS-DOS操作系统。这里并没有指定是那一个版本，只是从大体上去介绍内存管理方式。MS-DOS是一个单任务的批处理操作系统，同时只能有一个.COM或.EXE文件被执行，所做系统没有任务调度功能，使用.BAT文件可以实现批处理功能。而早期的Windows1.x也是基于MS-DOS，只是增加了图形界面。

##  

## 3.1 早期MS-DOS的内存布局

 

IBM和微软在设计DOS操作系统时，当时CPU主要是8086，采用了20位地址线，所以最大内存访问只1M。但是在在当时普片采用8bitCPU，最大内存访问64K的CPU来说，这个内存空间已经相当大了，所以MS-DOS就基于这个进行了设计。

![img](https://img-blog.csdn.net/20131024153758609)

DOS把内存划分问了2个区域：

- 常规内存： 也叫基本内存，是内存低端的640K。DOS系统本身、中断向量表、系统数据、驱动程序都会常驻在这一段内存中，剩余的就是用户程序可以使用的空间，大概600K，而随着MS-DOS功能越来越多，DOS本身所占用的空间也越来越大。而MS-DOS提供的内存管理，也主要是管理常规内存块。
- 上位内存：这一块内存区域是从640K-1M，是给外接卡设备的数据缓冲区以及ROM-BIOS使用的。 在这段内存空间中有一些空闲的空间，称为UMB(Upper Memory Block)。这些内存DOS系统无法进行管理。*EMM386.exe*可以管理这一部分内存。

 

## 3.2 MS-DOS 扩充内存（EMS）

 

随着PC的发展，越来越多硬件支持MS-DOS, 越来越多的软件开始在MS-DOS上被使用，于是640K内存中，操作系统，驱动程序，杀毒程序，常驻程序的体积越来越来，而应用程序可以使用的空间越来越小。为了能让8086使用更大的内存，Intel和MS联合推出了EMS（Expanded Memory Specification）扩充内存。通过主板上的扩展槽，最多可以支持32M内存。

 

但是CPU并不不能直接访问扩充的内存，通过扩充内存管理程序，使用了上位内存空间中的64K空余内存(UMB)，这64K内存被分成4个页，每页16K，这部分页称为“页框架”，EMS内存也分成一个个16K的页，总数可达2000个。使用EMS的程序最多允许同时访问4个页，当程序要访问到某个页时，内存控制板就把相应EMS页的内容复制到页框架中让程序读写，读写完后把页框架中页的内容复制回相应的EMS内存页，再把别的EMS页内容复制到页框架中让程序读写。所以也被称为“调页式扩充内存“。

 

 

 

## 3.3 实模式和高位内存（HMA）

 

1982年，Intel发布了新的80286 CPU，址线扩展到24位，最多可以访问16M内存。 但是在80286无法兼容8086上的程序，所以Intel提出了实模式和保护模式两种方式来解决这个问题。在实模式下，80286依然只使用20位的地址线，最多访问1M内存，以前的8086程序可以正常运行，而在保护模式下，程序可以使用全部的16M内存。所以实模式，其实就是8086的运行模式。

 

在8086内存访问时，当段值+偏移都为最大时：FFFF0h+FFFFh=10FFEFh=1M+64K，得到的地址超出了1M的范围，这一块地址称为高位地址。但是因为只有20根地址线，所以会采用一种wrap-around的技术，将地址对1M求模，得到内存地址。但是80286开始拥有了24条地址线，于是在实模式下，当使用10FFEFh地址访问时，因为A20地址线的存在，所以会直接访问内存这一块的地址。

![img](https://img-blog.csdn.net/20131024163339437)

IBM为了解决这个问题，采用了用键盘控制器来控制A20，称为A20 Gate，当打开的时候，可以访问到高位内存，而禁用时则和8086行为一样。IBM-PC大部分禁用了A20 Gate，现在大多PC通过BIOS调用来控制A20 Gate。 关于A20可以参考：[对A20 GATE的思考](http://www.cnblogs.com/sunt/articles/1889787.html)

 

 

## 3.4 实模式和扩展内存（XMS）

 

80286有24根地址线，最大内存容量可以达到16M，但是现在的问题在于MS-DOS本身是运行于实模式下的，所以即便处理器支持更大的内存，也无法使用。所以DOS上的应用程序最多只能使用640K的内存，这个也就是我们经常听到DOS程序的640K限制的问题。但这并不是8086时期硬件导致640K限制。

![img](https://img-blog.csdn.net/20131024171318953)

为了解决这个问题就使用看扩展内存XMS(Extended Memory Specification)。当然只有在80286和更高的处理器才才能支持。几乎所有使用DOS的机器上超过1M的内存都是扩展内存。扩展内存同样不能被DOS直接使用，DOS5.0以后提供了Himem.sys这个扩展内存管理程序，可以通过它来管理扩展内存。 E*mm386.exe*可以把扩展内存*(XMS)*仿真成扩充内存*(EMS)*，以满足一些要求使用扩充内存的程序。

 

 

## 3.5 MS-DOS和保护模式

 

在DOS操作系统下，无论CPU支持多大的内存空间，程序都只能使用常规内存空间的640K内存，运行在实模式中。但是80286之后的CPU可以支持保护模式，于是就有一些程序可以通过DPMI(DOS Protocted Mode Interface)， DOS扩展器程序比如*DOS4GW.exe使得CPU进入到保护模式，从而直接访问扩展的内存，但是此时，已经不是在DOS环境下了。而且对于80286来说一旦切换到保护模式就无法回到实模式，只能reset CPU。*

 

*实际上大多介绍的保护模式是指80386的32位保护模式，而非80286的16位保护模式。而80386之后，保护模式基本没有大的变化，后面将会详细介绍32位保护模式下的内存结构和管理。*

*![img](https://img-blog.csdn.net/20131024175105265)*

 



# *4. MS-DOS 内存管理*

 



了解了MS-DOS的内存结构，最后我们看看MS-DOS是怎么管理内存的。这一部分主要是看DOS如何管理常规内存的640K。

 

 

## 4.1 MS-DOS系统模块



![img](https://img-blog.csdn.net/20131024183056359)

我们可以看到在640K的常规内存中，一些系统模块专用了一部分内存。MS-DOS主要由一个引导程序好3个模块程序完成启动

- BOOT: 这是一个位于磁盘0扇区上的引导程序，主要是检查DOS系统盘，并把IO.SYS加载的
- IO.SYS是系统的输入输出模块，ROM-BIOS 是固化早BIOS中的设备驱动； 系统启动时IO.SYS接收到命令转换为设备控制命令，由ROM-BIOS中的驱动程序完成设备操作。
- MSDOS.SYS 这个是DOS的内核，主要进行内存，磁盘，外设的管理 
- COMMOND.COM: 这个是MS-DOS和用户之间的接口程序，用于接收用户输入的命令并执行。

 



## 4.2 MS-DOS 进程

 

MS-DOS是一个单任务的操作系统，所以不存在任务调度（80286也不支持多任务，80386支持任务切换）。当代码编写好生成可执行文件之后，被加载到内存空间时，而在程序内存空间前有一个256字节的程序段前缀（PSP）。

![img](https://img-blog.csdn.net/20131024184708062)

而在DOS中，还有一个环境块EVB用来记录环境变量，可以把他看做是PSP的扩展。

 

 

## 4.3 内存管理

 

当程序被加载的时候，需要向物理内存申请空间并加载程序。那么如何知道那些内存可以被使用呢？ MS-DOS采用了内存控制块（MCB）来标识物理内存块。DOS的内存块以节为单位，一节等于16个字节，每个内存块的前面都有一个一节的MCB来描述这个内存块。

![img](https://img-blog.csdn.net/20131024185255046)

- 标志位： 一个字节，Z标识是最后一个分区，M标识不是最后一个分区
- 拥有者：当这个字段为0时，标识是一个没有使用内存块，否则存放的是拥有此内存块的进程的PSP段地址。
- 内存块大小： 以节为单位，不包括MCB块。



所以通过一个MCB块，可以使用MCB块地址+内存块大小+1 就能知道下一个MCB块的地址。这样整个内存就被串联起来。下图展示了MS-DOS 3.3启动后内存的情况。

![img](https://img-blog.csdn.net/20131024190107453)

 

内存一共被分成了3部分：

1. 第一部分中的块主要是系统使用；
2. 第二部分是COMMAND.CMD程序，一共使用了3个块，其中程序数据和PSP还有环境各站一个块，构成了COMMAND的进程实体。
3. 最后一部分内存块是给暂驻程序TPA（Transient Program Area），也就是我们程序使用的用户内存空间。

 

 



## 4.4 用户程序的装载

 

当我们要把一个EXE程序装载到内存时，装载程序会检查EXE的头部信息，检查TAP的容量并确装载的段的地址。而装载时可以分为低位载入和高位载入。

 

![img](https://img-blog.csdn.net/20131024192013125)

 

而在载入一个COM文件时，因为COM文件没有头部信息，并且COM文件限制不能大于64K。

![img](https://img-blog.csdn.net/20131024192319750)

 

我们知道，在程序载入后，需要设置段寄存器的值，程序才能正确的被运行，下面列出了执行文件被加载后段寄存器的情况

![img](https://img-blog.csdn.net/20131024192456406)

具体可以看：[读书笔记DOS下可执行文件的加载](http://www.pediy.com/kssd/pediy10/77282.html)

 

 

 

# 总结：

 



这一篇主要介绍了x86-16 处理器的内存结构和访问方式，后面还介绍了MS-DOS操作系统是如何管理内存的。 因为这一部分很久远，并且我也没怎么接触过，所以查阅了很多料，费了好多时间。但是可以找到的资料并不是很多。 但是我们的主要目的是了解早期的CPU和操作系统是如何管理内存的，程序是如何加载运行的。

 

后面我们会介绍x86-32 CPU的内存管理，有了这里了解到的知识就能更好的理解为什么现在的电脑是这样运行的，为什么使用保护模式，为什么使用虚拟内存。

 

 


 

# 参考：



[Intel 8086](http://zh.wikipedia.org/wiki/Intel_8086)

[8086处理器六种模式](http://bbs.21ic.com/blog-161600-1225.html)

《80x86汇编语言程序设计教程》

[MS-DOS](http://zh.wikipedia.org/wiki/MS-DOS)

[DOS 内存的知识](http://blog.csdn.net/zhangking/article/details/6501384)

[纯DOS下内存的管理—实模式下访问4GB内存](http://blog.csdn.net/favory/article/details/3618387)

[实模式、保护模式和虚拟模式](http://www.cnblogs.com/chengxuyuancc/archive/2013/05/12/3073738.html)

[DOS下XMS,EMS,DPMI,DOS4GW研究 pdf](http://www.doc88.com/p-386623997570.html)

[DOS下可执行文件的加载](http://wenku.baidu.com/view/8f9c7eabd1f34693daef3e71)﻿﻿﻿﻿









# （6）-- x86-32 CPU和内存管理之分页管理





# 前言







上一篇我们了解了x86-16 CPU计算机的内存访问方式，寻址方式，以及基于MS-DOS的应用程序的内存布局。这一篇会主要介绍32位处理器的内存访问，内存管理以及应用程序的内存布局。虽然目前64位CPU已经非常普及了，不过相对于32位的内存管理方式并没有大的变化，而32位相对于16位却有了极大的改变。





# 1.  IA-32 CPU



1985年10月。Intel推出了80386 CPU 用来取代之前x86-16位的[架构](http://lib.csdn.net/base/architecture)，一直到现在差不多块20年的时间里，虽然处理的速度，制造工艺都在不断提升，但x86-32的架构都没有大的改变。一般我们说的IA-32, I386和x86-32是一个意思。

![img](https://img-blog.csdn.net/20131031215712765)

从80386开始，地址线变为了32位，和CPU寄存器以及运算单元位数一样，最大寻址范围增加到4G。所以在也不会出现16位CPU时访问内存出现的问题。另外80386处理器都可以支持虚拟存储器，支持实模式，保护模式和虚拟8086模式，支持多任务。 而之后的CPU，主要的改进就在于：

1. CPU内部集成DMA,计数器，定时器等；
2. 制造工艺的提示，更多的晶体管，更快的速度
3. 加入更多的指令集，如MMX,SSE,SSE2等
4. 集成L1，L2，L3高速缓存，减少外部总线的访问
5. 超线程，多核心提高CPU效率

但是在内存管理访问，却没有太大的变化，所以我们后面介绍的内容基本上可以试用所有的x86-32 CPU而不用特意去区分那个型号的CPU。





## 1.1 16位CPU内存访问的问题



前一篇我们已经比较详细的了解了16位CPU的内存访问技术，现在可以会头想想他所存在的缺点，

1. 单任务： 16位的CPU只支持单任务，也就是同时只有一个程序在运行，随着计算机的发展，单任务的缺点在于体验较差；
2. 内存小： 前面我们知道，在运行程序时，会把程序全部加载到内存中，但是当程序大于内存时，程序就无法运行了；
3. 地址不确定：每次程序装载时分配的地址可能都不一样，使得程序在编写时处理转跳等问题非常麻烦。
4. 安全差： 因为对于内存访问没有太多的限制，所以应用程序很容易去修改操作系统以及BIOS和硬件映射的内存空间，导致系统崩溃；



而当80386引入多任务的支持后，以前的内存管理方式已经不能满足现状的需求的了。于是我们需要新的内存管理方式来解决上面的问题：

1. 地址空间：这个是对物理内存的一个抽象，就好像进程是对CPU的一个抽象。一个进程可用于寻址的一套地址的集合，每个进程都有自己的地址空间，相互独立，这就解决了安全问题。
2. 交换：把程序全部加载到内存，当进程处于空闲时，把他移除内存，存放到硬盘上，然后载入其他程序。这样使得每个进程可以使用更多的内存。
3. 虚拟内存：在老的内存管理中，一次把程序加载到内存，而当程序过大时就无法正常运行了。而利用到计算机系统的局部性和存储分层，我们可以只加载一部分需要使用的代码和数据到内存，当访问的内容不在内存时，和已经使用完的部分进行交换，这样就能在小内存的机器上运行大的程序了。对于程序来说这是透明的，看起来自己好像使用了全部内存。而多个应用完全可以使用相同的虚拟地址。





## 1.2 IA-32 CPU的内存访问



32位CPU中开始，因为地址线和计算单元同为32位，所以采用了一种全新的内存访问方式，虚拟寻址。也就是CPU发出的地址并不是真正的物理地址，而是需要转换才能得到真实的物理地址。初看起来和16位计算机的分段内存访问好像差不多。但本质却不同。16位的分段访问是为了解决地址线位数大于CPU位数的问题。而虚拟寻址则是真正解决了上面提到的那些问题。  当然，Intel为了兼容，仍旧支持16位的分段式内存访问。



![img](https://img-blog.csdn.net/20131031224907906)

CPU在内部增加了一个MMU（Memory Management Unit）单元来管理内存地址的转换。我们知道在16位时代，仅仅使用一个地址加法器来计算地址，而这里MMU单元除了可以转换地址，还能提供内存访问控制。 比如[操作系统](http://lib.csdn.net/base/operatingsystem)固定使用一段内存地址，当进程请求访问这一段地址时，MMU可以拒绝访问以保证系统的稳定性。而MMU的翻译过程则需要操作系统的支持。所以可见硬件和软件在计算机发展过程中是密不可分的。后面会详细介绍虚拟地址转换的过程。这也是本文的重点。





## 1.3 IA-32 CPU工作模式



从80286开始为了兼容8086引入了实模式和保护模式。但是80386因为引入了对虚拟内存的支持，使得保护模式相对80286有了很大改变。而80286也受限于当时MS-DOS只能工作于实模式，所以无法使用到保护模式。所以我们一般谈到保护模式都是指386之后的32位保护模式。而CPU工作模式也和操作系统有关。

- 实模式： 实际就是8086的工作模式，可寻址空间为1M，采用分段式内存访问。程序可以直接访问BIOS和硬件的内存映射，所以目前计算机在启动时都是在实模式下。
- 保护模式： 80386以后因为引入了虚拟存储器，所以能更好的保护操作系统和各个进程的内存， 它主要体现在4G可寻址空间，采用段页式虚拟内存访问，支持多任务。当计算机启动后，BIOS把控制权交给操作系统，从实模式切换到保护模式。
- 虚拟8086模式：  主要是在保护模式下虚拟执行8086的实模式，这并不是一个CPU的模式，本质还是工作在实模式下，但可以实现多任务。



![img](https://img-blog.csdn.net/20131101004053218)



我们平时会经常听到实模式和保护模式，我们现在可以了解到他们主要的区别就在于内存访问的方式上，而CPU工作模式也离不开操作系统的支持。在DOS和Windows 1.X系统中，只支持实模式；Windows3.0中，同时支持实模式和保护模式；而到了Windows3.1，从上面微软操作系统和Intel CPU的版本图来看，当时主流已经是80386和80486了，所以移除了对实模式的支持。







## 1.4 IA-32 CPU寄存器



![img](https://img-blog.csdn.net/20131031234515734)



### 1.4.1 通用寄存器



IA32的CPU主要包含了8个32位的通用寄存器，

- EAX,EBX,ECX,EDX相对于16位的CPU来说扩展成了32为，当然为了兼容16为CPU，低位的16位和8位寄存器可以被单独使用。 
- ESI,EDI两个个变址寄存器升级到了32位，其低位的16位对应之前的SI,DI寄存器、
- ESP,EBP2个指针寄存器同样变为32位，其低位的16位对应之前的SP,BP寄存器。

在8086内存寻址中有介绍，只有BX,BP,SI,DI可以用来存放基址和变址的地址，但是80386开始，以上8个寄存器都可以用来存放指针地址，所以更加的通用。





### 1.4.2 段寄存器



32位CPU为了保持对16位CPU的兼容性，保留了4个16位段寄存器，CS,SS,DS,ES，同时增加了2个段寄存器FS,GS

- CS,SS,DS,ES： 工作在实模式时与16位CPU的段寄存器作用相同；工作在保护模式则不在存放段值，而是作为选择子，在虚拟地址转换时使用。
- FS和GS是新增的附加数据段，通过把段地址存入这两个寄存器可以实现自定义寻址。





### 1.4.3 指令指针寄存器和标志寄存器



EIP扩展到了32位，和数据线相同。 低16位作用和IP寄存器相同。在32位计算机中存放的是指令的虚拟地址，16位计算机中存放的是CS段内有效地址。EFLAGS寄存器同样扩展到32位，具体含义我们这里不做介绍。

 

 

### 1.4.4 新增的寄存器

 

另外我们也看到，在IA-32中也新增了一些寄存器，GDTR/IDTR/LDTR/TR。他们主要在CPU保护模式下需要用到的寄存器，具体使用在后面会介绍到。

- GDTR是全局描述附表寄存器，主要存放操作系统和各任务公用的描述符；
- LDTR是局部描述符表寄存器，主要存放各个任务的私有描述符；
- IDTR指出了保护模式中断向量表的起始地址和大小（2K，最多256个中断）；
- TR是任务寄存器；


 



# 2. 虚拟存储器

 

虚拟存储器我们一般也称为虚拟内存（和Windows中的虚拟内存不是一个概念，但是有关联），它的基本思想是：

- 每个进程都有自己的地址空间；
- 每个地址空间被分为多个块，每个块称为页，每个页有连续的地址空间；
- 这些页被映射到物理内存，但不是所有也都在内存中程序才能运行；
- 当使用的页不在物理内存中时，由操作系统负责载入相应的页；

 

在实模式下，CPU将偏移地址和段寄存器，基址寄存器等进行计算得到的实际的物理地址。 而在保护模式下，引入了虚拟内存的概念，在虚拟内存中使用的地址称为**虚拟地址（线性地址）**，虚拟地址通过MMU将虚拟地址映射为物理地址，然后送到总线，进行内存访问。这里最关键的就是虚拟地址的映射。

 

 

## 2.1 分页

 

对于虚拟内存来说，是对物理内存的抽象，整个虚拟内存空间被划分成了多个大小固定的页（page），每个页连续的虚拟地址，组合成了一个完整的虚拟地址空间。同样，操作系统也会把物理内存划分为多个大小固定的块，我们称为页框（page frame），它只是操作系统为了方便管理在逻辑上的划分的一个[数据结构](http://lib.csdn.net/base/datastructure)，并不存放实际内存数据，但是我们可以简单的认为它就是内存。这样一个虚拟内存的page就可以和一个物理内存的page frame对应起来，产生映射关系。

 ![img](https://img-blog.csdn.net/20140423180336000)

关于一个虚拟页的大小，现在的操作系统中一般是512B-64K（准确的说是地址范围大小，而非容纳数据的大小）。但是内存页的大小会对系统性能产生影响，内存页设得太小，内存页会很多，管理内存页的数组会比较大，耗内存。内存页设大了，因为一个进程拥有的内存是内存页大小的整数倍，会导致碎片，即申请了很多内存，真正用到的只有一点。目前Windows和[Linux](http://lib.csdn.net/base/linux)上默认的内存页面大小都是4K。

 

从上图我们也可以看出，虚拟内存的页和物理内存的页框并不一定是一一对应的，虚拟内存的大小和系统的寻址能力相关，也就是地址线的位数，而物理内存的页框数取决于实际的内存大小。所以可能只有一部分页有对应的页框，而当访问的数据不在物理内存中时就会出现缺页，这个时候操作系统会负责调入新的页，也就是建立新的映射。这样就不需要一次把程序全部加载到内存。

 

 

 

### 2.1.1 虚拟页是什么？

 

很多人会有一个疑问，虚拟页到底是实际存在的还是虚拟的？我们知道内存中存放的是执行文件的代码和数据，而程序在运行前，它的数据和代码是存放在这个程序的可执行文件中的（比如.exe和.so），而在运行时需要把可执行文件加载到内存。所以我们把这个硬盘上的文件也划分为4K大小的页（逻辑上划分，实际是加载过程中加载器完成的），这就是虚拟页里面实际的东西。但是程序在运行是可能会申请内存，这个时候需要新的虚拟页来映射，所以我们可以得知虚拟页应该有3种状态：

1. 已映射：虚拟页面被创建已经被加载到物理内存，和物理页之间存在映射关系。
2. 未映射：虚拟页面被创建，但是没有被加载到内存或已经被调出内存，和物理页面之间没有映射关系，当需要使用时调入内存建立映射。
3. 未创建：虚拟页面没有被创建，可能是因为还没有访问到此页面所以没有加载或者是调用macllo来分配内存，只有在运行是才会被创建。

 

 

### 2.1.2 存储器映射

 

加载应用程序到内存时，因为和虚拟地址有关，我们需要把应用程序文件和虚拟内存关联起来，在Linux中称为存储器映射，同时提供了一个mmap的系统调用来实现次功能。文件被分成页面大小的片，每一片包含一个虚拟页面的内容，因为页面调度程序是按需求调度，所以在这些虚拟页面并没有实际的进入内存，而是在CPU发出访问请求时，会创建一个虚拟页并加载到内存。我们在启动一个进程时，加载器为我们完成了文件映射的功能，所以此时我们的执行文件又可以称为**映像文件**。实际上加载器并不真正负责从磁盘加载数据到内存，因为和虚拟内存建立了映射关系，所以这个操作是虚拟内存自动进行的。 正是有了存储器映射的存在，使得我们可以很方便的将程序和数据加载到内存中。

 

 

### 2.1.3 交换分区

 

当CPU请求一个虚拟页是，虚拟页会被创建并加载到内存，而页面调度[算法](http://lib.csdn.net/base/datastructure)可能在页面休眠或在内存满的情况下更具调度算法将虚拟页交换出去，在适当的时候可能被交换回来。这个时候就需要一个区域来存放被交换出来的虚拟页，这个区域称为交换分区。 这个分区在Linux中称为swap分区，而在Windows中我们称为虚拟内存（注意这里和我们谈到的虚拟内存技术不是一回事）。

 

以前电脑内存很小，特别是玩一些游戏时经常会提示内存不足，网上一般会告诉你增大你的虚拟内存（交换分区），这样一来在内存不足的时候可以存放更多交换出来的虚拟页，看起来好像内存变大了一样。从这方面来说Windows把他叫虚拟内存（交换分区）也是很正确的。  交换分区虽然也是硬盘的一部分，但是交换分区没有普通的文件系统，这样消除了将文件偏移转换为页地址的开销。但是过于频繁的交换页面，IO操作会导致系统性能下降。但是在内存不足时可以保证系统 正常运行。当然这也和交换分区的大小有关。

 

而如今，一般使用的电脑都已经4G,8G内存了，对于普通需求来说足够大了。所以虚拟页会长时间存在与内存中而不被交换出去。所以我们可以禁用掉交换分区，以便提高性能。对于Windwos 从Vista开始有一个Superfetch的内存管理机制，而linux有Preload与之类似。这种内存机制会将用户经常用的应用的部分虚拟页提前加载到内存，当用户使用时就无需在从硬盘加载。而当应用休眠或关闭时，也不会将这些虚拟页交换出去。



如下图就是Windows 8上内存使用情况，其中最左灰色部分是给BIOS和硬件保留的内存映射区域；绿色为操作系统，驱动以及用户进程使用的内存；橙色表示已经修改的内存也，当交换出来时需要先写回到硬盘；而蓝色部分5G内存则是用来缓存了未激活进程的数据和代码页；最后剩余的3M才是空闲内存。 当活动进行需要更多内存时会优先使用可用部分，当可用部分没有内存可用时，会释放一部分备用区域的内存。

![img](https://img-blog.csdn.net/20140426204312953)
 



## 2.2 页表

 
 

上面我们看到当实际物理内存小于虚拟内存时，会存在缺页以及页面交换等问题。此时操作系统会处理这些事情，是的这一切对于程序来说是透明的，它们不知道发生了什么，只知道自己可以使用全部的虚拟内存空间。而对于操作系统来说，它们需要负责一切，需要知道程序的那些页在实际内存中，那些不在。于是出现了页表，就是用来记录虚拟内存的页和物理页框之间的映射关系。MMU也正是利用页表来进行虚拟地址和物理地址的转换。

![img](https://img-blog.csdn.net/20140423221213953)

上面这张图是一张虚拟内存页和物理内存页框之间通过页表的映射关系，其中虚拟页面从VP0-VP7，物理页为PP0-PP3，我们从图中可以得到几点信息：

1. 不是所有的虚拟内存页都加载到了物理内存中（VP3，VP6未映射状态）；
2. 不是所有虚拟内存页都被创建（VP0，VP5未被创建）
3. 所有的虚拟内存页在页表中都有一项纪录，我们称为PTE(Page Table Entry)；
4. 虚拟内存的页是存放在磁盘上的；
5. 页表纪录需要占用内存空间；
6. MMU通过页表，将虚拟地址转换为物理地址；

 

这里可能会有疑问，为什么VP5没有被创建？虚拟页不是应该连续的吗？这就涉及到内存分段，程序编译和加载一些列问题了，这个会在介绍程序加载时解释。

 

![img](https://img-blog.csdn.net/20140424140828609)

 

最后我们看下页表中PTE的结构，一个PTE大小是32位，系统在操作页表时则会根据这些属性进行相应操作。

 

- P：存在标志（1表示当前页是加载到了物理内存中）
- W：读写标志（0时表示只读）
- U/S： 用户/超级用户（0时表示用超级用户权限）
- PWT：连续写入
- PCD：禁用缓存
- A：访问过
- D：脏位（1表示被写过）
- PAT：页面属性索引表
- G：全局标志（TLB中使用）
- Avail：方便操作系统使用



 



## 2.3 虚拟地址转换



![img](https://img-blog.csdn.net/20140423225553703)



通过上图我们来分析一下虚拟地址转换的过程：

1. CPU送出要访问的虚拟地址，地址的结构是**【页号+页内地址**】；
2. 页表存放在内存中，页表的地址和长度信息则存放在一个页表专用的寄存器中；通过读取寄存器的信息获得页表的起始地址；
3. 将虚拟地址的页号与页表其实地址相加可以得到页表的实际地址
4. 通过页表的映射项目，可以得到对应的物理内存页的号码
5. 通过物理页号和业内偏移地址就能得到实际要访问的物理地址

 

当然，如果访问过程中出现缺页，会产生一个中断，然后操作系统会载入需要的页面并进行映射（设置页表），最后返回物理页号得到物理内存。从上面的过程我们可以知道，每次进行地址变化，MMU都要访问内存。回忆8086地址变换时是不需要访问内存的，于是虚拟地址的转换会影响系统性能，但是相当于虚拟内存带来的好处，这点代价还是值得的。

 



  

## 2.4 页表分级





在IA-32平台上，地址线为32位，所以最大的寻址范围是4G，那么最多能够支持使用4GB的内存（内存按字节编码）。那么对于虚拟内存来说，它的地址范围为4G（0x00000000 ~ 0xFFFFFFFF），而一个内存页的大小是4K，那么一个程序虚拟内存空间中有1048576个页（实际上进程可访问的虚拟地址范围没有4G，Linux是3G，Windows是2G或3G）。

从上面我们知道每个虚拟页都会在页表中有一个PTE，每个PTE为32位，那么对于一个进程至少需要4MB的内存来维护自己的页表；而一个系统中可能存在多个进行，仅仅维护页表这一项就需要消耗比较多内存。但实际上很多PTE项并没有映射到对应的物理页，这就造成了浪费。

 

有人会说那我们就动态建立页表，在映射时才增加这一项。但是从虚拟地址转换我们可以看到，找到PTE是通过PT首地址+页号得到的，所以页表PTE必须是连续的，但我们又知道并不是所有的虚拟页都会马上被创建，在访问是就会出现问题，比如VP0-VP8中的VP5没有被创建，当访问页号是5时，就会错误的访问到VP6。所以为了解决页表占用内存过多的问题，引入了**分级页表**。注意分级页表也需要CPU硬件提供支持。

 


 

### 2.4.1 二级页表

###  

![img](https://img-blog.csdn.net/20140424124921046)

 

上图是Linux系统上二级页表的示意图。与一级页表不同的是，多增加了一层目录，虚拟地址的组成变为了**【目录地址+页表地址+页内偏移】**。其中页内偏移地址为12位，页表（PGT）地址为10位，页表目录（PGD）地址为10位。因为总过是32位，他们表示的PTE的个数是不变的。同样，PEG的每一个项目也有自己的结构。

 

![img](https://img-blog.csdn.net/20140424141702390)

 

二级页表地址转换的过程也很简单：

1. 首先从cr3寄存器中找到PGD的首地址；（cr3寄存器用来保存PGD的地址）
2. PGD首地址和目录号进行计算得到页表的首地址；
3. 页表的首地址和页号进行计算得到物理页的首地址；
4. 页内偏移和物理页地址通过计算得到最终的物理地址；

 

现在谈一谈为什么页表分级可以解决内存问题。首先因为页表需要连续的大的内存空间，通过引入目录级，我们可以离散对连续大空间的需求，这样，只有在同一个目录下的页表才需要连续的空间。另一方面，如果某个目录下的页表中没有任何映射的记录，那么这一张页表就不需要加载。因为其他页表可以通过其他目录项来获得，而不会存在一级表中不加载页表项导致访问出错的问题。但是同一个页表中，如果只有一个PTE被使用，这张页表也是需要被加载的。分级的方法同样用到了程序的局部性原理。

 

 

### 2.4.2 多级页表

###  

 ![img](https://img-blog.csdn.net/20140424132242921)

 

 

对于32位系统最多能使用4G内存，为了让系统可以使用更多内存，加入了 **物理地址扩展（Physical Address Extension，缩写为PAE）**功能，可以支持36位。在前面8086时候我们见过类似的技术来使用更多内存。这个时候2级页表就无法满足要求了，于是引入了三级页表。其中增加了PMD中间目录一级。

 

 

![img](https://img-blog.csdn.net/20140424131436359)

为了适应64位CPU，操作系统又引入了4级页表。但是总体上来说他们工作原理都是相同的，这里就不叙述工作工程了。但是要注意的是，分级页表需要处理器的支持，对于只支持二级或三级页表的CPU来说，内核中体系结构相关的代码必须通过空页表对缺少的页表进行仿真。因此，内存管理代码剩余部分的实现是与CPU无关的。目前Windows 2000/XP使用的是二级页表，而使用PAE时则使用的是三级页表，对于64位操作系统则采用了四级页表。Linux则使用了四级页表。

 

 

### 2.4.3 倒排页表

###  

在64位操作系统中，因为有64位地址线，所以页表的大小可能非常非常大，虽然分级页表可以不必加载全部页表，IA-32,IA64系统一般使用四级页表来处理，而在PowerPC等体系中则使用倒排页表来解决这个问题。与传统页表的区别: 使用页框号而不是虚拟页号来索引页表项。因为不是X86体系中常用的方法，这里就不相信介绍了。具体可以查看《现代操作系统》P113


 



## 2.5 TLB缓存

##  

前面介绍虚拟地址转换时说过，相对于8086上的地址转换而言，这里多了一次内存访问查找页表的过程。我们知道内存速度比CPU慢很多，每次从内存取数据都要访问2次内存，会使得系统性能下降。

![img](https://img-blog.csdn.net/20140423230907296)



为了解决这个问题，在MMU中包含了一个关于PTE的缓冲区**TLB(Translation Lookaside Buffer )，**TLB是一个寄存器，所以它运行的速度和CPU速度相同。TLB中每一行保存了一个PTE，如上图所示，每次在去页表中查找之前，可以先在TLB中进行查找，如果命中则直接拿到物理页地址，如果不命中则再次访问内存。我们多次提到程序的局部性，在这里下一个要访问的地址很可能和前一个是在同一个内存页中，于是我们可以直接从寄存器中拿到物理内存页号，而不需要在访问内存，这样大大提高的了系统的速度。

 

![img](https://img-blog.csdn.net/20140424143253468)

上图是TLB的一个基本结构，对于多级页表来说，TLB可以缓存每一级的地址，所以同样能起作用。因为局部性原理，多级页表的访问速度并不比一级页表差。关于TLB更详细的内容，可以查看《深入理解计算机系统》P607 - P619

 

 

 

## 2.6 进程调度和虚拟内存

 

我们知道在系统中，每个进程都有自己独立的虚拟空间，于是每个进程都有一张属于自己的内页表。 而我们翻译地址时，从cr3中取出页表目录的首地址。对于不同的进程，他们都使用同一个寄存器。于是在CPU调度进程的时候，虚拟的地址空间也需要切换。于是对于普通用户程序需要做下面几件事情：

1. 保存当前进程的页表目录地址，也就是保存cr3中存放的地址到进程上下文中
2. 清空TLB中缓存的数据
3. 调度新的进程到CPU，并设置cr3寄存器的值为页表目录的首地址

 

但是内存中除了用户程序之外还存在操作系统自身占用的内存。我们可以简单的把操作系统看成一个超大的进程，他和其他普通进程一样需要使用虚拟内存，需要使用到页表。当然作为内核程序，它必须是有一些特权的，下一篇我们将会介绍虚拟内存的布局。而对于内核而言不是存在进程调度的。因为所有的内核进程都是共享内核的虚拟地址空间，而我们一般都称之为内核线程，而非进程。 当然对于Linux而言，没有线程的概念，线程和进程唯一不同就是是否共享虚拟地址空间。一般来说内核代码是常驻在内存的，那么内核会不会缺页呢？

 

 


 

# 3 小结 

 



这一篇文章主要介绍了IA-32上的虚拟内存管理，主要的核心就是虚拟内存分页。这也是现代操作系统和计算机的核心部分。整个虚拟内存部分涉及的内容也非常广，加上自己理解不深，很多东西就只能简单介绍了。而且内存管理这一块不同操作系统可能会有一些不同的，这里我尽量避开这些差异，总体来说都是比较通用的。在下一篇将主要介绍虚拟内存中的分段管理。



 


 

# 参考：



[实模式与保护模式](http://www.cppblog.com/mydriverc/articles/30719.html)

[Linux内核4级页表的演进](http://larmbr.me/2014/01/19/the-evolution-of-4-level-page-talbe-in-linux/) ﻿﻿

[我理解的逻辑地址、线性地址、物理地址和虚拟地址](http://bbs.chinaunix.net/thread-2083672-1-1.html)﻿﻿﻿﻿









# （7）-- x86-32 CPU和内存管理之分段管理







# 前言



 

前一篇我们介绍了内存管理中的分页试内存管理，分页的主要作用就是使得每个进程有一个独立的，完整的内存空间，通过虚拟内存技术，使得程序可以在较小的内存上运行，而进程之间内存空间相互独立，提高了安全性。这一篇将主要介绍内存管理中分段管理，以及两种的结合，也是目前计算机普遍采用的段页式内存管理。这也直接决定了的后面程序的编译，加载以及允许时的内存布局。

 

 

 

# 1. 内存分段

 

 

## 1.1 为什么分段？

 

 

在x86-16体系中，为了解决16位寄存器对20位地址线的寻址问题，引入了分段式内存管理。而CPU则使用CS，DS，ES，SS等寄存器来保存程序的段首地址。当CPU执行指令需要访问内存时，只会送出段内的偏移地址，而通过指令的类型类确定访问那一个段寄存器。具体可以参考：[计算机原理学习（5）-- x86-16 CPU和内存管理](http://blog.csdn.net/cc_net/article/details/12878935)

 

到了IA-32，Intel引入了保护模式，所以在IA-32中为了保持兼容性，所以同样支持内存分段管理。另外我们讨论过了内存分页，页面中包含了程序的代码，数据等信息，它们都有各自的地址。这些地址是在编译的时候就确定的，因为每个进程都有独立完整的内存空间，只需要把页和物理页映射就能运行，所以这个地址是可以在编译时就决定的。在编译时，编译器会等程序进行语法词法等分析，在编译过程中会建立许多的表，来确定代码和变量的虚拟地址：

- 被保存起来供打印清单的源程序正文；
- 符号表，包含变量的名字和属性；
- 包含所有用到的整形和浮点型数据的表；
- 语法分析树，包括程序语法分析的结果；
- 编译器内部过程调用的堆栈。

前面4张表会随着编译的进行不断增大，而堆栈的数据也会变化，现在的问题就是，每一张表的大小都不确定，那么如何指定每一张表在虚拟内存空间的地址呢？

![img](https://img-blog.csdn.net/20140429161241843)

如上图，没一张表都有自己的起始地址，但是当变量很多的时候，符号表需要的空间可能会超过程序正文的起始地址，这个时候就会把源程序的表的地址覆盖掉。当然编译器没有这么傻，它可以提示无法继续编译，当然这样并不合适，另一个办法就是拿出一部分没有使用的空间给符号表。造成这个问题的原因就是分页系统中的虚拟地址是一维的，所以在编译过程中必须给变量，代码分配虚拟地址。这个有点类似没有采用分页之前，进程之间使用物理地址导致相互覆盖的问题。

![img](https://img-blog.csdn.net/20140429162427171)

所以我们可以为不同的表分配自己的空间地址，也就是分段，这样他们地址都是相对地址，全部编译完成后确定了每张表的大小，就可以计算出实际的虚拟地址了。

 

 

 

## 1.2 分段的作用

 

 

分页实际是一个纯粹逻辑上的概念，因为实际的程序和内存并没有被真正的分为了不同的页面。而分段则不同，他是一个逻辑实体。一个段中可以是变量，源代码或者堆栈。一般来说每个段中不会包含不同类型的内容。而分段主要有以下几个作用：

1. **解决编译问题**： 前面提到过在编译时地址覆盖的问题，可以通过分段来解决，从而简化编译程序。
2. **重新编译**： 因为不同类型的数据在不同的段中，但其中一个段进行修改后，就不需要所有的段都重新进行编译。
3. **内存共享**： 对内存分段，可以很容易把其中的代码段或数据段共享给其他程序，分页中因为数据代码混合在一个页面中，所以不便于共享。
4. **安全性**： 将内存分为不同的段之后，因为不同段的内容类型不同，所以他们能进行的操作也不同，比如代码段的内容被加载后就不应该允许写的操作，因为这样会改变程序的行为。而在分页系统中，因为一个页不是一个逻辑实体，代码和数据可能混合在一起，无法进行安全上的控制。
5. **动态链接**： 动态链接是指在作业运行之前，并不把几个目标程序段链接起来。要运行时，先将主程序所对应的目标程序装入内存并启动运行，当运行过程中又需要调用某段时，才将该段(目标程序)调入内存并进行链接。可见，动态链接也要求以段作为管理的单位。
6. **保持兼容性**

所以在现在的x86的体系结构中分段内存管理是必选的，而分页管理则是可选的。

 

 

 

## 1.3 与x86-16分段管理的区别

 

 

Intel对分段内存管理逻辑地址的定义是**【段号+段内地址】**。在x86-16体系的分段管理中，CPU给出的内存地址是16位的段内偏移地址，段的基址从段寄存器中获得，最后计算出24位的物理地址。 而在IA-32体系中引入了保护模式，每个进程有4G的独立地址空间，CPU直接给出的是32位的段内偏移地址，段的基址从内存中获得，最后计算出32为的物理地址。他们最大的不同就在于获取基址的方式以及计算方法。

 

![img](https://img-blog.csdn.net/20140430170014796)

 

IA-32为了保持向前兼容，保留了CS/DS/ES/SS这4个寄存器，但因为不在从段寄存器中获得段价值，这4个段寄存器实际上已经失去了原本的作用（但不代表没有使用）。IA-32在内存中使用一张段表来记录各个段映射的物理内存地址（如下图）。

![img](https://img-blog.csdn.net/20140429171643234)

在译地的过程中，x86-16是通过16位的段基址和16位的段内偏移不是简单的相加，而是通过 **段值\*0x10 + 偏移地址** 对基址重定向的方式计算得到物理地址，而IA-32中则相对简单，不需要对基址重定向，这一点和前面分页内存管理是相似的。而CPU只需要为这个段表提供一个记录其首地址的寄存器就可以了。 同样也可以使用TLB来加速。

![img](https://img-blog.csdn.net/20140429170433968)

与x86-16中分段管理另一个不同是，在IA-32中，因为有了独立的地址空间，对多程序也支持的非常好。而分段可以很好的支持进程间数据的共享。

![img](https://img-blog.csdn.net/20140429171949046)

 

 

 

 

# 2. 分段内存管理

 

 

## 2.1 段选择器

##  

在IA-32中保留的CS/DS/ES/SS这4个16位段寄存器不再被解释为段的基地址，Intel为了保持兼容性将这些寄存器的16个位分成3个用于不同功能的域，称为**段选择器**。

![img](https://img-blog.csdn.net/20140430172430656)

其中3-15是选择子，存放的是段描述符的索引（可以理解为段号），该描述符为64bit用于描述存储器段的位置、长度和访问权限。而段描述符可以分为两种，**全局描述符(GDT)**和**局部描述符(LDT)**，对应着第2位，而0,1两位是表示CPU的权限级别（0-4级）。在IA-32中一共有6个段选择器

 

- CS保存了代码段描述符的索引；
- DS保存了数据段描述符的索引；
- SS保存堆栈段描述符索引；
- ES、FS、GS则作为一般用途，可以指向任意的数据段，实现自定义寻址。

 

 

## 2.2 段描述符

 

段描述符就是前面说到的段表中的每一个项目的，一个段描述符由8个字节组成。它描述了段的特征。前面提到段描述符可以分为GDT和LDT两类。通常来说系统只定义一个GDT，而每个进程如果需要放置一些自定义的段，就可以放在自己的LDT中。IA-32中引入了GDTR和LDTR两个寄存器，就是用来存放当前正在使用的GDT和LDT的首地址。

 

![img](https://img-blog.csdn.net/20140519165222265)![img](https://img-blog.csdn.net/20140519165227687)

上面的图是[Linux](http://lib.csdn.net/base/linux)中不同段的描述符，结构基本是一致的，只有少数字段有差别。其中最重要的就是BASE字段，一共32位，保存的是当前段的首地址。 在Linux系统中，每个CPU对应一个GDT。一个GDT中有18个段描述符和14个未使用或保留项。其中用户和内核各有一个代码段和数据段，然后还包含一个TSS任务段来保存寄存器的状态。其他的段则包括局部线程存储，电源管理，即插即用等多个段。而Linux系统中，大多数用户态的程序都不使用LDT。

 

 

 

## 2.3 段地址转换

 

 

在IA-32中，逻辑地址是16位的段选择符+32位偏移地址，段寄存器不在保存段基址，而是保存段描述符的索引。

 

 ![img](https://img-blog.csdn.net/20140519170754734)

1. IA-32首选确定要访问的段（方式x86-16相同），然后决定使用的段寄存器。
2. 根据段选择符号的TI字段决定是访问GDT还是LDT，他们的首地址则通过GTDR和LDTR来获得。
3. 将段选择符的Index字段的值*8，然后加上GDT或LDT的首地址，就能得到当前段描述符的地址。（乘以8是因为段描述符为8字节）
4. 得到段描述符的地址后，可以通过段描述符中BASE获得段的首地址。
5. 将逻辑地址中32位的偏移地址和段首地址相加就可以得到实际要访问的物理地址。

 

 

## 2.4 缓存段描述符

 

![img](https://img-blog.csdn.net/20140520155224734)

为了加速地址转换的过程，根据程序的局部性原理，我们可以讲当前段寄存器指向的段描述符缓存在特定的寄存器中。这里为6个段寄存器准备了6个用来缓存段描述符的非编程寄存器。这样就能加快地址转换的过程。而仅当段寄存器内容变化时，才有必要去访问内存中的GDT或LDT。

 

 

 

# 3. 段页式内存管理

#  

 

分段内存管理的优势在于内存共享和安全控制，而分页内存管理的优势在于提高内利用率。他们之间并不是相互对立的竞争关系，而是可以相互补充的。也就是可以把2种方式结合起来，也就是目前计算机中最普遍采用的**段页式内存管理**。段页式管理的核心就是对内存进行分段，对每个段进行分页。这样在拥有了分段的优势的同时，可以更加合理的使用内存的物理页。

![img](https://img-blog.csdn.net/20140429180533078)

 

 

## 2.1  段页式内存管理结构

 

![img](https://img-blog.csdn.net/20140429180347921)

对于段页式管理来说，我们需要通过段表来保存每一个段的信息，通过页表保存每个段中虚拟页的信息。在段页管理的系统中，CPU给出的不再是分页系统中的虚拟地址，而是给出的逻辑地址。（前面2篇有介绍逻辑地址和虚拟地址，简单的说逻辑地址是二维的，而虚拟地址是一维的，平坦的）。

 

 

 

## 2.2 段页式地址转换

 

 ![img](https://img-blog.csdn.net/20140519173235765)

 

上面的图简单的描述了在段页式内存管理的系统中，地址转换的过程。实际上就是我们前面介绍的分段和分页地址转换的结合。

1. CPU给出要访问的逻辑地址；
2. 通过分段内存管理的地址转换机制，将逻辑地址转换为线性地址，也就是分页系统中的虚拟地址；
3. 通过分页内存管理的地址转换机制，将虚拟地址转换为物理地址；

 

 

 

#  4. 总结

 

 

这一篇文章主要介绍了IA-32系统中分段式内存管理是如何工作的。而目前主流的系统中都采用了段页相结合的内存管理方式，当然不同的系统具体实现起来是不同的。比如在Linux中，所有段首地址都是从0x0000000开始，所以逻辑地址和转换得到的线性地址完全是一样的。到这一篇，有关x86的内存管理方式以及介绍晚了。内存的分页和分段也决定了程序的编译，可执行文件的结构，程序的内存布局等等。这个将在后面介绍到。

 

 

 

# 参考

 

《深入理解Linux内核第三版》

《现代[操作系统](http://lib.csdn.net/base/operatingsystem)》



