
# 了解GAN背后的设计，训练，损失函数和算法 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月30日 08:08:54[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：427


[https://www.toutiao.com/a6651201834939580941/](https://www.toutiao.com/a6651201834939580941/)
2019-01-27 23:40:04
假设我们有一个卧室图像数据集和一个在这个数据集上训练的图像分类器CNN，它告诉我们给定的输入图像是否是卧室。假设图像大小为16 * 16。每个像素可以有256个可能的值。所以存在无限大量的可能输入（即25616*16或~10616可能的组合）。这使得我们的分类器模型成为一个高维概率分布函数，它给出了来自这个大输入空间的给定输入作为卧室的概率。
那么，如果我们可以从卧室图像的数据分布中学习这种高维知识来进行分类，我们肯定能够利用相同的知识甚至生成全新的卧室图像。
虽然有多种生成建模方法，但我们将在本文中探讨生成对抗网络。GAN的原始论文（arxiv.org/pdf/1406.2661.pdf）发表于2014年，在这篇论文（arxiv.org/pdf/1511.06434.pdf）引入了深度卷积生成对抗网络(deep tional generate adversarial networks, DCGAN)，并作为一种流行的参考。这篇文章是基于这两篇论文的研究，对GAN做了很好的介绍。
GAN是同时训练生成模型G和判别模型D的网络。生成模型将通过捕获与训练数据集相关的数据分布来生成新的卧室图像。训练判别模型将给定的输入图像正确分类为真实(即来自训练数据集图像)或虚假(即生成模型生成的合成图像)。简单的说，判别模型是典型的CNN图像分类器模型，或者更具体地说是二元图像分类器。
生成模型与判别模型略有不同。它的目标不是分类而是生成。当判别模型给出一个代表不同的类的激活向量时给出一个输入图像，生成模型就会反向执行。
![了解GAN背后的设计，训练，损失函数和算法](http://p3.pstatp.com/large/pgc-image/d57ba80c5e934d4ea0fbdd244576d159)
生成与判别模型
它可以被认为是反向CNN，在某种意义上它将随机数的向量作为输入并生成图像作为输出，而正常的CNN则相反地将图像作为输入并生成数字向量或激活（对应于不同的类）作为输出。
但是这些不同的模型如何协同工作？下图给出了网络的图示。首先，我们将随机噪声向量作为生成模型的输入，生成模型生成图像输出。我们将这些生成的图像称为伪图像或合成图像。然后判别模型将训练数据集中的假图像和真图像都作为输入，并生成一个输出来分类图像是假图像还是真图像。
![了解GAN背后的设计，训练，损失函数和算法](http://p3.pstatp.com/large/pgc-image/88bfe37037d748a3959fe88b7fff0bc3)
生成敌对网络的说明
使用这两个模型对该网络的参数进行训练和优化。判别模型的目标是最大限度地正确分类图像的真伪。相反，生成模型的目标是最小化判别器正确地将假图像分类为假的。
反向传播和普通卷积神经网络（CNN）一样，是用来训练网络参数的，但是由于涉及到两个目标不同的模型，使得反向传播的应用有所不同。更具体地说，涉及的损失函数和在每个模型上执行的迭代次数是GAN不同的两个关键领域。
判别模型的损失函数将是一个与二元分类器相关的正则交叉熵损失函数。根据输入图像，损失函数中的一项或另一项将为0，结果将是模型预测图像被正确分类的概率的负对数。换句话说，在我们的上下文中，对于真实图像，“y”将为“1”，对于假图像，“1-y”将为“1”。“p”是图像是真实图像的预测概率，“1-p”是图像是假图像的预测概率。
![了解GAN背后的设计，训练，损失函数和算法](http://p3.pstatp.com/large/pgc-image/c630fac130194f08a766f153c4f0628f)
二元分类器的交叉熵损失
上面的概率p可以表示为D(x)，即判别器D估计的图像“x”是真实图像的概率。重写，如下图所示：
![了解GAN背后的设计，训练，损失函数和算法](http://p1.pstatp.com/large/pgc-image/a113ff368a7349c4be410f1556f5842a)
根据我们如何分配上下文，方程的第一部分将被激活，第二部分对于真实图像将为零。反之亦然。第二部分中图像“x”的表示可以用“G(z)”代替。也就是说，在给定输入z的情况下，将假图像表示为模型G的输出。“z”只不过是建模“G”产生“G（z）”的随机噪声输入向量。这些符号在初看时令人困惑，但论文中的算法通过“ascending”其随机梯度来更新判别器，这与上文所述的最小化损失函数相同。下面是论文中函数的快照：
![了解GAN背后的设计，训练，损失函数和算法](http://p1.pstatp.com/large/pgc-image/fb3a180904cd422abe154e7ceb6abd30)
回到生成函数G，G的损失函数将反过来，即最大化D的损失函数。但是等式的第一部分对生成器没有任何意义，所以我们真正说的是第二部分应该最大化。所以G的损失函数与D的损失函数相同，只是符号颠倒了，第一项被忽略了。
![了解GAN背后的设计，训练，损失函数和算法](http://p9.pstatp.com/large/pgc-image/bce637618af645caa2b5adb68c31c6a4)
生成器的损失函数
以下是论文中生成器损失函数的快照：
![了解GAN背后的设计，训练，损失函数和算法](http://p1.pstatp.com/large/pgc-image/d708dafcbd554979b1a4f739d1056fc0)
正如DCGAN内容所示，这是通过重塑和转置卷积的组合来实现的。以下是生成器的表示：
![了解GAN背后的设计，训练，损失函数和算法](http://p1.pstatp.com/large/pgc-image/f7819f66ff674cee92639a563ffbc0cf)
DCGAN生成器
转置卷积与卷积的逆不同，它不能恢复给定卷积输出的输入，只是改变了卷积的形状。下面的例子说明了上述生成器模型背后的数学原理，特别是卷积层。
![了解GAN背后的设计，训练，损失函数和算法](http://p1.pstatp.com/large/pgc-image/7b505dbd1ca443a8968853d4990d7f19)

> 在CNN中使用的常规卷积图示，以及通过转置卷积实现的上采样的两个示例。第一示例的结果用作第二和第三示例中具有相同内核的输入，以证明转换与反卷积不相同，并不是为了恢复原始输入。
论文算法的内部for循环。这意味着，对于k> 1，我们在G的每次迭代中对判别器D执行多次训练迭代。这是为了确保D'被充分训练并且比G更早地学习。我们需要一个好的D来欺骗G。
![了解GAN背后的设计，训练，损失函数和算法](http://p1.pstatp.com/large/pgc-image/a993d76b70164cc6a0df85b18e6d0d45)
另一个相关的重点是生成器可能记忆输入示例的问题，DCGAN通过使用3072-128-3072、降噪、dropout、正则化、RELU和自动编码器来解决，基本上是减少和重构机制，以最小化记忆。
DCGAN中还重点介绍了生成器在操作时如何忘记它正在生成的卧室图像中的某些对象。他们通过从第二层卷积层特征集中删除对应窗口的特征映射来实现这一点，并展示了网络是如何用其他对象替换窗口空间的。

