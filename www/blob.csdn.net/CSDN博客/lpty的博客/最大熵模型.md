
# 最大熵模型 - lpty的博客 - CSDN博客

2017年12月26日 19:33:11[lpty](https://me.csdn.net/sinat_33741547)阅读数：634标签：[最大熵																](https://so.csdn.net/so/search/s.do?q=最大熵&t=blog)个人分类：[机器学习																](https://blog.csdn.net/sinat_33741547/article/category/6482345)



## 一、概念
### 1、信息熵
信息论定义了信息熵，用以表示人们对客观事件不确定性的度量。信息熵考虑随机变量的所有可能取值，即所有可能发生事件带来的信息量的期望：
$H(X)=\sum_{i=1}^np_ilog(\frac{1}{p_i})=E(log(\frac{1}{p_i}))$
信息熵表示了一个不确定事件的所有可能状态所提供的信息量，信息熵越大，表明携带的信息量越大，不确定性也越大；反之携带的平均信息量越少，不确定性越少。
### 2、最大熵原理
最大熵原理认为，学习概率模型时，在所有可能的概率分布中，熵最大的模型是最好的模型。
上面的话可以表述为，
（1）对一个随机事件的概率分布进行预测时，应满足全部已知条件–即所有可能的模型
（2）而对未知的情况不要做任何主观假设。
在这种情况下，概率分布最均匀，预测的风险最小，熵也最大。
假设随机掷骰子，我们总会猜测出现任意一个点数的概率为1/6，与此类似，意为最小化经验风险。
## 二、最大熵模型
将最大熵原理应用到分类就得到最大熵模型，模型表示对于给定的输入X，以条件概率输出$P(Y|X)$。
### 1、经验分布
对于给定的训练集，可以确定联合分布$P(X,Y)$的经验分布和边缘分布$P(X)$的经验分布：
$\hat P(X,Y)=\frac{Count(X=x,Y=y)}{N}$
$\hat P(X)=\frac{Count(X=x)}{N}$
其中，N为样本数量，$Count(\cdot)为满足条件的样本数量。
### 2、约束条件
如果从样本中选取或者设计n个特征函数：
![这里写图片描述](https://img-blog.csdn.net/20171226184324938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzM3NDE1NDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)[ ](https://img-blog.csdn.net/20171226184324938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzM3NDE1NDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
这些特征在建模过程中，模型必须满足所有这些特征的约束，而所有满足这些约束的模型产生一个集合C，最大熵原理给出选择最优模型的一个准则。
[

](https://img-blog.csdn.net/20171226184324938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzM3NDE1NDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
### 3、期望
[
](https://img-blog.csdn.net/20171226184324938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzM3NDE1NDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)对于每一个特征函数，都有：
（1）关于经验分布$\hat P(X,Y)$的期望：
$E_{\hat P}(f)=\sum_{x,y}\hat P(X,Y)f(x,y)$
（2）关于理论模型$P(X,Y)$的期望：
$E_P(f)=\sum_{x,y}P(X,Y)f(x,y)\  
=\sum_{x,y}P(X)P(Y|X)f(x,y)$
我们认为经验分布是实际分布的无偏采样，依据经验分布训练出来的模型，应该满足$E_{\hat P}(f) = E_P(f)$：
$\sum_{x,y}\hat P(X,Y)f(x,y)=\sum_{x,y}\hat P(X) P(Y|X)f(x,y)$
定义在条件概率分布上的条件熵为：
$H(P)=H(Y|X)$
$=\sum_xP(x)H(Y|X=x)$
$=-\sum_xP(x)\sum_yP(y|x)log(P(y|x))$
$=-\sum_{x,y}P(x)P(y|x)log(P(y|x))$
所以，在满足所有约束条件的的模型集合C里面，满足条件熵最大的模型即是最大熵模型。
具体的模型推导优化过程这里不展开说明，具体可参考李航老师的《统计学习方法》，最终模型形式如下：
![这里写图片描述](https://img-blog.csdn.net/20171229163345856?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzM3NDE1NDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
### 4、特征
从上面模型的形式可以看到，最大熵模型其实和逻辑回归模型的形式非常相似，都是指数类模型。可以认为，逻辑回归只是类别数量为2的最大熵模型。
这里需要注意的是，最大熵模型的特征与普通机器学习算法的特征表示方式有所不同，普通机器学习算法抽取特征只是单纯的对输入X进行抽取，而最大熵模型则是对输入X及标签Y的联合特征抽取。
具体特征抽取形式可以参考：[http://blog.csdn.net/erli11/article/details/24718655](http://blog.csdn.net/erli11/article/details/24718655)
## 三、参考
1、《统计学习方法》  李航
2、[http://blog.csdn.net/u010487568/article/details/45512689](http://blog.csdn.net/u010487568/article/details/45512689)

