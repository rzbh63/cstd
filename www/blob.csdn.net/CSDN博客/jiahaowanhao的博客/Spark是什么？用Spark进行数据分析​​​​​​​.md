
# Spark是什么？用Spark进行数据分析​​​​​​​ - jiahaowanhao的博客 - CSDN博客


2018年07月18日 22:07:23[数据分析技术](https://me.csdn.net/jiahaowanhao)阅读数：70


[Spark是什么？用Spark进行数据分析](http://cda.pinggu.org/view/26127.html)
1什么是Apache Spark？
Apache Spark是一个为速度和通用目标设计的集群计算平台。
从速度的角度看，Spark从流行的MapReduce模型继承而来，可以更有效地支持多种类型的计算，如交互式查询和流处理。速度在大数据集的处理中非常重要，它可以决定用户可以交互式地处理数据，还是等几分钟甚至几小时。Spark为速度提供的一个重要特性是其可以在内存中运行计算，即使对基于磁盘的复杂应用，Spark依然比MapReduce更有效。
从通用性来说，Spark可以处理之前需要多个独立的分布式系统来处理的任务，这些任务包括批处理应用、交互式算法、交互式查询和数据流。通过用同一个引擎支持这些任务，Spark使得合并不同的处理类型变得简单，而合并操作在生产数据分析中频繁使用。而且，Spark降低了维护不同工具的管理负担。
Spark被设计的高度易访问，用Python、Java、Scala和SQL提供简单的API，而且提供丰富的内建库。Spark也与其他大数据工具进行了集成。特别地，Spark可以运行在Hadoop的集群上，可以访问任何Hadoop的数据源，包括Cassandra。2Spark 核心组件
![](http://www.cda.cn/uploadfile/image/20180718/20180718064907_80798.png)
Spark核心组件包含Spark的基本功能，有任务调度组件、内存管理组件、容错恢复组件、与存储系统交互的组件等。Spark核心组件提供了定义弹性分布式数据集（resilient distributed datasets，RDDs）的API，这组API是Spark主要的编程抽象。RDDs表示分布在多个不同机器节点上，可以被并行处理的数据集合。Spark核心组件提供许多API来创建和操作这些集合。
Spark SQLSpark SQL是Spark用来处理结构化数据的包。它使得可以像Hive查询语言（Hive Query Language, HQL）一样通过SQL语句来查询数据，支持多种数据源，包括Hive表、Parquet和JSON。除了为Spark提供一个SQL接口外，Spark SQL允许开发人员将SQL查询和由RDDs通过Python、Java和Scala支持的数据编程操作混合进一个单一的应用中，进而将SQL与复杂的分析结合。与计算密集型环境紧密集成使得Spark SQL不同于任何其他开源的数据仓库工具。Spark SQL在Spark 1.0版本中引入Spark。
Shark是一个较老的由加利福尼亚大学和伯克利大学开发的Spark上的SQL项目，通过修改Hive而运行在Spark上。现在已经被Spark SQL取代，以提供与Spark引擎和API更好的集成。
Spark流（Spark Streaming）Spark流作为Spark的一个组件，可以处理实时流数据。流数据的例子有生产环境的Web服务器生成的日志文件，用户向一个Web服务请求包含状态更新的消息。Spark流提供一个和Spark核心RDD API非常匹配的操作数据流的API，使得编程人员可以更容易地了解项目，并且可以在操作内存数据、磁盘数据、实时数据的应用之间快速切换。Spark流被设计为和Spark核心组件提供相同级别的容错性，吞吐量和可伸缩性。
MLlibSpark包含一个叫做MLlib的关于机器学习的库。MLlib提供多种类型的机器学习算法，包括分类、回归、聚类和协同过滤，并支持模型评估和数据导入功能。MLlib也提供一个低层的机器学习原语，包括一个通用的梯度下降优化算法。所有这些方法都可以应用到一个集群上。
GraphXGraphX是一个操作图（如社交网络的好友图）和执行基于图的并行计算的库。与Spark流和Spark SQL类似，GraphX扩展了Spark RDD API，允许我们用和每个节点和边绑定的任意属性来创建一个有向图。GraphX也提供了各种各样的操作图的操作符，以及关于通用图算法的一个库。
集群管理器Cluster Managers在底层，Spark可以有效地从一个计算节点扩展到成百上千个节点。为了在最大化灵活性的同时达到这个目标，Spark可以运行在多个集群管理器上，包括Hadoop YARN，Apache Mesos和一个包含在Spark中的叫做独立调度器的简易的集群管理器。如果你在一个空的机器群上安装Spark，独立调度器提供一个简单的方式；如果你已经有一个Hadoop YARN或Mesos集群，Spark支持你的应用允许在这些集群管理器上。第七章给出了不同的选择，以及如何选择正确的集群管理器。3谁使用Spark？用Spark做什么？由于Spark是一个面向集群计算的通用框架，可用于许多不同的应用。使用者主要有两种：数据科学家和数据工程师。我们仔细地分析一下这两种人和他们使用Spark的方式。明显地，典型的使用案例是不同的，但我们可以将他们粗略地分为两类，数据科学和数据应用。
数据科学的任务数据科学，近几年出现的一门学科，专注于分析数据。尽管没有一个标准的定义，我们认为一个数据科学家的主要工作是分析和建模数据。数据科学家可能会SQL，统计学，预测模型（机器学习），用Python、MATLAB或R编程。数据科学家能将数据格式化，用于进一步的分析。
数据科学家为了回答一个问题或进行深入研究，会使用相关的技术分析数据。通常，他们的工作包含特殊的分析，所以他们使用交互式shell，以使得他们能在最短的时间内看到查询结果和代码片段。Spark的速度和简单的API接口很好地符合这个目标，它的内建库意味着很多算法可以随时使用。
Spark通过若干组件支持不同的数据科学任务。Spark shell使得用Python或Scala进行交互式数据分析变得简单。Spark SQL也有一个独立的SQL shell，可以用SQL进行数据分析，也可以在Spark程序中或Spark shell中使用Spark SQL。MLlib库支持机器学习和数据分析。而且，支持调用外部的MATLAB或R语言编写的程序。Spark使得数据科学家可以用R或Pandas等工具处理包含大量数据的问题。
有时，经过初始的数据处理阶段后，数据科学家的工作将被产品化，扩展，加固（容错性），进而成为一个生产数据处理应用，作为商业应用的一个组件。例如，一个数据科学家的研究成果可能会产生一个产品推荐系统，集成到一个web应用上，用来向用户生成产品建议。通常由另外的人员（如工程师）对数据科学家的工作进行产品化。
数据处理应用Spark的另外一个主要的使用可以从工程师的角度进行描述。在这里，工程师指使用Spark来构建生产数据处理应用的大量的软件开发者。这些开发者了解软件工程的概念和原则，如封装、接口设计和面向对象编程。他们通常有计算机学科的学位。他们通过自己的软件工程技能来设计和构建实现某个商业使用场景的软件系统。
对工程师而言，Spark提供了一个简单的方式在集群之间并行化这些应用，隐藏了分布式系统、网络通信和容错处理的复杂性。系统使得工程师在实现任务的同时，有充足的权限监控、检查和调整应用。API的模块特性使得重用已有工作和本地测试变得简单。
Spark用户使用Spark作为其数据处理应用，因为他提供了丰富的功能，易于学习和使用，而且成熟可靠。

