
# 使用Flume+Kafka+SparkStreaming进行实时日志分析 - leofionn的博客 - CSDN博客


2018年05月04日 17:23:39[leofionn](https://me.csdn.net/qq_36142114)阅读数：126



每个公司想要进行数据分析或数据挖掘，收集日志、ETL都是第一步的，今天就讲一下如何实时地（准实时，每分钟分析一次）收集日志，处理日志，把处理后的记录存入Hive中，并附上完整实战代码
# 1. 整体架构
思考一下，正常情况下我们会如何收集并分析日志呢？
首先，业务日志会通过Nginx（或者其他方式，我们是使用Nginx写入日志）每分钟写入到磁盘中，现在我们想要使用Spark分析日志，就需要先将磁盘中的文件上传到HDFS上，然后Spark处理，最后存入Hive表中，如图所示：
![这里写图片描述](https://img-blog.csdn.net/20170518173937190?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVHJpZ2w=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
我们之前就是使用这种方式每天分析一次日志，但是这样有几个缺点：
首先我们的日志是通过Nginx每分钟存成一个文件，这样一天的文件数很多，不利于后续的分析任务，所以先要把一天的所有日志文件合并起来
合并起来以后需要把该文件从磁盘传到Hdfs上，但是我们的日志服务器并不在Hadoop集群内，所以没办法直接传到Hdfs上，需要首先把文件从日志服务器传输到Hadoop集群所在的服务器，然后再上传到Hdfs
最后也是最重要的，滞后一天分析数据已经不能满足我们新的业务需求了，最好能控制在一个小时的滞后时间
可以看出来我们以前收集分析日志的方式还是比较原始的，而且比较耗时，很多时间浪费在了网络传输上面，如果日志量大的话还有丢失数据的可能性，所以在此基础上改进了一下架构：
![这里写图片描述](https://img-blog.csdn.net/20170518175830889?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVHJpZ2w=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
整个过程就是，Flume会实时监控写入日志的磁盘，只要有新的日志写入，Flume就会将日志以消息的形式传递给Kafka，然后Spark Streaming实时消费消息传入Hive
那么Flume是什么呢，它为什么可以监控一个磁盘文件呢？简而言之，Flume是用来收集、汇聚并且移动大量日志文件的开源框架，所以很适合这种实时收集日志并且传递日志的场景
Kafka是一个消息系统，Flume收集的日志可以移动到Kafka消息队列中，然后就可以被多处消费了，而且可以保证不丢失数据
通过这套架构，收集到的日志可以及时被Flume发现传到Kafka，通过Kafka我们可以把日志用到各个地方，同一份日志可以存入Hdfs中，也可以离线进行分析，还可以实时计算，而且可以保证安全性，基本可以达到实时的要求
整个流程已经清晰了，下面各个突破，我们开始动手实现整套系统
# 2. 实战演练
## 2.1 安装Kafka
下载安装Kafka以及一些基本命令请传送到这里：[ Kafka安装与简介](http://blog.csdn.net/trigl/article/details/72581735)
安装好以后新建名为launcher_click的topic：
```python
bin/kafka
```
```python
-
```
```python
topics
```
```python
.
```
```python
sh
```
```python
-
```
```python
-
```
```python
create
```
```python
-
```
```python
-
```
```python
zookeeper
```
```python
hxf:2181
```
```python
,
```
```python
cfg:2181
```
```python
,
```
```python
jqs:2181
```
```python
,
```
```python
jxf:2181
```
```python
,
```
```python
sxtb:2181
```
```python
-
```
```python
-
```
```python
replication
```
```python
-
```
```python
factor
```
```python
2
```
```python
-
```
```python
-
```
```python
partitions
```
```python
2
```
```python
-
```
```python
-
```
```python
topic
```
```python
launcher_click
```
1
查看一下该topic：
```python
bin/kafka
```
```python
-
```
```python
topics
```
```python
.
```
```python
sh
```
```python
-
```
```python
-
```
```python
describe
```
```python
-
```
```python
-
```
```python
zookeeper
```
```python
hxf:2181
```
```python
,
```
```python
cfg:2181
```
```python
,
```
```python
jqs:2181
```
```python
,
```
```python
jxf:2181
```
```python
,
```
```python
sxtb:2181
```
```python
-
```
```python
-
```
```python
topic
```
```python
launcher_click
```
1
![这里写图片描述](https://img-blog.csdn.net/20170524141002525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVHJpZ2w=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
## 2.2 安装Flume
1、下载解压
下载地址：[https://flume.apache.org/download.html](https://flume.apache.org/download.html)
注意进入下载地址页面，使用清华大学的那个地址，否则会很慢
```python
wget http://apache
```
```python
.fayea
```
```python
.com
```
```python
/flume/
```
```python
1.7
```
```python
.0
```
```python
/apache-flume-
```
```python
1.7
```
```python
.0
```
```python
-bin
```
```python
.tar
```
```python
.gz
```
```python
tar -xvf apache-flume-
```
```python
1.7
```
```python
.0
```
```python
-bin
```
```python
.tar
```
```python
.gz
```
2、修改配置文件
进入flume目录，修改conf/flume-env.sh
```python
export
```
```python
JAVA_HOME=/data/
```
```python
install
```
```python
/jdk
```
```python
export
```
```python
JAVA_OPTS=
```
```python
"-Xms1000m -Xmx2000m -Dcom.sun.management.jmxremote"
```
添加配置文件：conf/flume_launcherclick.conf
```python
# logser可以看做是flume服务的名称，每个flume都由sources、channels和sinks三部分组成
```
```python
# sources可以看做是数据源头、channels是中间转存的渠道、sinks是数据后面的去向
```
```python
logser
```
```python
.sources
```
```python
= src_launcherclick
logser
```
```python
.sinks
```
```python
= kfk_launcherclick
logser
```
```python
.channels
```
```python
= ch_launcherclick
```
```python
# source
```
```python
# 源头类型是TAILDIR，就可以实时监控以追加形式写入文件的日志
```
```python
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.type
```
```python
= TAILDIR
```
```python
# positionFile记录所有监控的文件信息
```
```python
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.positionFile
```
```python
= /data/install/flume/position/launcherclick/taildir_position
```
```python
.json
```
```python
# 监控的文件组
```
```python
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.filegroups
```
```python
= f1
```
```python
# 文件组包含的具体文件，也就是我们监控的文件
```
```python
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.filegroups
```
```python
.f
```
```python
1 = /data/launcher/stat_app/.*
```
```python
# interceptor
```
```python
# 写kafka的topic即可
```
```python
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.interceptors
```
```python
= i1 i2
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.interceptors
```
```python
.i
```
```python
1
```
```python
.type
```
```python
=static
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.interceptors
```
```python
.i
```
```python
1
```
```python
.key
```
```python
= type
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.interceptors
```
```python
.i
```
```python
1
```
```python
.value
```
```python
= launcher_click
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.interceptors
```
```python
.i
```
```python
2
```
```python
.type
```
```python
=static
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.interceptors
```
```python
.i
```
```python
2
```
```python
.key
```
```python
= topic
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.interceptors
```
```python
.i
```
```python
2
```
```python
.value
```
```python
= launcher_click
```
```python
# channel
```
```python
logser
```
```python
.channels
```
```python
.ch
```
```python
_launcherclick
```
```python
.type
```
```python
= memory
logser
```
```python
.channels
```
```python
.ch
```
```python
_launcherclick
```
```python
.capacity
```
```python
=
```
```python
10000
```
```python
logser
```
```python
.channels
```
```python
.ch
```
```python
_launcherclick
```
```python
.transactionCapacity
```
```python
=
```
```python
1000
```
```python
# kfk sink
```
```python
# 指定sink类型是Kafka，说明日志最后要发送到Kafka
```
```python
logser
```
```python
.sinks
```
```python
.kfk
```
```python
_launcherclick
```
```python
.type
```
```python
= org
```
```python
.apache
```
```python
.flume
```
```python
.sink
```
```python
.kafka
```
```python
.KafkaSink
```
```python
# Kafka broker
```
```python
logser
```
```python
.sinks
```
```python
.kfk
```
```python
_launcherclick
```
```python
.brokerList
```
```python
=
```
```python
10.0
```
```python
.0
```
```python
.80
```
```python
:
```
```python
9092
```
```python
,
```
```python
10.0
```
```python
.0
```
```python
.140
```
```python
:
```
```python
9092
```
```python
# Bind the source and sink to the channel
```
```python
logser
```
```python
.sources
```
```python
.src
```
```python
_launcherclick
```
```python
.channels
```
```python
= ch_launcherclick
logser
```
```python
.sinks
```
```python
.kfk
```
```python
_launcherclick
```
```python
.channel
```
```python
= ch_launcherclick
```

3、启动
```python
nohup bin/flume-ng agent --conf conf/ --conf-file conf/flume_launcherclick
```
```python
.conf
```
```python
--name logser -Dflume
```
```python
.root
```
```python
.logger
```
```python
=INFO,console >> logs/flume_launcherclick
```
```python
.log
```
```python
&
```

此时Kafka和Flume都已经启动了，从配置可以看到Flume的监控文件是/data/launcher/stat_app/.*，所以只要该目录下文件内容有增加就会发送到Kafka，大家可以自己追加一些测试日志到这个目录的文件下，然后开一个Kafka Consumer看一下Kafka是否接收到消息，这里我们完成SparkStreaming以后再看测试结果
## 2.3 SparkStreaming编程
SparkStreaming是Spark用来处理实时流的，能够实时到秒级，我们这里不需要这么实时，是每分钟执行一次日志分析程序，主要代码如下：
```python
def main(args:
```
```python
Array
```
```python
[
```
```python
String
```
```python
]) {
    Logger.getLogger(
```
```python
"org.apache.spark"
```
```python
).setLevel(Level.WARN)
    System.setProperty(
```
```python
"spark.serializer"
```
```python
,
```
```python
"org.apache.spark.serializer.KryoSerializer"
```
```python
)
    val sparkConf =
```
```python
new
```
```python
SparkConf().setAppName(
```
```python
"LauncherStreaming"
```
```python
)
```
```python
//每60秒一个批次
```
```python
val ssc =
```
```python
new
```
```python
StreamingContext(sparkConf, Seconds(
```
```python
60
```
```python
))
```
```python
// 从Kafka中读取数据
```
```python
val kafkaStream = KafkaUtils.createStream(
      ssc,
```
```python
"hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181"
```
```python
,
```
```python
// Kafka集群使用的zookeeper
```
```python
"launcher-streaming"
```
```python
,
```
```python
// 该消费者使用的group.id
```
```python
Map[
```
```python
String
```
```python
, Int](
```
```python
"launcher_click"
```
```python
->
```
```python
0
```
```python
,
```
```python
"launcher_click"
```
```python
->
```
```python
1
```
```python
),
```
```python
// 日志在Kafka中的topic及其分区
```
```python
StorageLevel.MEMORY_AND_DISK_SER).map(_._2)
```
```python
// 获取日志内容
```
```python
kafkaStream.foreachRDD((rdd: RDD[
```
```python
String
```
```python
], time: Time) => {
      val result = rdd.map(log => parseLog(log))
```
```python
// 分析处理原始日志
```
```python
.filter(t => StringUtils.isNotBlank(t._1) && StringUtils.isNotBlank(t._2))
```
```python
// 存入hdfs
```
```python
result.saveAsHadoopFile(HDFS_DIR, classOf[
```
```python
String
```
```python
], classOf[
```
```python
String
```
```python
], classOf[LauncherMultipleTextOutputFormat[
```
```python
String
```
```python
,
```
```python
String
```
```python
]])
    })
    ssc.start()
```
```python
// 等待实时流
```
```python
ssc.awaitTermination()
  }
```

篇目有限，完整代码访问我的github：[https://github.com/Trigl/SparkLearning/blob/master/src/main/scala/com/trigl/spark/streaming/LauncherStreaming.scala](https://github.com/Trigl/SparkLearning/blob/master/src/main/scala/com/trigl/spark/streaming/LauncherStreaming.scala)
然后打包上传到master运行：
```python
nohup /data/install/spark-
```
```python
2.0
```
```python
.
```
```python
0
```
```python
-bin-hadoop2.
```
```python
7
```
```python
/bin/spark-submit  --master
```
```python
spark:
```
```python
/
```
```python
/hxf:7077  --executor-memory 1G --total-executor-cores 4   --class com.analysis.main.LauncherStreaming --jars /home
```
```python
/hadoop/jar
```
```python
/kafka-clients-0.10.0.0.jar,/home
```
```python
/hadoop/jar
```
```python
/metrics-core-2.2.0.jar,/home
```
```python
/hadoop/jar
```
```python
/zkclient-0.3.jar,/home
```
```python
/hadoop/jar
```
```python
/spark-streaming-kafka-0-8_2.11-2.0.0.jar,/home
```
```python
/hadoop/jar
```
```python
/kafka_2.11-0.8.2.1.jar  /home
```
```python
/hadoop/jar
```
```python
/SparkLearning.jar  >> /home
```
```python
/hadoop/logs
```
```python
/LauncherDM.log &
```

然后开始测试，往Flume监控目录/data/launcher/stat_app/.*写日志，原始日志内容类似下面这样：
```python
118.120
```
```python
.
```
```python
102.3
```
```python
|
```
```python
1495608541.238
```
```python
|UEsDBBQACAgIACB2uEoAAAAAAAAAAAAAAAABAAAAMGWUbW7bMAyGb6NfnUFRFEWhJ+gBdgBZVjpjjp04brMAO
```
```python
*yY2DKa9Y
```
```python
+B1+DnQ1LCztoITgK4wPGHfNUhmKGUPOn3DyP
```
```python
*zdOxSWM3T33XXMqy9OP7xXTZiTC1xlL0HgMEi
```
```python
+BfHoooBEGKr3fPpYy5jMse4Xzupus4TKkrs4kZOhI51CgWWKxsUQBRPMDr1
```
```python
*w5Hcuc0LiUEFBwdXQxAARXHb3
```
```python
+QXlOfzya0uZWOGwlEwBDwLD5oJBVFHsEEPF2U0EUToyr8k4tg9v8AkRrIcKmxGsU2eqQIM45dKuKFICo5oveEqOjh2JAIITImyIJqBk3JS4qh7Wby
```
```python
*TroxnL9ZKHXrsyWeBQoMXaEgXUKh6mOQ1l7NLc
```
```python
*Hwz8aDpAtndLFJEetkVc6S9V
```
```python
*bg
```
```python
+RFiKMvnTv6ahuGUTmWexqEfi3Elezx0botJrCCQn5jfCzWaqaUOqNpFYO23ckYl5GOlx4rLQuUllh27SsjZyLQTUn4K+
```
```python
3
```
```python
uVczlOi+
```
```python
7
```
```python
uuMzTYLoibeIspk71DtKuJC+
```
```python
7
```
```python
T5qXPg9lLddaZs6+Lolnj7ANW0dBGKOn72m3cbQJI2Kq4
```
```python
*C6Xhz9E5Pzeeg
```
```python
*i2l1IAJtpReILNq6DY4peFjHeO5vffPZd2UyejEJ28Puo0sI
```
```python
*2
```
```python
*5ojvhfNcquWomFMVp02Pz
```
```python
++M6Nach3e6XR5wOlrdSg4T7RkgtQAuC6HYl2sc62i6dUq
```
```python
*om
```
```python
+HWjvdHAPSk8hYkegHraxC8PwPons73XZeozDfXmaRzzzaD2XI4fX0QX
```
```python
*8BUEsHCKeftc48AgAAmQQAAA
```
```python
==
```

查看HDFS的对应目录是否有内容：
![这里写图片描述](https://img-blog.csdn.net/20170524145206940?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVHJpZ2w=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
HDFS存储的分析后的日志内容如下：
```python
99000945863664
```
```python
;864698037273329|119.176.140.248|1495594615129|2017-05-24 10:56:55|xiaomi|redmi4x|com.jingdong.app.mall&0ae359b6&1495534579412&1;com.autonavi.minimap&279f562f&1495534597934,1495534616627&2;com.android.contacts&91586932&1495538267103,1495540527138,1495576834653,1495583404117,1495591231535&5
```

SparkStreaming任务状态如下：
![这里写图片描述](https://img-blog.csdn.net/20170524145340469?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVHJpZ2w=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
可以看到的确是每分钟执行一次
---
Refer
[http://blog.xiaoxiaomo.com/2016/05/22/Flume-%E9%9B%86%E7%BE%A4%E5%8F%8A%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/](http://blog.xiaoxiaomo.com/2016/05/22/Flume-%E9%9B%86%E7%BE%A4%E5%8F%8A%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/)
[http://lxw1234.com/archives/2015/11/552.htm](http://lxw1234.com/archives/2015/11/552.htm)



