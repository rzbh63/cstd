
# 吴恩达深度学习笔记(67)-迁移学习（Transfer learning) - 喜欢打酱油的老鸟 - CSDN博客


2019年01月26日 21:22:06[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：78


[https://www.toutiao.com/a6644868806923518471/](https://www.toutiao.com/a6644868806923518471/)
2019-01-11 07:36:41
# 迁移学习（Transfer learning）
深度学习中，最强大的理念之一就是迁移学习，有的时候神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中。
所以例如，**也许你已经训练好一个神经网络，能够识别像猫这样的对象，然后使用那些知识，或者部分习得的知识去帮助您更好地阅读x射线扫描图，这就是所谓的迁移学习**。
我们来看看，假设你已经训练好一个图像识别神经网络，所以你首先用一个神经网络，并在(x,y)对上训练，其中x是图像，y是某些对象，图像是猫、狗、鸟或其他东西。
如果你把这个神经网络拿来，然后让它适应或者说迁移，在不同任务中学到的知识，比如放射科诊断，就是说阅读X射线扫描图。你可以做的是把神经网络最后的输出层拿走，就把它删掉，还有进入到最后一层的权重删掉，然后为最后一层重新赋予随机权重，然后让它在放射诊断数据上训练。
![吴恩达深度学习笔记(67)-迁移学习（Transfer learning)](http://p1.pstatp.com/large/pgc-image/f00ccb946c7a451aa25adba4b28799e5)
具体来说，**在第一阶段训练过程中，当你进行图像识别任务训练时，你可以训练神经网络的所有常用参数，所有的权重，所有的层，然后你就得到了一个能够做图像识别预测的网络**。
在训练了这个神经网络后，要实现迁移学习，你现在要做的是，**把数据集换成新的(x,y)对，现在这些变成放射科图像，而y是你想要预测的诊断，你要做的是初始化最后一层的权重，让我们称之为w^([L])和b^([L])随机初始化。**
![吴恩达深度学习笔记(67)-迁移学习（Transfer learning)](http://p1.pstatp.com/large/pgc-image/0be035e2be5b44dd82ba436321503b26)
现在，我们在**这个新数据集上重新训练网络，在新的放射科数据集上训练网络**。
要用放射科数据集重新训练神经网络有几种做法。
你可能，如果你的放射科数据集很小，你可能只需要重新训练最后一层的权重，就是w^([L])和b^([L])，并保持其他参数不变。
如果你有足够多的数据，你可以重新训练神经网络中剩下的所有层。
**经验规则是，如果你有一个小数据集，就只训练输出层前的最后一层，或者也许是最后一两层。但是如果你有很多数据，那么也许你可以重新训练网络中的所有参数。**
**如果你重新训练神经网络中的所有参数，那么这个在图像识别数据的初期训练阶段，有时称为预训练（pre-training），因为你在用图像识别数据去预先初始化，或者预训练神经网络的权重。然后，如果你以后更新所有权重，然后在放射科数据上训练，有时这个过程叫微调（fine tuning）。**如果你在深度学习文献中看到预训练和微调，你就知道它们说的是这个意思，预训练和微调的权重来源于迁移学习。
在这个例子中你做的是，把图像识别中学到的知识应用或迁移到放射科诊断上来，为什么这样做有效果呢？有很多低层次特征，比如说边缘检测、曲线检测、阳性对象检测（positive objects），从非常大的图像识别数据库中习得这些能力可能有助于你的学习算法在放射科诊断中做得更好，算法学到了很多结构信息，图像形状的信息，其中一些知识可能会很有用，所以学会了图像识别，它就可能学到足够多的信息，可以了解不同图像的组成部分是怎样的，学到线条、点、曲线这些知识，也许对象的一小部分，这些知识有可能帮助你的放射科诊断网络学习更快一些，或者需要更少的学习数据。
![吴恩达深度学习笔记(67)-迁移学习（Transfer learning)](http://p1.pstatp.com/large/pgc-image/196a56d8e5d34bdebcd56351e77f5170)
这里是另一个例子，假设你已经训练出一个语音识别系统，现在x是音频或音频片段输入，而y是听写文本，所以你已经训练了语音识别系统，让它输出听写文本。现在我们说你想搭建一个“唤醒词”或“触发词”检测系统，所谓唤醒词或触发词就是我们说的一句话，可以唤醒家里的语音控制设备，比如你说“Alexa”可以唤醒一个亚马逊Echo设备,或用“OK Google”来唤醒Google设备，用"Hey Siri"来唤醒苹果设备，用"你好百度"唤醒一个百度设备。要做到这点，你可能需要去掉神经网络的最后一层，然后加入新的输出节点，但有时你可以不只加入一个新节点，或者甚至往你的神经网络加入几个新层，然后把唤醒词检测问题的标签y喂进去训练。再次，这取决于你有多少数据，你可能只需要重新训练网络的新层，也许你需要重新训练神经网络中更多的层。
那么迁移学习什么时候是有意义的呢？
迁移学习起作用的场合是，在迁移来源问题中你有很多数据，但迁移目标问题你没有那么多数据。例如，假设图像识别任务中你有1百万个样本，所以这里数据相当多。可以学习低层次特征，可以在神经网络的前面几层学到如何识别很多有用的特征。但是对于放射科任务，也许你只有一百个样本，所以你的放射学诊断问题数据很少，也许只有100次X射线扫描，所以你从图像识别训练中学到的很多知识可以迁移，并且真正帮你加强放射科识别任务的性能，即使你的放射科数据很少。
![吴恩达深度学习笔记(67)-迁移学习（Transfer learning)](http://p1.pstatp.com/large/pgc-image/f220ed4adb554c90a1c70443b08fa146)
对于语音识别，也许你已经用10,000小时数据训练过你的语言识别系统，所以你从这10,000小时数据学到了很多人类声音的特征，这数据量其实很多了。但对于触发字检测，也许你只有1小时数据，所以这数据太小，不能用来拟合很多参数。所以在这种情况下，预先学到很多人类声音的特征人类语言的组成部分等等知识，可以帮你建立一个很好的唤醒字检测器，即使你的数据集相对较小。对于唤醒词任务来说，至少数据集要小得多。
![吴恩达深度学习笔记(67)-迁移学习（Transfer learning)](http://p3.pstatp.com/large/pgc-image/804d4948c15648cda71e09a5d1a7d4ab)
所以在这两种情况下，你从数据量很多的问题迁移到数据量相对小的问题。然后反过来的话，迁移学习可能就没有意义了。比如，你用100张图训练图像识别系统，然后有100甚至1000张图用于训练放射科诊断系统，人们可能会想，为了提升放射科诊断的性能，假设你真的希望这个放射科诊断系统做得好，那么用放射科图像训练可能比使用猫和狗的图像更有价值，所以这里（100甚至1000张图用于训练放射科诊断系统）的每个样本价值比这里（100张图训练图像识别系统）要大得多，至少就建立性能良好的放射科系统而言是这样。
所以，如果你的放射科数据更多，那么你这100张猫猫狗狗或者随机物体的图片肯定不会有太大帮助，因为来自猫狗识别任务中，每一张图的价值肯定不如一张X射线扫描图有价值，对于建立良好的放射科诊断系统而言是这样。
所以，**这是其中一个例子，说明迁移学习可能不会有害，但也别指望这么做可以带来有意义的增益。同样，如果你用10小时数据训练出一个语音识别系统。然后你实际上有10个小时甚至更多，比如说50个小时唤醒字检测的数据，你知道迁移学习有可能会有帮助，也可能不会，也许把这10小时数据迁移学习不会有太大坏处，但是你也别指望会得到有意义的增益。**
![吴恩达深度学习笔记(67)-迁移学习（Transfer learning)](http://p3.pstatp.com/large/pgc-image/09bcdf1c4c4a48429757a6c633ead4e2)
所以总结一下，什么时候迁移学习是有意义的？
**如果你想从任务A学习并迁移一些知识到任务B，那么当任务A和任务B都有同样的输入x时，迁移学习是有意义的**。在第一个例子中，A和B的输入都是图像，在第二个例子中，两者输入都是音频。
**当任务A的数据比任务B多得多时，迁移学习意义更大。**所有这些假设的前提都是，你希望提高任务B的性能，因为任务B每个数据更有价值，对任务B来说通常任务A的数据量必须大得多，才有帮助，因为任务A里单个样本的价值没有比任务B单个样本价值大。
**然后如果你觉得任务A的低层次特征，可以帮助任务B的学习，那迁移学习更有意义一些。**
而在这两个前面的例子中，也许学习图像识别教给系统足够多图像相关的知识，让它可以进行放射科诊断，也许学习语音识别教给系统足够多人类语言信息，能帮助你开发触发字或唤醒字检测器。
所以总结一下，**迁移学习最有用的场合是，如果你尝试优化任务B的性能，通常这个任务数据相对较少，**例如，在放射科中你知道很难收集很多X射线扫描图来搭建一个性能良好的放射科诊断系统
所以在这种情况下，你可能会找一个相关但不同的任务，如图像识别，其中你可能用1百万张图片训练过了，并从中学到很多低层次特征，所以那也许能帮助网络在任务B在放射科任务上做得更好，尽管任务B没有这么多数据。它确实可以显著提高你的学习任务的性能，但我有时候也见过有些场合使用迁移学习时，任务A实际上数据量比任务B要少，这种情况下增益可能不多。
好，这就是迁移学习，你从一个任务中学习，然后尝试迁移到另一个不同任务中。从多个任务中学习还有另外一个版本，就是所谓的多任务学习，当你尝试从多个任务中并行学习，而不是串行学习，在训练了一个任务之后试图迁移到另一个任务，所以在下一个笔记中，让我们来讨论多任务学习。

