
# NLP中的词向量及其应用 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月15日 08:07:59[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：310


[https://www.toutiao.com/a6643219722961682947/](https://www.toutiao.com/a6643219722961682947/)
2019-01-06 11:25:24
![NLP中的词向量及其应用](http://p1.pstatp.com/large/pgc-image/1546745074761c974ec399f)
词向量基本上是一种单词表示形式，它将人类对语言的理解与机器的理解连接起来。词向量是文本在n维空间中的分布式表示。这些是解决大多数NLP问题所必需的。
领域适应是一种技术，它允许机器学习和转移学习模型来映射小生境数据集，这些数据集都是用同一种语言编写的，但在语言上仍然不同。例如，法律文件、客户调查响应和新闻文章都是惟一的数据集，需要进行不同的分析。常见垃圾邮件过滤问题的任务之一是采用从一个用户(源分发版)到一个接收到明显不同的电子邮件的新用户(目标分发版)的模型。
词向量在深度学习领域的重要性，从该领域的研究数量可以明显看出。谷歌在向量领域进行的一项这样的研究导致了一组通常称为Word2Vec的相关算法的开发。
Word2Vec被维基百科描述为:
Word2vec以一个大型文本语料库作为输入，生成一个向量空间，通常有几百个维度，语料库中每个惟一的单词在该空间中分配一个对应的向量。单词向量被放置在向量空间中，这样在语料库中共享公共上下文的单词就会在空间中彼此靠近。
在这篇文章中，我们来看看单词嵌入(Word2Vec)和域适应的一些实际应用。我们还查看了Word2Vec的技术方面，以获得更好的理解。
分析调查结果
Word2Vec可用于从数千个客户的评论中获得可操作的指标。企业没有足够的时间和工具来分析调查结果并据此采取行动。这会导致投资回报率和品牌价值的损失。
在这种情况下，Word向量被证明是无价的。对调查数据集进行训练(或适应于调查数据集)的词的向量表示可以帮助在所审查的答复和作出答复的具体环境之间嵌入复杂的关系。机器学习算法可以利用这些信息为您的业务/产品确定可操作的见解。
逐字分析评论
利用词向量的机器学习在逐字注释分析领域取得了很大的进展。这种分析对于以客户为中心的企业非常重要。
在分析文本数据时，一个重要的用例是逐字分析注释。在这种情况下，数据科学家的任务是创建一种可以挖掘客户评论或评论的算法。
像Word2Vec这样的单词向量对于这种机器学习任务是必不可少的。经过客户评论和评论训练的单词的向量表示可以帮助映射出所分析的不同逐字评论和评论之间的复杂关系。像Word2Vec这样的词向量也有助于弄清楚特定评论的具体上下文。这种算法在理解买方或客户对特定业务或社交论坛的情绪方面非常有价值。
音乐/视频推荐系统
互联网上的流媒体服务彻底改变了我们体验内容的方式。在过去，推荐的重点是为您提供将来使用的内容。而现代的流媒体平台则专注于推荐那些能够而且将会被当下所享受的内容。流媒体模型以个性化广播和推荐播放列表的形式带来了新的发现方法。这里的重点是生成能够凝聚在一起的歌曲序列。为了增强用户体验，推荐系统的模型不仅应该捕捉相似的人通常对哪些歌曲感兴趣，而且还应该捕捉在非常相似的上下文中经常一起听哪些歌曲。
这些模型使用了Word2Vec。该算法将用户的收听队列解释为一个句子，并将每首歌视为句子中的一个单词。当Word2Vec模型在这样一个数据集上训练时，我们的意思是用户过去听过的每首歌和现在听过的每首歌在某种程度上都属于相同的上下文。Word2Vec精确地用一个坐标向量表示每首歌曲，这个坐标向量映射歌曲或视频播放的上下文。
词向量的技术方面
在NLP中，一个常见的实践是为各种下游任务使用预先训练的单词向量表示，也称为嵌入。直观地说，这些单词向量表示单词之间的隐式关系，这些关系在对可以受益于上下文信息的数据进行培训时非常有用。
以Mikolov等人的Word2Vec跳格模型为例——这是训练单词向量的两种最流行的方法之一(另一种是手套)。作者提出了一个类比推理问题，本质上需要问这样一个问题:“德国对柏林就像法国对___?”当你把这些词看成一个矢量时，这个问题的答案就由这个公式给出了
vec("柏林")- vec("德国")= x - vec("法国")
也就是说，向量集合之间的距离必须相等。因此,
x = vec("柏林")- vec("德国")+ vec("法国")
在正确学习向量表示的情况下，所需要的单词由最接近所获得的点的向量给出。另一个含义是具有相似语义和/或语法意义的单词将被组合在一起
![NLP中的词向量及其应用](http://p1.pstatp.com/large/pgc-image/cb0f8c468682499680dce0706e3430ae)
**改造**
虽然通用数据集通常受益于使用这些预先训练的词向量，但表示不一定总是很好地转移到专门的领域。这是因为向量已经在维基百科和类似来源创建的大量文本语料库上进行了训练。
例如，python这个词在日常上下文中还有其他含义，但它在计算机编程上下文中完全有其他含义。当您构建用于分析上下文关键数据(如医学和法律注释)的模型时，这些差异变得更加相关。
一种解决方案是简单地在领域特定的数据集上训练GloVe或skip-gram模型，但是在许多情况下，还没有足够大的数据集来获得实际相关/有意义的表示。
改造的目标是获取现成的预先训练过的单词向量，并使它们适应您的新域数据。与预先训练的向量相比，单词的结果表示更具有上下文感知能力。

