# 头部姿态估计总结





 版权声明：	https://blog.csdn.net/u013841196/article/details/81459714

## 头部姿态估计：

通常认为人体头部可以建模为一个无实体的刚体对象。根据这种假设，在姿势上人类的头部被限制为3个自由度，其特征分别是俯仰，坡度和偏航角在下图中体现。 
人脸姿态估计，顾名思义，给定一张人脸图像，确定其姿态，姿态由什么构成呢？很简单（pitch，yaw，roll）三种角度，分别代表上下翻转，左右翻转，平面内旋转的角度。 
**三个参数：** 
Yaw:摇头 左正右负 ； Pitch:点头 上负下正 ； Roll:摆头（歪头）左负 右正 
![这里写图片描述](https://img-blog.csdn.net/20180806192004350?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![这里写图片描述](https://img-blog.csdn.net/20180806192012950?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 
![这里写图片描述](https://img-blog.csdn.net/20180806192021323?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

------

## UMDFaces数据集：

该数据集包含367920张人脸，分别类属于8501个事件类别。提供的人脸信息包括，人脸框，人脸姿势，（yaw，pitch，roll），21个关键点，性别信息等。由于图片尺度，方向等的问题，使得该数据集不适合做人脸检测的训练，适合做人脸识别。 
该数据集，主要是使用GoogleScraper从网络爬下的图片，作者使用了DP2MFD model（Deep Pyramid Deformable Parts Model for Face Detection）和AMT（AmazonMechanical Turk） 
进行了先期的预处理，从而将人脸都检测出来。然后使用All-in-one CNN进行了关键点，姿势，性别等的估计。 
![这里写图片描述](https://img-blog.csdn.net/20180806192100264?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)





# 头部姿态估计两种评价准则

2016年12月05日 20:01:43 [机器学习的小学生](https://me.csdn.net/xuluhui123) 阅读数：1002



 版权声明：本文为博主原创文章，未经博主允许不得转载。	https://blog.csdn.net/xuluhui123/article/details/53469340

准则： 
![这里写图片描述](https://img-blog.csdn.net/20161205200105777)













# 头部姿态估计——multi-loss

2018年10月06日 11:13:13 [Peanut_范](https://me.csdn.net/u013841196) 阅读数：1001



 版权声明：	https://blog.csdn.net/u013841196/article/details/82949739

- 《Fine-Grained Head Pose Estimation Without Keypoints》
  2018，Nataniel Ruiz Eunji Chong James M. Rehg. multi-loss
  代码链接：<https://github.com/natanielruiz/deep-head-pose>

**1.引言：**
本文提出了一种简洁和鲁棒的方式来确定姿态，通过训练一个multi-loss的卷积神经网络。
直接使用RGB结合分类和回归损失来预测Euler angles（yaw，pitch and roll）。

**2.网络结构：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006110240267?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
本文提出使用3个分离的losses，为每一个角度。每个loss由两部分组成：a binned pose classification and a regression component 组成。

**最后为每一个欧拉角的损失为：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006110349883?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**实现细节：**
**1）** 对欧拉角（Yaw，Pitch，Roll）按角度区间进行分类，比如3度，那么Yaw：-90-+90，可以分成180/3= 60个类别，Pitch和Roll同Yaw角类似。这样就可以进行分类任务了。
**2）** 对分类的结果恢复成实际的角度，类别*3-90，在和实际的角度计算回归损失。
**3）** 最后将回归损失和分类损失进行合并来得到最后的损失，回归损失的前面增加了一个权重系数α。

**3.测试：**
**1）不同测试集的测试结果**
![在这里插入图片描述](https://img-blog.csdn.net/20181006110606768?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**2）不同网络结构设置α参数的结果比较**
![在这里插入图片描述](https://img-blog.csdn.net/20181006110648817?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
不同的网络结构需要自行去调节α进行训练。

**4.小结：**
使用分类和回归的方式进行约束，可以提升姿态估计的准确率。
但数据集构建比较麻烦。

**5.部分参考代码：**

```
伪代码：
# 类别数（yaw：60类）
idx_tensor1 = [idx for idx in xrange(60)]
idx_tensor1 = tf.convert_to_tensor(idx_tensor1,dtype=tf.float32)

# yaw角groundtruth获取
# continous labels
label_yaw_cont=ordinal_batch[:,0]
# binned labels				
label_yaw_bin=nominal_batch[:,0]

# 分类损失
# Cross entropy loss
loss_yaw_cl =tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits (labels=tf.cast(label_yaw_bin,tf.int64), logits=logits_nominal_yaw))

# 回归损失
# MSE loss
yaw_predict = tf.nn.softmax(logits_nominal_yaw)
logits_ordinal_yaw = tf.reduce_mean(yaw_predict*idx_tensor1,1)*3-90
loss_yaw_reg = tf.reduce_mean((logits_ordinal_yaw-label_yaw_cont)**2)

# 总损失	
#total loss
alpha=FLAGS.alpha
loss_yaw =tf.add_n([loss_yaw_cl,alpha * loss_yaw_reg])
12345678910111213141516171819202122232425
```

------

###### 











# 头部姿态估计——QuatNet

2018年10月06日 11:00:35 [Peanut_范](https://me.csdn.net/u013841196) 阅读数：619



 版权声明：	https://blog.csdn.net/u013841196/article/details/82949638

- 《QuatNet: Quaternion-based Head Pose Estimation with Multi-regression Loss》
  2018，Heng-Wei Hsu et al. QuatNet

**1.引言：**
作者提出了multi-regression loss function来使用CNNs进行头部姿态估计，输入采用RGB，并没有深度（Depth）信息。损失函数为L2 regression loss结合ordinal regression loss。
**1）** ordinal regression loss是被用于去处理non-stationary property，学习鲁棒的特征。
![在这里插入图片描述](https://img-blog.csdn.net/20181006103803272?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
The non-stationary property：不同的头部姿态角度面部的特征将会发生变化。
Ordinal regression learns 去预测labels的排序，而非label值本身，当labels的顺序扮演着更加重要的角色相比于它们实际的数值，这是非常重要和有效的。
**2）** L2 regression loss利用特征去提供更加精确的角度预测
**3）** Label：本文使用Euler angle和quaternions作为网络回归的结果，发现基于四元数的效果更好，因此论文题目为：QuatNet。

**2. QuatNet网络结构：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006103839732?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
QuatNet基础网络采用GoogLeNet model，最后一层池化层被替换为本文提出的regression nets和ranking nets。

**网络结构的细节：**
![在这里插入图片描述](https://img-blog.csdn.net/201810061039037?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**Regression Net:**
这4个回归网络采用相同的网络结构，一个256维的全连接层紧接着一个维度为1全连接层。
对应着四元数的4个值，如果为欧拉角就为3个值。
Ranking Net:
N多个二分类子网络，![在这里插入图片描述](https://img-blog.csdn.net/20181006103936189?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
注：ranking nets只在训练阶段进行约束，测试时去掉该网络。

**QuatNet的损失函数为：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006104020932?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)，其中λQ\lambda ^{Q}*λ**Q*=0.1。

**3.EulerNet网络细节：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006104339685?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**Ranking的角度限制：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006104631475?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**4.不同模型的结果比较：**
![在这里插入图片描述](https://img-blog.csdn.net/2018100610475129?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
从结果可以看出，加入ranking loss效果有提升。

**Methods所对应的方法为：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006104824329?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**regression net, ranking net and cross-entropy net结构的细节：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006104908290?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**5.评价标准：**
MAE（平均绝对误差）和每个角度的累计误差分布曲线（cumulative error distribution curve）
The cumulative error distribution curve reflects the proportion of test images whose errors are
below a certain threshold.
反应了测试数据在一定阈值下的错误数据比例。eg：±15o\pm 15^{o}±15*o*
**1）MAE：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006105246562?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**2）cumulative error distribution curve：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006105445820?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![在这里插入图片描述](https://img-blog.csdn.net/20181006105713432?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**6.小结：**
使用L2 regression loss结合ordinal regression loss可以提升姿态估计的准确率。







# 物体姿态估计——SilhoNet

2018年10月06日 11:46:13 [Peanut_范](https://me.csdn.net/u013841196) 阅读数：710



 版权声明：	https://blog.csdn.net/u013841196/article/details/82949995

- 《SilhoNet: An RGB Method for 3D Object Pose Estimation and Grasp Planning》
  2018，Gideon Billings and Matthew Johnson-Roberson，SilhoNet

**1.引言：**
自主机器人操纵通常涉及两项任务：
**1）估计待操纵物体的姿态**
**2）选择可行的抓取点**
在仅限于单目相机的场景时，过去的研究重点是分别解决这些问题。
本文中，作者引入了一种名为SilhoNet的新方法，它弥合了这两项任务之间的差距。使用卷积神经网络（CNN）管道，接受感兴趣区域（ROI）并同时预测具有相关遮挡掩模对象的中间轮廓表示，然后从预测的轮廓回归3D姿态。从预先计算的数据库中抓取点通过将它们反投影到遮挡物上进行过滤，以找到场景中可见的点。

本文中，**主要的贡献有：**
**1）** SilhoNet，一种新的基于RGB的深度学习方法，用于估计复杂场景中的姿态和遮挡；
**2）** 使用中间轮廓表示以便于在合成数据上学习模型以预测真实数据上的3D对象姿态，有效地桥接模拟到真实的域迁移；
**3）** 在新场景中使用推断轮廓的投影选择未被遮挡的抓握点的方法；
**4）** 对视觉上具有挑战性的YCB-Video数据集进行评估，其中提出的方法优于最先进的RGB方法。

**2.实现方案**
本文的方法：是结合1）深度学习对单目图像中3D姿态估计的能力和2）对象模型的先验知识，并通过考虑杂乱环境中其他对象的遮挡来过滤预先计算的抓握点。
![在这里插入图片描述](https://img-blog.csdn.net/20181006113844410?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**该方法主要由两个阶段组成：**
**1）预测对象的中间轮廓表示和遮挡掩模**
**2）从预测的轮廓回归3D方向四元数**
基于RGB视点中检测到的对象的估计遮挡和对象模型的先验知识，从预先计算的抓握数据库确定可行的抓握点。
**网络结构：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006113915465?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**A. Overview of the Network Pipeline**
网络的输入是RGB图像和识别物体的ROI边界框以及对应的类别标签。第一阶段使用VGG16主干末端带有反卷积层，从RGB输入图像生成特征图。输入图像中的提取特征与来自一组渲染对象视点的特征连接，然后通过两个相同结构的网络分支来预测完整的未被遮挡的轮廓和遮挡遮罩。网络的第二阶段通过ResNet-18架构传递预测的轮廓，在末端具有两个完全连接的层，已输出表示3D姿态的L2标准化四元数。

1. **Predicted ROIs:**
   ROI在特征提取阶段之后作为网络的输入特征，它主要用于从输入图像特征图中裁剪出相应的区域。然后通过缩小特征图或使用双线性插值将其缩放，将裁剪的特征图调整为高度和宽度=64x64。
2. **Rendered Model Viewpoints:**
   我们能够通过生成一组与检测到的对象类相关联的合成预渲染视点作为第一阶段的附加输入，来提高轮廓预测性能的网络。对于每个类，我们从对象模型渲染了一组12个视点，每个视点的维度为224x224。由于中间目标是轮廓预测，这些合成渲染在捕捉不同方向的真实物体的形状和轮廓方面做得非常出色，尽管模拟物体的视觉外观存在典型的域差异。
   识别物体的所有视图都通过特征提取阶段，然后通过4x4的最大池化，再增加两个反卷积层，将其调整为64x64大小，通道为32的特征映射。在实现中，本文为每个对象即时提取渲染视图的特征映射，然后与裁剪和调整大小的输入图像特征图进行连接，提供给网络。
3. **Silhouette Prediction:**
   网络的第一阶段将对象的中间轮廓表示预测为64x64的二维掩模。
   网络的第一部分是VGG16特征提取器，它以1/2，1/4，1/8和1/16比例生成特征图。1/8和1/16比例特征图都具有512的输出通道尺寸。使用两个卷积层将两者的通道减小到64。
   RGB的特征映射与渲染的视点特征映射连接，从而生成大小为64x64x448的单个特征向量矩阵。特征向量矩阵被传送到两个相同的网络分支中，其中一个输出轮廓预测而另一个输出遮挡掩模。
4. **3D Pose Regression:**
   本文使用四元数表示3D姿态，它可以表示连续空间中的作用于单位长度矢量的3D旋转。
   网络的第二阶段接收预测的轮廓概率图，将某些值阈值化为二进制掩模，并输出对象姿态的四元数预测。网络的这个阶段由ResNet-18骨干网组成。

**B. Grasp Point Detection**
最后一步是检测视觉上可行的抓取点。给定对象的估计3D姿态和预先计算的抓握点的数据库，我们将每个抓取点从对象框架投影到相机框架中的遮挡遮罩上。掩模的未被遮挡部分上的点被认为是有效的，并且可以从有效集中选择最高得分抓握。

**3.实验结果**
第一阶段数据结果显示：
![在这里插入图片描述](https://img-blog.csdn.net/20181006114010553?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
和其他网络结构进行比较：
![在这里插入图片描述](https://img-blog.csdn.net/20181006114033645?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**小结：**
特征提取用到了多尺度，使用识别物体的ROI作为输入，中间掩模进行遮挡检测和姿态估计。

**参考：**
<http://www.sohu.com/a/255001737_100177858>











# 物体姿态估计——DeepIM

2018年10月06日 11:36:28 [Peanut_范](https://me.csdn.net/u013841196) 阅读数：2610



 版权声明：	https://blog.csdn.net/u013841196/article/details/82949927

- 《DeepIM: Deep Iterative Matching for 6D Pose Estimation》
  2018，Yi Li et al. DeepIM

**1.引言：**
本文，作者提出了一种新的深度神经网络对物体的6D姿态（3D位置和3D方向）进行估计，命名为DeepIM。采用对图像进行直接回归物体姿态的方式，准确率是有限的，通过匹配物体的渲染图像可以进一步提高准确率。即给定初始姿态估计，对合成RGB图像进行渲染来和目标输入图像进行匹配，然后在计算出新的更准的姿态估计。
**1）Depth信息局限性：** 最新的技术已经使用深度相机（depth cameras）为物体的姿态估计，但这种相机在帧速率、视场、分辨率和深度范围等方面还存在相当大的局限性，一些小的、薄的、透明的或快速移动的物体检测起来还非常困难。
**2）RGB信息局限性：** 基于RGB的6D物体姿态估计仍然具有挑战性，因为目标图像的表观会受光照、姿态变化、遮挡等影响
此外，鲁棒的6D姿态估计方法还需要能处理有纹理和无纹理的目标。

传统的6D姿态估计：将2D图像中提取的局部特征与待检测物体3D基准模型中的特征相匹配来求解R和T，也就是基于2D-3D对应关系求解PnP问题。但是，这种方法对局部特征依赖太强，不能很好地处理无纹理的目标。
为了处理无纹理目标，目前的文献有两类方法：
**1）** 学习估计输入图像中的目标关键点或像素的3D模型坐标。
**2）** 通过离散化姿态空间将6D姿态估计问题转化为姿态分类问题，或转化为姿态回归问题。
这些方法虽然能够处理无纹理目标，但是精度不够高。

**2.DeepIM迭代结构：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006112021227?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**DeepIM：** 一种基于深度神经网络的迭代6D姿态匹配的新方法。给定测试图像中目标的初始6D姿态估计，DeepIM能够给出相对SE（3）变换符合目标渲染视图与观测图像之间的匹配关系。提高精度后的姿态估计迭代地对目标重新渲染，使得网络的两个输入图像会变得越来越相似，从而网络能够输出越来越精确的姿态估计。上图展示了作者提出网络用于姿态优化的迭代匹配过程。
这项工作主要的贡献有：
**1）** 将深度网络引入到基于图像的迭代姿态优化问题，而无需任何手工制作的图像特征，其能够自动学习内部优化机制。
**2）** 提出了一种旋转和平移解耦的SE（3）变换表示方法，能够实现精确的姿态估计，并且能使提出的方法适用于目标不在训练集时的姿态估计问题。
**3）** 作者在LINEMOD和Occlusion数据集上进行了大量实验，以评估DeepIM的准确性和各种性能。

**3.算法流程**
如果目标在输入图像上是非常小的，它是困难的去提取有用的特征。如下图所示，作者为了获得足够的信息进行姿态匹配，对观测图像进行放大，并在输入网络前进行渲染。要注意的是，在每次迭代过程中，都会根据上一次得到的姿态估计来重新渲染，这样才能通过迭代来增加姿态估计的准确度。
**Zoom in:**
![在这里插入图片描述](https://img-blog.csdn.net/20181006112208535?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**DeepIM网络结构图：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006112252328?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
输入观测图像、渲染图像以及对应的掩模。使用FlowNetSimple网络第11个卷积层输出的特征图作为输入，然后连接两个全连接层FC256，最后旋转和平移的估计分别使用两个全连接层FC3和FC4作为输入。（作者也尝试使用VGG16图像分类网络来作为基础网络，但效果很差，直觉：估计光流是有用的对于姿态匹配。）

**4.Untangled Transformation Representation**
通常使用四元数或欧拉角来表示旋转矩阵R△R_{\bigtriangleup}*R*△​和一个向量表示平移t△t_{\bigtriangleup}*t*△​，因此一个完成的转换矩阵可以写成[R△∣t△][R_{\bigtriangleup}|t_{\bigtriangleup}][*R*△​∣*t*△​]，给予一个原始目标姿态[R△∣tsrc][R_{\bigtriangleup}|t_{src}][*R*△​∣*t**s**r**c*​]。
通常目标从初始位置到新位置的旋转与平移变换关系如下所示：
![在这里插入图片描述](https://img-blog.csdn.net/20181006112711758?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
一般来说：旋转变换会影响影响最后的平移变换，即两者是耦合在一起的。
在本文中，作者让坐标轴平行于当前相机坐标轴，这样可以算得相对旋转，后续实验证明这样效果更好。还要解决相对平移估计的问题，一般的方法是直接在三维空间中计算原位置与新位置的xyz距离，但是这种方式既不利于网络训练，也不利于处理大小不一、边关相似的目标或未经训练的新目标。
**估计相对的平移：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006112748905?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
本文采用在二维图像空间进行回归估计平移变换，vx和vy分别表示图像水平和垂直方向上像素移动的距离，vz表示目标的尺度变化。其中，fx和fy是相机的焦距，由于是常数，在实际训练中作者将其设置为1。
![在这里插入图片描述](https://img-blog.csdn.net/20181006112822978?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这样一来，旋转和平移进行解耦了，这种表示表示方法不需要目标的任何先验知识，并且能处理一些特殊情况，比如两个外观相似的物体，唯一的区别就是大小不一样。

**5.Matching Loss**
模型训练的损失函数，通常直接的方法是将旋转和平移分开计算，比如用角度距离表示旋转误差，L1距离表示平移误差，但这种分离的方法很容易让旋转和平移两种损失在训练时失衡。本文作者提出了一种同时计算旋转和平移的**Point Matching Loss** 函数，来表示姿态真值和估计值之间的损失。其中，xj表示目标模型上的三维点，n表示总共用来计算损失函数的点个数，本文中n=3000。
![在这里插入图片描述](https://img-blog.csdn.net/20181006112905835?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
最后总的损失函数由
![在这里插入图片描述](https://img-blog.csdn.net/20181006112939279?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
组成，其中(α,β,γ)(\alpha ,\beta ,\gamma )(*α*,*β*,*γ*)分别为（0.1,0.25,0.03）。

**6.实验结果：**
作者主要使用了LINEMOD和OCCLUSION数据集。
在LINEMOD数据集上作者分别使用了PoseCNN和Faster R-CNN初始化DeepIM网络，发现即使两个网络性能差异很大，但是经过DeepIM之后仍能得到差不多的结果。
![在这里插入图片描述](https://img-blog.csdn.net/20181006113114257?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
LINEMOD数据集上的方法对比结果如下表所示，本文的方法是最好的。
![在这里插入图片描述](https://img-blog.csdn.net/20181006113142940?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**小结：**
使用一种同时计算旋转和平移的Point Matching Loss 函数，来表示姿态真值和估计值之间的损失。

**参考：**
<https://mp.weixin.qq.com/s?__biz=MzAxMzc2NDAxOQ==&mid=2650368997&idx=1&sn=8db28f163c639153e32a18b855762ec8&chksm=83905f39b4e7d62fd6f35b1e71d8b74277ebffd9d918c7b960bbd6bdd9383386971bb6ede35b&scene=0#rd>
将门创投公众号













# 头部姿态估计——adaptive gradient methods

2018年10月06日 10:08:37 [Peanut_范](https://me.csdn.net/u013841196) 阅读数：472



 版权声明：	https://blog.csdn.net/u013841196/article/details/82949283

- 《Head pose estimation in the wild using Convolutional Neural Networks and adaptive gradient methods》
  2017，Massimiliano Patacchiola, Angelo Cangelosi. adaptive gradient methods

本文使用4种最新的网络结构来训练头部姿态估计，作者也探讨dropout和adaptive gradient methods对结果性能得影响。
**网络结构：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006100238977?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**四种网络结构得细节：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006100307656?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**对于Dropout：**
The probability p is another hyperparameter to tune. However numerous experimental results suggest that a value of p = 0.5 produces the best performance , so we used this value in our experiments.

**实验结果：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006100431375?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![在这里插入图片描述](https://img-blog.csdn.net/20181006100440384?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
可以发现：**Adam**和**RMSProp**优化器效果最好。

**训练阶段比较：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006100547316?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**收敛速度比较：**
![在这里插入图片描述](https://img-blog.csdn.net/20181006100621577?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**小结：**
可以发现，在进行CNNs训练过程中，与更传统的优化器相比，Adam和RMSProp优化器效果和收敛速度都比较好，建议使用这两种优化器。







# 头部姿态估计——CNN

2018年08月06日 19:30:08 [Peanut_范](https://me.csdn.net/u013841196) 阅读数：2373



 版权声明：	https://blog.csdn.net/u013841196/article/details/81459824

- 《Face Alignment Assisted by Head Pose Estimation》

人脸对齐的错误案例分析：**头部姿态通常都是大角度的。** 
![这里写图片描述](https://img-blog.csdn.net/2018080619250634?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
**主旨**：通过人脸头部姿态来辅助关键点的检测，保证大角度人脸对齐的正确性。 
**动机**： 
1）大多数现有的方法在大姿态变化的人脸图像上表现的不好。 
2）最近的人脸对齐方法都是基于级联的方式和初始化于平均形状。 
**策略**： 
第一步，我们使用ConvNet直接从人脸图像上估计出头部姿态； 
第二步，两种方案来生成初始化， 
1）设计一个标准的3D人脸形状； 
2）从训练集中通过头部姿态最近邻的方法来搜索形状。

**数据准备：** 
![这里写图片描述](https://img-blog.csdn.net/20180806192544127?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 
使用3D头部姿态估计器来计算300W数据的头部姿态。

**头部姿态估计：** 
![这里写图片描述](https://img-blog.csdn.net/20180806192651364?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**Evaluation：** 
![这里写图片描述](https://img-blog.csdn.net/20180806192719870?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)













# 头部姿态估计：《Fine-Grained Head Pose Estimation Without Keypoints》

2018年12月06日 11:25:17 [邦戈栗子](https://me.csdn.net/qq_42189368) 阅读数：463



《Fine-Grained Head Pose Estimation Without Keypoints》
2018，Nataniel Ruiz Eunji Chong James M. Rehg. multi-loss
代码链接：https://github.com/natanielruiz/deep-head-pose
1.引言：
本文提出了一种简洁和鲁棒的方式来确定姿态，通过训练一个multi-loss的卷积神经网络。
直接使用RGB结合分类和回归损失来预测Euler angles（yaw，pitch and roll）。

2.网络结构：

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdn.net/20181006110240267?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

本文提出使用3个分离的losses，为每一个角度。每个loss由两部分组成：a binned pose classification and a regression component 组成。

最后为每一个欧拉角的损失为：

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdn.net/20181006110349883?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**实现细节：1） 对欧拉角（Yaw，Pitch，Roll）按角度区间进行分类，比如3度，那么Yaw：-90-+90，可以分成180/3= 60个类别，Pitch和Roll同Yaw角类似。这样就可以进行分类任务了。2） 对分类的结果恢复成实际的角度，类别\*3-90，在和实际的角度计算回归损失。3） 最后将回归损失和分类损失进行合并来得到最后的损失，回归损失的前面增加了一个权重系数α。**

3.测试：
1）不同测试集的测试结果

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdn.net/20181006110606768?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

2）不同网络结构设置α参数的结果比较

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdn.net/20181006110648817?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

不同的网络结构需要自行去调节α进行训练。

4.小结：
使用分类和回归的方式进行约束，可以提升姿态估计的准确率。
但数据集构建比较麻烦。

5.部分参考代码：

```
伪代码：



# 类别数（yaw：60类）



idx_tensor1 = [idx for idx in xrange(60)]



idx_tensor1 = tf.convert_to_tensor(idx_tensor1,dtype=tf.float32)



 



# yaw角groundtruth获取



# continous labels



label_yaw_cont=ordinal_batch[:,0]



# binned labels				



label_yaw_bin=nominal_batch[:,0]



 



# 分类损失



# Cross entropy loss



loss_yaw_cl =tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits (labels=tf.cast(label_yaw_bin,tf.int64), logits=logits_nominal_yaw))



 



# 回归损失



# MSE loss



yaw_predict = tf.nn.softmax(logits_nominal_yaw)



logits_ordinal_yaw = tf.reduce_mean(yaw_predict*idx_tensor1,1)*3-90



loss_yaw_reg = tf.reduce_mean((logits_ordinal_yaw-label_yaw_cont)**2)



 



# 总损失	



#total loss



alpha=FLAGS.alpha



loss_yaw =tf.add_n([loss_yaw_cl,alpha * loss_yaw_reg])
```

 

转自：

<https://blog.csdn.net/u013841196/article/details/82949739>















# 基于Dlib和OpenCV的人脸姿态估计(HeadPoseEstimation)

2017年09月02日 21:58:56 [二极管具有单向导电性](https://me.csdn.net/u013512448) 阅读数：7956



 版权声明：本文为博主原创文章，未经博主允许不得转载。需要转载请留言或发送邮件(tanyang1231@163.com)，并注明出处。 https://blog.csdn.net/u013512448/article/details/77804161

## 1.简介

人脸姿态估计主要是获得脸部朝向的角度信息。本文获得的人脸姿态信息用三个欧拉角（pitch，yaw，roll）表示。 
首先定义一个6关键点的3D脸部模型（左眼角，右眼角，鼻尖，左嘴角，右嘴角，下颌），然后采用Dlib检测出图片中对应的6个脸部关键点，采用OpenCV的solvePnP函数解出旋转向量，最后将旋转向量转换为欧拉角。 
![Pose估计结果](https://img-blog.csdn.net/20170902215020352?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUxMjQ0OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 2.定义6关键点的3D Model

![3D Model](https://img-blog.csdn.net/20170902215213773?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUxMjQ0OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

```
    // 3D model points.
    std::vector<cv::Point3d> model_points;
    model_points.push_back(cv::Point3d(0.0f, 0.0f, 0.0f));               // Nose tip
    model_points.push_back(cv::Point3d(0.0f, -330.0f, -65.0f));          // Chin
    model_points.push_back(cv::Point3d(-225.0f, 170.0f, -135.0f));       // Left eye left corner
    model_points.push_back(cv::Point3d(225.0f, 170.0f, -135.0f));        // Right eye right corner
    model_points.push_back(cv::Point3d(-150.0f, -150.0f, -125.0f));      // Left Mouth corner
    model_points.push_back(cv::Point3d(150.0f, -150.0f, -125.0f));       // Right mouth corner12345678
```

## 3.基于Dlib的人脸关键点检测

Dlib提供了一个68关键点的检测模型，而且是按照顺序排列的。因此可以直接索引到需要的6个关键点。

### 3.1 Dlib的使用

首先前往Dlib官网下载源码并解压。有多种编译方式，这里介绍比较简单的在VS2015的编译方式。 
首先把dlib/all/source.cpp添加到工程里，然后把Dlib的路径添加到工程属性表的include里。具体参见Dlib的how to compile页面。

> Compiling on Windows Using Visual Studio 2015 or Newer
>
> All you need to do is create an empty console project. Then add dlib/all/source.cpp to it and add the folder containing the dlib folder to the #include search path. Then you can compile any example program by adding it to your project.
>
> Again, note that dlib will only be able to work with jpeg and png files if you link in libjpeg and libpng. In Visual Studio, the easiest way to do this is to add all the libjpeg, libpng, and zlib source files in the dlib/external folder into your project and also define the DLIB_PNG_SUPPORT and DLIB_JPEG_SUPPORT preprocessor directives. If you don’t know how to configure Visual Studio then you should use CMake as shown above since it will take care of everything automatically.

### 3.2 关键点检测

Dlib提供一个68关键点的检测模型，并提供基于webcam的示例代码，参见examples文件夹下的的webcam_face_pose_ex.cpp。需要提前下载模型数据文件。下载链接：<http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2>

首先初始Dlib相关参数并进行人脸检测。注意后续需要用OpenCV的函数进行旋转向量运算，而Dlib在检测时会对图片进行一定的缩放，所以需要计算一个缩放比例ratio，以便将关键点的坐标还原为原图中的坐标。

```
// create a face detector
            frontal_face_detector detector = get_frontal_face_detector();

            char* fileName = "face_4.jpg";
            cv::Mat im = cv::imread(fileName);

            shape_predictor sp;
            deserialize("shape_predictor_68_face_landmarks.dat") >> sp;

            array2d<rgb_pixel> img;
            load_image(img, fileName);

            // Make the image larger so we can detect small faces.
            pyramid_up(img);

            //calculate the ratio of zoom
            int ratio = int(img.nc()) / int(im.cols);

            // bounding boxes  of detected faces
            std::vector<rectangle> dets = detector(img);

            cout << "Number of faces detected: " << dets.size() << endl;12345678910111213141516171819202122
```

接着对检测到的人脸进行关键点检测。经过我的实际测试，关键的索引顺序是固定的，所以我记录了需要的6个关键点的索引id，分别是： 
下巴：8 
鼻尖：30 
左眼角：36 
右眼角：45 
左嘴角：48 
右嘴角：54

```
            std::vector<full_object_detection> shapes;
            for (unsigned long j = 0; j < dets.size(); ++j)
            {
                full_object_detection shape = sp(img, dets[j]);
                shapes.push_back(shape);

                // landmarks
                //Dlib's feature points are arranged in order,so we can get 6 landmarks below by id
                std::vector<cv::Point2d> landmarks;
                landmarks.push_back(cv::Point2d(int(shape.part(30).x() / ratio), int(shape.part(30).y()) / ratio));    // Nose tip
                landmarks.push_back(cv::Point2d(int(shape.part(8).x() / ratio), int(shape.part(8).y()) / ratio));    // Chin
                landmarks.push_back(cv::Point2d(int(shape.part(36).x() / ratio), int(shape.part(36).y()) / ratio));     // Left eye left corner
                landmarks.push_back(cv::Point2d(int(shape.part(45).x() / ratio), int(shape.part(45).y()) / ratio));    // Right eye right corner
                landmarks.push_back(cv::Point2d(int(shape.part(48).x() / ratio), int(shape.part(48).y()) / ratio));    // Left Mouth corner
                landmarks.push_back(cv::Point2d(int(shape.part(54).x() / ratio), int(shape.part(54).y()) / ratio));    // Right mouth corner

            }1234567891011121314151617
```

## 4. 用OpenCV的solvePnP函数估计Pose

OpenCV中solvePnP 和 solvePnPRansac都可以用来估计Pose。可以通过改变flag来使用不同的计算方法，我采用的默认的方法。具体的参数可以参见OpenCV文档。

> solvePnP implements several algorithms for pose estimation which can be selected using the parameter flag. By default it uses the flag SOLVEPNP_ITERATIVE which is essentially the DLT solution followed by Levenberg-Marquardt optimization. SOLVEPNP_P3P uses only 3 points for calculating the pose and it should be used only when using solvePnPRansac.
>
> C++: bool solvePnP(InputArray objectPoints, InputArray imagePoints, InputArray cameraMatrix, InputArray distCoeffs, OutputArray rvec, OutputArray tvec, bool useExtrinsicGuess=false, int flags=SOLVEPNP_ITERATIVE )

确定pose也就是确定从3D model到图片中人脸的仿射变换矩阵，它包含旋转和平移的信息。solvePnP函数输出结果包括旋转向量(roatation vector)和平移向量(translation vector)。这里我们只关心旋转信息，所以主要将对 roatation vector进行操作。 
在调用solvePnP函数前需要初始化cameraMatrix，也就是相机内参，并调用solvePnP函数：

```
 // Camera internals
    double focal_length = im.cols; // Approximate focal length.
    cv::Point2d center = cv::Point2d(im.cols / 2, im.rows / 2);
    cv::Mat camera_matrix = (cv::Mat_<double>(3, 3) << focal_length, 0, center.x, 0, focal_length, center.y, 0, 0, 1);
    cv::Mat dist_coeffs = cv::Mat::zeros(4, 1, cv::DataType<double>::type); // Assuming no lens distortion

    cv::Mat rotation_vector; // Rotation in axis-angle form
    cv::Mat translation_vector;

    // Solve for pose
    cv::solvePnP(model_points, landmarks, camera_matrix, dist_coeffs, rotation_vector, translation_vector);1234567891011
```

（参考：<http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/>）

## 5.将旋转向量转换为欧拉角

rotation vector 是物体旋转信息的表示方式之一，是OpenCV常用的表示方式。除了rotation vector还有欧拉角(Euler angle)、旋转矩阵(Rotation Matrix)、方向余弦矩阵(Direction Cosine Matrix)、四元数(Quaternion) 和 轴-角表示(Axis-Angle)。 
因为我需要的是欧拉角，所以这里我只介绍将rotation vector 转换为欧拉角的方法。 
（其它表示方法参见：<http://blog.csdn.net/yuewei19/article/details/53023992>）

### 5.1 旋转向量(Rotation Vector) 和四元数(Quaternion)

三维空间的任意旋转，都可以用绕三维空间的某个轴旋转过某个角度来表示，即Axis-Angle表示方法。Axis可用一个三维向量(x,y,z)来表示，theta可以用一个角度值来表示，直观来讲，一个四维向量(theta,x,y,z)就可以表示出三维空间任意的旋转。 
注意，这里的三维向量(x,y,z)只是用来表示axis的方向朝向，因此更紧凑的表示方式是用一个单位向量来表示方向axis，而用该三维向量的长度来表示角度值theta。这样以来，可以用一个三维向量(theta*x,theta*y, theta*z)就可以表示出三维空间任意的旋转，前提是其中(x,y,z)是单位向量。这就是旋转向量(Rotation Vector)的表示方式。

四元数(Quaternion)也是一种常用的旋转表示方式。假设(x,y,z)是axis方向的单位向量，theta是绕axis转过的角度，那么四元数可以表示为[cos(theta/2),x*sin(theta/2), y*sin(theta/2), z*sin(theta/2)]。 
因为从四元数转换到欧拉角公式较简单，所以我先将rotation vector转换为四元数。

```
    //calculate rotation angles
    double theta = cv::norm(rotation_vector, CV_L2);

    //transformed to quaterniond
    Quaterniond q;
    q.w = cos(theta / 2);
    q.x = sin(theta / 2)*rotation_vector.at<double>(0, 0) / theta;
    q.y = sin(theta / 2)*rotation_vector.at<double>(0, 1) / theta;
    q.z = sin(theta / 2)*rotation_vector.at<double>(0, 2) / theta;123456789
```

### 5.2 欧拉角(Euler Angle)

在3D 空间中，表示物体的旋转可以由三个欧拉角来表示： 
pitch围绕X轴旋转，叫俯仰角。 
yaw围绕Y轴旋转，叫偏航角。 
roll围绕Z轴旋转，叫翻滚角。 
这三个角的顺序对旋转结果有影响。 
![欧拉角](https://img-blog.csdn.net/20170902225540903?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUxMjQ0OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

（欧拉角与四元数的转换关系： 
<http://www.cnblogs.com/wqj1212/archive/2010/11/21/1883033.html>）

四元数到欧拉角的转换公式如下： 
![四元数到欧拉角的转换公式](https://img-blog.csdn.net/20170902231812417?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUxMjQ0OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 
arctan和arcsin的结果为[-pi/2,pi/2]，不能覆盖所有的欧拉角，因此采用atan2代替arctan： 
![用atan2代替arctan](https://img-blog.csdn.net/20170902232051927?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzUxMjQ0OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

```cpp

void quaterniondToEulerAngle(Quaterniond& q, double& roll, double& yaw, double& pitch)
{
    double ysqr = q.y * q.y;

    // pitch (x-axis rotation)
    double t0 = +2.0 * (q.w * q.x + q.y * q.z);
    double t1 = +1.0 - 2.0 * (q.x * q.x + ysqr);
    pitch = std::atan2(t0, t1);

    // yaw (y-axis rotation)
    double t2 = +2.0 * (q.w * q.y - q.z * q.x);
    t2 = t2 > 1.0 ? 1.0 : t2;
    t2 = t2 < -1.0 ? -1.0 : t2;
    yaw = std::asin(t2);

    // roll (z-axis rotation)
    double t3 = +2.0 * (q.w * q.z + q.x * q.y);
    double t4 = +1.0 - 2.0 * (ysqr + q.z * q.z);
    roll = std::atan2(t3, t4);
}
```













# 基于深度图像的头部姿态估计

2016年11月15日 09:06:27 [AUTO1993](https://me.csdn.net/AUTO1993) 阅读数：4737



 版权声明：本文为博主原创文章，未经博主允许不得转载。	https://blog.csdn.net/AUTO1993/article/details/53167810

**1.头部姿态**

​      头部姿态估计通常是指利用计算机视觉和模式识别的方法在数字图像中判断人头部朝向问题[2]。更严格的说，头部姿态估计是在一个空间坐标系内识别头部的姿态参数，即头部位置参数()和方向度参数。描述头部方向度参数的有三个：水平转动的偏航角（Yaw）、垂直转动仰俯角（Pitch）以及左右转动的旋转角（Roll）。如下图所示，一般而言，一个正常的成年人的头部四周运动的范围为：左右偏角：-40.9度~63.3度，垂直偏角-60.4度~69.6度，水平偏角-79.8度~75.3度  

![img](https://img-blog.csdn.net/20161115163513869)

 图 三维空间坐标系中头部姿态描述

​       因此，在很大程度上，头部姿态反映了一个人的眼睛视线方向或注意力方向。当人眼被遮挡时，通过头部姿态估计可以大致辨别出被观察者的注视方向；当人眼未被遮挡时，头部姿态将成为精确预测被观察者注视方向的必要条件。        



**2.头部姿态估计方法分类**

​      从20世纪90年代至今，短短二十多年时间，头部姿态估计一跃成为国内外一大研究热点。围绕这一研究方向，广大学者提出几十种头部姿态估计方法。下面将头部姿态方法按照数据源的不同对方法进行分类。

**2.1数据源不同**

​      从所依赖的数据源的不同，大致地可以将这些方法分为基于二维彩色图像的方法，基于深度图像的方法以及基于三维图像的方法。

**2.1.1基于二维彩色图像的头部姿态估计方法**

​      该方法可以说是最为传统的一种方法，也是使用作为普遍的方法。是传统的主流方法，该类方法的优点是不依赖于任何特殊的硬件设施，只需一个简单的摄像头即可正常工作，成本低易于推广，相关研究成果最多。当然，采用该方法的缺点就是容易受光照变化，表情变化以及遮挡的影响。

**2.1.2基于深度图像的头部姿态估计方法**

​       采用这种方法的能在很大程度上克服对光照变化以及表情的变化，尤其得益于微软发明的Kinect设备，该款设备装备了一个RGB彩色摄像头，红外摄像头以及深度探测器。无论在任何的照明条线下，都可以利用Kinect的CMOS红外探测器来感知空间。它可以收集到视野里的每一个点的信息，并将其生成一张深度图像用以代表环境的信息。Kinect的COMS红外探测器可以实现实时的探测环境中物体的深度。而Kinect中间的CCD摄像头可以用来获取人体或者是物体的二维RGB彩色图像。随着Kinect设备价格不断下降，采用基于深度图像的头部姿态估计方法也是越来越多。

**2.1.3基于三维图像的头部姿态估计方法**

获得三维图像一般有三种方式：1）通过三维图像传感器直接获得；2）通过结合二维彩色图像的形状，纹理等特征实现三维重建；3）通过结合二维彩色图像和深度图像获取三维点云图像。采用该方法的优点是可以很好的保留彩色纹理信息以及深度信息，对于即使待检测人具有较高程度的化妆都能实现头部姿态估计，这是其他两种方法所实现不了的。但是，这类方法与前面三种方法相比，相关的研究相对来说是最少的，主要是考虑三个方面的原因：1）深度传感器价格昂贵；2）应用层面比较窄；3）三维重建技术目前尚未成熟。

​      当然还有根据实现原理和方法的不同对头部姿态方法进行分类，比如基于形状模板的方法，基于非线性回归的方法以及基于特征点几何关系的方法等等。这里就不一一概述了。

**3.基于深度图像的头部姿态估计**

​     这里介绍一下，我从github上下载的代码，进过配置环境，调试修改后能完全运行。以下是代码的主程序：



```cpp
#include <string>



#include <algorithm>



#include <iostream>



#include <vector>



#include "CRForestEstimator.h"



#include <stdint.h>



 



#define PATH_SEP "/"



 



using namespace std;



using namespace cv;



 



// Path to trees



string g_treepath;



// Number of trees



int g_ntrees;



// Patch width



int g_p_width;



// Patch height



int g_p_height;



//maximum distance form the sensor - used to segment the person



int g_max_z = 0;



//head threshold - to classify a cluster of votes as a head



int g_th = 400;



//threshold for the probability of a patch to belong to a head



float g_prob_th = 1.0f;



//threshold on the variance of the leaves



float g_maxv = 800.f;



//stride (how densely to sample test patches - increase for higher speed)



int g_stride = 5;



//radius used for clustering votes into possible heads



float g_larger_radius_ratio = 1.f;



//radius used for mean shift



float g_smaller_radius_ratio = 6.f;



 



//pointer to the actual estimator



CRForestEstimator* g_Estimate;



//input 3D image



Mat g_im3D;



 



std::vector< cv::Vec<float,POSE_SIZE> > g_means; //outputs



std::vector< std::vector< Vote > > g_clusters; //full clusters of votes



std::vector< Vote > g_votes; //all votes returned by the forest



 



bool loadDepthImageCompressed(Mat& depthImg, const char* fname ){



 



	//now read the depth image



	FILE* pFile = fopen(fname, "rb");



	if(!pFile){



		cerr << "could not open file " << fname << endl;



		return false;



	}



 



	int im_width = 0;



	int im_height = 0;



	bool success = true;



 



	success &= ( fread(&im_width,sizeof(int),1,pFile) == 1 ); // read width of depthmap



	success &= ( fread(&im_height,sizeof(int),1,pFile) == 1 ); // read height of depthmap



 



	depthImg.create( im_height, im_width, CV_16SC1 );



	depthImg.setTo(0);



 



 



	int numempty;



	int numfull;



	int p = 0;



 



	if(!depthImg.isContinuous())



	{



		cerr << "Image has the wrong size! (should be 640x480)" << endl;



		return false;



	}



 



	int16_t *data = depthImg.ptr<int16_t>(0);



	while(p < im_width*im_height ){



 



		success &= ( fread( &numempty,sizeof(int),1,pFile) == 1 );



 



		for(int i = 0; i < numempty; i++)



			data[ p + i ] = 0;



 



		success &= ( fread( &numfull,sizeof(int), 1, pFile) == 1 );



		success &= ( fread( &data[ p + numempty ], sizeof(int16_t), numfull, pFile) == (unsigned int) numfull );



		p += numempty+numfull;



 



	}



 



	fclose(pFile);



 



	return success;



}



 



void loadConfig(const char* filename) {



 



	ifstream in(filename);



	string dummy;



 



	if(in.is_open()) {



 



		// Path to trees



		in >> dummy;



		in >> g_treepath;



 



		// Number of trees



		in >> dummy;



		in >> g_ntrees;



 



		in >> dummy;



		in >> g_maxv;



 



		in >> dummy;



		in >> g_larger_radius_ratio;



 



		in >> dummy;



		in >> g_smaller_radius_ratio;



 



		in >> dummy;



		in >> g_stride;



 



		in >> dummy;



		in >> g_max_z;



 



		in >> dummy;



		in >> g_th;



 



 



	} else {



		cerr << "File not found " << filename << endl;



		exit(-1);



	}



	in.close();



 



	cout << endl << "------------------------------------" << endl << endl;



	cout << "Estimation:       " << endl;



	cout << "Trees:            " << g_ntrees << " " << g_treepath << endl;



	cout << "Stride:           " << g_stride << endl;



	cout << "Max Variance:     " << g_maxv << endl;



	cout << "Max Distance:     " << g_max_z << endl;



	cout << "Head Threshold:   " << g_th << endl;



 



	cout << endl << "------------------------------------" << endl << endl;



 



}



 



int main(int argc, char* argv[])



{//data\\frame_00100_depth.bin data//depth.cal



 



	if( argc != 3 ){



 



		cout << "usage: ./head_pose_estimation config_file depth_image" << endl;



		exit(-1);



	}



 



	loadConfig(argv[1]);



	CRForestEstimator estimator;



	if( !estimator.loadForest(g_treepath.c_str(), g_ntrees) ){



 



		cerr << "could not read forest!" << endl;



		exit(-1);



	}



 



	string depth_fname(argv[2]);



 



	//read calibration file (should be in the same directory as the depth image!)



	string cal_filename = depth_fname.substr(0,depth_fname.find_last_of('/'));



	cal_filename += "/depth.cal";



	ifstream is(cal_filename.c_str());



	if (!is){



 



		cerr << "depth.cal file not found in the same folder as the depth image! " << endl;



		return -1;



 



	}



	//read intrinsics only



	float depth_intrinsic[9];	



	for(int i =0; i<9; ++i)



		is >> depth_intrinsic[i];



	is.close();



 



	Mat depthImg;



	//read depth image (compressed!)



	if (!loadDepthImageCompressed( depthImg, depth_fname.c_str() ))



		return -1;



 



 



	Mat img3D;



	img3D.create( depthImg.rows, depthImg.cols, CV_32FC3 );



 



	//get 3D from depth



	for(int y = 0; y < img3D.rows; y++)



	{



		Vec3f* img3Di = img3D.ptr<Vec3f>(y);



		const int16_t* depthImgi = depthImg.ptr<int16_t>(y);



 



		for(int x = 0; x < img3D.cols; x++){



 



			float d = (float)depthImgi[x];



 



			if ( d < g_max_z && d > 0 ){



 



				img3Di[x][0] = d * (float(x) - depth_intrinsic[2])/depth_intrinsic[0];



				img3Di[x][1] = d * (float(y) - depth_intrinsic[5])/depth_intrinsic[4];



				img3Di[x][2] = d;



 



			}



			else{



 



				img3Di[x] = 0;



			}



 



		}



	}



 



	g_means.clear();



	g_votes.clear();



	g_clusters.clear();



 



	string pose_filename(depth_fname.substr(0,depth_fname.find_last_of('_')));



	pose_filename += "_pose.bin";



 



	cv::Vec<float,POSE_SIZE> gt;



	bool have_gt = false;



	//try to read in the ground truth from a binary file



	FILE* pFile = fopen(pose_filename.c_str(), "rb");



	if(pFile){



 



		have_gt = true;



		have_gt &= ( fread( >[0], sizeof(float),POSE_SIZE, pFile) == POSE_SIZE );



		fclose(pFile);



 



	}



 



	//do the actual estimate



	estimator.estimate( 	img3D,



							g_means,



							g_clusters,



							g_votes,



							g_stride,



							g_maxv,



							g_prob_th,



							g_larger_radius_ratio,



							g_smaller_radius_ratio,



							false,



							g_th



						);



 



	cout << "Heads found : " << g_means.size() << endl;



	imshow("img3D", img3D);



 



	//assuming there's only one head in the image!



	if(g_means.size()>0){



 



		cout << "Estimated: " << g_means[0][0] << " " << g_means[0][1] << " " << g_means[0][2] << " " << g_means[0][3] << " " << g_means[0][4] << " " << g_means[0][5] <<endl;



 



		float pt2d_est[2];



		float pt2d_gt[2];



 



		if(have_gt){



			cout << "Ground T.: " << gt[0] << " " << gt[1] << " " << gt[2] << " " << gt[3] << " " << gt[4] << " " << gt[5] <<endl;



 



			cv::Vec<float,POSE_SIZE> err = (gt-g_means[0]);



			//multiply(err,err,err);



			for(int n=0;n<POSE_SIZE;++n)



				err[n] = err[n]*err[n];



 



			float h_err = sqrt(err[0]+err[1]+err[2]);



			float a_err = sqrt(err[3]+err[4]+err[5]);



 



			cout << "Head error : " << h_err << " mm " << endl;



			cout << "Angle error : " << a_err <<" degrees " <<  endl;



 



			pt2d_gt[0] = depth_intrinsic[0]*gt[0]/gt[2] + depth_intrinsic[2];



			pt2d_gt[1] = depth_intrinsic[4]*gt[1]/gt[2] + depth_intrinsic[5];



 



		}



 



		pt2d_est[0] = depth_intrinsic[0]*g_means[0][0]/g_means[0][2] + depth_intrinsic[2];



		pt2d_est[1] = depth_intrinsic[4]*g_means[0][1]/g_means[0][2] + depth_intrinsic[5];



 



	}



	waitKey(0);



	return 0;



 



}
```

实验结果如下图所示

![img](https://img-blog.csdn.net/20161115165342402)![img](https://img-blog.csdn.net/20161115165359083)



这里说明一下，该代码并非我原创的，可以从github上下载的该代码，请点击[这里](https://github.com/peplin/head-pose-estimator)，不过下载的代码并非一个完整可运行的工程，需要配置一些环境变量Freeglut 2.8.1,修改部分代码才能运行，若是嫌麻烦的话，可以只直接下载我调试好的完整工程。请戳链接http://download.csdn.net/my/uploads下载。

#  





