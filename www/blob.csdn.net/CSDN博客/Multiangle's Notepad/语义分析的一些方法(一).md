
# 语义分析的一些方法(一) - Multiangle's Notepad - CSDN博客


2016年08月03日 18:49:56[multiangle](https://me.csdn.net/u014595019)阅读数：9949标签：[机器学习																](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)[nlp																](https://so.csdn.net/so/search/s.do?q=nlp&t=blog)[语义分析																](https://so.csdn.net/so/search/s.do?q=语义分析&t=blog)[自然语言处理																](https://so.csdn.net/so/search/s.do?q=自然语言处理&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=语义分析&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=nlp&t=blog)个人分类：[自然语言处理																](https://blog.csdn.net/u014595019/article/category/6183383)[机器学习&深度学习																](https://blog.csdn.net/u014595019/article/category/3051069)[
							](https://blog.csdn.net/u014595019/article/category/6183383)
所属专栏：[自然语言处理](https://blog.csdn.net/column/details/13461.html)
[
																	](https://so.csdn.net/so/search/s.do?q=nlp&t=blog)
[
				](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)
[
			](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)

**原文转自**[这里](http://www.flickering.cn/ads/2015/02/%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E7%9A%84%E4%B8%80%E4%BA%9B%E6%96%B9%E6%B3%95%E4%B8%80/)
语义分析，本文指运用各种机器学习方法，挖掘与学习文本、图片等的深层次概念。
wikipedia上的解释：In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents(or images)。
工作这几年，陆陆续续实践过一些项目，有搜索广告，社交广告，微博广告，品牌广告，内容广告等。要使我们广告平台效益最大化，首先需要理解用户，Context(将展示广告的上下文)和广告，才能将最合适的广告展示给用户。而这其中，就离不开对用户，对上下文，对广告的语义分析，由此催生了一些子项目，例如文本语义分析，图片语义理解，语义索引，短串语义关联，用户广告语义匹配等。
接下来我将写一写我所认识的语义分析的一些方法，虽说我们在做的时候，效果导向居多，方法理论理解也许并不深入，不过权当个人知识点总结，有任何不当之处请指正，谢谢。
本文主要由以下四部分组成：文本基本处理，文本语义分析，图片语义分析，语义分析小结。先讲述文本处理的基本方法，这构成了语义分析的基础。接着分文本和图片两节讲述各自语义分析的一些方法，值得注意的是，虽说分为两节，但文本和图片在语义分析方法上有很多共通与关联。最后我们简单介绍下语义分析在广点通“用户广告匹配”上的应用，并展望一下未来的语义分析方法。
# 1 文本基本处理
在讲文本语义分析之前，我们先说下文本基本处理，因为它构成了语义分析的基础。而文本处理有很多方面，考虑到本文主题，这里只介绍中文分词以及Term Weighting。
## 1.1中文分词
拿到一段文本后，通常情况下，首先要做分词。分词的方法一般有如下几种：
基于字符串匹配的分词方法。此方法按照不同的扫描方式，逐个查找词库进行分词。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分(即最短路径)；总之就是各种不同的启发规则。
全切分方法。它首先切分出与词库匹配的所有可能的词，再运用统计语言模型决定最优的切分结果。它的优点在于可以解决分词中的歧义问题。下图是一个示例，对于文本串“南京市长江大桥”，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型(例如n-gram)[18]找到最优路径，最后可能还需要命名实体识别。下图中“南京市 长江 大桥”的语言模型得分，即P(南京市，长江，大桥)最高，则为最优切分。
![](http://www.flickering.cn/wp-content/uploads/2015/01/rnnlm1.png)
图1. “南京市长江大桥”语言模型得分
由字构词的分词方法。可以理解为字的分类问题，也就是自然语言处理中的sequence labeling问题，通常做法里利用HMM，MAXENT，MEMM，CRF等预测文本串每个字的tag[62]，譬如B，E，I，S，这四个tag分别表示：beginning, inside, ending, single，也就是一个词的开始，中间，结束，以及单个字的词。 例如“南京市长江大桥”的标注结果可能为：“南(B)京(I)市(E)长(B)江(E)大(B)桥(E)”。由于CRF既可以像最大熵模型一样加各种领域feature，又避免了HMM的齐次马尔科夫假设，所以基于CRF的分词目前是效果最好的，具体请参考文献[61,62,63]。除了HMM，CRF等模型，分词也可以基于深度学习方法来做，如文献[9][10]所介绍，也取得了state-of-the-art的结果。
![](http://www.flickering.cn/wp-content/uploads/2015/01/word_segmentation1.png)
图2. 基于深度学习的中文分词
一个文本串除了分词，还需要做词性标注，命名实体识别，新词发现等。通常有两种方案，一种是pipeline approaches，就是先分词，再做词性标注；另一种是joint approaches，就是把这些任务用一个模型来完成。有兴趣可以参考文献[9][62]等。
一般而言，方法一和方法二在工业界用得比较多，方法三因为采用复杂的模型，虽准确率相对高，但耗时较大。
1.2 语言模型
前面在讲“全切分分词”方法时，提到了语言模型，并且通过语言模型，还可以引出词向量，所以这里把语言模型简单阐述一下。
语言模型是用来计算一个句子产生概率的概率模型，即P(w_1,w_2,w_3…w_m)，m表示词的总个数。
根据贝叶斯公式：P(w_1,w_2,w_3 … w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2) … P(w_m|w_1,w_2 … w_{m-1})。
最简单的语言模型是N-Gram，它利用马尔科夫假设，认为句子中每个单词只与其前n–1个单词有关，即假设产生w_m这个词的条件概率只依赖于前n–1个词，则有P(w_m|w_1,w_2…w_{m-1}) = P(w_m|w_{m-n+1},w_{m-n+2} … w_{m-1})。其中n越大，模型可区别性越强，n越小，模型可靠性越高。
N-Gram语言模型简单有效，但是它只考虑了词的位置关系，没有考虑词之间的相似度，词语法和词语义，并且还存在数据稀疏的问题，所以后来，又逐渐提出更多的语言模型，例如Class-based ngram model，topic-based ngram model，cache-based ngram model，skipping ngram model，指数语言模型（最大熵模型，条件随机域模型）等。若想了解更多请参考文章[18]。
最近，随着深度学习的兴起，神经网络语言模型也变得火热[4]。用神经网络训练语言模型的经典之作，要数Bengio等人发表的《A Neural Probabilistic Language Model》[3]，它也是基于N-Gram的，首先将每个单词w_{m-n+1},w_{m-n+2} … w_{m-1}映射到词向量空间，再把各个单词的词向量组合成一个更大的向量作为神经网络输入，输出是P(w_m)。本文将此模型简称为ffnnlm（Feed-forward Neural Net Language Model）。ffnnlm解决了传统n-gram的两个缺陷：(1)词语之间的相似性可以通过词向量来体现；(2)自带平滑功能。文献[3]不仅提出神经网络语言模型，还顺带引出了词向量，关于词向量，后文将再细述。
![](http://www.flickering.cn/wp-content/uploads/2015/01/ffnnlm1.png)
图3. 基于神经网络的语言模型
[
](http://www.flickering.cn/wp-content/uploads/2015/01/ffnnlm1.png)从最新文献看，目前state-of-the-art语言模型应该是基于循环神经网络(recurrent neural network)的语言模型，简称rnnlm[5][6]。循环神经网络相比于传统前馈神经网络，其特点是：可以存在有向环，将上一次的输出作为本次的输入。而rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n 要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息(例如下图中的context信息)可以在下一次预测里循环使用。
![](http://www.flickering.cn/wp-content/uploads/2015/01/simple_rnn.png)
图4. 基于simple RNN(time-delay neural network)的语言模型
[
](http://www.flickering.cn/wp-content/uploads/2015/01/simple_rnn.png)如上图所示，这是一个最简单的rnnlm，神经网络分为三层，第一层是输入层，第二层是隐藏层(也叫context层)，第三层输出层。 假设当前是t时刻，则分三步来预测P(w_m)：
[
](http://www.flickering.cn/wp-content/uploads/2015/01/simple_rnn.png)单词w_{m-1}映射到词向量，记作input(t)
连接上一次训练的隐藏层context(t–1)，经过sigmoid function，生成当前t时刻的context(t)
利用softmax function，预测P(w_m)
[
](http://www.flickering.cn/wp-content/uploads/2015/01/simple_rnn.png)参考文献[7]中列出了一个rnnlm的library，其代码紧凑。利用它训练中文语言模型将很简单，上面“南京市 长江 大桥”就是rnnlm的预测结果。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/simple_rnn.png)基于RNN的language model利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的vanishing gradient问题[55]（在RNN里，梯度计算随时间成指数倍增长或衰减，称之为Exponential Error Decay）。所以后来又提出基于LSTM(Long short term memory)的language model，LSTM也是一种RNN网络，关于LSTM的详细介绍请参考文献[54,49,52]。LSTM通过网络结构的修改，从而避免vanishing gradient问题。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/simple_rnn.png)
![](http://www.flickering.cn/wp-content/uploads/2015/01/lstm_unit.png)
图5. LSTM memory cell
[
](http://www.flickering.cn/wp-content/uploads/2015/01/lstm_unit.png)如上图所示，是一个LSTM unit。如果是传统的神经网络unit，output activation bi = activation_function(ai)，但LSTM unit的计算相对就复杂些了，它保存了该神经元上一次计算的结果，通过input gate，output gate，forget gate来计算输出，具体过程请参考文献[53，54]。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/lstm_unit.png)1.3 Term Weighting
[
](http://www.flickering.cn/wp-content/uploads/2015/01/lstm_unit.png)Term 权重计算
[
](http://www.flickering.cn/wp-content/uploads/2015/01/lstm_unit.png)对文本分词后，接下来需要对分词后的每个term计算一个权重，重要的term应该给与更高的权重。举例来说，“什么产品对减肥帮助最大？”的term weighting结果可能是: “什么 0.1，产品 0.5，对 0.1，减肥 0.8，帮助 0.3，最大 0.2”。Term weighting在文本检索，文本相关性，核心词提取等任务中都有重要作用。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/lstm_unit.png)Term weighting的打分公式一般由三部分组成：local，global和normalization [1,2]。即
TermWeight=L_{i,j} G_i N_j。L_{i,j}是term i在document j中的local weight，G_i是term i的global weight，N_j是document j的归一化因子。
常见的local，global，normalization weight公式[2]有：
![](http://www.flickering.cn/wp-content/uploads/2015/01/local_weight.png)
图6. Local weight formulas
[
](http://www.flickering.cn/wp-content/uploads/2015/01/local_weight.png)
![](http://www.flickering.cn/wp-content/uploads/2015/01/global_weight.png)
图7. Global weight formulas
[
](http://www.flickering.cn/wp-content/uploads/2015/01/global_weight.png)
![](http://www.flickering.cn/wp-content/uploads/2015/01/normlization_weight.png)
图8. Normalization factors
[
](http://www.flickering.cn/wp-content/uploads/2015/01/normlization_weight.png)Tf-Idf是一种最常见的term weighting方法。在上面的公式体系里，Tf-Idf的local weight是FREQ，glocal weight是IDFB，normalization是None。tf是词频，表示这个词出现的次数。df是文档频率，表示这个词在多少个文档中出现。idf则是逆文档频率，idf=log(TD/df)，TD表示总文档数。Tf-Idf在很多场合都很有效，但缺点也比较明显，以“词频”度量重要性，不够全面，譬如在搜索广告的关键词匹配时就不够用。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/normlization_weight.png)除了TF-IDF外，还有很多其他term weighting方法，例如Okapi，MI，LTU，ATC，TF-ICF[59]等。通过local，global，normalization各种公式的组合，可以生成不同的term weighting计算方法。不过上面这些方法都是无监督计算方法，有一定程度的通用性，但在一些特定场景里显得不够灵活，不够准确，所以可以基于有监督机器学习方法来拟合term weighting结果。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/normlization_weight.png)
![](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)
图9. Okapi计算公式
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)利用有监督机器学习方法来预测weight。这里类似于机器学习的分类任务，对于文本串的每个term，预测一个[0,1]的得分，得分越大则term重要性越高。既然是有监督学习，那么就需要训练数据。如果采用人工标注的话，极大耗费人力，所以可以采用训练数据自提取的方法，利用程序从搜索日志里自动挖掘。从海量日志数据里提取隐含的用户对于term重要性的标注，得到的训练数据将综合亿级用户的“标注结果”，覆盖面更广，且来自于真实搜索数据，训练结果与标注的目标集分布接近，训练数据更精确。下面列举三种方法(除此外，还有更多可以利用的方法)：
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)从搜索session数据里提取训练数据，用户在一个检索会话中的检索核心意图是不变的，提取出核心意图所对应的term，其重要性就高。
从历史短串关系资源库里提取训练数据，短串扩展关系中，一个term出现的次数越多，则越重要。
从搜索广告点击日志里提取训练数据，query与bidword共有term的点击率越高，它在query中的重要程度就越高。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)通过上面的方法，可以提取到大量质量不错的训练数据（数十亿级别的数据，这其中可能有部分样本不准确，但在如此大规模数据情况下，绝大部分样本都是准确的）。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)有了训练数据，接下来提取特征，基于逻辑回归模型来预测文本串中每个term的重要性。所提取的特征包括：
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)term的自解释特征，例如term专名类型，term词性，term idf，位置特征，term的长度等；
term与文本串的交叉特征，例如term与文本串中其他term的字面交叉特征，term转移到文本串中其他term的转移概率特征，term的文本分类、topic与文本串的文本分类、topic的交叉特征等。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)核心词、关键词提取
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)短文本串的核心词提取。对短文本串分词后，利用上面介绍的term weighting方法，获取term weight后，取一定的阈值，就可以提取出短文本串的核心词。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)长文本串(譬如web page)的关键词提取。这里简单介绍几种方法。想了解更多，请参考文献[69]。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)采用基于规则的方法。考虑到位置特征，网页特征等。
基于广告主购买的bidword和高频query建立多模式匹配树，在长文本串中进行全字匹配找出候选关键词，再结合关键词weight，以及某些规则找出优质的关键词。
类似于有监督的term weighting方法，也可以训练关键词weighting的模型。
基于文档主题结构的关键词抽取，具体可以参考文献[71]。
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)参考文献
[
](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)[Term-weighting approaches in automatic text retrieval，Gerard Salton et.](http://comminfo.rutgers.edu/~muresan/IR/Docs/Articles/ipmSalton1988.pdf)
[New term weighting formulas for the vector space method in information retrieval](http://www.sandia.gov/~tgkolda/pubs/pubfiles/ornl-tm-13756.pdf)
[A neural probabilistic language model 2003](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
[Deep Learning in NLP-词向量和语言模型](http://licstar.net/archives/328)
[Recurrent neural network based language models](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
Statistical Language Models based on Neural Networks，mikolov博士论文
[Rnnlm library](http://www.fit.vutbr.cz/~imikolov/rnnlm/)
[A survey of named entity recognition and classification](http://brown.cl.uni-heidelberg.de/~sourjiko/NER_Literatur/survey.pdf)
[Deep learning for Chinese word segmentation and POS tagging](http://www.aclweb.org/anthology/D13-1061)
[Max-margin tensor neural network for chinese word segmentation](http://aclweb.org/anthology/P14-1028)
[Learning distributed representations of concepts](http://www.cogsci.ucsd.edu/~ajyu/Teaching/Cogs202_sp12/Readings/hinton86.pdf)
[Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements](http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf)
[LightLda](http://arxiv.org/abs/1412.1576)
[word2vec](https://code.google.com/p/word2vec/)
[Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781v3.pdf)
[Deep Learning实战之word2vec](http://techblog.youdao.com/?p=915)
[word2vec中的数学原理详解](http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178)[出处2](http://blog.csdn.net/itplus/article/details/37969519)
[斯坦福课程-语言模型](http://52opencourse.com/111/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E5%9B%9B%E8%AF%BE-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88language-modeling%EF%BC%89)
[Translating Videos to Natural Language Using Deep Recurrent Neural Networks](http://arxiv.org/abs/1412.4729)
[Distributed Representations of Sentences and Documents](http://arxiv.org/pdf/1405.4053v2.pdf)
[Convolutional Neural Networks卷积神经网络](http://blog.csdn.net/zouxy09/article/details/8781543)
[A New, Deep-Learning Take on Image Recognition](http://research.microsoft.com/en-us/news/features/spp-102914.aspx)
[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](http://arxiv.org/pdf/1406.4729v1.pdf)
[A Deep Learning Tutorial: From Perceptrons to Deep Networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)
[Deep Learning for Computer Vision](http://cs.nyu.edu/~fergus/presentations/nips2013_final.pdf)
[Zero-shot leanring by convex combination of semantic embeddings](http://arxiv.org/pdf/1312.5650.pdf)
[Sequence to sequence learning with neural network](http://arxiv.org/pdf/1409.3215v3.pdf)
[Exploting similarities among language for machine translation](http://arxiv.org/pdf/1309.4168.pdf)
Grammar as Foreign Language Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton, arXiv 2014
[Deep Semantic Embedding](http://ceur-ws.org/Vol-1204/papers/paper_4.pdf)
张家俊. DNN Applications in NLP
[Deep learning for natural language processing and machine translation](http://cl.naist.jp/~kevinduh/notes/cwmt14tutorial.pdf)
Distributed Representations for Semantic Matching
distributed_representation_nlp
Deep Visual-Semantic Alignments for Generating Image Descriptions
[Convolutional Neural Networks for Sentence Classification](http://arxiv.org/pdf/1408.5882v2.pdf)
[Senna](http://ml.nec-labs.com/senna)
[ImageNet Large Scale Visual Recognition Challenge](http://arxiv.org/pdf/1409.0575v1.pdf)
Krizhevsky A, Sutskever I, Hinton G E. ImageNet Classification with Deep Convolutional Neural Networks
[Gradient-Based Learning Applied to Document Recognition](http://turing.iimas.unam.mx/~elena/CompVis/Lecun98.pdf)
Effetive use of word order for text categorization with convolutional neural network，Rie Johnson
[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/pdf/1406.1078.pdf)
[Show and Tell: A Neural Image Caption Generator](http://arxiv.org/pdf/1411.4555v1.pdf)
[Deep Image: Scaling up Image Recognition](http://arxiv.org/ftp/arxiv/papers/1501/1501.02876.pdf)
Large-Scale High-Precision Topic Modeling on Twitter
A. Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv:1404.5997, 2014
[A Brief Overview of Deep Learning](http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html)
Going deeper with convolutions. Christian Szegedy. Google Inc.[阅读笔记](http://www.gageet.com/2014/09203.php)
Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling
[Semi-Supervised Learning Tutorial](http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf)
http://www.zhihu.com/question/24904450
[LONG SHORT-TERM MEMORY BASED RECURRENT NEURAL NETWORK ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION](http://arxiv.org/pdf/1402.1128.pdf)
[LSTM Neural Networks for Language Modeling](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.248.4448&rep=rep1&type=pdf)
[LONG SHORT-TERM MEMORY](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)
Bengio, Y., Simard, P., Frasconi, P., “Learning long-term dependencies with gradient descent is difficult” IEEE Transactions on Neural Networks 5 (1994), pp. 157–166
[AliasLDA](http://www.sravi.org/pubs/fastlda-kdd2014.pdf)
[Gibbs sampling for the uninitiated](///Users/ling/Desktop/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf)
[Learning classifiers from only positive and unlabeled data](http://www.eecs.tufts.edu/~noto/pub/kdd08/elkan.kdd08.poster.pdf)
[TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams](http://cda.ornl.gov/publications/ICMLA06.pdf)
[LDA数学八卦](http://www.flickering.cn/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/2014/06/%E3%80%90lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6%E3%80%91%E7%A5%9E%E5%A5%87%E7%9A%84gamma%E5%87%BD%E6%95%B0/)
[Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields Models](http://www.aclweb.org/anthology/W06-0132)
[Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)
[Chinese Segmentation and New Word Detection using Conditional Random Fields](http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1091&context=cs_faculty_pubs)
[Gregor Heinrich. Parameter estimation for text analysis](http://www.arbylon.net/publications/text-est.pdf)
[Peacock：大规模主题模型及其在腾讯业务中的应用](http://km.oa.com/group/14352/articles/show/213192)
L. Yao, D. Mimno, and A. McCallum. Efficient methods for topic model inference on streaming document collections. In KDD, 2009.
[David Newman. Distributed Algorithms for Topic Models](http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf)
[Xuemin. LDA工程实践之算法篇](http://www.flickering.cn/nlp/2014/07/lda%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E4%B9%8B%E7%AE%97%E6%B3%95%E7%AF%87-1%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%AD%A3%E7%A1%AE%E6%80%A7%E9%AA%8C%E8%AF%81/)
[Brian Lott. Survey of Keyword Extraction Techniques](http://www.cs.unm.edu/~pdevineni/papers/Lott.pdf)
Yi Wang, Xuemin Zhao, Zhenlong Sun, Hao Yan, Lifeng Wang, Zhihui Jin, Liubin Wang, Yang Gao, Ching Law, and Jia Zeng. Peacock: Learning Long-Tail Topic Features for Industrial Applications. TIST’2015.
[刘知远. 基于文档主题结构的关键词抽取方法研究](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/phd_thesis.pdf)
[Hinton. Reducing the Dimensionality of Data with Neural Networks](http://www.cs.toronto.edu/~hinton/science.pdf)
[Samaneh Moghaddam. On the design of LDA models for aspect-based opinion mining](http://dl.acm.org/citation.cfm?id=2396863)；
The FLDA model for aspect-based opinion mining: addressing the cold start problem
[Ross Girshick et. Rich feature hierarchies for accurate object detection and semantic segmentation](http://www.cs.berkeley.edu/~rbg/papers/r-cnn-cvpr.pdf)
J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013.
[Baidu/UCLA: Explain Images with Multimodal Recurrent Neural Networks](http://arxiv.org/abs/1410.1090)
[Toronto: Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](http://arxiv.org/abs/1411.2539)
[Berkeley: Long-term Recurrent Convolutional Networks for Visual Recognition and Description](http://arxiv.org/abs/1411.4389)
[Xinlei Chen et. Learning a Recurrent Visual Representation for Image Caption Generation](http://arxiv.org/abs/1411.5654)
[Hao Fang et. From Captions to Visual Concepts and Back](http://arxiv.org/pdf/1411.4952v2)
[Modeling Documents with a Deep Boltzmann Machine](http://www.cs.toronto.edu/~nitish/uai13.pdf)
[A Deep Dive into Recurrent Neural Nets](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)
[Xiang zhang et. Text Understanding from Scratch](http://arxiv.org/abs/1502.01710?utm_source=dlvr.it&utm_medium=tumblr)
[            ](http://www.flickering.cn/wp-content/uploads/2015/01/okapi.png)
[
](http://www.flickering.cn/wp-content/uploads/2015/01/normlization_weight.png)
