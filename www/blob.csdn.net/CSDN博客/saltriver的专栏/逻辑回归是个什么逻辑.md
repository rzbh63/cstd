
# 逻辑回归是个什么逻辑 - saltriver的专栏 - CSDN博客


2017年03月19日 10:48:01[saltriver](https://me.csdn.net/saltriver)阅读数：6539


说到逻辑回归，可以先回顾下前期的文章《[线性回归](http://blog.csdn.net/saltriver/article/details/52423934)》。线性回归能够对连续值进行预测，如根据面积对房价进行预测。而在现实生活中，我们还有常见的另一类问题：分类问题。最简单的是二分类问题，即是与否的问题，如得病与否，交易是否合理，能否发放贷款，邮件是否垃圾邮件等。
逻辑回归（logistic regression），虽然名字上有“回归”两字，但它实际应用的是处理分类问题（classification）。它的核心思想是：如果回归的结果输出是一个连续值，而值的范围是无法限定的，那么想办法把这个连续结果值映射为可以帮助我们判断的结果值，从而进行分类。所以，从本质上讲，逻辑回归是在回归的基础上，进行了特殊的改进，而被用于分类问题上。
下面用一个最简单的例子来说明逻辑回归的使用过程。使用的是非常著名的[IRIS数据集](http://baike.baidu.com/link?url=DJ4b59CKkxgdsW5ldGFERmbi3FjSa3H6Ye5aAggUGB9wiAv0HG5otUFYzymhoGcHgU0FESy6IXmgOycRdh3FCa)，也称为鸢尾花数据集。下载地址为：[http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/)。数据集包含150条数据，每条数据包含4个属性，分别是花萼长度（sepal length）、花萼宽度（sepal width）、花瓣长度（petal length）、花瓣宽度（petal width），分为Setosa、Versicolour，Virginica这3个种类，每类50条数据。
![这里写图片描述](https://img-blog.csdn.net/20170319101949678?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FsdHJpdmVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
由于这个数据集是三分类问题，为了简便起见，重在理解逻辑回归的原理，这里对数据集进行了裁剪，只选取Setosa、Versicolour这两个种类进行二分类。
下面分别从策略、模型、算法三个方面给出问题解决框架。
**（1）模型**
模型就是所有学习的条件概率分布或决策函数。在这个实例中，我们已知4个影响戈尾花分类的变量花萼长度（sepal length）、花萼宽度（sepal width）、花瓣长度（petal length）、花瓣宽度（petal width），令其分别为x1,x2,x3,x4。我们构建的模型认为是这4个变量的线性组合，于是得到：

$$
z = {\theta _0} + {\theta _1}{x_1} + {\theta _2}{x_2} + {\theta _3}{x_3} + {\theta _4}{x_4}
$$
这里我们构建的是一个线性回归模型，前面提到，逻辑回归需要将线性模型进行一下映射，从而能用于分类。这里的映射函数或者叫分类器叫做sigmoid函数。详细的介绍见前期文章《[sigmoid函数](http://blog.csdn.net/saltriver/article/details/57531963)》。
通过sigmoid函数分类器，我们构建的逻辑回归模型是：

$$
P(y = 1|x;\theta ) = \frac{1}{{1 + {e^{ - z}}}} = \frac{1}{{1 + {e^{ - ({\theta _0} + {\theta _1}{x_1} + {\theta _2}{x_2} + {\theta _3}{x_3} + {\theta _4}{x_4})}}}} = \frac {1} {1 + e^{ - \theta ^Tx}}
$$
**（2）策略**
在模型确定后，需要用一个损失函数（loss function）或代价函数（cost function）来度量预测错误的程度。常用的损失函数有以下几种：
1）0-1损失函数：

$$
L(Y,f(X)) = \left\{ \begin{array}{l}
1,Y \ne f(X)\\
0,Y = f(X)
\end{array} \right.
$$
2）平方损失函数:

$$
L(Y,f(X)) = {(Y - f(X))^2}
$$
3）绝对损失函数:

$$
L(Y,f(X)) = |Y - f(X)|
$$
4）对数损失函数或对数似然损失函数：

$$
L(Y,P(Y|X)) =  - \log P(Y|X)
$$
对于逻辑回归模型，使用的是**对数损失函数作为代价函数**，至于为什么要选取这个损失函数，以后再说。则本例中，逻辑回归的损失函数为：

$$
\cos t(y,p(y|x)) = \left\{ \begin{array}{l}
 - \log p(y|x){\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} if{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} y = 1\\
 - \log (1 - p(y|x)){\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} if{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} y = 0
\end{array} \right.
$$
将上面的两个表达式合并，则得到单个数据点上的log损失为：

$$
\cos t(y,p(y|x)) =  - y\log p(y|x) - (1 - y)\log (1 - p(y|x))
$$
因为y只有两种取值情况，1或0，分别令y=1或y=0，即可得到原来的分段表达式，即两者是等价的。
全体样本的损失函数则可表达为：

$$
\cos t(y,p(y|x)) = \sum\limits_{i = 1}^m {( - {y_i}\log p({y_i}|{x_i}) - (1 - {y_i})\log (1 - p({y_i}|{x_i}))} )
$$
其中$p(y|x)$由前面的逻辑回归模型定义，令：

$$
p(y|x) = {h_\theta }(x) = \frac{1}{{1 + {e^{ - {\theta ^T}x}}}}
$$
则最终的损失函数为：

$$
\cos t(y,{h_\theta }(x)) = \sum\limits_{i = 1}^m {( - {y_i}\log \frac{1}{{1 + {e^{ - {\theta ^T}x}}}} - (1 - {y_i})\log (1 - \frac{1}{{1 + {e^{ - {\theta ^T}x}}}})} )
$$
**（3）算法**
算法是指学习模型的具体计算方法。在上述模型和损失函数定义后，剩下的就是基于训练集$({x_i},{y_i})$来求解模型中的参数$\theta$。于是该问题变成了一个求解最优化问题。如果最优化问题有显式的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。如何保证找到全局最优解，并使得求解过程非常的高效，就成为一个重要问题。
对于该优化问题，存在多种求解方法，比较常用的有梯度下降法、牛顿法、共轭梯度法，还有启发式算法，如模拟退火、遗传算法、粒子群算法等。可以参考前期文章《[梯度下降算法](http://blog.csdn.net/saltriver/article/details/52347726)》。这里不再赘述。
**（4）算例**
这里直接使用scikit-learn机器学习包进行计算：
```python
# -*- coding: utf-8 -*-
```
```python
from sklearn import datasets
import numpy as np
iris = datasets
```
```python
.load
```
```python
_iris()
```
```python
# 构建训练集和测试集
```
```python
iris_X_train = np
```
```python
.array
```
```python
(list(iris
```
```python
.data
```
```python
[:
```
```python
30
```
```python
]) + list(iris
```
```python
.data
```
```python
[
```
```python
50
```
```python
:
```
```python
80
```
```python
]))
iris_X_test = np
```
```python
.array
```
```python
(list(iris
```
```python
.data
```
```python
[
```
```python
30
```
```python
:
```
```python
50
```
```python
]) + list(iris
```
```python
.data
```
```python
[
```
```python
80
```
```python
:
```
```python
100
```
```python
]))
iris_Y_train = np
```
```python
.array
```
```python
(list(iris
```
```python
.target
```
```python
[:
```
```python
30
```
```python
]) + list(iris
```
```python
.target
```
```python
[
```
```python
50
```
```python
:
```
```python
80
```
```python
]))
iris_Y_test = np
```
```python
.array
```
```python
(list(iris
```
```python
.target
```
```python
[
```
```python
30
```
```python
:
```
```python
50
```
```python
]) + list(iris
```
```python
.target
```
```python
[
```
```python
80
```
```python
:
```
```python
100
```
```python
]))
from sklearn import linear_model
```
```python
# 构建模型
```
```python
logistic = linear_model
```
```python
.LogisticRegression
```
```python
()
```
```python
# 拟合数据
```
```python
logistic = logistic
```
```python
.fit
```
```python
(iris_X_train, iris_Y_train)
```
```python
# 显示参数
```
```python
print(logistic
```
```python
.coef
```
```python
_,logistic
```
```python
.intercept
```
```python
_)
```
```python
# 预测测试数据
```
```python
print(logistic
```
```python
.predict
```
```python
(iris_X_test))
```
```python
# 输出原始数据
```
```python
print(iris_Y_test)
```
输出结果为：
[[-0.32346426 -1.32886149 1.94671978 0.8778639 ]] [-0.23860313]
即各参数为：
${\theta _0} =-0.23860313$
${\theta _1} =-0.32346426$
${\theta _1} =-1.32886149$
${\theta _3} =1.94671978$
${\theta _4} =0.8778639$
最后逻辑回归的分类预测结果输出与测试集结果输出完全一致:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]

