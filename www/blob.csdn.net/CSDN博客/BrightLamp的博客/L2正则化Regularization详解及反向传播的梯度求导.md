
# L2正则化Regularization详解及反向传播的梯度求导 - BrightLamp的博客 - CSDN博客


2018年12月27日 16:48:28[BrightLampCsdn](https://me.csdn.net/oBrightLamp)阅读数：211



## 摘要
本文解释L2正则化Regularization, 求解其在反向传播中的梯度, 并使用TensorFlow和PyTorch验证.
## 相关
系列文章索引 :
[https://blog.csdn.net/oBrightLamp/article/details/85067981](https://blog.csdn.net/oBrightLamp/article/details/85067981)
## 正文
## 1. L2 正则原理
若某一个神经网络存在一个参数矩阵$W_{m\times n}$, 该网络在训练时输出一个损失值 error (标量$e_0$), 对 W 加上L2正则化项后的损失值为$e$. 已知$e_0$对 W 的梯度为$\nabla {e_0}_{(W)}$, 求 e 对 W 的梯度.
根据题意 :
$$
e = e_0+r\\
\;\\
r = \frac{\lambda}{2}\sum_{i=1}^{m}\sum_{j=1}^{n}w_{ij}^2
$$
其中$\lambda$为正则项衰减系数,$w_{ij}$为矩阵 W 的元素.
梯度求导 :
$$
\frac{de}{dW}=\frac{de_0}{dW} + \frac{dr}{dW}=\nabla {e_0}_{(W)} +\lambda W
$$
代入梯度下降公式, 加上正则化项前 :
$$
W^{(i+1)} = W^{(i)}-\eta\nabla {e_0}_{(W)}
$$
加上L2正则化项后 :
$$
W^{(i+1)} = W^{(i)}-\eta(\nabla {e_0}_{(W)}+\lambda W)= (1-\eta\lambda)W^{(i)}-\eta\nabla {e_0}_{(W)}
$$
我们可以看到, W 前面乘了一个小于 1 的系数.
故 L2 正则化又称为 L2 惩罚 (penalty) 或权值衰减 (Weight Decay).
## 2. 程序实现
### 2.1 TensorFlow
```python
import
```
```python
numpy
```
```python
as
```
```python
np
```
```python
import
```
```python
tensorflow
```
```python
as
```
```python
tf
tf
```
```python
.
```
```python
enable_eager_execution
```
```python
(
```
```python
)
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
seed
```
```python
(
```
```python
123
```
```python
)
```
```python
np
```
```python
.
```
```python
set_printoptions
```
```python
(
```
```python
8
```
```python
,
```
```python
suppress
```
```python
=
```
```python
True
```
```python
)
```
```python
x_numpy
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
random
```
```python
(
```
```python
(
```
```python
3
```
```python
,
```
```python
4
```
```python
)
```
```python
)
```
```python
.
```
```python
astype
```
```python
(
```
```python
np
```
```python
.
```
```python
double
```
```python
)
```
```python
x_tensor
```
```python
=
```
```python
tf
```
```python
.
```
```python
Variable
```
```python
(
```
```python
x_numpy
```
```python
)
```
```python
w_numpy
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
random
```
```python
(
```
```python
(
```
```python
4
```
```python
,
```
```python
5
```
```python
)
```
```python
)
```
```python
.
```
```python
astype
```
```python
(
```
```python
np
```
```python
.
```
```python
double
```
```python
)
```
```python
w_tensor
```
```python
=
```
```python
tf
```
```python
.
```
```python
Variable
```
```python
(
```
```python
w_numpy
```
```python
)
```
```python
weight_decay
```
```python
=
```
```python
0.9
```
```python
with
```
```python
tf
```
```python
.
```
```python
GradientTape
```
```python
(
```
```python
persistent
```
```python
=
```
```python
True
```
```python
)
```
```python
as
```
```python
tape
```
```python
:
```
```python
loss
```
```python
=
```
```python
tf
```
```python
.
```
```python
reduce_sum
```
```python
(
```
```python
tf
```
```python
.
```
```python
matmul
```
```python
(
```
```python
x_tensor
```
```python
,
```
```python
w_tensor
```
```python
)
```
```python
)
```
```python
loss2
```
```python
=
```
```python
loss
```
```python
+
```
```python
tf
```
```python
.
```
```python
reduce_sum
```
```python
(
```
```python
tf
```
```python
.
```
```python
square
```
```python
(
```
```python
w_tensor
```
```python
)
```
```python
)
```
```python
*
```
```python
weight_decay
grad
```
```python
=
```
```python
tape
```
```python
.
```
```python
gradient
```
```python
(
```
```python
loss
```
```python
,
```
```python
w_tensor
```
```python
)
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
grad2
```
```python
=
```
```python
tape
```
```python
.
```
```python
gradient
```
```python
(
```
```python
loss2
```
```python
,
```
```python
w_tensor
```
```python
)
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
print
```
```python
(
```
```python
"check_grad"
```
```python
)
```
```python
print
```
```python
(
```
```python
grad
```
```python
+
```
```python
2
```
```python
*
```
```python
w_numpy
```
```python
*
```
```python
weight_decay
```
```python
)
```
```python
print
```
```python
(
```
```python
grad2
```
```python
)
```
```python
"""
check_grad
[[ 2.6863001   2.00429027  2.61334972  3.22526179  2.22535517]
 [ 1.41717647  2.05815579  2.05865297  2.24328504  2.63034054]
 [ 2.85481325  2.65063599  2.85119176  2.13211971  2.20201325]
 [ 2.37606803  2.4938795   3.10095124  2.13098311  2.74585633]]
[[ 2.6863001   2.00429027  2.61334972  3.22526179  2.22535517]
 [ 1.41717647  2.05815579  2.05865297  2.24328504  2.63034054]
 [ 2.85481325  2.65063599  2.85119176  2.13211971  2.20201325]
 [ 2.37606803  2.4938795   3.10095124  2.13098311  2.74585633]]
"""
```
### 2.2 PyTorch
PyTorch 在 torch.optim.SGD 方法中使用下式实现L2正则化 :
$$
W^{(i+1)} = W^{(i)}-\eta G^{(i)}\\
\;\\
G^{(i)} =\frac{dL}{dW^{(i)}} + \lambda W^{(i)}\\
$$
$\frac{dL}{dW^{(i)}}$是 L2 正则化前的梯度,$\lambda$是权重衰减系数,$G^{(i)}$是更新梯度.
验证代码 :
```python
import
```
```python
torch
```
```python
import
```
```python
numpy
```
```python
as
```
```python
np
np
```
```python
.
```
```python
random
```
```python
.
```
```python
seed
```
```python
(
```
```python
123
```
```python
)
```
```python
np
```
```python
.
```
```python
set_printoptions
```
```python
(
```
```python
8
```
```python
,
```
```python
suppress
```
```python
=
```
```python
True
```
```python
)
```
```python
x_numpy
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
random
```
```python
(
```
```python
(
```
```python
3
```
```python
,
```
```python
4
```
```python
)
```
```python
)
```
```python
.
```
```python
astype
```
```python
(
```
```python
np
```
```python
.
```
```python
double
```
```python
)
```
```python
x_torch
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
x_numpy
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
x_torch2
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
x_numpy
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
w_numpy
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
random
```
```python
(
```
```python
(
```
```python
4
```
```python
,
```
```python
5
```
```python
)
```
```python
)
```
```python
.
```
```python
astype
```
```python
(
```
```python
np
```
```python
.
```
```python
double
```
```python
)
```
```python
w_torch
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
w_numpy
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
w_torch2
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
w_numpy
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
lr
```
```python
=
```
```python
0.1
```
```python
weight_decay
```
```python
=
```
```python
0.9
```
```python
sgd
```
```python
=
```
```python
torch
```
```python
.
```
```python
optim
```
```python
.
```
```python
SGD
```
```python
(
```
```python
[
```
```python
w_torch
```
```python
]
```
```python
,
```
```python
lr
```
```python
=
```
```python
lr
```
```python
,
```
```python
weight_decay
```
```python
=
```
```python
0
```
```python
)
```
```python
sgd2
```
```python
=
```
```python
torch
```
```python
.
```
```python
optim
```
```python
.
```
```python
SGD
```
```python
(
```
```python
[
```
```python
w_torch2
```
```python
]
```
```python
,
```
```python
lr
```
```python
=
```
```python
lr
```
```python
,
```
```python
weight_decay
```
```python
=
```
```python
weight_decay
```
```python
)
```
```python
y_torch
```
```python
=
```
```python
torch
```
```python
.
```
```python
matmul
```
```python
(
```
```python
x_torch
```
```python
,
```
```python
w_torch
```
```python
)
```
```python
y_torch2
```
```python
=
```
```python
torch
```
```python
.
```
```python
matmul
```
```python
(
```
```python
x_torch2
```
```python
,
```
```python
w_torch2
```
```python
)
```
```python
loss
```
```python
=
```
```python
y_torch
```
```python
.
```
```python
sum
```
```python
(
```
```python
)
```
```python
loss2
```
```python
=
```
```python
y_torch2
```
```python
.
```
```python
sum
```
```python
(
```
```python
)
```
```python
sgd
```
```python
.
```
```python
zero_grad
```
```python
(
```
```python
)
```
```python
sgd2
```
```python
.
```
```python
zero_grad
```
```python
(
```
```python
)
```
```python
loss
```
```python
.
```
```python
backward
```
```python
(
```
```python
)
```
```python
loss2
```
```python
.
```
```python
backward
```
```python
(
```
```python
)
```
```python
sgd
```
```python
.
```
```python
step
```
```python
(
```
```python
)
```
```python
sgd2
```
```python
.
```
```python
step
```
```python
(
```
```python
)
```
```python
w_grad
```
```python
=
```
```python
w_torch
```
```python
.
```
```python
grad
```
```python
.
```
```python
data
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
w_grad2
```
```python
=
```
```python
w_torch2
```
```python
.
```
```python
grad
```
```python
.
```
```python
data
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
print
```
```python
(
```
```python
"check_grad"
```
```python
)
```
```python
print
```
```python
(
```
```python
w_grad
```
```python
+
```
```python
weight_decay
```
```python
*
```
```python
w_numpy
```
```python
)
```
```python
print
```
```python
(
```
```python
w_grad2
```
```python
)
```
```python
"""
check_grad
[[ 2.29158508  1.95058016  2.25510989  2.56106592  2.06111261]
 [ 1.25926989  1.57975955  1.58000814  1.67232418  1.86585193]
 [ 2.20280346  2.10071483  2.20099271  1.84145669  1.87640346]
 [ 2.17063112  2.22953686  2.53307273  2.04808866  2.35552527]]
[[ 2.29158508  1.95058016  2.25510989  2.56106592  2.06111261]
 [ 1.25926989  1.57975955  1.58000814  1.67232418  1.86585193]
 [ 2.20280346  2.10071483  2.20099271  1.84145669  1.87640346]
 [ 2.17063112  2.22953686  2.53307273  2.04808866  2.35552527]]
 """
```

