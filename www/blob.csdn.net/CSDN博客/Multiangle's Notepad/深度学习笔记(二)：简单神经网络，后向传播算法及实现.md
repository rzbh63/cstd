
# 深度学习笔记(二)：简单神经网络，后向传播算法及实现 - Multiangle's Notepad - CSDN博客


2016年09月16日 22:31:45[multiangle](https://me.csdn.net/u014595019)阅读数：12951标签：[神经网络																](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)[深度学习																](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)[算法																](https://so.csdn.net/so/search/s.do?q=算法&t=blog)[分类算法																](https://so.csdn.net/so/search/s.do?q=分类算法&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=算法&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)个人分类：[机器学习&深度学习																](https://blog.csdn.net/u014595019/article/category/3051069)[算法																](https://blog.csdn.net/u014595019/article/category/6163865)[
							](https://blog.csdn.net/u014595019/article/category/3051069)
所属专栏：[深度学习&tensorflow笔记](https://blog.csdn.net/column/details/13414.html)
[
																	](https://so.csdn.net/so/search/s.do?q=深度学习&t=blog)
[
				](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)
[
			](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)

[深度学习笔记(一)：logistic分类](http://blog.csdn.net/u014595019/article/details/52554582)
[深度学习笔记(二)：简单神经网络，后向传播算法及实现](http://blog.csdn.net/u014595019/article/details/52557623)
[深度学习笔记(三)：激活函数和损失函数](http://blog.csdn.net/u014595019/article/details/52562159)
[深度学习笔记：优化方法总结(BGD,SGD,Momentum,AdaGrad,RMSProp,Adam)](http://blog.csdn.net/u014595019/article/details/52989301)
[深度学习笔记(四)：循环神经网络的概念，结构和代码注释](http://blog.csdn.net/u014595019/article/details/52571966)
[深度学习笔记(五)：LSTM](http://blog.csdn.net/u014595019/article/details/52605693)
[深度学习笔记(六)：Encoder-Decoder模型和Attention模型](http://blog.csdn.net/u014595019/article/details/52826423)
---
在之前的[深度学习笔记(一):logistic分类](http://blog.csdn.net/u014595019/article/details/52554582)中，已经描述了普通logistic回归以及如何将logistic回归用于多类分类。在这一节，我们再进一步，往其中加入隐藏层，构建出最简单的神经网络
---


# 2.简单神经网络及后向传播算法
---


## 2.1 大概描述和公式表达
神经网络的大概结构如图所示，
![这里写图片描述](https://img-blog.csdn.net/20160916202938313)
从左往右，分别是输入层，隐藏层，输出层，分别记为$\mathbf x$,$\mathbf h$,$\mathbf y$. 从输入层到隐藏层的矩阵记为$W_{hx}$, 偏置向量$b_h$; 从隐藏层到输出层的矩阵记为$W_{yh}$, 偏置向量为$b_y$. 那么根据之前logistic分类的公式稍作扩展，不难得到

$$
\begin{align}
&  \mathbf h_z = W_{hx}\mathbf x+\mathbf b_h \\
&  \mathbf h_a = \sigma(\mathbf h_z) \\
& \mathbf y_z = W_{yh}\mathbf h_a + \mathbf b_y \\
& \mathbf y_a = \sigma(\mathbf y_z)
 \end{align}
$$
其实就是两层logistic分类的堆叠，将前一个分类器的输出作为后一个的输入。得到输出$\mathbf y_a$以后的判断方法也比较类似，哪项最高就判定属于哪一类。真正值得写一下的是神经网络中的后向算法。按照传统的logistic分类，只能做到根据误差来更新$W_{yh}$和$\mathbf b_y$那么如何来更新从输入层到隐藏层的参数$W_{hx}$和$\mathbf b_h$呢？这就要用到后向算法了。所谓后向算法，就是指误差由输出层逐层往前传递，进而逐层更新参数矩阵和偏执向量。后向算法的核心其实就4个字：链式法则。首先来看$W_{yh}$和$\mathbf b_y$的更新

$$
\begin{align}
&   C =\frac{1}{2}(\mathbf y_a-\mathbf y)^2 \\
&   \frac{\partial C}{\partial \mathbf y_z} =  C'\sigma'(\mathbf y_z) 
																		=(\mathbf y_a-\mathbf y).\times\mathbf a.\times (1-\mathbf a) \\
&   \frac{\partial C}{\partial W_{yh}} =\frac{\partial C}{\partial \mathbf y_z} 
																	\frac{\partial \mathbf y_z}{\partial W_{yh}} =C'\sigma'(\mathbf y_z)\mathbf h_a^T \\
& \frac{\partial C}{\partial \mathbf b_y} =\frac{\partial C}{\partial \mathbf y_z}
\frac{\partial \mathbf y_z}{\partial \mathbf b_y}
																		 =C'\sigma'(\mathbf y_z)
\end{align}
$$
其实在上面的公式中，已经用到了链式法则。 类似的，可以得到

$$
\begin{align}
 &  \frac{\partial C}{\partial \mathbf h_a} = \frac{\partial C}{\partial \mathbf y_z}
																		 \frac{\partial \mathbf y_z}{\partial \mathbf h_a}
																		= W_{yh}^T[C'\sigma'(\mathbf y_z)]\\
& \frac{\partial C}{\partial W_{hx}} =  \frac{\partial C}{\partial \mathbf h_a}
																	 \frac{\partial \mathbf h_a}{\partial W}
															    = [\frac{\partial C}{\partial \mathbf h_a}
																    \sigma'(\mathbf h_z)]\mathbf x^T \\
& \frac{\partial C}{\partial \mathbf b_h} =  \frac{\partial C}{\partial \mathbf h_a}
																	 \frac{\partial \mathbf h_a}{\partial \mathbf b_h}
															    = [\frac{\partial C}{\partial \mathbf h_a}
																    \sigma'(\mathbf h_z)] \\
\end{align}
$$
可以看到，在$W_{hx}$和$\mathbf b_h$的计算中都用到了$\frac{\partial C}{\partial \mathbf h_a}$这可以看成由输出层传递到中间层的误差。那么在获得了各参数的偏导数以后，就可以对参数进行修正了

$$
\begin{align}
 & W_{yh} := W_{yh} - \eta\frac{\partial C}{\partial W_{yh}} \\
 & \mathbf b_y := \mathbf b_y - \eta\frac{\partial C}{\partial \mathbf b_y} \\
 & W_{hx} := W_{hx} - \eta\frac{\partial C}{\partial W_{hx}} \\
 & \mathbf b_h := \mathbf b_h - \eta\frac{\partial C}{\partial \mathbf b_h} \\
\end{align}
$$

---


## 2.2 神经网络的简单实现
为了加深印象，我自己实现了一个神经网络分类器，分类效果如下图所示
![这里写图片描述](https://img-blog.csdn.net/20160916222448503)
上图中，左上角显示的是实际的分类，右上角显示的是分类器判断出的各点分类。靠下的图显示的是分类器的判断准确率随迭代次数的变化情况。可以看到，经过训练以后，分类器的判断准确率还是可以的。
下面是代码部分
```python
import
```
```python
numpy
```
```python
as
```
```python
np
```
```python
import
```
```python
matplotlib.pyplot
```
```python
as
```
```python
plt
```
```python
import
```
```python
random
```
```python
import
```
```python
math
```
```python
# 构造各个分类
```
```python
def
```
```python
gen_sample
```
```python
()
```
```python
:
```
```python
data = []
    radius = [
```
```python
0
```
```python
,
```
```python
50
```
```python
]
```
```python
for
```
```python
i
```
```python
in
```
```python
range(
```
```python
1000
```
```python
):
```
```python
# 生成10k个点
```
```python
catg = random.randint(
```
```python
0
```
```python
,
```
```python
1
```
```python
)
```
```python
# 决定分类
```
```python
r = random.random()*
```
```python
10
```
```python
arg = random.random()*
```
```python
360
```
```python
len = r + radius[catg]
        x_c = math.cos(math.radians(arg))*len
        y_c = math.sin(math.radians(arg))*len
        x = random.random()*
```
```python
30
```
```python
+ x_c
        y = random.random()*
```
```python
30
```
```python
+ y_c
        data.append((x,y,catg))
```
```python
return
```
```python
data
```
```python
def
```
```python
plot_dots
```
```python
(data)
```
```python
:
```
```python
data_asclass = [[]
```
```python
for
```
```python
i
```
```python
in
```
```python
range(
```
```python
2
```
```python
)]
```
```python
for
```
```python
d
```
```python
in
```
```python
data:
        data_asclass[int(d[
```
```python
2
```
```python
])].append((d[
```
```python
0
```
```python
],d[
```
```python
1
```
```python
]))
    colors = [
```
```python
'r.'
```
```python
,
```
```python
'b.'
```
```python
,
```
```python
'r.'
```
```python
,
```
```python
'b.'
```
```python
]
```
```python
for
```
```python
i,d
```
```python
in
```
```python
enumerate(data_asclass):
```
```python
# print(d)
```
```python
nd = np.array(d)
        plt.plot(nd[:,
```
```python
0
```
```python
],nd[:,
```
```python
1
```
```python
],colors[i])
    plt.draw()
```
```python
def
```
```python
train
```
```python
(input, output, Whx, Wyh, bh, by)
```
```python
:
```
```python
"""
    完成神经网络的训练过程
    :param input:   输入列向量， 例如 [x,y].T
    :param output:  输出列向量, 例如[0,1,0,0].T
    :param Whx:     x->h 的参数矩阵
    :param Wyh:     h->y 的参数矩阵
    :param bh:      x->h 的偏置向量
    :param by:      h->y 的偏置向量
    :return:
    """
```
```python
h_z = np.dot(Whx, input) + bh
```
```python
# 线性求和
```
```python
h_a =
```
```python
1
```
```python
/(
```
```python
1
```
```python
+np.exp(-
```
```python
1
```
```python
*h_z))
```
```python
# 经过sigmoid激活函数
```
```python
y_z = np.dot(Wyh, h_a) + by
    y_a =
```
```python
1
```
```python
/(
```
```python
1
```
```python
+np.exp(-
```
```python
1
```
```python
*y_z))
    c_y = (y_a-output)*y_a*(
```
```python
1
```
```python
-y_a)
    dWyh = np.dot(c_y, h_a.T)
    dby = c_y
    c_h = np.dot(Wyh.T, c_y)*h_a*(
```
```python
1
```
```python
-h_a)
    dWhx = np.dot(c_h,input.T)
    dbh = c_h
```
```python
return
```
```python
dWhx,dWyh,dbh,dby,c_y
```
```python
def
```
```python
test
```
```python
(train_set, test_set, Whx, Wyh, bh, by)
```
```python
:
```
```python
train_tag = [int(x)
```
```python
for
```
```python
x
```
```python
in
```
```python
train_set[:,
```
```python
2
```
```python
]]
    test_tag = [int(x)
```
```python
for
```
```python
x
```
```python
in
```
```python
test_set[:,
```
```python
2
```
```python
]]
    train_pred = []
    test_pred = []
```
```python
for
```
```python
i,d
```
```python
in
```
```python
enumerate(train_set):
        input = train_set[i:i+
```
```python
1
```
```python
,
```
```python
0
```
```python
:
```
```python
2
```
```python
].T
        tag = predict(input,Whx,Wyh,bh,by)
        train_pred.append(tag)
```
```python
for
```
```python
i,d
```
```python
in
```
```python
enumerate(test_set):
        input = test_set[i:i+
```
```python
1
```
```python
,
```
```python
0
```
```python
:
```
```python
2
```
```python
].T
        tag = predict(input,Whx,Wyh,bh,by)
        test_pred.append(tag)
```
```python
# print(train_tag)
```
```python
# print(train_pred)
```
```python
train_err =
```
```python
0
```
```python
test_err =
```
```python
0
```
```python
for
```
```python
i
```
```python
in
```
```python
range(train_pred.__len__()):
```
```python
if
```
```python
train_pred[i]!=int(train_tag[i]):
            train_err +=
```
```python
1
```
```python
for
```
```python
i
```
```python
in
```
```python
range(test_pred.__len__()):
```
```python
if
```
```python
test_pred[i]!=int(test_tag[i]):
            test_err +=
```
```python
1
```
```python
# print(test_tag)
```
```python
# print(test_pred)
```
```python
train_ratio = train_err / train_pred.__len__()
    test_ratio = test_err / test_pred.__len__()
```
```python
return
```
```python
train_err,train_ratio,test_err,test_ratio
```
```python
def
```
```python
predict
```
```python
(input,Whx,Wyh,bh,by)
```
```python
:
```
```python
# print('-----------------')
```
```python
# print(input)
```
```python
h_z = np.dot(Whx, input) + bh
```
```python
# 线性求和
```
```python
h_a =
```
```python
1
```
```python
/(
```
```python
1
```
```python
+np.exp(-
```
```python
1
```
```python
*h_z))
```
```python
# 经过sigmoid激活函数
```
```python
y_z = np.dot(Wyh, h_a) + by
    y_a =
```
```python
1
```
```python
/(
```
```python
1
```
```python
+np.exp(-
```
```python
1
```
```python
*y_z))
```
```python
# print(y_a)
```
```python
tag = np.argmax(y_a)
```
```python
return
```
```python
tag
```
```python
if
```
```python
__name__==
```
```python
'__main__'
```
```python
:
    input_dim   =
```
```python
2
```
```python
output_dim  =
```
```python
2
```
```python
hidden_size =
```
```python
200
```
```python
Whx = np.random.randn(hidden_size, input_dim)*
```
```python
0.01
```
```python
Wyh = np.random.randn(output_dim, hidden_size)*
```
```python
0.01
```
```python
bh  = np.zeros((hidden_size,
```
```python
1
```
```python
))
    by  = np.zeros((output_dim,
```
```python
1
```
```python
))
    data = gen_sample()
    plt.subplot(
```
```python
221
```
```python
)
    plot_dots(data)
    ndata = np.array(data)
    train_set = ndata[
```
```python
0
```
```python
:
```
```python
800
```
```python
,:]
    test_set = ndata[
```
```python
800
```
```python
:
```
```python
1000
```
```python
,:]
    train_ratio_list = []
    test_ratio_list = []
```
```python
for
```
```python
times
```
```python
in
```
```python
range(
```
```python
10000
```
```python
):
        i = times%train_set.__len__()
        input = train_set[i:i+
```
```python
1
```
```python
,
```
```python
0
```
```python
:
```
```python
2
```
```python
].T
        tag = int(train_set[i,
```
```python
2
```
```python
])
        output = np.zeros((
```
```python
2
```
```python
,
```
```python
1
```
```python
))
        output[tag,
```
```python
0
```
```python
] =
```
```python
1
```
```python
dWhx,dWyh,dbh,dby,c_y = train(input,output,Whx,Wyh,bh,by)
```
```python
if
```
```python
times%
```
```python
100
```
```python
==
```
```python
0
```
```python
:
            train_err,train_ratio,test_err,test_ratio = test(train_set,test_set,Whx,Wyh,bh,by)
            print(
```
```python
'times:{t}\t train ratio:{tar}\t test ratio: {ter}'
```
```python
.format(tar=train_ratio,ter=test_ratio,t=times))
            train_ratio_list.append(train_ratio)
            test_ratio_list.append(test_ratio)
```
```python
for
```
```python
param, dparam
```
```python
in
```
```python
zip([Whx, Wyh, bh, by],
                                 [dWhx,dWyh,dbh,dby]):
            param -=
```
```python
0.01
```
```python
*dparam
```
```python
for
```
```python
i,d
```
```python
in
```
```python
enumerate(ndata):
        input = ndata[i:i+
```
```python
1
```
```python
,
```
```python
0
```
```python
:
```
```python
2
```
```python
].T
        tag = predict(input,Whx,Wyh,bh,by)
        ndata[i,
```
```python
2
```
```python
] = tag
    plt.subplot(
```
```python
222
```
```python
)
    plot_dots(ndata)
```
```python
# plt.figure()
```
```python
plt.subplot(
```
```python
212
```
```python
)
    plt.plot(train_ratio_list)
    plt.plot(test_ratio_list)
    plt.show()
```

