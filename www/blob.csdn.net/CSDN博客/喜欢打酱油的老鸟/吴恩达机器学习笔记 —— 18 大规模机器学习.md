
# 吴恩达机器学习笔记 —— 18 大规模机器学习 - 喜欢打酱油的老鸟 - CSDN博客


2018年08月05日 08:43:03[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：48标签：[吴恩达																](https://so.csdn.net/so/search/s.do?q=吴恩达&t=blog)[机器学习																](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=吴恩达&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)


[http://www.cnblogs.com/xing901022/p/9417633.html](http://www.cnblogs.com/xing901022/p/9417633.html)
本章讲了梯度下降的几种方式：batch梯度下降、mini-batch梯度下降、随机梯度下降。也讲解了如何利用mapreduce或者多cpu的思想加速模型的训练。
更多内容参考机器学习&深度学习
有的时候数据量会影响算法的结果，如果样本数据量很大，使用梯度下降优化参数时，一次调整参数需要计算全量的样本，非常耗时。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_wuenda18_1.png)
如果训练集和验证集的误差像左边的图形这样，就可以证明随着数据量的增加，将会提高模型的准确度。而如果像右边的图，那么增加样本的数量就没有什么意义了。
因此可以考虑缩小m的使用量，可以使用随机梯度下降。随机梯度下降的过程是：随机打散所有的样本，然后从第一个样本开始计算误差值，优化参数；遍历所有的样本。这样虽然优化的方向比较散乱，但是最终还是会趋于最优解。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_wuenda18_2.png)
还有一种方式叫做小批量梯度下降，每次使用一小部分的数据进行验证。比批量梯度下降更快，但是比随机梯度下降更稳定。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_wuenda18_3.png)
针对损失函数和batch的数量，可以画出下面的图：图1的震荡曲线可以忽略，此时的震荡可能是由于局部最小值造成的；图2如果增加数量能使得曲线更平滑，那么可以考虑增加batch的数量。图3 可能是模型根本没有在学习，可以考虑修改一下其他的参数。图4可能是因为学习太高，可以使用更小的学习率。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_wuenda18_4.png)
在线学习就是随着数据的获取，增量的来当做每个batch进行训练。
如果数据的样本很大，其实也可以通过map reduce的方式来进行并行处理，比如把数据切分成很多块，每个map运行完，统一在reduce端进行参数梯度下降学习。多CPU的情况下，也是同样的道理。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_wuenda18_5.png)

