# 机器翻译汇总

2018年02月02日 10:57:24 [kula147](https://me.csdn.net/kula147) 阅读数：2369



 版权声明：本文为博主原创文章，未经博主允许不得转载。	https://blog.csdn.net/kula147/article/details/79236585

斯坦福大学的NLP 机器翻译的项目 
<https://nlp.stanford.edu/projects/nmt/> 
tf中的NMT项目：<https://github.com/tensorflow/nmt>

最新进展 
Facebook AI Research论文《Convolutional Sequence to Sequence Learning》，该文章所提出的模型（简称ConvS2S）不仅仅在翻译任务上效果显著，而且所需训练时间也很短。 
<https://arxiv.org/abs/1705.03122> 
实现： 
pytorch版本：<https://github.com/facebookresearch/fairseq> 
Google团队论文《Attention is All You Need》，将机器翻译任务水平提高了2 BLEU。翻译效果上超越了ConvS2S，而且，其模型（Transformer）所需的训练时间也比ConvS2S要更短。 
<https://arxiv.org/pdf/1706.03762> 
实现： 
tf版本：<https://github.com/DongjunLee/transformer-tensorflow>

对两篇论文的理解： 
<https://zhuanlan.zhihu.com/p/27464080>

2017年神经机器翻译（NMT）的一些重要资源汇总： 
<https://www.zhihu.com/question/65340310/answer/229955388>