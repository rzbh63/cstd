
# 深度学习（上） - 蜗牛 - CSDN博客


2015年05月29日 19:20:14[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：2014


深度学习允许由多个处理层组成的计算模型来学习多个抽象层数据的表示。这些方法大大提高了目前最先进的语音识别，可视对象识别，目标检测和其他诸多领域如药物发现和基因组。深度学习发现大数据集结构很复杂，该结构使用BP算法来指示机器应该如何改变内部参数，这些参数是用于从前一层的表示来计算每层的表示。深度卷积网已经在处理图像，视频，语音和音频方面取得了突破，而递归网已经触及到连续数据，如文本和语音。
机器学习技术为现代化社会的许多方面提供了动力：从网络搜索到社交网络上的内容过滤到电子商务网站的推荐，并且越来越多的存在于消费类产品中，如照相机和智能手机。机器学习系统用于识别图像中的问题，语音转录成文本，匹配用户感兴趣的新闻、帖子或产品，选择相关的搜索结果。这些应用程序使用了一类称为深度学习的技术。
常规的机器学习技术用他们原始的形式处理自然数据的能力是有限的。几十年来，构成模式识别和机器学习系统需要细致的工程和大量的专业领域知识来设计特征提取器，它从学习子系统中转换原始数据（如图像的像素值）到合适的内部表示或特征向量，这通常是一个分类器，可以探测或分类输入模式。
表示学习是一套方法，这套方法可以将原始数据送给机器并且机器能自动发现需要检测或分类的表示。深度学习方法是多层次表示的表示方法，通过组合简单非线性的模块得到，每个模块变换一层（从原始输入开始）的表示到更高且稍微抽象的层。随着这种变换足够多的合成，可以学到很复杂的函数。对于分类任务，高层表示放大了输入层的样子，输入层对于判别非常重要并抑制了不相干的变化。例如，图像表示成像素值阵列的形式，第一层表示的学习特征通常表示图像中特定方向和位置是否存在边缘。第二层通常通过发现边缘的特定布置来检测图案，不论边缘位置的变化是多么小。第三层可能将图案组装成较大的对应于熟悉物体的部件组合，后续层将检测作为这些部件组合的对象。深度学习的关键方面是这些层的特征不是由人来设计的：它们使用一个通用的学习过程从数据中学到的。
许多年来，一直拒绝人工智能界来尝试解决的问题，现在利用深度学习取得了重大进展。它已被证明非常善于发现高维数据中复杂的结构，因此适用于科学，商业和政府的许多领域。除了在图像识别和语音识别中打破记录外，在预测潜在药物分子活性，分析粒子加速器数据，重构大脑电路和预测在基因序列上非编码DNA突变和疾病的影响上都打破了其他机器学习方法的记录。也许更奇怪的是，深度学习产生了在自然语言理解的各项任务中产生了非常有前景的结果，特别是话题分类，情感分析，问题解答和语言翻译。
我们认为，深度学习在不久的将来会有更多的成就，因为它需要很少的人工操作，所以在可用计算和数据量增加是它很容易处理。新的学习算法和架构（目前正在发展的是深度神经网络）只会加速这一进程。
## 监督学习
机器学习（不管是深度还是非深度）最常见的形式是监督学习。试想一下，我们要建立一个系统，它可以对包含房子，汽车，人或宠物的图片进行分类。首先，我们收集大量房子，汽车，人与宠物的图像数据集，每个图像都标有其类别。在训练期间，给机器展示图像并产生一个得分向量形式的输出。我们希望所需的类别在所有类别中得分最高，但是在训练之前这是不可能发生的。我们计算一个目标函数，它测量输出分数和期望的分数之间的误差（或距离）。然后该机器修改它的内部可调节参数来降低
这个误差。这些可调的参数（通常被称为权重）是实数，可以看作是定义机器输入-输出函数的“按钮”（knobs）。在典型的深度学习系统中，有数以亿计的调整权重和数以亿计的训练机器的标记实例。
为了正确地调整权重向量，学习算法计算一个梯度向量，该梯度向量表示方法是：对每个权重，当权重增加一个微小的量时误差增加或减少的量。然后在相反的方向上调整权重向量来适应梯度向量。
目标函数（求所有训练实例的平均）可以看作是高维空间中权值的丘陵地貌。负梯度向量指示下降速度最快的方向，接近最小值，其中输出误差平均来说是比较低的。
在实践中，大多数实践者使用一种称为随机梯度下降（SGD）的过程。这包括展示几个实例的输入向量，计算输出和误差，计算这些实例的平均梯度以及调整权重。该过程在训练集中得到的许多小实例集上一直重复进行，直到目标函数的平均值停止下降。因为每个小实例集给出了所有实例上平均梯度的噪声估计，所以称为随机。相比更复杂的最优化方法，这个简单的程序通常可以很快地找到一组好的权重。训练结束后，该系统的性能在一组不同的实例上（称为测试集）进行测量。这是用于测试机器的泛化能力 - 对训练阶段没有见过的新输入产生明显答案的能力。
当前许多机器学习的实际应用是在手工设计的特征上进行分类的。两类线性分类器计算特征向量成分的加权和。如果加权和高于阈值，输入就被分到它所属的类别。
从1960年以来，我们已经知道线性分类器只能刻出他们的输入空间到非常简单的区域，即由超平面分隔的半空间（half-spaces）。但是诸如图像和语音识别这样的问题需要对不相干输入变化不敏感的输入- 输出函数，如对象的位置，方向或明亮变化，或语音中音调或重音变化，而对特定的微小变化非常敏感（例如，一个白狼和跟狼一个品种的白狗“Samoyed”之间的差异）。在像素级上，不同姿势和环境中的两个Samoyeds图像可能非常不同，而相同姿势和环境中Samoyed和狼的两幅图像是非常相似的。在原始像素上操作的线性分类器或任何其他’浅’（shallow）分类器不可能辨别后两种，即便是能将前两者放到同一类别中。这就是为什么浅分类需要一个好的特征提取器，因为它可以解决选择不变困境（selectivity-invariance-dilema），其产生的用于选择图像的表示对判别是非常重要的，而对不相关方面（例如，动物的姿势）是不变的。为了使分类器更强大，可以使用通用的非线性特征，就像核方法一样，但通用的功能（如会出现高斯核的方法）不能让学习者从训练实例中很好的泛化。常规选择是手工设计好的特征提取器，这需要相当大的工程技术和领域的专业知识。但是，如果好的特征可以用通用学习过程自动学习的话，可以避免这样的问题。这是深度学习的主要优点。
深度学习架构是一个简单模块的多层堆叠，所有（或大部分）模块都需要学习，许多模块计算非线性输入-输出映射。堆叠中的每个模块变换输入来增加表示的选择性和不变性。一个带有多个非线性层（5〜20个深度）的系统可以实现极其复杂的输入函数，输入对微小的细节敏感（如区别白狼和Samoyeds），同时对非常不相关的变化不敏感（如背景，姿势，照明和周围物体）。
## 反向传播来训练多层架构
在早期的模式识别领域，研究人员的目的一直是用可训练的多层网络取代手工设计的特征，但是尽管它是简单的，这个方法一直没有被广泛理解，直到20世纪80年代中期。事实证明，多层架构可以通过简单的随机梯度下降来训练。只要模块是输入和内部权重的相对平滑函数，就可以用反向传播过程来训练梯度。这个想法（它是可以实现的并能够工作）是20世纪70年代和80年代期间几个小组独立发现的。
用反向传播过程来计算多层堆叠模块权重目标函数的梯度比导数链式法则的实际应用更加容易。关键的见解是，（相对于模块输入来说）目标的导数（或梯度）可以通过逆向操作（相对于模块或输入后续模块的输出来说）梯度计算得到（图1）。反向传播方程可以反复应用到整个模块的传播梯度上，从输出顶端（其中网络产生了预测）开始一路到底部（其中提供了外部输入）。一旦这些梯度被计算出来，那么计算相对于每个模块权重的梯度就是非常简单的。
许多深度学习应用使用前馈神经网络架构（图1），它学习将一个固定大小的输入（例如，一幅图像）映射到固定大小的输出（例如，每个类别的概率）。从一层到下一层，一组单元计算一个输入的加权和，该输出来自前一层。并且通过非线性函数传递结果。目前，最流行的非线性函数是修正线性单元（ReLU），这是简单的半波修正f（z）=max（z，0）。在过去的几十年里，神经网络使用平滑的非线性关系，如
tanh（z）或1 /（1+exp（-z）），但ReLU通常在多层网络中学习更快，在没有无监督预训练的情况下可以训练一个深度监督网络。不在输入或输出层的单元通常被称为隐层。隐层可以看作是用非线性的方式来变形输入使得在最后一层类别是线性可分的（图1）。
![这里写图片描述](https://img-blog.csdn.net/20150529170045206)[ ](https://img-blog.csdn.net/20150529170045206)
图1 |多层神经网络和反向传播。
a，多层神经网络（通过连接点显示）可以变形输入空间使得数据（实例在红线和蓝线上）的类别是线性可分的。注意，输入空间中规则的网格（如左图所示）也被隐层转化（中间平面显示）。这是只有两个输入单元，两个隐层单元和一个输出单元组成的例子，但用于目标识别或自然语言处理的网络包含几十万或几百万的单元。
b，导数的链式法则告诉我们两个小的影响（即x小的变化对y的影响，y的变化对z的影响）是怎么组成的。x上的小变化Δx首先转化成y的小变化Δy，通过乘以![这里写图片描述](https://img-blog.csdn.net/20150529184341380)[（即，偏导数的定义）。类似地，变化Δy创建了z上的变化Δz。将一个方程式带到其他式子中得到导数的链式法则 - Δx如何通过乘以](https://img-blog.csdn.net/20150529184341380)![这里写图片描述](https://img-blog.csdn.net/20150529184447135)[的乘积变成Δz。当x，y和z是向量时（导数是Jacobian矩阵）该法则依然成立。  ](https://img-blog.csdn.net/20150529184447135)
c，等式用于计算神经网络的正向传递，该网络有两个隐层和一个输出层，各层通过反向传播梯度组成一个模块。对每一层，我们首先计算总输入z，这是下层输出的加权和。然后一个非线性函数f（.）被施加到z上来获取单元的输出。为简单起见，我们省略偏置项。用于神经网络的非线性函数包括修正线性单元（ReLU）f（z）=max（0，z），近年来比较常用，还有更常规的sigmoids，如双曲正切，f（z）=（exp（z）-exp（-z））/（exp（z）+exp（-z））和逻辑函数，f（z）=1/（1+exp（-z））。
d，等式用于计算反向传递。对每个隐层，我们计算各单元输出的误差导数，这是相对于上层全部输入的误差导数加权和。然后，通过乘以f（z）的梯度，我们转换相对于输出的误差导数到相对于输入的误差导数。在输出层中，相对于输出单元的误差导数通过微分代价函数计算出来。如果单元l的代价函数为![这里写图片描述](https://img-blog.csdn.net/20150529190558699)[，那么误差导数是](https://img-blog.csdn.net/20150529190558699)![这里写图片描述](https://img-blog.csdn.net/20150529190640648)[，其中t1是目标值。一旦](https://img-blog.csdn.net/20150529190640648)![这里写图片描述](https://img-blog.csdn.net/20150529190721114)[已知，单元j到下层的连接权值wjk的误差导数是](https://img-blog.csdn.net/20150529190721114)![这里写图片描述](https://img-blog.csdn.net/20150529190916922)[。](https://img-blog.csdn.net/20150529190916922)
[
](https://img-blog.csdn.net/20150529190721114)在20世纪90年代后期，神经网络和反向传播被机器学习社区抛弃也被计算机视觉和语音识别社区忽视。大部分认为在具有很少先验知识的情况下，学习有用的，多阶段的特征提取是不可行的。尤其是，普遍认为简单的梯度下降会陷入局部最小的困境 - 权值配置不小的改变会降低平均误差。
[
](https://img-blog.csdn.net/20150529190721114)在实践中，对于大型网路来说局部最小是罕见的问题。无论初始条件是什么，系统几乎总能达到相似质量的解决方案。最近的理论和经验结果有力地表明局部最小值不是一个严重的问题。相反，曲线充满了大量的鞍点（梯度为0），并且向上弯曲的区域占据了大部分而小部分是向下弯曲的。分析似乎表明向下弯曲的部分出现了大量的鞍点，但他们几乎都有非常相似的目标函数值。因此，这些算法陷入的鞍点并不重要。
[
](https://img-blog.csdn.net/20150529190721114)深度前馈网络的兴趣在2006年左右开始恢复，这归因于加拿大高级研究所（CIFAR）汇集了一组研究人员。研究人员提出无监督的学习过程不需要标记数据就可以创建特征检测器。每层特征检测器学习的目标是能够重建或模拟下一层特征检测器（或原始输入）的活动。通过使用这个重建对象来“预训练”几层越来越复杂的特征检测器，深度网络的权重可以被初始化为合理值。然后最后一层输出单元添加到网络的顶部，使用标准的反向传播对整个深度系统进行微调。在识别手写数字或检测行人，尤其是标记数据的数量非常有限时该网络工作都非常出色。
[
](https://img-blog.csdn.net/20150529190721114)这种预训练方法的第一个主要应用是语音识别方面，并且快速图形处理单元（GPUs）的来临使它成为了可能，GPUs对于程序来说非常便利并且允许研究人员以10倍或20倍快的速度训练网络。在2009年，该方法被用来映射短时间窗口系数，该系数从声波到一组各种语音片段概率中提取出来的，这个语音片段可能由窗口中心的结构来表示。在一个标准语音识别基准测试上（用一个很小的词汇量）它打破了记录，并很快在大词汇量的任务中打破了记录。从2009年到2012年，许多主要语音研究组一直在开发深度网，并已经将其部署在Android手机上。对于较小的数据集，无监督的前期训练有助于防止过度拟合，这样的话在标记实例数较小或我们有许多“源”任务的实例但“目标”任务却很少时，可以到的非常好的泛化。一旦深度学习被修复（rehabilitated：平反，修复），原来的预训练阶段只需要较小的数据集。
[
](https://img-blog.csdn.net/20150529190721114)然而，出现了一种特定类型的深度前馈网络，这比相邻层之间全联接的网络更容易训练和泛化。这就是卷积神经网络（ConvNet）。在神经网络不受青睐那段时间里，它实现了许多实际的成就。最近，计算机视觉社区正在广泛的使用它。
[
](https://img-blog.csdn.net/20150529190721114)今天先写到这里，下一篇会接着介绍卷积神经网络等知识
to be continue……
[
						](https://img-blog.csdn.net/20150529190721114)
[
	](https://img-blog.csdn.net/20150529190721114)[
](https://img-blog.csdn.net/20150529190640648)
[
  ](https://img-blog.csdn.net/20150529190558699)