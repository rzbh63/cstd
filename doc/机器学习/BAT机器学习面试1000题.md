# BAT机器学习面试1000题

1 - 750 缺431-440, 651-655

有三点得强调下：

  1.虽然本系列主要是机器学习、深度学习相关的考题，其他类型的题不多，但不代表应聘机器学习或深度学习的岗位时，公司或面试官就只问这两项，虽说是做数据或AI相关，但基本的语言（比如Python）、编码coding能力（对于开发，编码coding能力怎么强调都不过分，比如最简单的手写快速排序、手写二分查找）、数据结构、算法、计算机体系结构、操作系统、概率统计等等也必须掌握。对于数据结构和算法，一者 重点推荐前面说的微软面试100题系列（后来这个系列整理成了新书《编程之法：面试和算法心得》），二者 多刷 leetcode，看1000道题不如实际动手刷100道。

  2.本系列会尽量让考察同一个部分（比如同是模型/算法相关的）、同一个方向（比如同是属于最优化的算法）的题整理到一块，为的是让大家做到举一反三、构建完整知识体系，在准备笔试面试的过程中，通过懂一题懂一片。

  3.本系列每一道题的答案都会确保逻辑清晰、通俗易懂（当你学习某个知识点感觉学不懂时，十有八九不是你不够聪明，十有八九是你所看的资料不够通俗、不够易懂），如有更好意见，欢迎在评论下共同探讨。

-------------------------------------------------------------我是分割线------------------------------------------------------------

【BAT机器学习面试1000题】

### 1.请简要介绍下SVM

  SVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。

  扩展：这里有篇文章详尽介绍了SVM的原理、推导，

http://blog.csdn.net/v_july_v/ ... 24837

。

  此外，这里有个视频也是关于SVM的推导：

http://www.julyedu.com/video/play/18/429

### 2.请简要介绍下tensorflow的计算图

  @寒小阳：Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。

[![2-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/92e77089047c6446a4bbb44c9844ed1c.jpg)](https://ask.julyedu.com/uploads/questions/20171019/92e77089047c6446a4bbb44c9844ed1c.jpg)

### 3.在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别。

  欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 $x=(x_1,\ldots,x_n)$  和 $y = (y_1,...,y_n)$ 之间的距离为：

$d(x, y):=\sqrt {(x_1-y_1)^2+(x_2-y_2)^2+\cdots+(x_n-y_n)^2} = \sqrt {\displaystyle \sum^{n}_{i=1}(x_i-y_i)^2}$ 

  曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：$|x_1-x_2| + |y_1-y_2|​$

[![3-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/46a80d55ea9eaa7e4b70b656bcfc495f.jpg)](https://ask.julyedu.com/uploads/questions/20171019/46a80d55ea9eaa7e4b70b656bcfc495f.jpg)

，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。

 

  通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。

  另，关于各种距离的比较参看

http://blog.csdn.net/v_july_v/ ... 03674

  通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。

  另，关于各种距离的比较参看

http://blog.csdn.net/v_july_v/ ... 03674

### 4.百度2015校招机器学习笔试题

参见

http://www.itmian4.com/thread-7042-1-1.html

### 5.关于LR

  @rickjin：把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。

[![5-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/bd4613ad1a23b51739650a46552b3361.jpg)](https://ask.julyedu.com/uploads/questions/20171019/bd4613ad1a23b51739650a46552b3361.jpg)

  另外，关于答案这篇文章可以做参考：

http://blog.csdn.net/cyh_24/ar ... .html

http://blog.csdn.net/zouxy09/a ... 19673

### 6.overfitting怎么解决？

  dropout、regularization、batch normalizatin

### 7.LR和SVM的联系与区别

  @朝阳在望，联系：

  1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）

 

  2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。

 

  区别： 1、LR是参数模型，SVM是非参数模型。

 

  2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。

 

  3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。

 

  4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。

 

  5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

  来源：

http://blog.csdn.net/timcompp/ ... 37986

### 8.说说你知道的核函数

通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：  

[![8-1.png](https://ask.julyedu.com/uploads/questions/20171019/91161a809b93bd9a2be904c30b28f771.png)](https://ask.julyedu.com/uploads/questions/20171019/91161a809b93bd9a2be904c30b28f771.png)

线性核

[![8-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/7170a7e54497f0c74573da104fab86b2.jpg)](https://ask.julyedu.com/uploads/questions/20171019/7170a7e54497f0c74573da104fab86b2.jpg)

，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了(意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)。

### 9.LR与线性回归的区别与联系

  @nishizhen：个人感觉逻辑回归和线性回归首先都是广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

  @乖乖癞皮狗：逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

### 10.请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么？

  关于决策树，这里有篇《决策树算法》。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文"Adaptive Boosting"（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。

  @Xijun LI：xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：

  1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数

  2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性

  3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的

  更多详见：

https://xijunlee.github.io/2017/06/03/

集成学习总结/

### 11.为什么xgboost要用泰勒展开，优势在哪里？

  @AntZ：xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得二阶倒数形式, 可以在不选定损失函数具体形式的情况下用于算法优化分析.本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性。

### 12.xgboost如何寻找最优特征？是又放回还是无放回的呢？

  @AntZ：xgboost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性. xgboost利用梯度优化模型算法, 样本是不放回的(想象一个样本连续重复抽出,梯度来回踏步会不会高兴). 但xgboost支持子采样, 也就是每轮计算可以不使用全部样本。

### 13.谈谈判别式模型和生成式模型？

  判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。

  生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。

  由生成模型可以得到判别模型，但由判别模型得不到生成模型。

  常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场

  常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机

### 14.L1和L2的区别

  L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。

 

  比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|.

  简单总结一下就是：

 

  L1范数: 为x向量各个元素绝对值之和。

 

  L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数

 

  Lp范数: 为x向量各个元素绝对值p次方和的1/p次方.

  在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。

 

  L1范数可以使权值稀疏，方便特征提取。

 

  L2范数可以防止过拟合，提升模型的泛化能力。

### 15.L1和L2正则先验分别服从什么分布

  @齐同学：面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。

### 16.CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？

  @许韩，来源：

https://zhuanlan.zhihu.com/p/25005808

Deep Learning -Yann LeCun, Yoshua Bengio & Geoffrey Hinton

Learn TensorFlow and deep learning, without a Ph.D.

The Unreasonable Effectiveness of Deep Learning -LeCun 16 NIPS Keynote

  以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。

[![16-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/9b43a050ffd2bb719ea7c5a8f0acfdc2.jpg)](https://ask.julyedu.com/uploads/questions/20171019/9b43a050ffd2bb719ea7c5a8f0acfdc2.jpg)

  CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。

局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：

[![16-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/064ef311ebec604640ac2b759c42f53b.jpg)](https://ask.julyedu.com/uploads/questions/20171019/064ef311ebec604640ac2b759c42f53b.jpg)

http://blog.csdn.net/v_july_v/ ... 12459

### 17.说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式。

[![17-1.png](https://ask.julyedu.com/uploads/questions/20171019/9a7ba14ae7754a2dead93511ae927350.png)](https://ask.julyedu.com/uploads/questions/20171019/9a7ba14ae7754a2dead93511ae927350.png)

  由上述式子可知，em <= 1/2时，am >= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。

  d. 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代

[![17-2.png](https://ask.julyedu.com/uploads/questions/20171019/a8611ac0b98394e95280ebf72a78c804.png)](https://ask.julyedu.com/uploads/questions/20171019/a8611ac0b98394e95280ebf72a78c804.png)

  更多请查看此文：

http://blog.csdn.net/v_july_v/ ... 18799

### 18.LSTM结构推导，为什么比RNN好？

  推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸

### 19.经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：

[![19-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/bbf594decd1ad85120709ed40810f697.jpg)](https://ask.julyedu.com/uploads/questions/20171019/bbf594decd1ad85120709ed40810f697.jpg)

  这叫做拼写检查。根据谷歌一员工写的文章(

http://norvig.com/spell-correct.htm

l)显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。

[![19-2.png](https://ask.julyedu.com/uploads/questions/20171019/4b1e8ab8238bb5eb32a9eda60ef5d3e3.png)](https://ask.julyedu.com/uploads/questions/20171019/4b1e8ab8238bb5eb32a9eda60ef5d3e3.png)

  P(c)表示某个正确的词的出现"概率"，它可以用"频率"代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P(c)就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。

  P(w|c)表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见

http://blog.csdn.net/v_july_v/ ... %23t4

  所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。具体的计算过程及此方法的缺陷请参见

http://norvig.com/spell-correct.html

### 20.为什么朴素贝叶斯如此“朴素”？

  因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

### 21.请大致对比下plsa和LDA的区别

[![21-1.png](https://ask.julyedu.com/uploads/questions/20171019/627b85f381928223949d8757fa6ee64b.png)](https://ask.julyedu.com/uploads/questions/20171019/627b85f381928223949d8757fa6ee64b.png)

  但在贝叶斯框架下的LDA中，我们不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。但一篇文档总得对应一个主题分布和一个词分布吧，怎么办呢？LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。

  文档d产生主题z（准确的说，其实是Dirichlet先验为文档d生成主题分布Θ，然后根据主题分布Θ产生主题z）的概率，主题z产生单词w的概率都不再是某两个确定的值，而是随机变量。

  还是再次举下文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布{ P(zi|d), i = 1,2,3 }可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，可能是P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，也有可能是P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6等等，而主题分布到底是哪个取值集合我们不确定（为什么？这就是贝叶斯派的核心思想，把未知参数当作是随机变量，不再认为是某一个确定的值），但其先验分布是dirichlet 分布，所以可以从无穷多个主题分布中按照dirichlet 先验随机抽取出某个主题分布出来。如下图所示（图截取自沈博PPT上）：

[![21-2.png](https://ask.julyedu.com/uploads/questions/20171019/39e67cd2087edca57ec9aad55e1cc093.png)](https://ask.julyedu.com/uploads/questions/20171019/39e67cd2087edca57ec9aad55e1cc093.png)

  综上，LDA真的只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布，只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。

更多请参见：

http://blog.csdn.net/v_july_v/ ... 09515

### 22.请简要说说EM算法

  @tornadomeet，本题解析来源：

http://www.cnblogs.com/tornadomeet/p/3395593.html

有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步：

  E步：选取一组参数，求出在该参数下隐含变量的条件概率值；

　　M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。

　　重复上面2步直至收敛。

　　公式如下所示：

[![22-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/83b4f094fbcb69416a0d145dde69af07.jpg)](https://ask.julyedu.com/uploads/questions/20171019/83b4f094fbcb69416a0d145dde69af07.jpg)

　　M步公式中下界函数的推导过程：

[![22-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/dfe7bc517885fe83878946e892e9afa5.jpg)](https://ask.julyedu.com/uploads/questions/20171019/dfe7bc517885fe83878946e892e9afa5.jpg)

　　EM算法一个常见的例子就是GMM模型，每个样本都有可能由k个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。

　　GMM的E步公式如下（计算每个样本对应每个高斯的概率）：

[![22-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/f5df020538abd8a286d6a4b2ad739e52.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f5df020538abd8a286d6a4b2ad739e52.jpg)

　　更具体的计算公式为：

[![22-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/905e5a14db7e74d5bcc002daf1d8ff8c.jpg)](https://ask.julyedu.com/uploads/questions/20171019/905e5a14db7e74d5bcc002daf1d8ff8c.jpg)

　　M步公式如下（计算每个高斯的比重，均值，方差这3个参数）：

[![22-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/ed7f9491a31620bb5b9cbacb1a486441.jpg)](https://ask.julyedu.com/uploads/questions/20171019/ed7f9491a31620bb5b9cbacb1a486441.jpg)

### 23.KNN中的K如何选取的？

　　关于什么是KNN，可以查看此文：

http://blog.csdn.net/v_july_v/ ... 03674

。KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：

　　1.如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；

　　2.如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。

　　3.K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。

　　在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。

### 24.防止过拟合的方法

 　　过拟合的原因是算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。

 

　　处理方法有：

a.早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练

b.数据集扩增：原有数据增加、原有数据加随机噪声、重采样

c.正则化

d.交叉验证

e.特征选择/特征降维

### 25.机器学习中，为何要经常对数据做归一化

　　@zhanlijun，本题解析来源：http://www.cnblogs.com/LBSer/p/4440590.html

　　机器学习模型被互联网行业广泛应用，如排序（参见http://www.cnblogs.com/LBSer/p/4439542.html）、推荐、反作弊、定位（参见http://www.cnblogs.com/LBSer/p/4020370.html）等。一般做机器学习应用的时候大部分时间是花费在特征处理上，其中很关键的一步就是对特征数据进行归一化，为什么要归一化呢？很多同学并未搞清楚，维基百科给出的解释：1）归一化后加快了梯度下降求最优解的速度；2）归一化有可能提高精度。下面再简单扩展解释下这两点。

　　1) 归一化为什么能提高梯度下降法求解最优解的速度？

　　斯坦福机器学习视频做了很好的解释：https://class.coursera.org/ml-003/lecture/21

　　如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；

　　而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。

　　因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。

[![25-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/1718077431c8e1bc7bf584ad11f0d00c.jpg)](https://ask.julyedu.com/uploads/questions/20171019/1718077431c8e1bc7bf584ad11f0d00c.jpg)

　　2) 归一化有可能提高精度

　　一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。

　　3) 归一化的类型

　　a. 线性归一化

[![25-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/07f2d7df9a9a69e1681cb28c3a5c3fbb.jpg)](https://ask.julyedu.com/uploads/questions/20171019/07f2d7df9a9a69e1681cb28c3a5c3fbb.jpg)

　　这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。

　　b. 标准差标准化

　　经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：

[![25-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/fc72ea4e5fac36385b6bcc5df6e7ef38.jpg)](https://ask.julyedu.com/uploads/questions/20171019/fc72ea4e5fac36385b6bcc5df6e7ef38.jpg)

　　其中μ为所有样本数据的均值，σ为所有样本数据的标准差。

　　c.非线性归一化

　　经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。

### 26.谈谈深度学习中的归一化问题

[![26-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/23edca99e015b1149122ff069ddf55a1.jpg)](https://ask.julyedu.com/uploads/questions/20171019/23edca99e015b1149122ff069ddf55a1.jpg)

　　详情参见此视频：

http://www.julyedu.com/video/play/69/686

### 27.哪些机器学习算法不需要做归一化处理？

　　概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。

　　@管博士：我理解归一化和标准化主要是为了使计算更方便 比如两个变量的量纲不同 可能一个的数值远大于另一个那么他们同时作为变量的时候 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能 量纲也需要调整 所以我估计lr 和 knn 保准话一下应该有好处。至于其他的算法 我也觉得如果变量量纲差距很大的话 先标准化一下会有好处。

　　@寒小阳：一般我习惯说树形模型，这里说的概率模型可能是差不多的意思。

### 28.对于树形结构为什么不需要归一化？

　　数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。

　　另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。

### 29.数据归一化（或者标准化，注意归一化和标准化不同）的原因

　　@我愛大泡泡，来源：http://blog.csdn.net/woaidapao ... 06273

　　要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。

　　有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。

　　有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。

　　补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。

### 30.请简要说说一个完整机器学习项目的流程

  @寒小阳、龙心尘

  1 抽象成数学问题

  明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。

  这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。

  2 获取数据

  数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。

  数据要有代表性，否则必然会过拟合。

  而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。

  而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

  3 特征预处理与特征选择

  良好的数据要能够提取出良好的特征才能真正发挥效力。

特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。

  筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

  4 训练模型与调优

  直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。

  5 模型诊断

  如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。

过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。

  误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……

诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

  6 模型融合

  一般来说，模型融合后都能使得效果有一定提升。而且效果很好。

工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

  7 上线运行

  这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。

  这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。

  故，基于此，七月在线每一期ML算法班都特此增加特征工程、模型调优等相关课。比如，这里有个公开课视频

http://www.julyedu.com/video/play/18/186

### 31.逻辑斯特回归为什么要对特征进行离散化

  @严林，本题解析来源：

https://www.zhihu.com/question/31989952

  在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：

  0. 离散特征的增加和减少都很容易，易于模型的快速迭代；

  1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；

  2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；

  3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；

  4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；

  5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；

  6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

  李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

### 32.new 和 malloc的区别

  @Sommer_Xia，来源：

http://blog.csdn.net/shymi1991 ... 32775

  1. malloc与free是C++/C语言的标准库函数，new/delete是C++的运算符。它们都可用于申请动态内存和释放内存。

  2. 对于非内部数据类型的对象而言，光用maloc/free无法满足动态对象的要求。对象在创建的同时要自动执行构造函数，对象在消亡之前要自动执行析构函数。由于malloc/free是库函数而不是运算符，不在编译器控制权限之内，不能够把执行构造函数和析构函数的任务强加于malloc/free。

  3. 因此C++语言需要一个能完成动态内存分配和初始化工作的运算符new，以一个能完成清理与释放内存工作的运算符delete。注意new/delete不是库函数。

  4. C++程序经常要调用C函数，而C程序只能用malloc/free管理动态内存

### 33.hash 冲突及解决办法

  @Sommer_Xia，来源：

http://blog.csdn.net/shymi1991 ... 32775

  关键字值不同的元素可能会映象到哈希表的同一地址上就会发生哈希冲突。解决办法：

  1）开放定址法：当冲突发生时，使用某种探查(亦称探测)技术在散列表中形成一个探查(测)序列。沿此序列逐个单元地查找，直到找到给定 的关键字，或者碰到一个开放的地址(即该地址单元为空)为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。查找时探查到开放的地址则表明表中无待查的关键字，即查找失败。

  2）再哈希法：同时构造多个不同的哈希函数。

  3）链地址法：将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。

  4）建立公共溢出区：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。

### 34.下列哪个不属于CRF模型对于HMM和MEMM模型的优势（B ）

A. 特征灵活 B. 速度快 C. 可容纳较多上下文信息 D. 全局最优

  首先，CRF，HMM(隐马模型)，MEMM(最大熵隐马模型)都常用来做序列标注的建模.

隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择

  最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉

  条件随机场则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。

### 35.什么是熵

  从名字上来看，熵给人一种很玄乎，不知道是啥的感觉。其实，熵的定义很简单，即用来表示随机变量的不确定性。之所以给人玄乎的感觉，大概是因为为何要取这样的名字，以及怎么用。

  熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。

熵的引入

  事实上，熵的英文原文为entropy，最初由德国物理学家鲁道夫·克劳修斯提出，其表达式为：$\Delta S=\frac Q T$

  它表示一个系系统在不受外部干扰时，其内部最稳定的状态。后来一中国学者翻译entropy时，考虑到entropy是能量Q跟温度T的商，且跟火有关，便把entropy形象的翻译成“熵”。

  我们知道，任何粒子的常态都是随机运动，也就是"无序运动"，如果让粒子呈现"有序化"，必须耗费能量。所以，温度（热能）可以被看作"有序化"的一种度量，而"熵"可以看作是"无序化"的度量。

  如果没有外部能量输入，封闭系统趋向越来越混乱（熵越来越大）。比如，如果房间无人打扫，不可能越来越干净（有序化），只可能越来越乱（无序化）。而要让一个系统变得更有序，必须有外部能量的输入。

  1948年，香农Claude E. Shannon引入信息（熵），将其定义为离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。

  更多请查看

http://blog.csdn.net/v_july_v/ ... 08465

### 36.熵、联合熵、条件熵、相对熵、互信息的定义

  为了更好的理解，需要了解的概率必备知识有：

  1.大写字母X表示随机变量，小写字母x表示随机变量X的某个具体的取值；

  2.P(X)表示随机变量X的概率分布，P(X,Y)表示随机变量X、Y的联合概率分布，P(Y|X)表示已知随机变量X的情况下随机变量Y的条件概率分布；

  3.p(X = x)表示随机变量X取某个具体值的概率，简记为p(x)；

  4.p(X = x, Y = y) 表示联合概率，简记为p(x,y)，p(Y = y|X = x)表示条件概率，简记为p(y|x)，且有：p(x,y) = p(x) * p(y|x)。

  熵：如果一个随机变量X的可能取值为X = {x1, x2,…, xk}，其概率分布为P(X = xi) = pi（i = 1,2, ..., n），则随机变量X的熵定义为：$H(X)=- \displaystyle \sum_x p(x)log\ p(x) ​$

  把最前面的负号放到最后，便成了：$H(X)=\displaystyle \sum_x p(x) log\ \frac{1}{p(x)}​$

  上面两个熵的公式，无论用哪个都行，而且两者等价，一个意思（这两个公式在下文中都会用到）。

  联合熵：两个随机变量X，Y的联合分布，可以形成联合熵Joint Entropy，用H(X,Y)表示。

  条件熵：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。

  且有此式子成立：$H(Y|X) = H(X,Y) – H(X)$，整个式子表示$(X,Y)$发生所包含的熵减去X单独发生包含的熵。至于怎么得来的请看推导：

[![36-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/477252ebbb5473420a026293da4109bc.jpg)](https://ask.julyedu.com/uploads/questions/20171019/477252ebbb5473420a026293da4109bc.jpg)
$$
\begin{align} &H(X,Y)-H(X) \\ 
&=-\sum_{x,y}p(x,y)log\ p(x,y)+\sum_x p(x)log\ p(x)\\
&=-\sum_{x,y}p(x,y)log\ p(x,y)+\sum_x(\sum_yp(x,y))log\ p(x)\\
&=-\sum_{x,y}p(x,y)log\ p(x,y)+\sum_{x,y}p(x,y)log\ p(x)\\
&=-\sum_{x,y}p(x,y)log\frac{p(x,y)}{p(x)}\\
&=-\sum_{x,y}p(x,y)log\ p(y|x)\\
\end{align}
$$


  简单解释下上面的推导过程。整个式子共6行，其中

  第二行推到第三行的依据是边缘分布p(x)等于联合分布p(x,y)的和；

  第三行推到第四行的依据是把公因子logp(x)乘进去，然后把x,y写在一起；

  第四行推到第五行的依据是：因为两个sigma都有p(x,y)，故提取公因子p(x,y)放到外边，然后把里边的-（log p(x,y) - log p(x)）写成- log (p(x,y)/p(x) ) ；

  第五行推到第六行的依据是：p(x,y) = p(x) * p(y|x)，故p(x,y) / p(x) = p(y|x)。

  相对熵：又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等。设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是：

[![36-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/c571abd846cd5742571e367b6c9b9f96.jpg)](https://ask.julyedu.com/uploads/questions/20171019/c571abd846cd5742571e367b6c9b9f96.jpg)
$$
D(p||q)=\sum_x p(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}
$$


  在一定程度上，相对熵可以度量两个随机变量的“距离”，且有$D(p||q) ≠D(q||p)$。另外，值得一提的是，$D(p||q)$是必然大于等于0的。

  互信息：两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，用I(X,Y)表示：

[![36-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/5f0dd26c5f6757bf1d559bcfc5405b63.jpg)](https://ask.julyedu.com/uploads/questions/20171019/5f0dd26c5f6757bf1d559bcfc5405b63.jpg)
$$
I(X,Y)=\sum_{x,y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
$$


  且有$I(X,Y)=D(P(X,Y) || P(X)P(Y))$。下面，咱们来计算下$H(Y)-I(X,Y)$的结果，如下：

[![36-6.jpg](https://ask.julyedu.com/uploads/questions/20171019/0d74a163ed3933161f789ab033e2be64.jpg)](https://ask.julyedu.com/uploads/questions/20171019/0d74a163ed3933161f789ab033e2be64.jpg)

  通过上面的计算过程，我们发现竟然有$H(Y)-I(X,Y) = H(Y|X)$。故通过条件熵的定义，有：$H(Y|X) = H(X,Y) - H(X)$，而根据互信息定义展开得到$H(Y|X) = H(Y) - I(X,Y)$，把前者跟后者结合起来，便有$I(X,Y)= H(X) + H(Y) - H(X,Y)$，此结论被多数文献作为互信息的定义。更多请查看

http://blog.csdn.net/v_july_v/ ... 08465

### 37.什么是最大熵

  熵是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0。如果没有外界干扰，随机变量总是趋向于无序，在经过足够时间的稳定演化，它应该能够达到的最大程度的熵。

  为了准确的估计随机变量的状态，我们一般习惯性最大化熵，认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。换言之，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，其原则是承认已知事物（知识），且对未知事物不做任何假设，没有任何偏见。

  例如，投掷一个骰子，如果问"每个面朝上的概率分别是多少"，你会说是等概率，即各点出现的概率均为1/6。因为对这个"一无所知"的色子，什么都不确定，而假定它每一个朝上概率均等则是最合理的做法。从投资的角度来看，这是风险最小的做法，而从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。

  3.1 无偏原则

  下面再举个大多数有关最大熵模型的文章中都喜欢举的一个例子。

  例如，一篇文章中出现了“学习”这个词，那这个词是主语、谓语、还是宾语呢？换言之，已知“学习”可能是动词，也可能是名词，故“学习”可以被标为主语、谓语、宾语、定语等等。

  令x1表示“学习”被标为名词， x2表示“学习”被标为动词。

  令y1表示“学习”被标为主语， y2表示被标为谓语， y3表示宾语， y4表示定语。

  且这些概率值加起来的和必为1，即，$p(x_1)+p(x_2)=1​$， 则根据无偏原则，认为这个分布中取各个值的概率是相等的，故得到：

[![37-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/18e4ec1100df8e1a5dbb22e4699c7927.jpg)](https://ask.julyedu.com/uploads/questions/20171019/18e4ec1100df8e1a5dbb22e4699c7927.jpg)

[![37-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/4de938bd66434d37dd3f5314b3b23da6.jpg)](https://ask.julyedu.com/uploads/questions/20171019/4de938bd66434d37dd3f5314b3b23da6.jpg)
$$
p(x_1) = p(x_2) = 0.5\\
p(y_1)=p(y_2)=p(y_3)=p(y_4)=0.25
$$
  因为没有任何的先验知识，所以这种判断是合理的。如果有了一定的先验知识呢？

  即进一步，若已知：“学习”被标为定语的可能性很小，只有0.05，即$p(y_4)=0.05$

[![37-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/238e765530bc0538ca0aeba77a0133e8.jpg)](https://ask.julyedu.com/uploads/questions/20171019/238e765530bc0538ca0aeba77a0133e8.jpg)

，剩下的依然根据无偏原则，可得：

[![37-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/a27095f99f4e0151186266121f61423b.jpg)](https://ask.julyedu.com/uploads/questions/20171019/a27095f99f4e0151186266121f61423b.jpg)

[![37-6.jpg](https://ask.julyedu.com/uploads/questions/20171019/7823fbc5065cd7748769d8b8c5a6bc5f.jpg)](https://ask.julyedu.com/uploads/questions/20171019/7823fbc5065cd7748769d8b8c5a6bc5f.jpg)

  再进一步，当“学习”被标作名词x1的时候，它被标作谓语y2的概率为0.95，即

[![37-7.jpg](https://ask.julyedu.com/uploads/questions/20171019/a912c7be8d706df1e1a7ca70cedc5562.jpg)](https://ask.julyedu.com/uploads/questions/20171019/a912c7be8d706df1e1a7ca70cedc5562.jpg)

，此时仍然需要坚持无偏见原则，使得概率分布尽量平均。但怎么样才能得到尽量无偏见的分布？

  实践经验和理论计算都告诉我们，在完全无约束状态下，均匀分布等价于熵最大（有约束的情况下，不一定是概率相等的均匀分布。 比如，给定均值和方差，熵最大的分布就变成了正态分布 ）。

  于是，问题便转化为了：计算X和Y的分布，使得H(Y|X)达到最大值，并且满足下述条件：

[![37-8.jpg](https://ask.julyedu.com/uploads/questions/20171019/1a67deab6eba50ad4faff257e93fa70b.jpg)](https://ask.julyedu.com/uploads/questions/20171019/1a67deab6eba50ad4faff257e93fa70b.jpg)

  因此，也就引出了最大熵模型的本质，它要解决的问题就是已知X，计算Y的概率，且尽可能让Y的概率最大（实践中，X可能是某单词的上下文信息，Y是该单词翻译成me，I，us、we的各自概率），从而根据已有信息，尽可能最准确的推测未知信息，这就是最大熵模型所要解决的问题。

  相当于已知X，计算Y的最大可能的概率，转换成公式，便是要最大化下述式子H(Y|X)：

[![37-9.jpg](https://ask.julyedu.com/uploads/questions/20171019/7e49b314b9f89d60d0889371e9bfcb47.jpg)](https://ask.julyedu.com/uploads/questions/20171019/7e49b314b9f89d60d0889371e9bfcb47.jpg)

  且满足以下4个约束条件：

[![37-10.jpg](https://ask.julyedu.com/uploads/questions/20171019/4b0bd8422166937e66c6202de8fc877d.jpg)](https://ask.julyedu.com/uploads/questions/20171019/4b0bd8422166937e66c6202de8fc877d.jpg)

### 38.简单说下有监督学习和无监督学习的区别

  有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）

  无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)

### 39.了解正则化么

  正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。

  奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。

### 40.协方差和相关性有什么区别？

  相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。

### 41.线性分类器与非线性分类器的区别以及优劣

  如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。

  常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归

  常见的非线性分类器：决策树、RF、GBDT、多层感知机

  SVM两种都有(看线性核还是高斯核)

  线性分类器速度快、编程方便，但是可能拟合效果不会很好

  非线性分类器编程复杂，但是效果拟合能力强

### 42.数据的逻辑存储结构（如数组，队列，树等）对于软件开发具有十分重要的影响，试对你所了解的各种存储结构从运行速度、存储效率和适用场合等方面进行简要地分析。

 

运行速度 存储效率 适用场合

数组 快 高 比较适合进行查找操作，还有像类似于矩阵等的操作

链表 较快 较高 比较适合增删改频繁操作，动态的分配内存

队列 较快 较高 比较适合进行任务类等的调度

栈 一般 较高 比较适合递归类程序的改写

二叉树（树） 较快 一般 一切具有层次关系的问题都可用树来描述

图 一般 一般 除了像最小生成树、最短路径、拓扑排序等经典用途。还被用于像神经网络等人工智能领域等等。

### 43.什么是分布式数据库？

分布式数据库系统是在集中式数据库系统成熟技术的基础上发展起来的，但不是简单地把集中式数据库分散地实现，它具有自己的性质和特征。集中式数据库系统的许多概念和技术，如数据独立性、数据共享和减少冗余度、并发控制、完整性、安全性和恢复等在分布式数据库系统中都有了不同的、更加丰富的内容。

### 44.简单说说贝叶斯定理。

  在引出贝叶斯定理之前，先学习几个定义：

条件概率（又称后验概率）就是事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。

比如，在同一个样本空间Ω中的事件或者子集A与B，如果随机从Ω中选出的一个元素属于B，那么这个随机选择的元素还属于A的概率就定义为在B的前提下A的条件概率，所以：P(A|B) = |A∩B|/|B|，接着分子、分母都除以|Ω|得到

[![44-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/d0a20db238281c91834c2879dde0ff41.jpg)](https://ask.julyedu.com/uploads/questions/20171019/d0a20db238281c91834c2879dde0ff41.jpg)

  联合概率表示两个事件共同发生的概率。A与B的联合概率表示为

[![44-2.png](https://ask.julyedu.com/uploads/questions/20171019/384a210b26d06421cbff9a8a7cf31742.png)](https://ask.julyedu.com/uploads/questions/20171019/384a210b26d06421cbff9a8a7cf31742.png)

  边缘概率（又称先验概率）是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)。

 

  接着，考虑一个问题：P(A|B)是在B发生的情况下A发生的可能性。

  1.首先，事件B发生之前，我们对事件A的发生有一个基本的概率判断，称为A的先验概率，用P(A)表示；

  2.其次，事件B发生之后，我们对事件A的发生概率重新评估，称为A的后验概率，用P(A|B)表示；

  3.类似的，事件A发生之前，我们对事件B的发生有一个基本的概率判断，称为B的先验概率，用P(B)表示；

  4.同样，事件A发生之后，我们对事件B的发生概率重新评估，称为B的后验概率，用P(B|A)表示。

  贝叶斯定理便是基于下述贝叶斯公式：

[![44-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/399472eb52bb9f8d1400a54ce1281069.jpg)](https://ask.julyedu.com/uploads/questions/20171019/399472eb52bb9f8d1400a54ce1281069.jpg)

  上述公式的推导其实非常简单，就是从条件概率推出。

  根据条件概率的定义，在事件B发生的条件下事件A发生的概率是

[![44-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/df37b83e7bcbbbd735d57a0ffb1d9de5.jpg)](https://ask.julyedu.com/uploads/questions/20171019/df37b83e7bcbbbd735d57a0ffb1d9de5.jpg)

  同样地，在事件A发生的条件下事件B发生的概率

[![44-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/ba5016b42f9ae4bb7d41854e8a13c0da.jpg)](https://ask.julyedu.com/uploads/questions/20171019/ba5016b42f9ae4bb7d41854e8a13c0da.jpg)

  整理与合并上述两个方程式，便可以得到：

[![44-6.jpg](https://ask.julyedu.com/uploads/questions/20171019/6ce70ebc26a92517c9e507974f344b93.jpg)](https://ask.julyedu.com/uploads/questions/20171019/6ce70ebc26a92517c9e507974f344b93.jpg)

  接着，上式两边同除以P(B)，若P(B)是非零的，我们便可以得到贝叶斯定理的公式表达式：

[![44-7.jpg](https://ask.julyedu.com/uploads/questions/20171019/457cf698ac926c29eb9ecee982d34040.jpg)](https://ask.julyedu.com/uploads/questions/20171019/457cf698ac926c29eb9ecee982d34040.jpg)

  所以，贝叶斯公式可以直接根据条件概率的定义直接推出。即因为P(A,B) = P(A)P(B|A) = P(B)P(A|B)，所以P(A|B) = P(A)P(B|A) / P(B)。更多请参见此文

http://blog.csdn.net/v_july_v/ ... 84699

### 45.#include和#include“filename.h”有什么区别？

  用 #include 格式来引用标准库的头文件（编译器将从标准库目录开始搜索）。

  用 #include “filename.h” 格式来引用非标准库的头文件（编译器将从用户的工作目录开始搜索）。

### 46.某超市研究销售纪录数据后发现，买啤酒的人很大概率也会购买尿布，这种属于数据挖掘的哪类问题？(A)

A. 关联规则发现 B. 聚类 C. 分类 D. 自然语言处理

### 47.将原始数据进行集成、变换、维度规约、数值规约是在以下哪个步骤的任务？(C)

A. 频繁模式挖掘 B. 分类和预测 C. 数据预处理 D. 数据流挖掘

### 48.下面哪种不属于数据预处理的方法？ (D)

A变量代换 B离散化 C 聚集 D 估计遗漏值

 

### 49.什么是KDD？ (A)

A. 数据挖掘与知识发现 B. 领域知识发现 C. 文档知识发现 D. 动态知识发现

### 50.当不知道数据所带标签时，可以使用哪种技术促使带同类标签的数据与带其他标签的数据相分离？(B)

A. 分类 B. 聚类 C. 关联分析 D. 隐马尔可夫链

### 51.建立一个模型，通过这个模型根据已知的变量值来预测其他某个变量值属于数据挖掘的哪一类任务？(C)

A. 根据内容检索 B. 建模描述 C. 预测建模 D. 寻找模式和规则

### 52.以下哪种方法不属于特征选择的标准方法： (D)

A嵌入 B 过滤 C 包装 D 抽样

 

### 53.请用python编写函数find_string，从文本中搜索并打印内容，要求支持通配符星号和问号。

  例子：

```python
>>> find_string('hello\nworld\n','wor')
['wor']
  
>>> find_string('hello\nworld\n','l*d')
['ld']
  
>>> find_string('hello\nworld\n','o.')
['or']
```
  答案:

```python
def find_string(str,pat):
  import re
  return re.findall(pat,str,re.I)
```

 

### 54.说下红黑树的五个性质

  红黑树，一种二叉查找树，但在每个结点上增加一个存储位表示结点的颜色，可以是Red或Black。

  通过对任何一条从根到叶子的路径上各个结点着色方式的限制，红黑树确保没有一条路径会比其他路径长出俩倍，因而是接近平衡的。

[![54-1.png](https://ask.julyedu.com/uploads/questions/20171019/b8f403f154799087ba225cfb1d930ba9.png)](https://ask.julyedu.com/uploads/questions/20171019/b8f403f154799087ba225cfb1d930ba9.png)

  红黑树，作为一棵二叉查找树，满足二叉查找树的一般性质。下面，来了解下 二叉查找树的一般性质。

  二叉查找树，也称有序二叉树（ordered binary tree），或已排序二叉树（sorted binary tree），是指一棵空树或者具有下列性质的二叉树：

  若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；

  若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；

  任意节点的左、右子树也分别为二叉查找树。

  没有键值相等的节点（no duplicate nodes）。

  因为一棵由n个结点随机构造的二叉查找树的高度为lgn，所以顺理成章，二叉查找树的一般操作的执行时间为O(lgn)。但二叉查找树若退化成了一棵具有n个结点的线性链后，则这些操作最坏情况运行时间为O(n)。

  红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质使得红黑树相对平衡，从而保证了红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。

  但它是如何保证一棵n个结点的红黑树的高度始终保持在logn的呢？这就引出了红黑树的5个性质：

  每个结点要么是红的要么是黑的。

  根结点是黑的。

  每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。

  如果一个结点是红的，那么它的两个儿子都是黑的。

  对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。

 

  正是红黑树的这5条性质，使一棵n个结点的红黑树始终保持了logn的高度，从而也就解释了上面所说的“红黑树的查找、插入、删除的时间复杂度最坏为O(log n)”这一结论成立的原因。更多请参见此文：

http://blog.csdn.net/v_july_v/ ... 05630

### 55.简单说下sigmoid激活函数

  常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。

  sigmoid的函数表达式如下

[![55-1.png](https://ask.julyedu.com/uploads/questions/20171019/4569dbbffb17cbe6f64a0bb49d8785bd.png)](https://ask.julyedu.com/uploads/questions/20171019/4569dbbffb17cbe6f64a0bb49d8785bd.png)

  也就是说，sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。

  压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。

  举个例子，如下图（图引自Stanford机器学习公开课）

[![55-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/1439e1c5d14d7f733ddfbb26af08ce9e.jpg)](https://ask.julyedu.com/uploads/questions/20171019/1439e1c5d14d7f733ddfbb26af08ce9e.jpg)

[![55-3.png](https://ask.julyedu.com/uploads/questions/20171019/5df425a12425201a8faeae78cdcbc12c.png)](https://ask.julyedu.com/uploads/questions/20171019/5df425a12425201a8faeae78cdcbc12c.png)

### 56.什么是卷积

  对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。

  非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。

[![56-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/c0ff2b671f97799e8b86a26a485b14d2.jpg)](https://ask.julyedu.com/uploads/questions/20171019/c0ff2b671f97799e8b86a26a485b14d2.jpg)

  OK，举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。

[![56-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/e5d4b1c4400ede2974f5f8e7fbbd2644.jpg)](https://ask.julyedu.com/uploads/questions/20171019/e5d4b1c4400ede2974f5f8e7fbbd2644.jpg)

  分解下上图

[![56-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/92bae91e9ab8a610c216c53fa519c3d8.jpg)](https://ask.julyedu.com/uploads/questions/20171019/92bae91e9ab8a610c216c53fa519c3d8.jpg)

对应位置上是数字先相乘后相加

# [![56-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/6803e9924863c7d696271270b4aaa7c3.jpg)](https://ask.julyedu.com/uploads/questions/20171019/6803e9924863c7d696271270b4aaa7c3.jpg)

[![56-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/3f44adeb15433afa5b0308d28c2847f4.jpg)](https://ask.julyedu.com/uploads/questions/20171019/3f44adeb15433afa5b0308d28c2847f4.jpg)

  中间滤波器filter与数据窗口做内积，其具体计算过程则是：4*0 + 0*0 + 0*0 + 0*0 + 0*1 + 0*1 + 0*0 + 0*1 + -4*2 = -8

### 57.什么是CNN的池化pool层

  池化，简言之，即取区域平均或最大，如下图所示（图引自cs231n）

[![57-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/520ffe5d64d3293f44d1d8dff2bf9fb2.jpg)](https://ask.julyedu.com/uploads/questions/20171019/520ffe5d64d3293f44d1d8dff2bf9fb2.jpg)

  上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。很简单不是？

### 58.简述下什么是生成对抗网络

  GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。

  更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。

[![58-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/142addda2419ff8632a52d143d3c3259.jpg)](https://ask.julyedu.com/uploads/questions/20171019/142addda2419ff8632a52d143d3c3259.jpg)

  如下图中的左右两个场景：

[![58-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/b0250aaa4c370c5374e6737ca6e71323.jpg)](https://ask.julyedu.com/uploads/questions/20171019/b0250aaa4c370c5374e6737ca6e71323.jpg)

  更多请参见此课程：

https://www.julyedu.com/course/getDetail/83

### 59.学梵高作画的原理是啥

  这里有篇如何做梵高风格画的实验教程《教你从头到尾利用DL学梵高作画：GTX 1070 cuda 8.0 tensorflow gpu版》，至于其原理请看这个视频：NeuralStyle艺术化图片（学梵高作画背后的原理）。

60.现在有 a 到 z 26 个元素， 编写程序打印 a 到 z 中任取 3 个元素的组合（比如 打印 a b c ，d y z等）

  解析参考：

http://blog.csdn.net/lvonve/ar ... 20680



### 61.说说梯度下降法

  @LeftNotEasy，本题解析来源：

http://www.cnblogs.com/LeftNot ... .html

  下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。

[![61-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/17d5ba34c5afd89aadebbc3327a4563f.jpg)](https://ask.julyedu.com/uploads/questions/20171019/17d5ba34c5afd89aadebbc3327a4563f.jpg)

  我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向等等，我们可以做出一个估计函数：

[![61-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/16a5df367f3de598508fecfe72b41e7a.jpg)](https://ask.julyedu.com/uploads/questions/20171019/16a5df367f3de598508fecfe72b41e7a.jpg)

  θ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。为了如果我们令X0 = 1，就可以用向量的方式来表示了：

[![61-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/e0e04c3c047f31e47913e5d702d490fc.jpg)](https://ask.julyedu.com/uploads/questions/20171019/e0e04c3c047f31e47913e5d702d490fc.jpg)

  我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个进行评估的函数称为损失函数（loss function），描述h函数不好的程度，在下面，我们称这个函数为J函数

  在这儿我们可以做出下面的一个损失函数：

[![61-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/103e296fc1a969532d7b6b051b871c1f.jpg)](https://ask.julyedu.com/uploads/questions/20171019/103e296fc1a969532d7b6b051b871c1f.jpg)

  换言之，我们把对x(i)的估计值与真实值y(i)差的平方和作为损失函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。

  如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，另外一种就是梯度下降法。

  梯度下降法的算法流程如下：

  1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。

  2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。

  为了描述的更清楚，给出下面的图：

[![61-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/ddcb0939f30c87fcb8b4942586784cb5.jpg)](https://ask.julyedu.com/uploads/questions/20171019/ddcb0939f30c87fcb8b4942586784cb5.jpg)

 这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低，也就是达到深蓝色的部分。θ0，θ1表示θ向量的两个维度。

  在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。

  然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如下图所示，算法的结束将是在θ下降到无法继续下降为止。

[![61-6.jpg](https://ask.julyedu.com/uploads/questions/20171019/f6ec1704ca3764c946ec67966abb4c70.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f6ec1704ca3764c946ec67966abb4c70.jpg)

  当然，可能梯度下降的最终点并非是全局最小点，即也可能是一个局部最小点，如下图所示：

[![61-7.jpg](https://ask.julyedu.com/uploads/questions/20171019/e8a5e85529ee59c057e4b10301cebbd7.jpg)](https://ask.julyedu.com/uploads/questions/20171019/e8a5e85529ee59c057e4b10301cebbd7.jpg)

  上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。

  下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J：

[![61-8.jpg](https://ask.julyedu.com/uploads/questions/20171019/f1134b4e107bbdd53e7bbef03018dafe.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f1134b4e107bbdd53e7bbef03018dafe.jpg)

  下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。

[![61-9.jpg](https://ask.julyedu.com/uploads/questions/20171019/e185889af969dbf1fb11d99a13001e7e.jpg)](https://ask.julyedu.com/uploads/questions/20171019/e185889af969dbf1fb11d99a13001e7e.jpg)

  一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。

  用更简单的数学语言进行描述步骤2）是这样的：

[![61-10.jpg](https://ask.julyedu.com/uploads/questions/20171019/57d5e9a2806520e5c7d361711061c510.jpg)](https://ask.julyedu.com/uploads/questions/20171019/57d5e9a2806520e5c7d361711061c510.jpg)

### 62.梯度下降法找到的一定是下降最快的方向么？

  梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。by林小溪（

https://www.zhihu.com/question ... 89869

）。

  一般解释梯度下降，会用下山来举例。假设你现在在山顶处，必须抵达山脚下（也就是山谷最低处）的湖泊。但让人头疼的是，你的双眼被蒙上了无法辨别前进方向。换句话说，你不再能够一眼看出哪条路径是最快的下山路径，如下图（图片来源：

http://blog.csdn.net/wemedia/details.html?id=45460

）：

[![62-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/0aa0d28e2d234e578211f48330f1cf75.jpg)](https://ask.julyedu.com/uploads/questions/20171019/0aa0d28e2d234e578211f48330f1cf75.jpg)

  最好的办法就是走一步算一步，先用脚向四周各个方向都迈出一步，试探一下周围的地势，用脚感觉下哪个方向是下降最大的方向。换言之，每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向（当前最陡峭的位置向下）走一步。就这样，每要走一步都根据上一步所在的位置选择当前最陡峭最快下山的方向走下一步，一步步走下去，一直走到我们感觉已经到了山脚。

  当然这样走下去，我们走到的可能并不一定是真正的山脚，而只是走到了某一个局部的山峰低处。换句话说，梯度下降不一定能够找到全局的最优解，也有可能只是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

[![62-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/d8f2ae0d9a48d1ba6ba8d6895c793135.jpg)](https://ask.julyedu.com/uploads/questions/20171019/d8f2ae0d9a48d1ba6ba8d6895c793135.jpg)

  @zbxzc（

http://blog.csdn.net/u01456892 ... 56915

）：更进一步，我们来定义输出误差，即对于任意一组权值向量，那它得到的输出和我们预想的输出之间的误差值。定义误差的方法很多，不同的误差计算方法可以得到不同的权值更新法则，这里我们先用这样的定义：

[![62-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/71f0835a2740d62e89720d02a9ff6e18.jpg)](https://ask.julyedu.com/uploads/questions/20171019/71f0835a2740d62e89720d02a9ff6e18.jpg)

  上面公式中D代表了所有的输入实例，或者说是样本，d代表了一个样本实例，od表示感知器的输出，td代表我们预想的输出。

  这样，我们的目标就明确了，就是想找到一组权值让这个误差的值最小，显然我们用误差对权值求导将是一个很好的选择，导数的意义是提供了一个方向，沿着这个方向改变权值，将会让总的误差变大，更形象的叫它为梯度。

[![62-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/8d4d20bfda89241a7d15ce4bcfe7854c.jpg)](https://ask.julyedu.com/uploads/questions/20171019/8d4d20bfda89241a7d15ce4bcfe7854c.jpg)

  既然梯度确定了E最陡峭的上升的方向，那么梯度下降的训练法则是：

[![62-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/d2c6a24d3d544220a97c28a6bf030455.jpg)](https://ask.julyedu.com/uploads/questions/20171019/d2c6a24d3d544220a97c28a6bf030455.jpg)

  梯度上升和梯度下降其实是一个思想，上式中权值更新的+号改为-号也就是梯度上升了。梯度上升用来求函数的最大值，梯度下降求最小值。

这样每次移动的方向确定了，但每次移动的距离却不知道。这个可以由步长（也称学习率）来确定，记为α。这样权值调整可表示为：

[![62-6.jpg](https://ask.julyedu.com/uploads/questions/20171019/e3e6c2735a62740e51b02be16e185521.jpg)](https://ask.julyedu.com/uploads/questions/20171019/e3e6c2735a62740e51b02be16e185521.jpg)

  总之，梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是“最速下降法”。最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的搜索迭代示意图如下图所示：

[![62-7.jpg](https://ask.julyedu.com/uploads/questions/20171019/223c5615f79843e7ec58594b1c40f259.jpg)](https://ask.julyedu.com/uploads/questions/20171019/223c5615f79843e7ec58594b1c40f259.jpg)

  正因为梯度度下降法在接近最优解的区域收敛速度明显变慢，所以利用梯度下降法求解需要很多次的迭代。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。by@wtq1993，

http://blog.csdn.net/wtq1993/a ... 07040

随机梯度下降

  普通的梯度下降算法在更新回归系数时要遍历整个数据集，是一种批处理方法，这样训练数据特别忙庞大时，可能出现如下问题：

  1）收敛过程可能非常慢；

  2）如果误差曲面上有多个局极小值，那么不能保证这个过程会找到全局最小值。

  为了解决上面的问题，实际中我们应用的是梯度下降的一种变体被称为随机梯度下降。

  上面公式中的误差是针对于所有训练样本而得到的，而随机梯度下降的思想是根据每个单独的训练样本来更新权值，这样我们上面的梯度公式就变成了：

[![62-8.jpg](https://ask.julyedu.com/uploads/questions/20171019/074b62cc95e005ccd6ddac2f7f2ff0a9.jpg)](https://ask.julyedu.com/uploads/questions/20171019/074b62cc95e005ccd6ddac2f7f2ff0a9.jpg)

  经过推导后，我们就可以得到最终的权值更新的公式：

[![62-9.jpg](https://ask.julyedu.com/uploads/questions/20171019/94e007bf4679961a9e5384777e4b2544.jpg)](https://ask.julyedu.com/uploads/questions/20171019/94e007bf4679961a9e5384777e4b2544.jpg)

  有了上面权重的更新公式后，我们就可以通过输入大量的实例样本，来根据我们预期的结果不断地调整权值，从而最终得到一组权值使得我们的算法能够对一个新的样本输入得到正确的或无限接近的结果。

这里做一个对比

设代价函数为

[![62-10.png](https://ask.julyedu.com/uploads/questions/20171019/330a4c9949b89f12188f06f138852938.png)](https://ask.julyedu.com/uploads/questions/20171019/330a4c9949b89f12188f06f138852938.png)

 批量梯度下降

[![62-11.png](https://ask.julyedu.com/uploads/questions/20171019/49539ad5df0de7795c16a42532bc6462.png)](https://ask.julyedu.com/uploads/questions/20171019/49539ad5df0de7795c16a42532bc6462.png)

  参数更新为：

[![62-12.jpg](https://ask.julyedu.com/uploads/questions/20171019/43d76018c2f3549c172908ba1d20658a.jpg)](https://ask.julyedu.com/uploads/questions/20171019/43d76018c2f3549c172908ba1d20658a.jpg)

  i是样本编号下标，j是样本维数下标，m为样例数目，n为特征数目。所以更新一个θj需要遍历整个样本集

[![62-13.jpg](https://ask.julyedu.com/uploads/questions/20171019/f3466878c27e47b2ee8f7ab21c6b17d8.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f3466878c27e47b2ee8f7ab21c6b17d8.jpg)

随机梯度下降

  参数更新为：

[![62-13.png](https://ask.julyedu.com/uploads/questions/20171019/e769e77c199c623a4b4ccd0cd37e9172.png)](https://ask.julyedu.com/uploads/questions/20171019/e769e77c199c623a4b4ccd0cd37e9172.png)

  i是样本编号下标，j是样本维数下标，m为样例数目，n为特征数目。所以更新一个θj只需要一个样本就可以。

[![62-14.png](https://ask.julyedu.com/uploads/questions/20171019/d68779584bad7c3700e22ca4f937b822.png)](https://ask.julyedu.com/uploads/questions/20171019/d68779584bad7c3700e22ca4f937b822.png)

  下面两幅图可以很形象的对比各种优化方法（图来源：

http://sebastianruder.com/opti ... cent/

）：

[![62-15.gif](https://ask.julyedu.com/uploads/questions/20171019/9f521b260a0df62a4f98f40f729ee4ac.gif)](https://ask.julyedu.com/uploads/questions/20171019/9f521b260a0df62a4f98f40f729ee4ac.gif)

SGD各优化方法在损失曲面上的表现

  从上图可以看出， Adagrad、Adadelta与RMSprop在损失曲面上能够立即转移到正确的移动方向上达到快速的收敛。而Momentum 与NAG会导致偏离(off-track)。同时NAG能够在偏离之后快速修正其路线，因为其根据梯度修正来提高响应性。

[![62-16.gif](https://ask.julyedu.com/uploads/questions/20171019/5bb65265a4f6cdeb1b46c7d3dbca3131.gif)](https://ask.julyedu.com/uploads/questions/20171019/5bb65265a4f6cdeb1b46c7d3dbca3131.gif)

SGD各优化方法在损失曲面鞍点处上的表现

### 63.牛顿法和梯度下降法有什么不同

  @wtq1993，

http://blog.csdn.net/wtq1993/a ... 07040

  1）牛顿法（Newton's method）

  牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

  具体步骤：

  首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f ' (x0)（这里f ' 表示函数 f 的导数）。然后我们计算穿过点(x0, f (x0)) 并且斜率为f '(x0)的直线和 x 轴的交点的x坐标，也就是求如下方程的解：

[![63-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/824e7a3125842b0d6c06bf9dd1fd0805.jpg)](https://ask.julyedu.com/uploads/questions/20171019/824e7a3125842b0d6c06bf9dd1fd0805.jpg)

  我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f (x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示：

[![63-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/7b023e10f82f780719f90dbb3a827b6f.jpg)](https://ask.julyedu.com/uploads/questions/20171019/7b023e10f82f780719f90dbb3a827b6f.jpg)

  已经证明，如果f ' 是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值x0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果f ' (x)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。

  由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示：

[![63-3.gif](https://ask.julyedu.com/uploads/questions/20171019/ebe76498fe28ca3de30c521192fdc8a4.gif)](https://ask.julyedu.com/uploads/questions/20171019/ebe76498fe28ca3de30c521192fdc8a4.gif)

  关于牛顿法和梯度下降法的效率对比：

  a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。

  b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

[![63-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/360858df5a9496b92082967f43945366.jpg)](https://ask.julyedu.com/uploads/questions/20171019/360858df5a9496b92082967f43945366.jpg)

  注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。

  牛顿法的优缺点总结：

  优点：二阶收敛，收敛速度快；

  缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

### 64.什么是拟牛顿法（Quasi-Newton Methods）

  @wtq1993，

http://blog.csdn.net/wtq1993/a ... 07040

  拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。

  拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

  具体步骤：

  拟牛顿法的基本思想如下。首先构造目标函数在当前迭代xk的二次模型：

[![64-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/d4c39d0ba24d61d461ad75bb51ae3227.jpg)](https://ask.julyedu.com/uploads/questions/20171019/d4c39d0ba24d61d461ad75bb51ae3227.jpg)

  这里Bk是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：

[![64-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/852138be30bc8a284a06dc819e4d3da7.jpg)](https://ask.julyedu.com/uploads/questions/20171019/852138be30bc8a284a06dc819e4d3da7.jpg)

　　其中我们要求步长ak满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hessian矩阵Bk代替真实的Hessian矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型：

[![64-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/2bc74eb77c8a54d14ca22a9e225ea2a8.jpg)](https://ask.julyedu.com/uploads/questions/20171019/2bc74eb77c8a54d14ca22a9e225ea2a8.jpg)

  我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求

[![64-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/46d441d5c5d036cfa840b8f70d197a6e.jpg)](https://ask.julyedu.com/uploads/questions/20171019/46d441d5c5d036cfa840b8f70d197a6e.jpg)

  从而得到

[![64-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/7380947831db756edf2fa502738b72e8.jpg)](https://ask.julyedu.com/uploads/questions/20171019/7380947831db756edf2fa502738b72e8.jpg)

  这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。

### 65.请说说随机梯度下降法的问题和挑战

[![65-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/8729d21b68d3740d3865960d28e9c4e6.jpg)](https://ask.julyedu.com/uploads/questions/20171019/8729d21b68d3740d3865960d28e9c4e6.jpg)

[![65-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/05b0128720851a27b65e1ce104e2a639.jpg)](https://ask.julyedu.com/uploads/questions/20171019/05b0128720851a27b65e1ce104e2a639.jpg)

[![65-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/b8c33bea06ee7fad1098f5c4fd5b45d6.jpg)](https://ask.julyedu.com/uploads/questions/20171019/b8c33bea06ee7fad1098f5c4fd5b45d6.jpg)

那到底如何优化随机梯度法呢？详情请点击：

https://ask.julyedu.com/question/7913

### 66.说说共轭梯度法

  @wtq1993，

http://blog.csdn.net/wtq1993/a ... 07040

  共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。

  下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：

[![66-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/b1bdb1ff74a4f7ead6bd813e88933762.jpg)](https://ask.julyedu.com/uploads/questions/20171019/b1bdb1ff74a4f7ead6bd813e88933762.jpg)

注：绿色为梯度下降法，红色代表共轭梯度法

### 67.对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法?

  @抽象猴，来源：

https://www.zhihu.com/question ... 04190

[![67-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/33492b5bf25e0e9e62d1f7f165ac614e.jpg)](https://ask.julyedu.com/uploads/questions/20171019/33492b5bf25e0e9e62d1f7f165ac614e.jpg)

  没有免费的午餐定理：

  对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。

  也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。

  但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。

### 68.什么最小二乘法？

  我们口头中经常说：一般来说，平均来说。如平均来说，不吸烟的健康优于吸烟者，之所以要加“平均”二字，是因为凡事皆有例外，总存在某个特别的人他吸烟但由于经常锻炼所以他的健康状况可能会优于他身边不吸烟的朋友。而最小二乘法的一个最简单的例子便是算术平均。

  最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。用函数表示为：

[![68-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/524b7dc783688acfb80199f585af4dae.jpg)](https://ask.julyedu.com/uploads/questions/20171019/524b7dc783688acfb80199f585af4dae.jpg)

  使误差「所谓误差，当然是观察值与实际真实值的差量」平方和达到最小以寻求估计值的方法，就叫做最小二乘法，用最小二乘法得到的估计，叫做最小二乘估计。当然，取平方和作为目标函数只是众多可取的方法之一。

  最小二乘法的一般形式可表示为：

[![68-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/dc59d273aed4c562348caa0719cd1a17.jpg)](https://ask.julyedu.com/uploads/questions/20171019/dc59d273aed4c562348caa0719cd1a17.jpg)

  有效的最小二乘法是勒让德在 1805 年发表的，基本思想就是认为测量中有误差，所以所有方程的累积误差为

[![68-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/0e5e70d88fd072b755cd2b0bd696be48.jpg)](https://ask.julyedu.com/uploads/questions/20171019/0e5e70d88fd072b755cd2b0bd696be48.jpg)

 我们求解出导致累积误差最小的参数即可：

[![68-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/ed65a1b4bace0209f4865225f71c0698.jpg)](https://ask.julyedu.com/uploads/questions/20171019/ed65a1b4bace0209f4865225f71c0698.jpg)

  勒让德在论文中对最小二乘法的优良性做了几点说明：

最小二乘使得误差平方和最小，并在各个方程的误差之间建立了一种平衡，从而防止某一个极端误差取得支配地位

计算中只要求偏导后求解线性方程组，计算过程明确便捷

最小二乘可以导出算术平均值作为估计值

  对于最后一点，从统计学的角度来看是很重要的一个性质。推理如下：假设真值为 θ, x1,⋯,xn为n次测量值, 每次测量的误差为ei=xi−θ，按最小二乘法，误差累积为

[![68-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/b6cb8ef2948271070dbb0b32127cbcb1.jpg)](https://ask.julyedu.com/uploads/questions/20171019/b6cb8ef2948271070dbb0b32127cbcb1.jpg)

[![68-6.png](https://ask.julyedu.com/uploads/questions/20171019/92da390df8536e2971cfbef6b1a933ab.png)](https://ask.julyedu.com/uploads/questions/20171019/92da390df8536e2971cfbef6b1a933ab.png)

  由于算术平均是一个历经考验的方法，而以上的推理说明，算术平均是最小二乘的一个特例，所以从另一个角度说明了最小二乘方法的优良性，使我们对最小二乘法更加有信心。

  最小二乘法发表之后很快得到了大家的认可接受，并迅速的在数据分析实践中被广泛使用。不过历史上又有人把最小二乘法的发明归功于高斯，这又是怎么一回事呢。高斯在1809年也发表了最小二乘法，并且声称自己已经使用这个方法多年。高斯发明了小行星定位的数学方法，并在数据分析中使用最小二乘方法进行计算，准确的预测了谷神星的位置。

  对了，最小二乘法跟SVM有什么联系呢？请参见

http://blog.csdn.net/v_july_v/ ... 24837

### 69.看你T恤上印着：人生苦短，我用Python，你可否说说Python到底是什么样的语言？你可以比较其他技术或者语言来回答你的问题。

  @David 9，  

http://nooverfit.com/wp/15%25E ... 259F/

 

  这里是一些关键点：Python是解释型语言。这意味着不像C和其他语言，Python运行前不需要编译。其他解释型语言包括PHP和Ruby。

  Python是动态类型的，这意味着你不需要在声明变量时指定类型。你可以先定义x=111，然后 x=”I’m a string”。

  Python是面向对象语言，所有允许定义类并且可以继承和组合。Python没有访问访问标识如在C++中的public, private, 这就非常信任程序员的素质，相信每个程序员都是“成人”了~

  在Python中，函数是一等公民。这就意味着它们可以被赋值，从其他函数返回值，并且传递函数对象。类不是一等公民。

  写Python代码很快，但是跑起来会比编译型语言慢。幸运的是，Python允许使用C扩展写程序，所以瓶颈可以得到处理。Numpy库就是一个很好例子，因为很多代码不是Python直接写的，所以运行很快。

  Python使用场景很多 – web应用开发、大数据应用、数据科学、人工智能等等。它也经常被看做“胶水”语言，使得不同语言间可以衔接上。

  Python能够简化工作 ，使得程序员能够关心如何重写代码而不是详细看一遍底层实现。

  @July：Python目前早已成为AI时代的第一语言，为帮助大家更好的学习Python语言、数据分析、爬虫等相关知识，七月在线特开一系列Python课程，有需要的亲们可以看下，比如《Python数据分析集训营》

http://www.julyedu.com/weekend/python

### 70.Python是如何进行内存管理的？

  @Tom_junsong，来源：

http://www.cnblogs.com/tom-gao/p/6645859.html

  从三个方面来说,一对象的引用计数机制,二垃圾回收机制,三内存池机制

一、对象的引用计数机制

  Python内部使用引用计数，来保持追踪内存中的对象，所有对象都有引用计数。

引用计数增加的情况：

  1，一个对象分配一个新名称

  2，将其放入一个容器中（如列表、元组或字典）

引用计数减少的情况：

  1，使用del语句对对象别名显示的销毁

  2，引用超出作用域或被重新赋值

  sys.getrefcount( )函数可以获得对象的当前引用计数

  多数情况下，引用计数比你猜测得要大得多。对于不可变数据（如数字和字符串），解释器会在程序的不同部分共享内存，以便节约内存。

二、垃圾回收

  1，当一个对象的引用计数归零时，它将被垃圾收集机制处理掉。

  2，当两个对象a和b相互引用时，del语句可以减少a和b的引用计数，并销毁用于引用底层对象的名称。然而由于每个对象都包含一个对其他对象的应用，因此引用计数不会归零，对象也不会销毁。（从而导致内存泄露）。为解决这一问题，解释器会定期执行一个循环检测器，搜索不可访问对象的循环并删除它们。

三、内存池机制

  Python提供了对内存的垃圾收集机制，但是它将不用的内存放到内存池而不是返回给操作系统。

  1，Pymalloc机制。为了加速Python的执行效率，Python引入了一个内存池机制，用于管理对小块内存的申请和释放。

  2，Python中所有小于256个字节的对象都使用pymalloc实现的分配器，而大的对象则使用系统的malloc。

  3，对于Python对象，如整数，浮点数和List，都有其独立的私有内存池，对象间不共享他们的内存池。也就是说如果你分配又释放了大量的整数，用于缓存这些整数的内存就不能再分配给浮点数。

### 71.请写出一段Python代码实现删除一个list里面的重复元素

  @Tom_junsong，

http://www.cnblogs.com/tom-gao/p/6645859.html

1,使用set函数，set(list)

2，使用字典函数，

```python
a=[1,2,4,2,4,5,6,5,7,8,9,0]
b={}
b=b.fromkeys(a)
c=list(b.keys())
c
```

### 72.编程用sort进行排序，然后从最后一个元素开始判断

  a=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3]

  @Tom_junsong，

http://www.cnblogs.com/tom-gao/p/6645859.html

```python
a.sort()
last=a[-1]
for i inrange(len(a)-2,-1,-1):
    if last==a[i]:
        del a[i]
    else:
        last=a[i]
print(a)
```

### 73.Python里面如何生成随机数？

  @Tom_junsong，

http://www.cnblogs.com/tom-gao/p/6645859.html

  random模块

  随机整数：random.randint(a,b)：返回随机整数x,a<=x<=b

  random.randrange(start,stop,[,step])：返回一个范围在(start,stop,step)之间的随机整数，不包括结束值。

  随机实数：random.random( ):返回0到1之间的浮点数

  random.uniform(a,b):返回指定范围内的浮点数。更多Python笔试面试题请看：

http://python.jobbole.com/85231/

74.说说常见的损失函数

  对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致（要知道，有时损失或误差是不可避免的），用一个损失函数来度量预测错误的程度。损失函数记为L(Y, f(X))。

  常用的损失函数有以下几种（基本引用自《统计学习方法》）：

[![74-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/542590ed4bfe2ede9c1f67455cc7188c.jpg)](https://ask.julyedu.com/uploads/questions/20171019/542590ed4bfe2ede9c1f67455cc7188c.jpg)

[![74-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/65f7986f77ad0fccda3b842a4c3e6dfa.jpg)](https://ask.julyedu.com/uploads/questions/20171019/65f7986f77ad0fccda3b842a4c3e6dfa.jpg)

  如此，SVM有第二种理解，即最优化+损失最小，或如@夏粉_百度所说“可从损失函数和优化算法角度看SVM，boosting，LR等算法，可能会有不同收获”。关于SVM的更多理解请参考：

http://blog.csdn.net/v_july_v/ ... 24837

75.简单介绍下logistics回归

  Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。

  假设函数

[![75-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/0063d0c70caa98d65983072bf760f232.jpg)](https://ask.julyedu.com/uploads/questions/20171019/0063d0c70caa98d65983072bf760f232.jpg)

  其中x是n维特征向量，函数g就是logistic函数。

  而

[![75-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/4782bc67cecdacc4733f70510c912883.jpg)](https://ask.julyedu.com/uploads/questions/20171019/4782bc67cecdacc4733f70510c912883.jpg)

的图像是

[![75-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/e6d2ce1a84d87f4b0506355f837587d4.jpg)](https://ask.julyedu.com/uploads/questions/20171019/e6d2ce1a84d87f4b0506355f837587d4.jpg)

  可以看到，将无穷映射到了(0,1)。

  而假设函数就是特征属于y=1的概率。

[![75-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/afaf1524cdcb63d57b908b9f60bad3bf.jpg)](https://ask.julyedu.com/uploads/questions/20171019/afaf1524cdcb63d57b908b9f60bad3bf.jpg)

[![75-5.png](https://ask.julyedu.com/uploads/questions/20171019/a8b877c5c4470bc11275c0d730d229ac.png)](https://ask.julyedu.com/uploads/questions/20171019/a8b877c5c4470bc11275c0d730d229ac.png)

76.看你是搞视觉的，熟悉哪些CV框架，顺带聊聊CV最近五年的发展史如何？

  原英文：adeshpande3.github.io

  作者：Adit Deshpande，UCLA CS研究生

  译者：新智元闻菲、胡祥杰

  译文链接：

https://mp.weixin.qq.com/s%3F_ ... irect

的

  本段结构如下：

　　AlexNet(2012年)

　　ZF Net(2013年)

　　VGG Net(2014年)

　　GoogLeNet (2015年)

　　微软 ResNet (2015年)

　　区域 CNN(R-CNN - 2013年，Fast R-CNN - 2015年，Faster R-CNN - 2015年)

　　生成对抗网络(2014年)

　　生成图像描述(2014年)

　　空间转化器网络(2015年)

　　AlexNet(2012年)

　　一切都从这里开始(尽管有些人会说是Yann LeCun 1998年发表的那篇论文才真正开启了一个时代)。这篇论文，题目叫做“ImageNet Classification with Deep Convolutional Networks”，迄今被引用6184次，被业内普遍视为行业最重要的论文之一。Alex Krizhevsky、Ilya Sutskever和 Geoffrey Hinton创造了一个“大型的深度卷积神经网络”，赢得了2012 ILSVRC(2012年ImageNet 大规模视觉识别挑战赛)。稍微介绍一下，这个比赛被誉为计算机视觉的年度奥林匹克竞赛，全世界的团队相聚一堂，看看是哪家的视觉模型表现最为出色。2012年是CNN首次实现Top 5误差率15.4%的一年(Top 5误差率是指给定一张图像，其标签不在模型认为最有可能的5个结果中的几率)，当时的次优项误差率为26.2%。这个表现不用说震惊了整个计算机视觉界。可以说，是自那时起，CNN才成了家喻户晓的名字。

　　论文中，作者讨论了网络的架构(名为AlexNet)。相比现代架构，他们使用了一种相对简单的布局，整个网络由5层卷积层组成，最大池化层、退出层(dropout layer)和3层全卷积层。网络能够对1000种潜在类别进行分类。

[![76-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/61b2516393de311b5cfac0b6ef0aa1a6.jpg)](https://ask.julyedu.com/uploads/questions/20171019/61b2516393de311b5cfac0b6ef0aa1a6.jpg)

　　AlexNet 架构：看上去有些奇怪，因为使用了两台GPU训练，因而有两股“流”。使用两台GPU训练的原因是计算量太大，只能拆开来。

　　要点

　　使用ImageNet数据训练网络，ImageNet数据库含有1500多万个带标记的图像，超过2.2万个类别。

　　使用ReLU代替传统正切函数引入非线性(ReLU比传统正切函数快几倍，缩短训练时间)。

　　使用了图像转化(image translation)、水平反射(horizontal reflection)和补丁提取(patch extraction)这些数据增强技术。

　　用dropout层应对训练数据过拟合的问题。

　　使用批处理随机梯度下降训练模型，注明动量衰减值和权重衰减值。

　　使用两台GTX 580 GPU，训练了5到6天

　　为什么重要?

　　Krizhevsky、Sutskever 和 Hinton 2012年开发的这个神经网络，是CNN在计算机视觉领域的一大亮相。这是史上第一次有模型在ImageNet 数据库表现这么好，ImageNet 数据库难度是出了名的。论文中提出的方法，比如数据增强和dropout，现在也在使用，这篇论文真正展示了CNN的优点，并且以破纪录的比赛成绩实打实地做支撑。

　　ZF Net(2013年)

　　2012年AlexNet出尽了风头，ILSVRC 2013就有一大批CNN模型冒了出来。2013年的冠军是纽约大学Matthew Zeiler 和 Rob Fergus设计的网络 ZF Net，错误率 11.2%。ZF Net模型更像是AlexNet架构的微调优化版，但还是提出了有关优化性能的一些关键想法。还有一个原因，这篇论文写得非常好，论文作者花了大量时间阐释有关卷积神经网络的直观概念，展示了将滤波器和权重可视化的正确方法。

　　在这篇题为“Visualizing and Understanding Convolutional Neural Networks”的论文中，Zeiler和Fergus从大数据和GPU计算力让人们重拾对CNN的兴趣讲起，讨论了研究人员对模型内在机制知之甚少，一针见血地指出“发展更好的模型实际上是不断试错的过程”。虽然我们现在要比3年前知道得多一些了，但论文所提出的问题至今仍然存在!这篇论文的主要贡献在于提出了一个比AlexNet稍微好一些的模型并给出了细节，还提供了一些制作可视化特征图值得借鉴的方法。

[![76-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/0142eed0d78a5a51b50c4da7804752a0.jpg)](https://ask.julyedu.com/uploads/questions/20171019/0142eed0d78a5a51b50c4da7804752a0.jpg)

　　要点

　　除了一些小的修改，整体架构非常类似AlexNet。

　　AlexNet训练用了1500万张图片，而ZFNet只用了130万张。

　　AlexNet在第一层中使用了大小为11×11的滤波器，而ZF使用的滤波器大小为7x7，整体处理速度也有所减慢。做此修改的原因是，对于输入数据来说，第一层卷积层有助于保留大量的原始象素信息。11×11的滤波器漏掉了大量相关信息，特别是因为这是第一层卷积层。

　　随着网络增大，使用的滤波器数量增多。

　　利用ReLU的激活函数，将交叉熵代价函数作为误差函数，使用批处理随机梯度下降进行训练。

　　使用一台GTX 580 GPU训练了12天。

　　开发可视化技术“解卷积网络”(Deconvolutional Network)，有助于检查不同的特征激活和其对输入空间关系。名字之所以称为“deconvnet”，是因为它将特征映射到像素(与卷积层恰好相反)。

　　DeConvNet

　　DeConvNet工作的基本原理是，每层训练过的CNN后面都连一层“deconvet”，它会提供一条返回图像像素的路径。输入图像进入CNN之后，每一层都计算激活。然而向前传递。现在，假设我们想知道第4层卷积层某个特征的激活值，我们将保存这个特征图的激活值，并将这一层的其他激活值设为0，再将这张特征图作为输入送入deconvnet。Deconvnet与原来的CNN拥有同样的滤波器。输入经过一系列unpool(maxpooling倒过来)，修正，对前一层进行过滤操作，直到输入空间满。

　　这一过程背后的逻辑在于，我们想要知道是激活某个特征图的是什么结构。下面来看第一层和第二层的可视化。

[![76-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/7a829ad85dc31ac2dca9ae1c69fc1447.jpg)](https://ask.julyedu.com/uploads/questions/20171019/7a829ad85dc31ac2dca9ae1c69fc1447.jpg)

　　ConvNet的第一层永远是低层特征检测器，在这里就是对简单的边缘、颜色进行检测。第二层就有比较圆滑的特征了。再来看第三、第四和第五层。

[![76-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/ebbe2e28a017cd9c2b530afdaf9e8454.jpg)](https://ask.julyedu.com/uploads/questions/20171019/ebbe2e28a017cd9c2b530afdaf9e8454.jpg)

　　这些层展示出了更多的高级特征，比如狗的脸和鲜花。值得一提的是，在第一层卷积层后面，我们通常会跟一个池化层将图像缩小(比如将 32x32x32 变为16x16x3)。这样做的效果是加宽了第二层看原始图像的视野。更详细的内容可以阅读论文。

　　为什么重要?

　　ZF Net不仅是2013年比赛的冠军，还对CNN的运作机制提供了极好的直观信息，展示了更多提升性能的方法。论文所描述的可视化方法不仅有助于弄清CNN的内在机理，也为优化网络架构提供了有用的信息。Deconv可视化方法和 occlusion 实验也让这篇论文成了我个人的最爱。

　　VGG Net(2015年)

　　简单、有深度，这就是2014年错误率7.3%的模型VGG Net(不是ILSVRC 2014冠军)。牛津大学的Karen Simonyan 和 Andrew Zisserman Main Points创造了一个19层的CNN，严格使用3x3的过滤器(stride =1，pad= 1)和2x2 maxpooling层(stride =2)。简单吧?

[![76-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/6dbbfd3783012788dd97ebe36b1787d0.jpg)](https://ask.julyedu.com/uploads/questions/20171019/6dbbfd3783012788dd97ebe36b1787d0.jpg)

　　要点

　　这里使用3x3的滤波器和AlexNet在第一层使用11x11的滤波器和ZF Net 7x7的滤波器作用完全不同。作者认为两个3x3的卷积层组合可以实现5x5的有效感受野。这就在保持滤波器尺寸较小的同时模拟了大型滤波器，减少了参数。此外，有两个卷积层就能够使用两层ReLU。

　　3卷积层具有7x7的有效感受野。

　　每个maxpool层后滤波器的数量增加一倍。进一步加强了缩小空间尺寸，但保持深度增长的想法。

　　图像分类和定位任务都运作良好。

　　使用Caffe工具包建模。

　　训练中使用scale jittering的数据增强技术。

　　每层卷积层后使用ReLU层和批处理梯度下降训练。

　　使用4台英伟达Titan Black GPU训练了两到三周。

　　为什么重要?

　　在我看来，VGG Net是最重要的模型之一，因为它再次强调CNN必须够深，视觉数据的层次化表示才有用。深的同时结构简单。

　　GoogLeNet(2015年)

　　理解了我们刚才所说的神经网络架构中的简化的概念了吗?通过推出 Inception 模型，谷歌从某种程度上把这一概念抛了出来。GoogLeNet是一个22层的卷积神经网络，在2014年的ILSVRC2014上凭借6.7%的错误率进入Top 5。据我所知，这是第一个真正不使用通用方法的卷积神经网络架构，传统的卷积神经网络的方法是简单堆叠卷积层，然后把各层以序列结构堆积起来。论文的作者也强调，这种新的模型重点考虑了内存和能量消耗。这一点很重要，我自己也会经常忽略：把所有的层都堆叠、增加大量的滤波器，在计算和内存上消耗很大，过拟合的风险也会增加。

[![76-6.jpg](https://ask.julyedu.com/uploads/questions/20171019/d9f3c9f55db1b537558e3ca1986d5eb0.jpg)](https://ask.julyedu.com/uploads/questions/20171019/d9f3c9f55db1b537558e3ca1986d5eb0.jpg)

　　换一种方式看 GoogLeNet：

[![76-7.jpg](https://ask.julyedu.com/uploads/questions/20171019/ceb7dc7533e858825deeafa42c068931.jpg)](https://ask.julyedu.com/uploads/questions/20171019/ceb7dc7533e858825deeafa42c068931.jpg)

　　Inception 模型

　　第一次看到GoogLeNet的构造时，我们立刻注意到，并不是所有的事情都是按照顺序进行的，这与此前看到的架构不一样。我们有一些网络，能同时并行发生反应。

[![76-8.jpg](https://ask.julyedu.com/uploads/questions/20171019/9b18dd25f1942a9316370c669560126e.jpg)](https://ask.julyedu.com/uploads/questions/20171019/9b18dd25f1942a9316370c669560126e.jpg)

  这个盒子被称为 Inception 模型。可以近距离地看看它的构成。　

[![76-9.jpg](https://ask.julyedu.com/uploads/questions/20171019/7be3588150d82b2ab0275597a7dd43e7.jpg)](https://ask.julyedu.com/uploads/questions/20171019/7be3588150d82b2ab0275597a7dd43e7.jpg)

　　底部的绿色盒子是我们的输入层，顶部的是输出层(把这张图片向右旋转90度，你会看到跟展示了整个网络的那张图片相对应的模型)。基本上，在一个传统的卷积网络中的每一层中，你必须选择操作池还是卷积操作(还要选择滤波器的大小)。Inception 模型能让你做到的就是并行地执行所有的操作。事实上，这就是作者构想出来的最“初始”的想法。

[![76-10.jpg](https://ask.julyedu.com/uploads/questions/20171019/87ade64df1de3a720c8e0d6ed6f050ba.jpg)](https://ask.julyedu.com/uploads/questions/20171019/87ade64df1de3a720c8e0d6ed6f050ba.jpg)

　　现在，来看看它为什么起作用。它会导向许多不同的结果，我们会最后会在输出层体积上获得极端大的深度通道。作者处理这个问题的方法是，在3X3和5X5层前，各自增加一个1X1的卷积操作。1X1的卷积(或者网络层中的网络)，提供了一个减少维度的方法。比如，我们假设你拥有一个输入层，体积是100x100x60(这并不定是图像的三个维度，只是网络中每一层的输入)。增加20个1X1的卷积滤波器，会让你把输入的体积减小到100X100X20。这意味着，3X3层和5X5层不需要处理输入层那么大的体积。这可以被认为是“池特征”(pooling of feature)，因为我们正在减少体积的高度，这和使用常用的最大池化层(maxpooling layers)减少宽度和长度类似。另一个需要注意的是，这些1X1的卷积层后面跟着的是ReLU 单元，这肯定不会有害。

　　你也许会问，“这个架构有什么用?”这么说吧，这个模型由一个网络层中的网络、一个中等大小的过滤卷积、一个大型的过滤卷积、一个操作池(pooling operation)组成。网络卷积层中的网络能够提取输入体积中的每一个细节中的信息，同时 5x5 的滤波器也能够覆盖大部分接受层的的输入，进而能提起其中的信息。你也可以进行一个池操作，以减少空间大小，降低过度拟合。在这些层之上，你在每一个卷积层后都有一个ReLU，这能改进网络的非线性特征。基本上，网络在执行这些基本的功能时，还能同时考虑计算的能力。这篇论文还提供了更高级别的推理，包括的主题有稀疏和紧密联结(见论文第三和第四节)。

　　要点

　　整个架构中使用了9个Inception 模型，总共超过100层。这已经很深了……没有使用完全连接的层。他们使用一个平均池代替，从 7x7x1024 的体积降到了 1x1x1024，这节省了大量的参数。比AlexNet的参数少了12X在测试中，相同图像的多个剪裁建立，然后填到网络中，计算softmax probabilities的均值，然后我们可以获得最后的解决方案。在感知模型中，使用了R-CNN中的概念。Inception有一些升级的版本(版本6和7)，“少数高端的GPU”一周内就能完成训练。

　　为什么重要?

　　GoogLeNet 是第一个引入了“CNN 各层不需要一直都按顺序堆叠”这一概念的模型。用Inception模型，作者展示了一个具有创造性的层次机构，能带来性能和计算效率的提升。这篇论文确实为接下来几年可能会见到的令人惊叹的架构打下了基础。

　　微软 ResNet(2015年)

　　想象一个深度CNN架构，再深、再深、再深，估计都还没有 ILSVRC 2015 冠军，微软的152层ResNet架构深。除了在层数上面创纪录，ResNet 的错误率也低得惊人，达到了3.6%，人类都大约在5%~10%的水平。

　　为什么重要?

　　只有3.6%的误差率，这应该足以说服你。ResNet模型是目前最好的CNN架构，而且是残差学习理念的一大创新。从2012年起，错误率逐年下降，我怀疑到ILSVRC2016，是否还会一直下降。我相信，我们现在堆放更多层将不会实现性能的大幅提升。我们必须要创造新的架构。

　　区域 CNN：R-CNN(2013年)、Fast R-CNN(2015年)、Faster R-CNN(2015年)

　　一些人可能会认为，R-CNN的出现比此前任何关于新的网络架构的论文都有影响力。第一篇关于R-CNN的论文被引用了超过1600次。Ross Girshick 和他在UC Berkeley 的团队在机器视觉上取得了最有影响力的进步。正如他们的文章所写， Fast R-CNN 和 Faster R-CNN能够让模型变得更快，更好地适应现代的物体识别任务。

　　R-CNN的目标是解决物体识别的难题。在获得特定的一张图像后， 我们希望能够绘制图像中所有物体的边缘。这一过程可以分为两个组成部分，一个是区域建议，另一个是分类。

　　论文的作者强调，任何分类不可知区域的建议方法都应该适用。Selective Search专用于RCNN。Selective Search 的作用是聚合2000个不同的区域，这些区域有最高的可能性会包含一个物体。在我们设计出一系列的区域建议之后，这些建议被汇合到一个图像大小的区域，能被填入到经过训练的CNN(论文中的例子是AlexNet)，能为每一个区域提取出一个对应的特征。这个向量随后被用于作为一个线性SVM的输入，SVM经过了每一种类型和输出分类训练。向量还可以被填入到一个有边界的回归区域，获得最精准的一致性。

[![76-11.jpg](https://ask.julyedu.com/uploads/questions/20171019/65927b4a78fdf6566893b40ff7f353ca.jpg)](https://ask.julyedu.com/uploads/questions/20171019/65927b4a78fdf6566893b40ff7f353ca.jpg)

　　非极值压抑后被用于压制边界区域，这些区域相互之间有很大的重复。

　　Fast R-CNN

　　原始模型得到了改进，主要有三个原因：训练需要多个步骤，这在计算上成本过高，而且速度很慢。Fast R-CNN通过从根本上在不同的建议中分析卷积层的计算，同时打乱生成区域建议的顺利以及运行CNN，能够快速地解决问题。

[![76-12.jpg](https://ask.julyedu.com/uploads/questions/20171019/8af58d0e0025168ae7d2425aca430c55.jpg)](https://ask.julyedu.com/uploads/questions/20171019/8af58d0e0025168ae7d2425aca430c55.jpg)

　　Faster R-CNN的工作是克服R-CNN和 Fast R-CNN所展示出来的，在训练管道上的复杂性。作者 在最后一个卷积层上引入了一个区域建议网络(RPN)。这一网络能够只看最后一层的特征就产出区域建议。从这一层面上来说，相同的R-CNN管道可用。

[![76-13.jpg](https://ask.julyedu.com/uploads/questions/20171019/e858ddb34777ba7d93ad814e416046c6.jpg)](https://ask.julyedu.com/uploads/questions/20171019/e858ddb34777ba7d93ad814e416046c6.jpg)

　　为什么重要?

　　能够识别出一张图像中的某一个物体是一方面，但是，能够识别物体的精确位置对于计算机知识来说是一个巨大的飞跃。更快的R-CNN已经成为今天标准的物体识别程序。

　　生成对抗网络(2015年)

　　按照Yann LeCun的说法，生成对抗网络可能就是深度学习下一个大突破。假设有两个模型，一个生成模型，一个判别模型。判别模型的任务是决定某幅图像是真实的(来自数据库)，还是机器生成的，而生成模型的任务则是生成能够骗过判别模型的图像。这两个模型彼此就形成了“对抗”，发展下去最终会达到一个平衡，生成器生成的图像与真实的图像没有区别，判别器无法区分两者。

[![76-14.jpg](https://ask.julyedu.com/uploads/questions/20171019/71d1a6feb56f6d0cebce09e3cf41a81c.jpg)](https://ask.julyedu.com/uploads/questions/20171019/71d1a6feb56f6d0cebce09e3cf41a81c.jpg)

　　左边一栏是数据库里的图像，也即真实的图像，右边一栏是机器生成的图像，虽然肉眼看上去基本一样，但在CNN看起来却十分不同。

　　为什么重要?

　　听上去很简单，然而这是只有在理解了“数据内在表征”之后才能建立的模型，你能够训练网络理解真实图像和机器生成的图像之间的区别。因此，这个模型也可以被用于CNN中做特征提取。此外，你还能用生成对抗模型制作以假乱真的图片。

　　生成图像描述(2014年)

　　把CNN和RNN结合在一起会发生什么?Andrej Karpathy 和李飞飞写的这篇论文探讨了结合CNN和双向RNN生成不同图像区域的自然语言描述问题。简单说，这个模型能够接收一张图片，然后输出

[![76-15.jpg](https://ask.julyedu.com/uploads/questions/20171019/9f15ec2e4b3659a5e5bc5862178898a4.jpg)](https://ask.julyedu.com/uploads/questions/20171019/9f15ec2e4b3659a5e5bc5862178898a4.jpg)

　　很神奇吧。传统CNN，训练数据中每幅图像都有单一的一个标记。这篇论文描述的模型则是每幅图像都带有一句话(或图说)。这种标记被称为弱标记，使用这种训练数据，一个深度神经网络“推断句子中的部分与其描述的区域之间的潜在对齐(latent alignment)”，另一个神经网络将图像作为输入，生成文本的描述。

　　为什么重要?

　　使用看似不相关的RNN和CNN模型创造了一个十分有用的应用，将计算机视觉和自然语言处理结合在一起。这篇论文为如何建模处理跨领域任务提供了全新的思路。

　　空间转换器网络(2015年)

　　最后，让我们来看该领域最近的一篇论文。本文是谷歌DeepMind的一个团队在一年前写的。这篇论文的主要贡献是介绍了空间变换器(Spatial Transformer)模块。基本思路是，这个模块会转变输入图像，使随后的层可以更轻松地进行分类。作者试图在图像到达特定层前改变图像，而不是更改主CNN架构本身。该模块希望纠正两件事：姿势标准化(场景中物体倾斜或缩放)和空间注意力(在密集的图像中将注意力集中到正确的物体)。对于传统的CNN，如果你想使你的模型对于不同规格和旋转的图像都保持不变，那你需要大量的训练样本来使模型学习。让我们来看看这个模块是如何帮助解决这一问题。

　　传统CNN模型中，处理空间不变性的是maxpooling层。其原因是，一旦我们知道某个特定特性还是起始输入量(有高激活值)，它的确切位置就没有它对其他特性的相对位置重要，其他功能一样重要。这个新的空间变换器是动态的，它会对每个输入图像产生不同的行为(不同的扭曲/变形)。这不仅仅是像传统 maxpool 那样简单和预定义。让我们来看看这个模块是如何工作的。该模块包括：

　　一个本地化网络，会吸收输入量，并输出应施加的空间变换的参数。参数可以是6维仿射变换。

　　采样网格，这是由卷曲规则网格和定位网络中创建的仿射变换(theta)共同产生的。

　　一个采样器，其目的是执行输入功能图的翘曲。

[![76-16.jpg](https://ask.julyedu.com/uploads/questions/20171019/3a7baa5e7e7123ac502967970af29c47.jpg)](https://ask.julyedu.com/uploads/questions/20171019/3a7baa5e7e7123ac502967970af29c47.jpg)

　　该模块可以放入CNN的任何地方中，可以帮助网络学习如何以在训练过程中最大限度地减少成本函数的方式来变换特征图。

[![76-17.jpg](https://ask.julyedu.com/uploads/questions/20171019/6f6abc1c664f0551e119f8f55d6a5e1f.jpg)](https://ask.julyedu.com/uploads/questions/20171019/6f6abc1c664f0551e119f8f55d6a5e1f.jpg)

　　为什么重要?

　　CNN的改进不一定要到通过网络架构的大改变来实现。我们不需要创建下一个ResNet或者 Inception 模型。本文实现了对输入图像进行仿射变换的简单的想法，以使模型对平移，缩放和旋转保持不变。更多请查看

https://pan.baidu.com/s/1dFyVLst#list/path=%2F

77.深度学习在视觉领域有何前沿进展

  @元峰，本题解析来源：

https://zhuanlan.zhihu.com/p/24699780

  引言

  在今年的神经网络顶级会议NIPS2016上，深度学习三大牛之一的Yann Lecun教授给出了一个关于机器学习中的有监督学习、无监督学习和增强学习的一个有趣的比喻，他说：如果把智能（Intelligence）比作一个蛋糕，那么无监督学习就是蛋糕本体，增强学习是蛋糕上的樱桃，那么监督学习，仅仅能算作蛋糕上的糖霜（图1）。

[![77-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/f536ee6fe0ec374b1aff639bde266e44.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f536ee6fe0ec374b1aff639bde266e44.jpg)

图1. Yann LeCun 对监督学习，增强学习和无监督学习的价值的形象比喻

\1. 深度有监督学习在计算机视觉领域的进展

1.1 图像分类（Image Classification）

  自从Alex和他的导师Hinton（深度学习鼻祖）在2012年的ImageNet大规模图像识别竞赛（ILSVRC2012）中以超过第二名10个百分点的成绩(83.6%的Top5精度)碾压第二名（74.2%，使用传统的计算机视觉方法）后，深度学习真正开始火热，卷积神经网络（CNN）开始成为家喻户晓的名字，从12年的AlexNet（83.6%），到2013年ImageNet 大规模图像识别竞赛冠军的88.8%，再到2014年VGG的92.7%和同年的GoogLeNet的93.3%，终于，到了2015年，在1000类的图像识别中，微软提出的残差网（ResNet）以96.43%的Top5正确率，达到了超过人类的水平（人类的正确率也只有94.9%）.

  Top5精度是指在给出一张图片，模型给出5个最有可能的标签，只要在预测的5个结果中包含正确标签，即为正确

[![77-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/dfe01ec834b9b2eb9b0dadde71c888d1.jpg)](https://ask.julyedu.com/uploads/questions/20171019/dfe01ec834b9b2eb9b0dadde71c888d1.jpg)

图２. 2010-2015年ILSVRC竞赛图像识别错误率演进趋势

1.2 图像检测（Image Dection）

  伴随着图像分类任务，还有另外一个更加有挑战的任务–图像检测，图像检测是指在分类图像的同时把物体用矩形框给圈起来。从14年到16年，先后涌现出R-CNN,Fast R-CNN, Faster R-CNN, YOLO, SSD等知名框架，其检测平均精度（mAP），在计算机视觉一个知名数据集上PASCAL VOC上的检测平均精度（mAP），也从R-CNN的53.3%，到Fast RCNN的68.4%，再到Faster R-CNN的75.9%，最新实验显示，Faster RCNN结合残差网（Resnet-101），其检测精度可以达到83.8%。深度学习检测速度也越来越快，从最初的RCNN模型，处理一张图片要用2秒多，到Faster RCNN的198毫秒/张，再到YOLO的155帧/秒（其缺陷是精度较低，只有52.7%），最后出来了精度和速度都较高的SSD，精度75.1%，速度23帧/秒。

 

[![77-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/b7bb19b0688e4c411cf5bdedc2ffc972.jpg)](https://ask.julyedu.com/uploads/questions/20171019/b7bb19b0688e4c411cf5bdedc2ffc972.jpg)

图3. 图像检测示例

1.3 图像分割（Semantic Segmentation）

  图像分割也是一项有意思的研究领域，它的目的是把图像中各种不同物体给用不同颜色分割出来，如下图所示，其平均精度（mIoU，即预测区域和实际区域交集除以预测区域和实际区域的并集），也从最开始的FCN模型（图像语义分割全连接网络，该论文获得计算机视觉顶会CVPR2015的最佳论文的）的62.2%，到DeepLab框架的72.7%，再到牛津大学的CRF as RNN的74.7%。该领域是一个仍在进展的领域，仍旧有很大的进步空间。

[![77-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/515f4cba626217151f1d0d7a98aad984.jpg)](https://ask.julyedu.com/uploads/questions/20171019/515f4cba626217151f1d0d7a98aad984.jpg)

图4. 图像分割的例子

1.4 图像标注–看图说话（Image Captioning）

  图像标注是一项引人注目的研究领域，它的研究目的是给出一张图片，你给我用一段文字描述它，如图中所示，图片中第一个图，程序自动给出的描述是“一个人在尘土飞扬的土路上骑摩托车”，第二个图片是“两只狗在草地上玩耍”。由于该研究巨大的商业价值（例如图片搜索），近几年，工业界的百度，谷歌和微软 以及学术界的加大伯克利，深度学习研究重地多伦多大学都在做相应的研究。

[![77-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/0573a858167bcc8bc02595246ea1e527.jpg)](https://ask.julyedu.com/uploads/questions/20171019/0573a858167bcc8bc02595246ea1e527.jpg)

图5.图像标注，根据图片生成描述文字

1.5 图像生成–文字转图像（Image Generator）

  图片标注任务本来是一个半圆，既然我们可以从图片产生描述文字，那么我们也能从文字来生成图片。如图6所示，第一列“一架大客机在蓝天飞翔”，模型自动根据文字生成了16张图片，第三列比较有意思，“一群大象在干燥草地行走”（这个有点违背常识，因为大象一般在雨林，不会在干燥草地上行走），模型也相应的生成了对应图片，虽然生成的质量还不算太好，但也已经中规中矩。

[![77-6.jpg](https://ask.julyedu.com/uploads/questions/20171019/8845df0c553320be301e3edecdc02d34.jpg)](https://ask.julyedu.com/uploads/questions/20171019/8845df0c553320be301e3edecdc02d34.jpg)

图6.根据文字生成图片

2.强化学习（Reinforcement Learning）

  在监督学习任务中，我们都是给定样本一个固定标签，然后去训练模型，可是，在真实环境中，我们很难给出所有样本的标签，这时候，强化学习就派上了用场。简单来说，我们给定一些奖励或惩罚，强化学习就是让模型自己去试错，模型自己去优化怎么才能得到更多的分数。2016年大火的AlphaGo就是利用了强化学习去训练，它在不断的自我试错和博弈中掌握了最优的策略。利用强化学习去玩flyppy bird，已经能够玩到几万分了。

[![77-7.jpg](https://ask.julyedu.com/uploads/questions/20171019/1992a0169d394431874e902e14784f0a.jpg)](https://ask.julyedu.com/uploads/questions/20171019/1992a0169d394431874e902e14784f0a.jpg)

图７. 强化学习玩flappy bird

  谷歌DeepMind发表的使用增强学习来玩Atari游戏，其中一个经典的游戏是打砖块（breakout），DeepMind提出的模型仅仅使用像素作为输入，没有任何其他先验知识，换句话说，模型并不认识球是什么，它玩的是什么，令人惊讶的是，在经过240分钟的训练后，它不光学会了正确的接球，击打砖块，它甚至学会了持续击打同一个位置，游戏就胜利的越快（它的奖励也越高）。

  视频链接:

http://v.youku.com/v_show/id_X ... .html

[![77-8.jpg](https://ask.julyedu.com/uploads/questions/20171019/8850a4e136639422868fe3281a115448.jpg)](https://ask.julyedu.com/uploads/questions/20171019/8850a4e136639422868fe3281a115448.jpg)

图8.使用深度增强学习来玩Atari Breakout　

  强化学习在机器人领域和自动驾驶领域有极大的应用价值，当前arxiv上基本上每隔几天就会有相应的论文出现。机器人去学习试错来学习最优的表现，这或许是人工智能进化的最优途径，估计也是通向强人工智能的必经之路。

3深度无监督学习（Deep Unsupervised Learning）–预测学习

  相比有限的监督学习数据，自然界有无穷无尽的未标注数据。试想，如果人工智能可以从庞大的自然界自动去学习，那岂不是开启了一个新纪元？当前，最有前景的研究领域或许应属无监督学习，这也正是Yann Lecun教授把无监督学习比喻成人工智能大蛋糕的原因吧。

  深度学习牛人Ian Goodfellow在2014年提出生成对抗网络（

https://arxiv.org/abs/1406.2661

）后，该领域越来越火，成为16年研究最火热的一个领域之一。大牛Yann LeCun曾说：“对抗网络是切片面包发明以来最令人激动的事情。”这句话足以说明生成对抗网络有多重要。

 

  生成对抗网络的一个简单解释如下：假设有两个模型，一个是生成模型（Generative Model，下文简写为G），一个是判别模型（Discriminative Model，下文简写为D），判别模型(D)的任务就是判断一个实例是真实的还是由模型生成的，生成模型(G)的任务是生成一个实例来骗过判别模型（D），两个模型互相对抗，发展下去就会达到一个平衡，生成模型生成的实例与真实的没有区别，判别模型无法区分自然的还是模型生成的。以赝品商人为例，赝品商人（生成模型）制作出假的毕加索画作来欺骗行家（判别模型D），赝品商人一直提升他的高仿水平来区分行家，行家也一直学习真的假的毕加索画作来提升自己的辨识能力，两个人一直博弈，最后赝品商人高仿的毕加索画作达到了以假乱真的水平，行家最后也很难区分正品和赝品了。下图是Goodfellow在发表生成对抗网络论文中的一些生成图片，可以看出，模型生成的模型与真实的还是有大差别，但这是14年的论文了，16年这个领域进展非常快，相继出现了条件生成对抗网络（Conditional Generative Adversarial Nets）和信息生成对抗网络（InfoGAN），深度卷积生成对抗网络（Deep Convolutional Generative Adversarial Network, DCGAN），更重要的是，当前生成对抗网络把触角伸到了视频预测领域，众所周知，人类主要是靠视频序列来理解自然界的，图片只占非常小的一部分，当人工智能学会理解视频后，它也真正开始显现出威力了。

  这里推荐一篇2017年初Ian GoodFellow结合他在NIPS2016的演讲写出的综述性论文NIPS 2016 Tutorial: Generative Adversarial Networks

[![77-9.jpg](https://ask.julyedu.com/uploads/questions/20171019/2813e32ec6fe8f5f2e44761cd5917dab.jpg)](https://ask.julyedu.com/uploads/questions/20171019/2813e32ec6fe8f5f2e44761cd5917dab.jpg)

图9 生成对抗网络生成的一些图片，最后边一列是与训练集中图片最相近的生产图片

3.1条件生成对抗网络（Conditional Generative Adversarial Nets，CGAN）

  生成对抗网络一般是根据随机噪声来生成特定类型的图像等实例，条件生成对抗网络则是根据一定的输入来限定输出，例如根据几个描述名词来生成特定的实例，这有点类似1.5节介绍的由文字生成图像，下图是Conditioanal Generative Adversarial Nets论文中的一张图片，根据特定的名词描述来生成图片。（注意：左边的一列图片的描述文字是训练集中不存在的，也就是说是模型根据没有见过的描述来生成的图片，右边的一列图片的描述是训练集中存在的）

[![77-10.jpg](https://ask.julyedu.com/uploads/questions/20171019/1fb22080311b9b3bb686529982e7395e.jpg)](https://ask.julyedu.com/uploads/questions/20171019/1fb22080311b9b3bb686529982e7395e.jpg)

图10. 根据文字来生成图片

  条件生成对抗网络的另一篇有意思的论文是图像到图像的翻译，该论文提出的模型能够根据一张输入图片，然后给出模型生成的图片，下图是论文中的一张图，其中左上角第一对非常有意思，模型输入图像分割的结果，给出了生成的真实场景的结果，这类似于图像分割的反向工程。

[![77-11.jpg](https://ask.julyedu.com/uploads/questions/20171019/11478fd0e3b69727e0d88eeb466a3364.jpg)](https://ask.julyedu.com/uploads/questions/20171019/11478fd0e3b69727e0d88eeb466a3364.jpg)

图11. 根据特定输入来生成一些有意思的输出图片

  生成对抗网络也用在了图像超分辨率上，2016年有人提出SRGAN模型（

https://arxiv.org/abs/1609.04802

），它把原高清图下采样后，试图用生成对抗网络模型来还原图片来生成更为自然的，更逼近原图像的图像。下图中最右边是原图，把他降采样后采用三次差值（Bicubic Interpolation）得到的图像比较模糊，采用残差网络的版本（SRResNet）已经干净了很多，我们可以看到SRGAN生成的图片更为真实一些。

[![77-12.jpg](https://ask.julyedu.com/uploads/questions/20171019/67a8336881f9a5b99d7b316f52950867.jpg)](https://ask.julyedu.com/uploads/questions/20171019/67a8336881f9a5b99d7b316f52950867.jpg)

图12.生成对抗网络做超分辨率的例子，最右边是原始图像

  生成对抗网络的另一篇有影响力的论文是深度卷积生成对抗网络DCGAN（

https://arxiv.org/abs/1511.06434

）,作者把卷积神经网络和生成对抗网络结合起来，作者指出该框架可以很好的学习事物的特征，论文在图像生成和图像操作上给出了很有意思的结果，例如图13，带眼睛的男人-不戴眼镜的男人+不带眼睛的女人=带眼睛的女人,该模型给出了图片的类似向量化操作。

[![77-13.jpg](https://ask.julyedu.com/uploads/questions/20171019/f50abfbed20937a9997533609e81d0d8.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f50abfbed20937a9997533609e81d0d8.jpg)

图13. DCGAN论文中的例图

  生成对抗网络的发展是在是太火爆，一篇文章难以罗列完全，对此感兴趣的朋友们可以自己在网络搜素相关论文来研究

  openAI的一篇描述生成对抗网络的博客非常棒，因为Ian Goodfellow就在OpenAI工作，所以这篇博客的质量还是相当有保障的。链接为：

https://openai.com/blog/generative-models/

3.2 视频预测

  该方向是笔者自己最感兴趣的方向，Yann LeCun也提出，“用预测学习来替代无监督学习”,预测学习通过观察和理解这个世界是如何运作的，然后对世界的变化做出预测，机器学会了感知世界的变化，然后对世界的状态进行了推断。

  今年的NIPS上，MIT的学者Vondrick等人发表了一篇名为Generating Videos with Scene Dynamics（

http://carlvondrick.com/tinyvideo/

）的论文,该论文提出了基于一幅静态的图片，模型自动推测接下来的场景，例如给出一张人站在沙滩的图片，模型自动给出一段接下来的海浪涌动的小视频。该模型是以无监督的方式，在大量的视频上训练而来的。该模型表明它可以自动学习到视频中有用的特征。下图是作者的官方主页上给出的图，是动态图，如果无法正常查看，请转入

http://carlvondrick.com/tinyvideo/

  视频生成例子，下图的视频是模型自动生成的，我们可以看到图片不太完美，但已经能相当好的表示一个场景了。

 

[![77-14-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/4ef51b539e80af46c0a5dfc2dd2c77d2.jpg)](https://ask.julyedu.com/uploads/questions/20171019/4ef51b539e80af46c0a5dfc2dd2c77d2.jpg)

[![77-14-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/a6f19c120c3209765e66f93d5ca3f02c.jpg)](https://ask.julyedu.com/uploads/questions/20171019/a6f19c120c3209765e66f93d5ca3f02c.jpg)

图14. 随机生成的视频，沙滩上波涛涌动，火车奔驰的场景

  条件视频生成，下图是输入一张静态图，模型自动推演出一段小视频。

[![77-15-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/27af9a01bbdc09a1bf1755690ec1d840.jpg)](https://ask.julyedu.com/uploads/questions/20171019/27af9a01bbdc09a1bf1755690ec1d840.jpg)

[![77-15-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/f00e42fef6658154de94a41602bd3861.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f00e42fef6658154de94a41602bd3861.jpg)

图15.根据一张草地静态图，模型自动推测人的移动场景,该图为动图，如果无法查看，请访问

http://carlvondrick.com/tinyvideo/

[![77-16-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/991c9747ff7619af9f9a4d99fa66ddc4.jpg)](https://ask.julyedu.com/uploads/questions/20171019/991c9747ff7619af9f9a4d99fa66ddc4.jpg)

[![77-16-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/8e1b3defd690e167bc239411c532cedb.jpg)](https://ask.julyedu.com/uploads/questions/20171019/8e1b3defd690e167bc239411c532cedb.jpg)

图16.给出一张铁道图，模型自动推测火车跑过的样子,该图为动图，如果无法查看，请访问

http://carlvondrick.com/tinyvideo/

  MIT的CSAIL实验室也放出了一篇博客，题目是《教会机器去预测未来》（

http://news.mit.edu/2016/teach ... -0621

）,该模型在youtube视频和电视剧上（例如The Office和《绝望主妇》）训练，训练好以后，如果你给该模型一个亲吻之前的图片，该模型能自动推测出加下来拥抱亲吻的动作，具体的例子见下图。

 

[![77-17.jpg](https://ask.julyedu.com/uploads/questions/20171019/fedf64dcc9e0a7dd5b3470c5e7edd76c.jpg)](https://ask.julyedu.com/uploads/questions/20171019/fedf64dcc9e0a7dd5b3470c5e7edd76c.jpg)

图17. 给出一张静态图，模型自动推测接下来的动作

  哈佛大学的Lotter等人提出了PredNet（

https://coxlab.github.io/prednet/

），该模型也是在KITTI数据集（

http://www.cvlibs.net/datasets/kitti/

）上训练,然后该模型就可以根据前面的视频，预测行车记录仪接下来几帧的图像，模型是用长短期记忆神经网络（LSTM）训练得到的。具体例子见下图,给出行车记录仪前几张的图片，自动预测接下来的五帧场景，模型输入几帧图像后，预测接下来的5帧，由图可知，越往后，模型预测的越是模糊,但模型已经可以给出有参加价值的预测结果了。图片是动图，如果无法正常查看，请访问论文作者的博客

https://coxlab.github.io/prednet/

[![77-18.jpg](https://ask.julyedu.com/uploads/questions/20171019/f663005ae1acda6bfb7745fc962f7acc.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f663005ae1acda6bfb7745fc962f7acc.jpg)

图18. 给出行车记录仪前几张的图片，自动预测接下来的五帧场景,该图为动图，如果无法查看，请访问

https://coxlab.github.io/prednet/

4 总结

  生成对抗网络，无监督学习视频预测的论文实在是太多，本人精力实在有限，对此感兴趣的读者可以每天刷一下arxiv的计算机视觉版块的计算机视觉和模型识别，神经网络和进化计算和人工智能等相应版块，基本上每天都有这方面新论文出现。图像检测和分割，增强学习，生成对抗网络，预测学习都是人工智能发展火热的方向，希望对深度学习感兴趣的我们在这方面能做出来点成果。谢谢朋友们的阅读，对深度无监督学习感兴趣的朋友，欢迎一起学习交流，请私信我。

5 参考文献

  在写本文的过程中，我尽量把论文网址以链接的形式附着在正文中.本文参考的大部分博客和论文整理如下，方便大家和自己以后研究查看。

  参考博客

1.NIPS 主旨演讲】Yann LeCun：用预测学习替代无监督学习

https://mp.weixin.qq.com/s/VJkiVmGBMv3sL94mivjNHg

2.计算机视觉和 CNN 发展十一座里程碑

https://mp.weixin.qq.com/s/eosTWBbLpwVroYPEb9Q0wA

3.Generative Models

https://blog.openai.com/generative-models/

4.Generating Videos with Scene Dynamics

http://carlvondrick.com/tinyvideo/

5.Teaching machines to predict the future

http://news.mit.edu/2016/teach ... -0621

  参考论文

1.Resnet模型，图像分类，超过人类的计算机识别水平。Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification

https://arxiv.org/abs/1502.01852

2.图像检测 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

https://arxiv.org/abs/1506.01497

3.图像分割Conditional Random Fields as Recurrent Neural Networks

http://www.robots.ox.ac.uk/~szheng/CRFasRNN.html

4.图像标注，看图说话 Show and Tell: A Neural Image Caption Generator

https://arxiv.org/abs/1411.4555

5.文字生成图像Generative Adversarial Text to Image Synthesis

https://arxiv.org/abs/1605.05396

6.强化学习玩flyppy bird Using Deep Q-Network to Learn How To Play Flappy Bird

https://github.com/yenchenlin/ ... yBird

7.强化学习玩Atari游戏 Playing Atari with Deep Reinforcement Learning

https://arxiv.org/abs/1312.5602

8.生成对抗网络 Generative Adversarial Networks

https://arxiv.org/abs/1406.2661

9.条件生成对抗网络Conditional Generative Adversarial Nets

https://arxiv.org/abs/1411.1784

10.生成对抗网络做图像超分辨率Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network

https://arxiv.org/abs/1609.04802

11.深度卷积生成对抗网络Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks

https://arxiv.org/abs/1511.06434

12.由图片推演视频Generating Videos with Scene Dynamics

http://carlvondrick.com/tinyvideo/

13.视频预测和无监督学习Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning

https://coxlab.github.io/prednet/

78.HashMap与HashTable区别

　　点评：HashMap基于Hashtable实现，不同之处在于HashMap是非同步的，并且允许null，即null value和null key，Hashtable则不允许null，详见：

http://oznyang.iteye.com/blog/30690

。此外，记住一点：hashmap/hashset等凡是带有hash字眼的均基于hashtable实现，没带hash字眼的如set/map均是基于红黑树实现，前者无序，后者有序，详见此文第一部分：《教你如何迅速秒杀掉：99%的海量数据处理面试题》

http://blog.csdn.net/v_july_v/ ... 82693

　　不过，估计还是直接来图更形象点，故直接上图（图片来源：July9月28日在上海交大面试&算法讲座的PPT

http://vdisk.weibo.com/s/zrFL6OXKg_1me

）：

[![78-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/fc848c2e7704762ba9414a3bd10b682f.jpg)](https://ask.julyedu.com/uploads/questions/20171019/fc848c2e7704762ba9414a3bd10b682f.jpg)





79.在分类问题中，我们经常会遇到正负样本数据量不等的情况，比如正样本为10w条数据，负样本只有1w条数据，以下最合适的处理方法是( )

A 将负样本重复10次，生成10w样本量，打乱顺序参与分类

B 直接进行分类，可以最大限度利用数据

C 从10w正样本中随机抽取1w参与分类

D 将负样本每个权重设置为10，正样本权重为1，参与训练过程

  @管博士：准确的说，其实选项中的这些方法各有优缺点，需要具体问题具体分析，有篇文章对各种方法的优缺点进行了分析，讲的不错 感兴趣的同学可以参考一下：

https://www.analyticsvidhya.co ... blem/

。

80.以下第80题~第94题来自：<http://blog.csdn.net/u011204487>

  深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为

m∗n，n∗p，p∗q，且m<n<p<q，以下计算顺序效率最高的是（A） 

A.(AB)C

B.AC(B)

C.A(BC)

D.所以效率都相同

  @BlackEyes_SGC： m*n*p<m*n*q,m*p*q< n*p*q, 所以 (AB)C 最小

81.Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是:(C)

 

  A.各类别的先验概率P(C)是相等的

  B.以0为均值，sqr(2)/2为标准差的正态分布

  C.特征变量X的各个维度是类别条件独立随机变量

  D.P(X|C)是高斯分布

  @BlackEyes_SGC：朴素贝叶斯的条件就是每个变量相互独立。

82.关于支持向量机SVM,下列说法错误的是（C）

 

  A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力

  B.Hinge 损失函数，作用是最小化经验分类错误

  C.分类间隔为1/||w||，||w||代表向量的模

  D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习

  @BlackEyes_SGC：A正确。考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。B正确。 C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。 D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和ai∗yi∗xi，a变小使得w变小，因此间隔2/||w||变大

83.在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计(D)

 

A.EM算法 B.维特比算法 C.前向后向算法 D.极大似然估计

  @BlackEyes_SGC： EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法

  维特比算法： 用动态规划解决HMM的预测问题，不是参数估计

  前向后向算法：用来算概率

  极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数

  注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。

84.假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是(BD)：

 

A.这个被重复的特征在模型中的决定作用会被加强

B.模型效果相比无重复特征的情况下精确度会降低

C.如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。

D.当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题

E.NB可以用来做最小二乘回归

F.以上说法都不正确

  @BlackEyes_SGC：NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分

85.以下哪些方法不可以直接来对文本分类？ (A)

A、Kmeans B、决策树 C、支持向量机 D、KNN

  @BlackEyes_SGC：A：Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。

86.已知一组数据的协方差矩阵P,下面关于主分量说法错误的是(C)

 

A、主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小 

B、在经主分量分解后,协方差矩阵成为对角矩阵 

C、主分量分析就是K-L变换 

D、主分量是通过求协方差矩阵的特征值得到

  @BlackEyes_SGC：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。

87.kmeans的复杂度

[![87-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/cfe655b3a426e365e6113bc14d17db65.jpg)](https://ask.julyedu.com/uploads/questions/20171019/cfe655b3a426e365e6113bc14d17db65.jpg)

  时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数

88.关于logit 回归和SVM 不正确的是（A）

A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。A错误

B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确

C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。

D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。

  @BlackEyes_SGC：Logit回归目标函数是最小化后验概率，Logit回归可以用于预测事件发生概率的大小，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。

89.输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为(97)：

 

  @BlackEyes_SGC：计算尺寸不被整除只在GoogLeNet中遇到过。卷积向下取整，池化向上取整。

  本题 （200-5+2*1）/2+1 为99.5，取99

  （99-3）/1+1 为97

  （97-3+2*1）/1+1 为97

  研究过网络的话看到stride为1的时候，当kernel为 3 padding为1或者kernel为5 padding为2 一看就是卷积前后尺寸不变。计算GoogLeNet全过程的尺寸也一样。

90.影响聚类算法结果的主要因素有（B、C、D ）。

A.已知类别的样本质量； B.分类准则； C.特征选取； D.模式相似性测度 

91.模式识别中，马式距离较之于欧式距离的优点是（C、D）。

A.平移不变性； B.旋转不变性； C尺度不变性； D.考虑了模式的分布

92.影响基本K-均值算法的主要因素有(ABD）。

A.样本输入顺序；

B.模式相似性测度；

C.聚类准则；

D.初始类中心的选取 

93.在统计模式分类问题中，当先验概率未知时，可以使用（BD）。

A. 最小损失准则； 

B. 最小最大损失准则；

C. 最小误判概率准则； 

D. N-P判决

94.如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（BC）。

A. 已知类别样本质量； B. 分类准则； C. 特征选取； D. 量纲

95.欧式距离具有（A B ）；马式距离具有（A B C D ）。

A. 平移不变性； B. 旋转不变性； C. 尺度缩放不变性； D. 不受量纲影响的特性

96.你有哪些deep learning（rnn、cnn）调参的经验？

参考

https://www.zhihu.com/question/41631631

97.简单说说RNN的原理

  我们升学到高三准备高考时，此时的知识是由高二及高二之前所学的知识加上高三所学的知识合成得来，即我们的知识是由前序铺垫，是有记忆的，好比当电影字幕上出现：“我是”时，你会很自然的联想到：“我是中国人”。

[![97-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/4a97699aa3a223a0a4d956abc48145df.jpg)](https://ask.julyedu.com/uploads/questions/20171019/4a97699aa3a223a0a4d956abc48145df.jpg)

  关于RNN，这里有课程详细讲RNN，包括RNN条件生成、attention，以及LSTM等等均有细致讲解：

http://www.julyedu.com/category/index/8/21

98.什么是RNN？

  @一只鸟的天空，本题解析来源：

http://blog.csdn.net/heyongluo ... 36251

  RNNs的目的使用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。理论上，RNNs能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关，下图便是一个典型的RNNs： 

[![98-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/be54602c1742d204ef4d566db49f6912.jpg)](https://ask.julyedu.com/uploads/questions/20171019/be54602c1742d204ef4d566db49f6912.jpg)

[![98-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/992a4623cead4868d5f425eb117bcde1.jpg)](https://ask.julyedu.com/uploads/questions/20171019/992a4623cead4868d5f425eb117bcde1.jpg)

From Nature 

  RNNs包含输入单元(Input units)，输入集标记为{x0,x1,...,xt,xt+1,...}，而输出单元(Output units)的输出集则被标记为{y0,y1,...,yt,yt+1.,..}。RNNs还包含隐藏单元(Hidden units)，我们将其输出集标记为{s0,s1,...,st,st+1,...}，这些隐藏单元完成了最为主要的工作。你会发现，在图中：有一条单向流动的信息流是从输入单元到达隐藏单元的，与此同时另一条单向流动的信息流从隐藏单元到达输出单元。在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”，并且隐藏层的输入还包括上一隐藏层的状态，即隐藏层内的节点可以自连也可以互连。 

  上图将循环神经网络进行展开成一个全神经网络。例如，对一个包含5个单词的语句，那么展开的网络便是一个五层的神经网络，每一层代表一个单词。对于该网络的计算过程如下：

  xt表示第t,t=1,2,3...步(step)的输入。比如，x1为第二个词的one-hot向量(根据上图，x0为第一个词)； 

  st为隐藏层的第t步的状态，它是网络的记忆单元。 st根据当前输入层的输出与上一步隐藏层的状态进行计算。st=f(Uxt+Wst−1)，其中f一般是非线性的激活函数，如tanh（

https://reference.wolfram.com/ ... .html

）或ReLU（

https://en.wikipedia.org/wiki/ ... %2529

），在计算s0时，即第一个单词的隐藏层状态，需要用到s−1，但是其并不存在，在实现中一般置为0向量；

  ot是第t步的输出，如下个单词的向量表示，ot=softmax(Vst). 

  更多请看此文：

http://blog.csdn.net/heyongluo ... 36251

\99. RNN是怎么从单层网络一步一步构造的的?

  @何之源，本题解析来源：

https://zhuanlan.zhihu.com/p/28054589

  一、从单层网络谈起

  在学习RNN之前，首先要了解一下最基本的单层网络，它的结构如图：

[![99-1.jpg](https://ask.julyedu.com/uploads/questions/20171019/b928bd2e9b7a55aa1b928994869030be.jpg)](https://ask.julyedu.com/uploads/questions/20171019/b928bd2e9b7a55aa1b928994869030be.jpg)

  输入是x，经过变换Wx+b和激活函数f得到输出y。相信大家对这个已经非常熟悉了。

  二、经典的RNN结构（N vs N）

  在实际应用中，我们还会遇到很多序列形的数据：

[![99-2.jpg](https://ask.julyedu.com/uploads/questions/20171019/2380f19a56b8c6b21939a7c47a50f1f3.jpg)](https://ask.julyedu.com/uploads/questions/20171019/2380f19a56b8c6b21939a7c47a50f1f3.jpg)

如：

自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。

语音处理。此时，x1、x2、x3……是每帧的声音信号。

时间序列问题。例如每天的股票价格等等。

  序列形的数据就不太好用原始的神经网络处理了。为了建模序列问题，RNN引入了隐状态h（hidden state）的概念，h可以对序列形的数据提取特征，接着再转换为输出。先从h1的计算开始看：

[![99-3.jpg](https://ask.julyedu.com/uploads/questions/20171019/fa070df85771400de847da1a35c29f54.jpg)](https://ask.julyedu.com/uploads/questions/20171019/fa070df85771400de847da1a35c29f54.jpg)

  图示中记号的含义是：

圆圈或方块表示的是向量。

一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换。

  在很多论文中也会出现类似的记号，初学的时候很容易搞乱，但只要把握住以上两点，就可以比较轻松地理解图示背后的含义。

  h2的计算和h1类似。要注意的是，在计算时，每一步使用的参数U、W、b都是一样的，也就是说每个步骤的参数都是共享的，这是RNN的重要特点，一定要牢记。

[![99-4.jpg](https://ask.julyedu.com/uploads/questions/20171019/d34c561a201016676393f9a82e46c24f.jpg)](https://ask.julyedu.com/uploads/questions/20171019/d34c561a201016676393f9a82e46c24f.jpg)

  依次计算剩下来的（使用相同的参数U、W、b）：

[![99-5.jpg](https://ask.julyedu.com/uploads/questions/20171019/63aac25b4b25c143275713df2a672907.jpg)](https://ask.julyedu.com/uploads/questions/20171019/63aac25b4b25c143275713df2a672907.jpg)

  我们这里为了方便起见，只画出序列长度为4的情况，实际上，这个计算过程可以无限地持续下去。

  我们目前的RNN还没有输出，得到输出值的方法就是直接通过h进行计算：

[![99-6.jpg](https://ask.julyedu.com/uploads/questions/20171019/04a42e44fdbfb5ca72d512dfe9a98300.jpg)](https://ask.julyedu.com/uploads/questions/20171019/04a42e44fdbfb5ca72d512dfe9a98300.jpg)

  正如之前所说，一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1。

  剩下的输出类似进行（使用和y1同样的参数V和c）：

[![99-7.jpg](https://ask.julyedu.com/uploads/questions/20171019/9732ba6fe59c5842b300583ce577a6d9.jpg)](https://ask.julyedu.com/uploads/questions/20171019/9732ba6fe59c5842b300583ce577a6d9.jpg)

  OK！大功告成！这就是最经典的RNN结构，我们像搭积木一样把它搭好了。它的输入是x1, x2, .....xn，输出为y1, y2, ...yn，也就是说，输入和输出序列必须要是等长的。

  由于这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模，如：

计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。

输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：The Unreasonable Effectiveness of Recurrent Neural Networks，Char RNN可以用来生成文章、诗歌，甚至是代码。此篇博客里有自动生成歌词的实验教程《基于torch学汪峰写歌词、聊天机器人、图像着色/生成、看图说话、字幕生成》）。

  三、N VS 1

  有的时候，我们要处理的问题输入是一个序列，输出是一个单独的值而不是序列，应该怎样建模呢？实际上，我们只在最后一个h上进行输出变换就可以了：

[![99-8.jpg](https://ask.julyedu.com/uploads/questions/20171019/a76549fd29a8cdfc1a7c1a9c0a322586.jpg)](https://ask.julyedu.com/uploads/questions/20171019/a76549fd29a8cdfc1a7c1a9c0a322586.jpg)

  这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。

  四、1 VS N

  输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算：

[![99-9.jpg](https://ask.julyedu.com/uploads/questions/20171019/937e874c80548215d416572217e91d53.jpg)](https://ask.julyedu.com/uploads/questions/20171019/937e874c80548215d416572217e91d53.jpg)

  还有一种结构是把输入信息X作为每个阶段的输入：

[![99-10.jpg](https://ask.julyedu.com/uploads/questions/20171019/da3fb1d65d9f00d284be0cf4485b0e3e.jpg)](https://ask.julyedu.com/uploads/questions/20171019/da3fb1d65d9f00d284be0cf4485b0e3e.jpg)

  下图省略了一些X的圆圈，是一个等价表示：

[![99-11.jpg](https://ask.julyedu.com/uploads/questions/20171019/29a0d1d5ff358b974558a8885b7dff56.jpg)](https://ask.julyedu.com/uploads/questions/20171019/29a0d1d5ff358b974558a8885b7dff56.jpg)

这种1 VS N的结构可以处理的问题有：

从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子

从类别生成语音或音乐等

  五、N vs M

  下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。

  原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。

  为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：

[![99-12.jpg](https://ask.julyedu.com/uploads/questions/20171019/f30b2aa8cc13067a475318d8c8750c72.jpg)](https://ask.julyedu.com/uploads/questions/20171019/f30b2aa8cc13067a475318d8c8750c72.jpg)

  得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。

  拿到c之后，就用另一个RNN网络对其进行解码，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：

[![99-13.jpg](https://ask.julyedu.com/uploads/questions/20171019/6f061978109bd6b2cd0dd1ef169ea63b.jpg)](https://ask.julyedu.com/uploads/questions/20171019/6f061978109bd6b2cd0dd1ef169ea63b.jpg)

  还有一种做法是将c当做每一步的输入：

[![99-14.jpg](https://ask.julyedu.com/uploads/questions/20171019/20bdb104ced5e23964aeb71b8cb22cbd.jpg)](https://ask.julyedu.com/uploads/questions/20171019/20bdb104ced5e23964aeb71b8cb22cbd.jpg)

  由于这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：

机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的

文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列。

阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。

语音识别。输入是语音信号序列，输出是文字序列。

100.RNN中只能采用tanh而不是ReLu作为激活函数么？

  解析详见：

https://www.zhihu.com/question/61265076

![end.jpg](https://ask.julyedu.com/uploads/questions/20171019/e2490643dd233b6b8ea7df3f75f686a7.jpg)



101.深度学习（CNN RNN Attention）解决大规模文本分类问题

  

https://zhuanlan.zhihu.com/p/25928551

102.如何解决RNN梯度爆炸和弥散的问题的？

  本题解析来源：

http://blog.csdn.net/han_xiaoy ... 32536

  为了解决梯度爆炸问题，Thomas Mikolov首先提出了一个简单的启发性的解决方案，就是当梯度大于一定阈值的的时候，将它截断为一个较小的数。具体如算法1所述：

  算法：当梯度爆炸时截断梯度（伪代码）

[![102.png](https://ask.julyedu.com/uploads/questions/20171116/c7179190c8e267fb859cb177912d89bd.png)](https://ask.julyedu.com/uploads/questions/20171116/c7179190c8e267fb859cb177912d89bd.png)

  下图可视化了梯度截断的效果。它展示了一个小的rnn（其中W为权值矩阵，b为bias项）的决策面。这个模型是一个一小段时间的rnn单元组成；实心箭头表明每步梯度下降的训练过程。当梯度下降过程中，模型的目标函数取得了较高的误差时，梯度将被送到远离决策面的位置。截断模型产生了一个虚线，它将误差梯度拉回到离原始梯度接近的位置。

[![102.1_.png](https://ask.julyedu.com/uploads/questions/20171116/5bb2c967ba0702a70ee3b18daad6536b.png)](https://ask.julyedu.com/uploads/questions/20171116/5bb2c967ba0702a70ee3b18daad6536b.png)

梯度爆炸，梯度截断可视化 

  为了解决梯度弥散的问题，我们介绍了两种方法。第一种方法是将随机初始化W(hh)改为一个有关联的矩阵初始化。第二种方法是使用ReLU（Rectified Linear Units）代替sigmoid函数。ReLU的导数不是0就是1.因此，神经元的梯度将始终为1，而不会当梯度传播了一定时间之后变小。

103.如何提高深度学习的性能

  

http://blog.csdn.net/han_xiaoy ... 54879

104.RNN、LSTM、GRU区别

  @我愛大泡泡，本题解析来源：

http://blog.csdn.net/woaidapao ... 06273

  RNN引入了循环的概念，但是在实际过程中却出现了初始信息随时间消失的问题，即长期依赖（Long-Term Dependencies）问题，所以引入了LSTM。

  LSTM：因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸的变化是关键，下图非常明确适合记忆：

[![104.1_.png](https://ask.julyedu.com/uploads/questions/20171116/b72dce624e03b19167070c8c033d2003.png)](https://ask.julyedu.com/uploads/questions/20171116/b72dce624e03b19167070c8c033d2003.png)

 GRU是LSTM的变体，将忘记门和输入们合成了一个单一的更新门。

[![104.2_.png](https://ask.julyedu.com/uploads/questions/20171116/30c87552f8aaf47df031edfbc856e7c1.png)](https://ask.julyedu.com/uploads/questions/20171116/30c87552f8aaf47df031edfbc856e7c1.png)

105.当机器学习性能遭遇瓶颈时，你会如何优化的？

  可以从这4个方面进行尝试：、基于数据、借助算法、用算法调参、借助模型融合。当然能谈多细多深入就看你的经验心得了。这里有一份参考清单：机器学习性能改善备忘单（

http://blog.csdn.net/han_xiaoy ... 53145

）

106.做过什么样的机器学习项目？比如如何从零构建一个推荐系统

  这里有一个推荐系统的公开课

http://www.julyedu.com/video/play/18/148

，另，再推荐一个课程：机器学习项目班 [10次纯项目讲解，100%纯实战]

107.什么样的资料集不适合用深度学习?

  @抽象猴，来源：

https://www.zhihu.com/question/41233373

  a.数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

  b.数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

108.广义线性模型是怎被应用在深度学习中?

  @许韩，来源：

https://www.zhihu.com/question ... 04190

A Statistical View of Deep Learning (I): Recursive GLMs

  深度学习从统计学角度，可以看做递归的广义线性模型。

  广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。

  深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。下图是一个对照表

[![108.1_.png](https://ask.julyedu.com/uploads/questions/20171116/1f42c3b983593a0c3def11f694b3ed7e.png)](https://ask.julyedu.com/uploads/questions/20171116/1f42c3b983593a0c3def11f694b3ed7e.png)

109.准备机器学习面试应该了解哪些理论知识

  @穆文，来源：

https://www.zhihu.com/question/62482926

[![109.png](https://ask.julyedu.com/uploads/questions/20171116/999c1419d30f1450f598a1ae89b91d6a.png)](https://ask.julyedu.com/uploads/questions/20171116/999c1419d30f1450f598a1ae89b91d6a.png)

  看下来，这些问题的答案基本都在本BAT机器学习面试1000题系列里了。

110.标准化与归一化的区别?

  简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：

  特征向量的缺失值处理

  1. 缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。

  2. 缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:

  1) 把NaN直接作为一个特征，假设用0表示；

  2) 用均值填充；

  3) 用随机森林等算法预测填充

111.随机森林如何处理缺失值

  方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。

  方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似1缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩2。

112.随机森林如何评估特征重要性

  衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：

  1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。

  2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。

113.优化Kmeans

  使用kd树或者ball tree，将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。

114.KMeans初始类簇中心点的选取

  k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。

  1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心

  2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)

  3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大

  4. 重复2和3直到k个聚类中心被选出来

  5. 利用这k个初始的聚类中心来运行标准的k-means算法

115.解释对偶的概念

  一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

 

116.如何进行特征选择？

  特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解

  常见的特征选择方式：

  1. 去除方差较小的特征

  2. 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。

  3. 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。

  4. 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。

117.数据预处理

  1. 缺失值，填充缺失值fillna：

  i. 离散：None,

  ii. 连续：均值。

  iii. 缺失值太多，则直接去除该列

  2. 连续值：离散化。有的模型（如决策树）需要离散值

  3. 对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作

  4. 皮尔逊相关系数，去除高度相关的列

118.你知道有哪些数据处理和特征工程的处理？

[![119.png](https://ask.julyedu.com/uploads/questions/20171116/508c916b571e42afa2676981c4d02c2d.png)](https://ask.julyedu.com/uploads/questions/20171116/508c916b571e42afa2676981c4d02c2d.png)

  更多请查看此课程《机器学习工程师 第八期 [六大阶段、层层深入]》第7次课 特征工程（

https://www.julyedu.com/course/getDetail/65

）

119.简单说说特征工程

[![118.png](https://ask.julyedu.com/uploads/questions/20171116/6e13c3149bdea7fde4d42db6812bb66e.png)](https://ask.julyedu.com/uploads/questions/20171116/6e13c3149bdea7fde4d42db6812bb66e.png)

上图来源：

http://www.julyedu.com/video/play/18

120.请对比下Sigmoid、Tanh、ReLu这三个激活函数

[![120.png](https://ask.julyedu.com/uploads/questions/20171116/3064c1e03e62b48cf7cc6a82f2719058.png)](https://ask.julyedu.com/uploads/questions/20171116/3064c1e03e62b48cf7cc6a82f2719058.png)

  logistic函数，应用在Logistic回归中。<span style="color: rgb(51, 51, 51); font-family:;" new="" times="" 14px;"="">logistic回归的目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。

[![120.1_.png](https://ask.julyedu.com/uploads/questions/20171116/69bddde88c19d75a8c2f51757fe72429.png)](https://ask.julyedu.com/uploads/questions/20171116/69bddde88c19d75a8c2f51757fe72429.png)

[![120.2_.png](https://ask.julyedu.com/uploads/questions/20171116/840e44a7ac1816cd393a0ee247572140.png)](https://ask.julyedu.com/uploads/questions/20171116/840e44a7ac1816cd393a0ee247572140.png)

更多详见：

https://mp.weixin.qq.com/s/7DgiXCNBS5vb07WIKTFYRQ

121.Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不足，有没改进的激活函数

@我愛大泡泡，来源：

http://blog.csdn.net/woaidapao ... 06273

[![122.png](https://ask.julyedu.com/uploads/questions/20171116/b75c2c66b68e9f4a31731ad884ef8b76.png)](https://ask.julyedu.com/uploads/questions/20171116/b75c2c66b68e9f4a31731ad884ef8b76.png)

122.怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感？

https://www.zhihu.com/question/58230411

123.为什么引入非线性激励函数？

  @Begin Again，来源：

https://www.zhihu.com/question/29021768

  如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。

  正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释）。

124.请问人工神经网络中为什么ReLu要好过于tanh和sigmoid function?

  @Begin Again，来源：

https://www.zhihu.com/question/29021768

  第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。

  第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。

  第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。当然现在也有一些对relu的改进，比如prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的大家可以找相关的paper看。

  多加一句，现在主流的做法，会多做一步batch normalization，尽可能保证每一层网络的输入具有相同的分布[1]。而最新的paper[2]，他们在加入bypass connection之后，发现改变batch normalization的位置会有更好的效果。大家有兴趣可以看下。

[1] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.

[2] He, Kaiming, et al. "Identity Mappings in Deep Residual Networks." arXiv preprint arXiv:1603.05027 (2016). 

125.为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数？

  为什么不是选择统一一种sigmoid或者tanh，而是混合使用呢？这样的目的是什么？

[![125.png](https://ask.julyedu.com/uploads/questions/20171116/96bdc0fd7aa6768e020fbc1f1d93fe0d.png)](https://ask.julyedu.com/uploads/questions/20171116/96bdc0fd7aa6768e020fbc1f1d93fe0d.png)

  本题解析来源：

https://www.zhihu.com/question/46197687

  @beanfrog：二者目的不一样：sigmoid 用在了各种gate上，产生0~1之间的值，这个一般只有sigmoid最直接了。tanh 用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。

  @hhhh：另可参见A Critical Review of Recurrent Neural Networks for Sequence Learning的section4.1，说了那两个tanh都可以替换成别的。

126.衡量分类器的好坏。

　　@我愛大泡泡，来源：

http://blog.csdn.net/woaidapao ... 06273

　　这里首先要知道TP、FN（真的判成假的）、FP（假的判成真）、TN四种（可以画一个表格）。 

　　几种常用的指标：

　　精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）

　　召回率 recall = TP/(TP+FN) = TP/ P

　　F1值： 2/F1 = 1/recall + 1/precision

　　ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N

127.机器学习和统计里面的auc的物理意义是什么？

　　详情参见

https://www.zhihu.com/question/39840928

128.观察增益gain, alpha和gamma越大，增益越小？

　　@AntZ：xgboost寻找分割点的标准是最大化gain. 考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中计算Gain按最大值找出最佳的分割点。它的计算公式分为四项, 可以由正则化项参数调整(lamda为叶子权重平方和的系数, gama为叶子数量):

[![128.png](https://ask.julyedu.com/uploads/questions/20171116/b528b5b4227d845595159da4642aad17.png)](https://ask.julyedu.com/uploads/questions/20171116/b528b5b4227d845595159da4642aad17.png)

　　第一项是假设分割的左孩子的权重分数, 第二项为右孩子, 第三项为不分割总体分数, 最后一项为引入一个节点的复杂度损失

由公式可知, gama越大gain越小, lamda越大, gain可能小也可能大。

　　原问题是alpha而不是lambda, 这里paper上没有提到, xgboost实现上有这个参数. 上面是我从paper上理解的答案,下面是搜索到的:

https://zhidao.baidu.com/quest ... 3Dgbk

　　lambda[默认1]权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。11、alpha[默认1]权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。

　　gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。

129.什么造成梯度消失问题? 推导一下

　　@许韩，来源：

https://www.zhihu.com/question ... 04190

Yes you should understand backdrop－Andrej Karpathy

How does the ReLu solve the vanishing gradient problem?

　　神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。

　　梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。

[![129.png](https://ask.julyedu.com/uploads/questions/20171116/d8ebb9dc795455dbfea4ade8eccec461.png)](https://ask.julyedu.com/uploads/questions/20171116/d8ebb9dc795455dbfea4ade8eccec461.png)

130.什么是梯度消失和梯度爆炸？

  @寒小阳，反向传播中链式法则带来的连乘，如果有数很小趋于0，结果就会特别小（梯度消失）；如果数都比较大，可能结果会很大（梯度爆炸）。

  @单车，下段来源：

https://zhuanlan.zhihu.com/p/25631496

  层数比较多的神经网络模型在训练时也是会出现一些问题的，其中就包括梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。

  例如，对于下图所示的含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。

[![130.1_.png](https://ask.julyedu.com/uploads/questions/20171116/23771243a0224190bd990bf2271395e3.png)](https://ask.julyedu.com/uploads/questions/20171116/23771243a0224190bd990bf2271395e3.png)

[![130.2_.png](https://ask.julyedu.com/uploads/questions/20171116/32c9473280aa621b337b2e62a4f53313.png)](https://ask.julyedu.com/uploads/questions/20171116/32c9473280aa621b337b2e62a4f53313.png)

  其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。

131.如何解决梯度消失和梯度膨胀?

  （1）梯度消失：

  根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0

  可以采用ReLU激活函数有效的解决梯度消失的情况

  （2）梯度膨胀

  根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大

  可以通过激活函数来解决

132.推导下反向传播Backpropagation

  @我愛大泡泡，来源：

http://blog.csdn.net/woaidapao ... 06273

  反向传播是在求解损失函数L对参数w求导时候用到的方法，目的是通过链式法则对参数进行一层一层的求导。这里重点强调：要将参数进行随机初始化而不是全部置0，否则所有隐层的数值都会与输入相关，这称为对称失效。 

  大致过程是:

[![132.png](https://ask.julyedu.com/uploads/questions/20171116/6e11e96ae361163e94b1fcc740d3ebe4.png)](https://ask.julyedu.com/uploads/questions/20171116/6e11e96ae361163e94b1fcc740d3ebe4.png)

133.SVD和PCA

　　PCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。

134.数据不平衡问题

　　这主要是由于数据分布不平衡造成的。解决方法如下：

  1）采样，对小样本加噪声采样，对大样本进行下采样

  2）进行特殊的加权，如在Adaboost中或者SVM中

  3）采用对不平衡数据集不敏感的算法

  4）改变评价标准：用AUC/ROC来进行评价

  5）采用Bagging/Boosting/ensemble等方法

  6）考虑数据的先验分布

135.简述神经网络的发展

 

  MP模型+sgn—->单层感知机（只能线性）+sgn— Minsky 低谷 —>多层感知机+BP+sigmoid—- (低谷) —>深度学习+pre-training+ReLU/sigmoid

136.深度学习常用方法

  @SmallisBig，来源：

http://blog.csdn.net/u01049616 ... 50487

  全连接DNN（相邻层相互连接、层内无连接）： 

  AutoEncoder(尽可能还原输入)、Sparse Coding（在AE上加入L1规范）、RBM（解决概率问题）—–>特征探测器——>栈式叠加 贪心训练 

  RBM—->DBN 

  解决全连接DNN的全连接问题—–>CNN 

  解决全连接DNN的无法对时间序列上变化进行建模的问题—–>RNN—解决时间轴上的梯度消失问题——->LSTM

137.神经网络模型（Neural Network）因受人类大脑的启发而得名。

[![137.1_.jpg](https://ask.julyedu.com/uploads/questions/20171116/0c3a861a31cd7eba552c8ec8b72ac1a0.jpg)](https://ask.julyedu.com/uploads/questions/20171116/0c3a861a31cd7eba552c8ec8b72ac1a0.jpg)

　　神经网络由许多神经元（Neuron）组成，每个神经元接受一个输入，对输入进行处理后给出一个输出，如下图所示。请问下列关于神经元的描述中，哪一项是正确的？

[![137.2_.jpg](https://ask.julyedu.com/uploads/questions/20171116/dd8edc5f70c192a4802a69ef8560505e.jpg)](https://ask.julyedu.com/uploads/questions/20171116/dd8edc5f70c192a4802a69ef8560505e.jpg)

　　A.每个神经元只有一个输入和一个输出

　　B.每个神经元有多个输入和一个输出

　　C.每个神经元有一个输入和多个输出

　　D.每个神经元有多个输入和多个输出

　　E.上述都正确

　　答案：（E），每个神经元可以有一个或多个输入，和一个或多个输出。

138.下图是一个神经元的数学表示

[![138.1_.jpg](https://ask.julyedu.com/uploads/questions/20171116/5ea89a457c468178be157fe8aa005fce.jpg)](https://ask.julyedu.com/uploads/questions/20171116/5ea89a457c468178be157fe8aa005fce.jpg)

　　这些组成部分分别表示为：

　　- x1, x2,…, xN：表示神经元的输入。可以是输入层的实际观测值，也可以是某一个隐藏层（Hidden Layer）的中间值

　　- w1, w2,…,wN：表示每一个输入的权重

　　- bi：表示偏差单元/偏移量（bias unit）。作为常数项加到激活函数的输入当中，类似截距（Intercept）

　　- a：作为神经元的激励函数（Activation），可以表示为

[![138.2_.jpg](https://ask.julyedu.com/uploads/questions/20171116/ae1bb473643b380089fa91dd5b60feaf.jpg)](https://ask.julyedu.com/uploads/questions/20171116/ae1bb473643b380089fa91dd5b60feaf.jpg)

　　- y：神经元输出

　　考虑上述标注，线性等式（y = mx + c）可以被认为是属于神经元吗：

　　答案：是的，一个不包含非线性的神经元可以看作是线性回归函数（Linear Regression Function）。

139.在一个神经网络中，知道每一个神经元的权重和偏差是最重要的一步。如果知道了神经元准确的权重和偏差，便可以近似任何函数，但怎么获知每个神经的权重和偏移呢？

　　A 搜索每个可能的权重和偏差组合，直到得到最佳值

　　B 赋予一个初始值，然后检查跟最佳值的差值，不断迭代调整权重

　　C 随机赋值，听天由命

　　D 以上都不正确的

  答案：（C）选项C是对梯度下降的描述。

140.梯度下降算法的正确步骤是什么？

  1.计算预测值和真实值之间的误差

  2.重复迭代，直至得到网络权重的最佳值

  3.把输入传入网络，得到输出值

  4.用随机值初始化权重和偏差

  5.对每一个产生误差的神经元，调整相应的（权重）值以减小误差

  答案：正确步骤排序是：4, 3, 1, 5, 2

141.已知：

　　- 大脑是有很多个叫做神经元的东西构成，神经网络是对大脑的简单的数学表达。

　　- 每一个神经元都有输入、处理函数和输出。

　　- 神经元组合起来形成了网络，可以拟合任何函数。

　　- 为了得到最佳的神经网络，我们用梯度下降方法不断更新模型

　　给定上述关于神经网络的描述，什么情况下神经网络模型被称为深度学习模型？

A 加入更多层，使神经网络的深度增加

B 有维度更高的数据

C 当这是一个图形识别的问题时

D 以上都不正确

  答案：（A）更多层意味着网络更深。没有严格的定义多少层的模型才叫深度模型，目前如果有超过2层的隐层，那么也可以及叫做深度模型。

142.卷积神经网络可以对一个输入进行多种变换（旋转、平移、缩放），这个表述正确吗？

　　答案：错误，把数据传入神经网络之前需要做一系列数据预处理（也就是旋转、平移、缩放）工作，神经网络本身不能完成这些变换。

143.下面哪项操作能实现跟神经网络中Dropout的类似效果？

　　A Boosting　　B Bagging　　C Stacking　　D Mapping

　　答案：B，Dropout可以认为是一种极端的Bagging，每一个模型都在单独的数据上训练，同时，通过和其他模型对应参数的共享，从而实现模型参数的高度正则化。

144.下列哪一项在神经网络中引入了非线性？

　　A 随机梯度下降　　B 修正线性单元（ReLU）　　C 卷积函数　　D 以上都不正确

　　答案：（B），修正线性单元是非线性的激活函数。

145.在训练神经网络时，损失函数(loss)在最初的几个epochs时没有下降，可能的原因是？（）

[![145.jpg](https://ask.julyedu.com/uploads/questions/20171116/3a3585425c4bf5f520f2a079aa17f268.jpg)](https://ask.julyedu.com/uploads/questions/20171116/3a3585425c4bf5f520f2a079aa17f268.jpg)

　　A 学习率(learning rate)太低　　B 正则参数太高　　C 陷入局部最小值　　D 以上都有可能

　　答案：（A）

146.下列哪项关于模型能力（model capacity）的描述是正确的？（指神经网络模型能拟合复杂函数的能力）

　　A 隐藏层层数增加，模型能力增加

　　B Dropout的比例增加，模型能力增加

　　C 学习率增加，模型能力增加

　　D 都不正确

　　答案：（A）

147.如果增加多层感知机（Multilayer Perceptron）的隐藏层层数，分类误差便会减小。这种陈述正确还是错误？

　　答案：错误。过拟合可能会导致错误增加。

148.构建一个神经网络，将前一层的输出和它自身作为输入。

[![148.png](https://ask.julyedu.com/uploads/questions/20171116/ca44a00c419f1c7b56f7846743b2fc95.png)](https://ask.julyedu.com/uploads/questions/20171116/ca44a00c419f1c7b56f7846743b2fc95.png)

下列哪一种架构有反馈连接？

　　A 循环神经网络　　B 卷积神经网络　　C 限制玻尔兹曼机　　D 都不是

　　答案：（A）

149.下列哪一项在神经网络中引入了非线性？在感知机中（Perceptron）的任务顺序是什么？

　　1.随机初始化感知机的权重

　　2.去到数据集的下一批（batch）

　　3.如果预测值和输出不一致，则调整权重

　　4.对一个输入样本，计算输出值

　　答案：1 - 4 - 3 - 2

150.假设你需要调整参数来最小化代价函数（cost function），可以使用下列哪项技术？

　　A． 穷举搜索　　B． 随机搜索　　C． Bayesian优化　　D． 以上任意一种

　　答案：（D）

151.在下面哪种情况下，一阶梯度下降不一定正确工作（可能会卡住）？

[![151.png](https://ask.julyedu.com/uploads/questions/20171116/334e2c3e35f848af39bb1a3a01af5f95.png)](https://ask.julyedu.com/uploads/questions/20171116/334e2c3e35f848af39bb1a3a01af5f95.png)

　　答案：（B）

　　这是鞍点（Saddle Point）的梯度下降的经典例子。另，本题来源于：

https://www.analyticsvidhya.co ... ning/

。

152.下图显示了训练过的3层卷积神经网络准确度，与参数数量(特征核的数量)的关系。

[![152.jpg](https://ask.julyedu.com/uploads/questions/20171116/732262298f46f13dc861d51946d9645c.jpg)](https://ask.julyedu.com/uploads/questions/20171116/732262298f46f13dc861d51946d9645c.jpg)

　　从图中趋势可见，如果增加神经网络的宽度，精确度会增加到一个特定阈值后，便开始降低。造成这一现象的可能原因是什么？

　　A 即使增加卷积核的数量，只有少部分的核会被用作预测

　　B 当卷积核数量增加时，神经网络的预测能力（Power）会降低

　　C 当卷积核数量增加时，它们之间的相关性增加(correlate)，导致过拟合

　　D 以上都不正确

　　答案：（C）

　　如C选项指出的那样，可能的原因是核之间的相关性。

153.假设我们有一个如下图所示的隐藏层。隐藏层在这个网络中起到了一定的降纬作用。假如现在我们用另一种维度下降的方法，比如说主成分分析法(PCA)来替代这个隐藏层。



[![153.jpg](https://ask.julyedu.com/uploads/questions/20171116/1890dc115086926be8f66ec6450f46ff.jpg)](https://ask.julyedu.com/uploads/questions/20171116/1890dc115086926be8f66ec6450f46ff.jpg)

　　那么，这两者的输出效果是一样的吗？

　　答案：不同，因为PCA用于相关特征而隐层用于有预测能力的特征

154.神经网络能组成函数(y=1/x)吗？

　　答案：可以，因为激活函数可以是互反函数

155.下列哪个神经网络结构会发生权重共享？

　　A.卷积神经网络　　B.循环神经网络　　C.全连接神经网络　　D.选项A和B

　　答案：（D）

156.批规范化(Batch Normalization)的好处都有啥？

　　A.在将所有的输入传递到下一层之前对其进行归一化（更改）

　　B.它将权重的归一化平均值和标准差

　　C.它是一种非常有效的反向传播(BP)方法

　　D.这些均不是

　　答案：（A）

157.在一个神经网络中，下面哪种方法可以用来处理过拟合？

　　A Dropout　　B 分批归一化(Batch Normalization)　　C 正则化(regularization)　　D 都可以

　　答案：（D）

158.如果我们用了一个过大的学习速率会发生什么？

　　答案：神经网络不会收敛

159.下图所示的网络用于训练识别字符H和T，如下所示：

[![159.png](https://ask.julyedu.com/uploads/questions/20171116/c979e84f38ec9a4d8809142433a4726b.png)](https://ask.julyedu.com/uploads/questions/20171116/c979e84f38ec9a4d8809142433a4726b.png)

　　D.可能是A或B，取决于神经网络的权重设置

　　答案：（D）

　　不知道神经网络的权重和偏差是什么，则无法判定它将会给出什么样的输出。 

160.假设我们已经在ImageNet数据集(物体识别)上训练好了一个卷积神经网络。然后给这张卷积神经网络输入一张全白的图片。对于这个输入的输出结果为任何种类的物体的可能性都是一样的，对吗？

　　答案：不对，各个神经元的反应是不一样的

161.当在卷积神经网络中加入池化层(pooling layer)时，变换的不变性会被保留，是吗？

　　答案：是，使用池化时会导致出现不变性。

162.当数据过大以至于无法在RAM中同时处理时，哪种梯度下降方法更加有效？

　　A 随机梯度下降法(Stochastic Gradient Descent)　　B 不知道　　C 整批梯度下降法(Full Batch Gradient Descent)　　D 都不是

　　答案：（A）

163.下图是一个利用sigmoid函数作为激活函数的含四个隐藏层的神经网络训练的梯度下降图。这个神经网络遇到了梯度消失的问题。下面哪个叙述是正确的？

[![163.png](https://ask.julyedu.com/uploads/questions/20171116/e8029bfc43461bcb05c9d7a0718893cc.png)](https://ask.julyedu.com/uploads/questions/20171116/e8029bfc43461bcb05c9d7a0718893cc.png)

　　A 第一隐藏层对应D，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应A

　　B 第一隐藏层对应A，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应D

　　C 第一隐藏层对应A，第二隐藏层对应B，第三隐藏层对应C，第四隐藏层对应D

　　D 第一隐藏层对应B，第二隐藏层对应D，第三隐藏层对应C，第四隐藏层对应A 

　　答案：（A），由于反向传播算法进入起始层，学习能力降低，这就是梯度消失。 

164.对于一个分类任务，如果开始时神经网络的权重不是随机赋值的，二是都设成0，下面哪个叙述是正确的？

　　A 其他选项都不对

　　B 没啥问题，神经网络会正常开始训练

　　C 神经网络可以训练，但是所有的神经元最后都会变成识别同样的东西

　　D 神经网络不会开始训练，因为没有梯度改变

　　答案：（C）

165.下图显示，当开始训练时，误差一直很高，这是因为神经网络在往全局最小值前进之前一直被卡在局部最小值里。为了避免这种情况，我们可以采取下面哪种策略？

[![165.png](https://ask.julyedu.com/uploads/questions/20171116/f9cfec1d4006cda66cf9baee91437291.png)](https://ask.julyedu.com/uploads/questions/20171116/f9cfec1d4006cda66cf9baee91437291.png)

　　A 改变学习速率，比如一开始的几个训练周期不断更改学习速率

　　B 一开始将学习速率减小10倍，然后用动量项(momentum)

　　C 增加参数数目，这样神经网络就不会卡在局部最优处

　　D 其他都不对 

　　答案：（A），选项A可以将陷于局部最小值的神经网络提取出来。

166.对于一个图像识别问题(在一张照片里找出一只猫)，下面哪种神经网络可以更好地解决这个问题？

　　A 循环神经网络　　B 感知机　　C 多层感知机　　D 卷积神经网络

　　卷积神经网络将更好地适用于图像相关问题，因为考虑到图像附近位置变化的固有性质。

　　答案：（D）

167.假设在训练中我们突然遇到了一个问题，在几次循环之后，误差瞬间降低

[![167.png](https://ask.julyedu.com/uploads/questions/20171116/db515b12144d6bc8189e8263722f85cb.png)](https://ask.julyedu.com/uploads/questions/20171116/db515b12144d6bc8189e8263722f85cb.png)

你打算怎么做来处理这个问题？

　　A 对数据作归一化　　B 对数据取对数变化　　C 都不对　　D 对数据作主成分分析(PCA)和归一化

　　答案：（D），首先将相关的数据去掉，然后将其置零。 

168.下面那个决策边界是神经网络生成的？



[![168.png](https://ask.julyedu.com/uploads/questions/20171116/c2353e9108d6566083fa5f8b81523d2b.png)](https://ask.julyedu.com/uploads/questions/20171116/c2353e9108d6566083fa5f8b81523d2b.png)

　　A. A　　B. D　　C. C　　D. B 　　E. 以上都有 

　　答案：（E）

169.在下图中，我们可以观察到误差出现了许多小的"涨落"。 这种情况我们应该担心吗？

[![169.png](https://ask.julyedu.com/uploads/questions/20171116/e08de9e9fbceb5f99a4d017119f598c8.png)](https://ask.julyedu.com/uploads/questions/20171116/e08de9e9fbceb5f99a4d017119f598c8.png)

　　答案：不需要，只要在训练集和交叉验证集上有累积的下降就可以了。为了减少这些“起伏”，可以尝试增加批尺寸(batch size)。 

170.在选择神经网络的深度时，下面那些参数需要考虑？

　　1 神经网络的类型(如MLP,CNN)　　2 输入数据　　3 计算能力(硬件和软件能力决定)

　　4 学习速率　　5 映射的输出函数

　　A 1,2,4,5　　B 2,3,4,5　　C 都需要考虑　　D 1,3,4,5

　　答案：（C），所有上述因素对于选择神经网络模型的深度都是重要的。

171.考虑某个具体问题时，你可能只有少量数据来解决这个问题。不过幸运的是你有一个类似问题已经预先训练好的神经网络。可以用下面哪种方法来利用这个预先训练好的网络？

　　A 把除了最后一层外所有的层都冻住，重新训练最后一层

　　B 对新数据重新训练整个模型

　　C 只对最后几层进行调参(fine tune)

　　D 对每一层模型进行评估，选择其中的少数来用

　　答案：（C）

172.增加卷积核的大小对于改进卷积神经网络的效果是必要的吗？

　　答案：不是，增加核函数的大小不一定会提高性能。这个问题在很大程度上取决于数据集。

173.请简述神经网络的发展史

　　@SIY.Z。本题解析来源：

https://zhuanlan.zhihu.com/p/29435406

　　sigmoid会饱和，造成梯度消失。于是有了ReLU。——ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。——强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。——太深了，梯度传不下去，于是有了highway。——干脆连highway的参数都不要，直接变残差，于是有了ResNet。——强行稳定参数的均值和方差，于是有了BatchNorm。——在梯度流中增加噪声，于是有了 Dropout。——RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。——LSTM简化一下，有了GRU。——GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。——WGAN对梯度的clip有问题，于是有了WGAN-GP。

174.说说spark的性能调优

　　

https://tech.meituan.com/spark-tuning-basic.html

　　

https://tech.meituan.com/spark-tuning-pro.html

175.如何理解LSTM网络？

　　@Not_GOD，本题解析来源：

http://www.jianshu.com/p/9dc9f41f0b29/

　　

Recurrent Neural Networks

　　人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。

　　传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。

　　RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。

[![103.1_.png](https://ask.julyedu.com/uploads/questions/20171116/ca6edcfdefdb6af23aac29ff79573861.png)](https://ask.julyedu.com/uploads/questions/20171116/ca6edcfdefdb6af23aac29ff79573861.png)

RNN 包含循环

　　在上面的示例图中，神经网络的模块，A，正在读取某个输入 x_i，并输出一个值 h_i。循环可以使得信息可以从当前步传递到下一步。

　　RNN 可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开：

[![103.2_.png](https://ask.julyedu.com/uploads/questions/20171116/0a7daf66541ba38122375f6d66e1974c.png)](https://ask.julyedu.com/uploads/questions/20171116/0a7daf66541ba38122375f6d66e1974c.png)

展开的 RNN

　　链式的特征揭示了 RNN 本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。

　　在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。我建议大家参考 Andrej Karpathy 的博客文章——The Unreasonable Effectiveness of Recurrent Neural Networks （

http://karpathy.github.io/2015 ... ness/

）

　　而这些成功应用的关键之处就是 LSTM 的使用，这是一种特别的 RNN，比标准的 RNN 在很多的任务上都表现得更好。这篇博文也会就 LSTM 进行展开。

　　

长期依赖（Long-Term Dependencies）问题

　　RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。当然还有很多依赖因素。

　　有时我们仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。

[![103.3_.png](https://ask.julyedu.com/uploads/questions/20171116/c704f5a93787b5afd66338a640f918a9.png)](https://ask.julyedu.com/uploads/questions/20171116/c704f5a93787b5afd66338a640f918a9.png)

不太长的相关信息和位置间隔

　　但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France... I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。

　　但在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。

[![103.4_.png](https://ask.julyedu.com/uploads/questions/20171116/856be7c918a8cc54a6bccc1a0ef06841.png)](https://ask.julyedu.com/uploads/questions/20171116/856be7c918a8cc54a6bccc1a0ef06841.png)

相当长的相关信息和位置间隔

　　在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。

　　然而，幸运的是，LSTM 并没有这个问题！

　　

LSTM 网络

　　Long Short Term 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。如@寒小阳所说：LSTM和基线RNN并没有特别大的结构不同，但是它们用了不同的函数来计算隐状态。LSTM的“记忆”我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的输入为前状态ht−1和当前输入xt。这些“细胞”会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。

　　LSTM 由Hochreiter & Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。

　　LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！

　　所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。

[![103.5_.png](https://ask.julyedu.com/uploads/questions/20171116/4c0a34314f88e336054b0a42b936195d.png)](https://ask.julyedu.com/uploads/questions/20171116/4c0a34314f88e336054b0a42b936195d.png)

标准 RNN 中的重复模块包含单一的层

　　LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互

[![103.6_.png](https://ask.julyedu.com/uploads/questions/20171116/c4850fec81a9cd774406936d807d0f16.png)](https://ask.julyedu.com/uploads/questions/20171116/c4850fec81a9cd774406936d807d0f16.png)

LSTM 中的重复模块包含四个交互的层

　　现在，我们先来熟悉一下图中使用的各种元素的图标。

[![103.7_.png](https://ask.julyedu.com/uploads/questions/20171116/d42e6ac5fe0513270f25e29d1c6f4739.png)](https://ask.julyedu.com/uploads/questions/20171116/d42e6ac5fe0513270f25e29d1c6f4739.png)

LSTM 中的图标

　　在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。

　　

LSTM 的核心思想

　　LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。

　　细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。

[![103.8_.png](https://ask.julyedu.com/uploads/questions/20171116/4ba171dc0c7817d0ac44bb2c3ee2e82d.png)](https://ask.julyedu.com/uploads/questions/20171116/4ba171dc0c7817d0ac44bb2c3ee2e82d.png)

　　LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。

[![103.9_.png](https://ask.julyedu.com/uploads/questions/20171116/4e96a9ff3e8e02cf55226ee0f4f45173.png)](https://ask.julyedu.com/uploads/questions/20171116/4e96a9ff3e8e02cf55226ee0f4f45173.png)

　　Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！

　　LSTM 拥有三个门，来保护和控制细胞状态。

　　

逐步理解 LSTM

　　在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取 h_{t-1} 和x_t，输出一个在 0 到 1 之间的数值给每个在细胞状态 C_{t-1} 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。

　　让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。

[![103.10_.png](https://ask.julyedu.com/uploads/questions/20171116/c5b49ca158d5d6dd4512293cd41e6cc8.png)](https://ask.julyedu.com/uploads/questions/20171116/c5b49ca158d5d6dd4512293cd41e6cc8.png)

决定丢弃信息

　　下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，\tilde{C}_t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。

　　在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。

[![103.11_.png](https://ask.julyedu.com/uploads/questions/20171116/07e93339da409ec89b00708b798868af.png)](https://ask.julyedu.com/uploads/questions/20171116/07e93339da409ec89b00708b798868af.png)

确定更新的信息

　　更新旧细胞状态，C_{t-1} 更新为 C_t。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。

　　把旧状态与 f_t 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i_t * \tilde{C}_t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。

　　在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。

[![103.12_.png](https://ask.julyedu.com/uploads/questions/20171116/f6a6d988bd325558b556ea12de94bc1b.png)](https://ask.julyedu.com/uploads/questions/20171116/f6a6d988bd325558b556ea12de94bc1b.png)

更新细胞状态

　　最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。

　　在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。

[![103.13_.png](https://ask.julyedu.com/uploads/questions/20171116/7f70b2e7011e1779958ae620efc99725.png)](https://ask.julyedu.com/uploads/questions/20171116/7f70b2e7011e1779958ae620efc99725.png)

输出信息

　　

LSTM 的变体

　　几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。

　　流形的 LSTM 变体，由 Gers & Schmidhuber (2000) 提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。

[![103.14_.png](https://ask.julyedu.com/uploads/questions/20171116/56ef84f22f43375828bc66e5dab2e470.png)](https://ask.julyedu.com/uploads/questions/20171116/56ef84f22f43375828bc66e5dab2e470.png)

peephole 连接

　　上面的图例中，我们增加了 peephole 到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。

　　还有通过使用 coupled 忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。

[![103.15_.png](https://ask.julyedu.com/uploads/questions/20171116/476a49380964f6e93268139ff5b9545d.png)](https://ask.julyedu.com/uploads/questions/20171116/476a49380964f6e93268139ff5b9545d.png)

coupled 忘记门和输入门

　　另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014) 提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。

[![103.16_.png](https://ask.julyedu.com/uploads/questions/20171116/3068e1e8236bf08bf2944192fba4ca6b.png)](https://ask.julyedu.com/uploads/questions/20171116/3068e1e8236bf08bf2944192fba4ca6b.png)

GRU

　　这里只是部分流行的 LSTM 变体。当然还有很多其他的，如Yao, et al. (2015) 提出的 Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik, et al. (2014) 提出的 Clockwork RNN。

　　要问哪个变体是最好的？其中的差异性真的重要吗？Greff, et al. (2015) 给出了流行变体的比较，结论是他们基本上是一样的。Jozefowicz, et al. (2015) 则在超过 1 万种 RNN 架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。

[![103.17_.png](https://ask.julyedu.com/uploads/questions/20171116/0ec6ecf25237f09db0fd0cc1cff8b9de.png)](https://ask.julyedu.com/uploads/questions/20171116/0ec6ecf25237f09db0fd0cc1cff8b9de.png)

Jozefowicz等人论文截图

　　

结论

　　刚开始，我提到通过 RNN 得到重要的结果。本质上所有这些都可以使用 LSTM 完成。对于大多数任务确实展示了更好的性能！

　　由于 LSTM 一般是通过一系列的方程表示的，使得 LSTM 有一点令人费解。然而本文中一步一步地解释让这种困惑消除了不少。

　　LSTM 是我们在 RNN 中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢？在研究人员间普遍的观点是：“Yes! 下一步已经有了——那就是注意力！” 这个想法是让 RNN 的每一步都从更加大的信息集中挑选信息。例如，如果你使用 RNN 来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，Xu, et al.(2015)已经这么做了——如果你希望深入探索注意力可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索……

　　注意力也不是 RNN 研究领域中唯一的发展方向。例如，Kalchbrenner, et al. (2015) 提出的 Grid LSTM 看起来也是很有前途。使用生成模型的 RNN，诸如Gregor, et al. (2015) Chung, et al. (2015) 和 Bayer & Osendorfer (2015) 提出的模型同样很有趣。在过去几年中，RNN 的研究已经相当的燃，而研究成果当然也会更加丰富！

再次说明下，本题解析基本取自Not_GOD翻译Christopher Olah 博文的《理解LSTM网络》，致谢。

176.常见的分类算法有哪些？

 

　　SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯

177.常见的监督学习算法有哪些？

 

　　感知机、svm、人工神经网络、决策树、逻辑回归

178.在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）

　　A. 增加训练集量　　B. 减少神经网络隐藏层节点数　　C. 删除稀疏的特征　　D. SVM算法中使用高斯核/RBF核代替线性核

　　正确答案：D，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

　　一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。

　　B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合

　　D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数之一。

179.下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测？

　　A.AR模型　　B.MA模型　　C.ARMA模型　　D.GARCH模型

　　正确答案：D，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

　　GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。

180.以下哪个属于线性分类器最佳准则?

　　A.感知准则函数　　B.贝叶斯分类　　C.支持向量机　　D.Fisher准则　　

　　正确答案：ACD，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

　　线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

181.基于二次准则函数的H-K算法较之于感知器算法的优点是（）?

　　A.计算量小　　B.可以判别问题是否线性可分　　C.其解完全适用于非线性可分的情况　　D.其解的适应性更好

　　正确答案：BD，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

　　HK算法思想很朴实,就是在最小均方误差准则下求得权矢量.

　　他相对于感知器算法的优点在于,他适用于线性可分和非线性可分得情况,对于线性可分的情况,给出最优权矢量,对于非线性可分得情况,能够判别出来,以退出迭代过程.

182.以下说法中正确的是（）？

　　A.SVM对噪声(如来自其他分布的噪声样本)鲁棒

　　B.在AdaBoost算法中,所有被分错的样本的权重更新比例相同

　　C.Boosting和Bagging都是组合多个分类器投票的方法,二者都是根据单个分类器的正确率决定其权重

　　D.给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少

　　正确答案：BD，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

183.输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为（）：

　　A. 95　　B. 96　　C. 97　　D. 98

　　正确答案：C ，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

　　卷积或者池化后大小的计算公式： 

out_height=（(input_height - filter_height + padding_top+padding_bottom)/stride_height ）+1

out_width=（(input_width - filter_width + padding_left+padding_right)/stride_width ）+1

　　其中，padding指的是向外扩展的边缘大小，而stride则是步长，即每次移动的长度。

　　这样一来就容易多了，首先长宽一般大，所以我们只需要计算一个维度即可，这样，经过第一次卷积后的大小为: （200-5+2）/2+1，取99；经过第一次池化后的大小为：（99-3）/1+1 为97；经过第二次卷积后的大小为： （97-3+2）/1+1 为97

184.在spss的基础分析模块中，作用是“以行列表的形式揭示数据之间的关系”的是（ ）

　　A. 数据描述　　B. 相关　　C. 交叉表　　D. 多重相应

　　正确答案：C

185.一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：（）。

　　A. 二分类问题　　B. 多分类问题　　C. 层次聚类问题　　D. k-中心点聚类问题

　　正确答案：B，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

 

　　二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。

　　层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其 他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。

　　K-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。

　　多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。

186.关于 logit 回归和 SVM 不正确的是（）

　　A.Logit回归目标函数是最小化后验概率　　B. Logit回归可以用于预测事件发生概率的大小　　C. SVM目标是结构风险最小化　　D.SVM可以有效避免模型过拟合

　　正确答案： A，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

，Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误

187.有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是()

　　A. 2x+y=4　　B. x+2y=5　　C. x+2y=3　　D. 2x-y=0

　　正确答案：C，这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。

188.下面有关分类算法的准确率，召回率，F1 值的描述，错误的是？

　　A.准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率

　　B.召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率

　　C.正确率、召回率和 F 值取值都在0和1之间，数值越接近0，查准率或查全率就越高

　　D.为了解决准确率和召回率冲突问题，引入了F1分数

　　正确答案：C

　　解析：对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：TP——将正类预测为正类数，FN——将正类预测为负类数，FP——将负类预测为正类数，TN——将负类预测为负类数

　　由此，精准率定义为：P = TP / (TP + FP)，召回率定义为：R = TP / (TP + FN)，F1值定义为： F1 = 2 P R / (P + R)

　　精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。

189.以下几种模型方法属于判别式模型(Discriminative Model)的有()

1)混合高斯模型 2)条件随机场模型 3)区分度训练 4)隐马尔科夫模型

　　A.2,3　　B.3,4　　C.1,4　　D.1,2

　　正确答案：A，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

，生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。

190.SPSS中，数据整理的功能主要集中在（ ）等菜单中

　　A.数据　　B.直销　　C.分析　　D.转换

　　正确答案：AD，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

 解析：对数据的整理主要在数据和转换功能菜单中。

191.深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为m∗n，n∗p，p∗q，且m<n<p<q，以下计算顺序效率最高的是（）

 

　　A.(AB)C　　B.AC(B)　　C.A(BC)　　D.所以效率都相同

　　正确答案：A，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

　　因为 A*B ， A 的列数必须和 B 的行数相等。因此，可以排除 B 选项， 在 A 选项中，m∗n 的矩阵 A 和n∗p的矩阵 B 的乘积，得到 m∗p的矩阵 A*B ，而 A∗B的每个元素需要 n 次乘法和 n-1 次加法，忽略加法，共需要 m∗n∗p次乘法运算。同样情况分析 A*B 之后再乘以 C 时的情况，共需要 m∗p∗q次乘法运算。因此， A 选项 (AB)C 需要的乘法次数是 m∗n∗p+m∗p∗q 。同理分析， C 选项 A (BC) 需要的乘法次数是 n∗p∗q+m∗n∗q。 

　　由于m∗n∗p<m∗n∗q，m∗p∗q<n∗p∗q，显然 A 运算次数更少，故选 A 。

192.Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是:()

 

　　A.各类别的先验概率P(C)是相等的　　B.以0为均值，sqr(2)/2为标准差的正态分布　　C.特征变量X的各个维度是类别条件独立随机变量　　D.P(X|C)是高斯分布

　　正确答案：C，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

，朴素贝叶斯的条件就是每个变量相互独立。

193.关于支持向量机SVM,下列说法错误的是（）

 

　　A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力

　　B.Hinge 损失函数，作用是最小化经验分类错误

　　C.分类间隔为1/||w||，||w||代表向量的模

　　D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习

　　正确答案：C，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

194.在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计()

 

　　A.EM算法　　B.维特比算法　　C.前向后向算法　　D.极大似然估计

　　正确答案：D，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

195.假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是：

 

　　A.这个被重复的特征在模型中的决定作用会被加强

　　B.模型效果相比无重复特征的情况下精确度会降低

　　C.如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。

　　D.当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题

　　E.NB可以用来做最小二乘回归

　　正确答案：BD

196.在Logistic Regression 中,如果同时加入L1和L2范数,会产生什么效果()

　　A.可以做特征选择,并在一定程度上防止过拟合　　B.能解决维度灾难问题　　C.能加快计算速度　　D.可以获得更准确的结果

　　正确答案: A，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

197.机器学习中L1正则化和L2正则化的区别是？

　　A.使用L1可以得到稀疏的权值　　B.使用L1可以得到平滑的权值　　C.使用L2可以得到稀疏的权值　　D.使用L2可以得到平滑的权值

　　正确答案:AD，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

，L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

198.位势函数法的积累势函数K(x)的作用相当于Bayes判决中的()

　　A.后验概率　　B.先验概率　　C.类概率密度　　D.类概率密度与先验概率的乘积

　　正确答案:AD，@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

事实上，AD说的是一回事。 具体的，势函数详解请看——《势函数法》。

199.隐马尔可夫模型三个基本问题以及相应的算法说法正确的是（ ）

　　A.评估—前向后向算法　　B.解码—维特比算法　　C.学习—Baum-Welch算法　　D.学习—前向后向算法

　　正确答案: ABC，评估问题，可以使用前向算法、后向算法、前向后向算法。

200.特征比数据量还大时，选择什么样的分类器？

　　答案：线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。



  **201.下列属于无监督学习的是：** 
　　A.k-means
　　B.SVM
　　C.最大熵
　　D.CRF
　　正确答案：A
　　解析： A是聚类，BC是分类，D是序列化标注，也是有监督学习。

**202.下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）** 
　　A.特征灵活
　　B.速度快
　　C.可容纳较多上下文信息
　　D.全局最优
　　正确答案：B
　　解析： CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢
　　CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较
　　同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较
　　CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较

**203.数据清理中，处理缺失值的方法是?** 
　　A.估算
　　B.整例删除
　　C.变量删除
　　D.成对删除
　　正确答案：ABCD
　　@刘炫320，本题题目及解析来源：<http://blog.csdn.net/column/details/16442.html>
　　由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。
　　估算(estimation)。最简单的办法就是用某个变量的样本均值、中位数或众数代替无效值和缺失值。这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。另一种办法就是根据调查对象对其他问题的答案，通过变量之间的相关分析或逻辑推论进行估计。例如，某一产品的拥有情况可能与家庭收入有关，可以根据调查对象的家庭收入推算拥有这一产品的可能性。
　　整例删除(casewise deletion)是剔除含有缺失值的样本。由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。
变量删除(variable deletion)。如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。
　　成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。这是一种保守的处理方法，最大限度地保留了数据集中的可用信息。
　　采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在调查中应当尽量避免出现无效值和缺失值，保证数据的完整性。

**204.关于线性回归的描述,以下正确的有:** 
　　A.基本假设包括随机干扰项是均值为0,方差为1的标准正态分布
　　B.基本假设包括随机干扰下是均值为0的同方差正态分布
　　C.在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量
　　D.在违背基本假设时,模型不再可以估计
　　E.可以用DW检验残差是否存在序列相关性
　　F.多重共线性会使得参数估计值方差减小 
　　正确答案：ACEF

　　@刘炫320，本题题目及解析来源：<http://blog.csdn.net/column/details/16442.html>
　　1、AB一元线性回归的基本假设有：
　　（1）随机误差项是一个期望值或平均值为0的随机变量； 
　　（2）对于解释变量的所有观测值，随机误差项有相同的方差； 
　　（3）随机误差项彼此不相关； 
　　（4）解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立； 
　　（5）解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵； 
　　（6）随机误差项服从正态分布
　　2、CD 违背基本假设的计量经济学模型还是可以估计的，只是不能使用普通最小二乘法进行估计。
　　当存在异方差时，普通最小二乘法估计存在以下问题： 参数估计值虽然是无偏的，但不是最小方差线性无偏估计。
　　3、E杜宾-瓦特森（DW）检验，计量经济，统计分析中常用的一种检验序列一阶 自相关 最常用的方法。
　　4、F所谓多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。影响 
　　（1）完全共线性下参数估计量不存在 
　　（2）近似共线性下OLS估计量非有效 
多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF) 
　　（3）参数估计量经济含义不合理 
　　（4）变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外 
　　（5）模型的预测功能失效。变大的方差容易使区间预测的“区间”变大，使预测失去意义。
　　对于线性回归模型,当响应变量服从正态分布,误差项满足高斯–马尔科夫条件（零均值、等方差、不相关）时,回归参数的最小二乘估计是一致最小方差无偏估计。
　　当然，该条件只是理想化的假定，为的是数学上有相应的较为成熟的结论。其实大多数实际问题都不完全满足这些理想化的假定。
　　线性回归模型理论的发展正是在不断克服理想化条件不被满足时得到许多新方法。如加权LSE、岭估计、压缩估计、BOX_COX变换等一系列段。做实际工作时一定是要超越书本上的理想化条件的。

**205.影响聚类算法效果的主要原因有：**
　　A.特征选取
　　B.模式相似性测度
　　C.分类准则
　　D.已知类别的样本质量
　　正确答案：ABC
　　@刘炫320，本题题目及解析来源：<http://blog.csdn.net/column/details/16442.html>
　　解析：这道题应该是很简单的，D之所以不正确，是因为聚类是对无类别的数据进行聚类，不使用已经标记好的数据。
　　前面的ABC选项，可以参考：《聚类分析》与《各类算法的比较》。

206.以下哪个是常见的时间序列算法模型
　　A.RSI
　　B.MACD
　　C.ARMA
　　D.KDJ

　　正确答案：C
　　解析： 自回归滑动平均模型(ARMA) ，其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。
　　其他三项都不是一个层次的。 
　　A.相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .
　　B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .
　　D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 .

207.下列不是SVM核函数的是：
　　A.多项式核函数
　　B.logistic核函数
　　C.径向基核函数
　　D.Sigmoid核函数

　　正确答案：B
　　@刘炫320，本题题目及解析来源：http://blog.csdn.net/column/details/16442.html 
　　SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数.
　　核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：
　　(1)线性核函数 ：K ( x , x i ) = x ⋅ x i
　　(2)多项式核 ：K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d
　　(3)径向基核（RBF）：K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 ) 
　　Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。
　　(4)傅里叶核 ：K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 )
　　(5)样条核 ：K ( x , x i ) = B 2 n + 1 ( x − x i )
　　(6)Sigmoid核函数 ：K ( x , x i ) = tanh ( κ ( x , x i ) − δ )
　　采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。
　　在选取核函数解决实际问题时，通常采用的方法有：
　　一是利用专家的先验知识预先选定核函数；
　　二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．
　　三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．

208.已知一组数据的协方差矩阵P,下面关于主分量说法错误的是()
　　A.主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小
　　B.在经主分量分解后,协方差矩阵成为对角矩阵
　　C.主分量分析就是K-L变换
　　D.主分量是通过求协方差矩阵的特征值得到

　　正确答案：C
　　解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。

209.在分类问题中,我们经常会遇到正负样本数据量不等的情况,比如正样本为10w条数据,负样本只有1w条数据,以下最合适的处理方法是()
　　A.将负样本重复10次,生成10w样本量,打乱顺序参与分类
　　B.直接进行分类,可以最大限度利用数据
　　C.从10w正样本中随机抽取1w参与分类
　　D.将负样本每个权重设置为10,正样本权重为1,参与训练过程

　　正确答案:ACD
　　解析：1. 重采样。 A可视作重采样的变形。改变数据分布消除不平衡，可能导致过拟合。 
  　　2. 欠采样。 C的方案 提高少数类的分类性能，可能丢失多数类的重要信息。 
      如果1：10算是均匀的话，可以将多数类分割成为1000份。然后将每一份跟少数类的样本组合进行训练得到分类器。而后将这1000个分类器用assemble的方法组合位一个分类器。A选项可以看作此方式，因而相对比较合理。 
      　　另：如果目标是 预测的分布 跟训练的分布一致，那就加大对分布不一致的惩罚系数。 
  　　3. 权值调整。 D方案也是其中一种方式。

    当然，这只是在数据集上进行相应的处理，在算法上也有相应的处理方法。

210.在统计模式识分类问题中，当先验概率未知时，可以使用()?
　　A.最小损失准则
　　B.N-P判决
　　C.最小最大损失准则
　　D.最小误判概率准则

　　正确答案:BC
　　@刘炫320，本题题目及解析来源：http://blog.csdn.net/column/details/16442.html
　　选项 A ,最小损失准则中需要用到先验概率
　　选项B ,在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。 
  　　1. p(y)已知，直接使用贝叶斯公式求后验概率即可； 
  　　2. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。 
      聂曼-皮尔逊决策（N-P判决）可以归结为找阈值a，即： 
      　　如果p（x|w1）/p（x|w2）>a，则 x属于w1； 
      　　如果p（x|w1）/p（x|w2）<a，则 x属于w 2；
      　　选项C ,最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。



211.解决隐马模型中预测问题的算法是?

　　A.前向算法

　　B.后向算法

　　C.Baum-Welch算法

　　D.维特比算法

　　正确答案：D

　　@刘炫320，本题题目及解析来源：

http://blog.csdn.net/column/details/16442.html

　　A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。

　　C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；

　　D：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。

212.一般，k-NN最近邻方法在( )的情况下效果较好

　　A.样本较多但典型性不好

　　B.样本较少但典型性好

　　C.样本呈团状分布

　　D.样本呈链状分布

　　正确答案：B

　　解析：K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B

　　样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。

213.下列方法中，可以用于特征降维的方法包括（）

　　A.主成分分析PCA

　　B.线性判别分析LDA

　　C.深度学习SparseAutoEncoder

　　D.矩阵奇异值分解SVD

　　E.最小二乘法LeastSquares

　　正确答案：ABCD

　　解析：降维的3种常见方法ABD，都是线性的。深度学习是降维的方法这个就比较新鲜了，事实上，细细想来，也是降维的一种方法，因为如果隐藏层中的神经元数目要小于输入层，那就达到了降维，但如果隐藏层中的神经元如果多余输入层，那就不是降维了。

最小二乘法是线性回归的一种解决方法，其实也是投影，但是并没有进行降维。

214.下面哪些是基于核的机器学习算法?()

　　A.Expectation Maximization（EM）（最大期望算法）

　　B.Radial Basis Function（RBF）（径向基核函数）

　　C.Linear Discrimimate Analysis（LDA）（主成分分析法）

　　D.Support Vector Machine（SVM）（支持向量机）

　　正确答案：BCD

　　解析：径向基核函数是非常常用的核函数，而主成分分析法的常规方法是线性的，但是当遇到非线性的时候，同样可以使用核方法使得非线性问题转化为线性问题。支持向量机处理非线性的问题的时候，核函数也是非常重要的。

215.试推导样本空间中任意点x到超平面（w,b）的距离公式。

[![3.jpg](https://ask.julyedu.com/uploads/questions/20171122/200bad874676bcebc126a58c98a19983.jpg)](https://ask.julyedu.com/uploads/questions/20171122/200bad874676bcebc126a58c98a19983.jpg)



216.从网上下载或自己编程实现一个卷积神经网络，并在手写字符识别数据MNIST上进行试验测试。

　　解析详见：

http://blog.csdn.net/snoopy_yu ... 03019

217.神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属是好的属性但不必要的？

　　@Hengkai Guo，本题解析来源：

https://www.zhihu.com/question/67366051

　　说说我对一个好的激活函数的理解吧，有些地方可能不太严谨，欢迎讨论。（部分参考了Activation function。）

　　1. 非线性：即导数不是常数。这个条件前面很多答主都提到了，是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。

　　2. 几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。

　　3. 计算简单：正如题主所说，非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。

　　4. 非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x>0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x<0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。

　　5. 单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。

　　6. 输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。

　　7. 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x>0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。

　　8. 参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。

　　9. 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。

参考文献：

[1] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.

[2] Lin M, Chen Q, Yan S. Network in network[J]. arXiv preprint arXiv:1312.4400, 2013.

[3] Maas A L, Hannun A Y, Ng A Y. Rectifier nonlinearities improve neural network acoustic models[C]//Proc. ICML. 2013, 30(1).

[4] He K, Zhang X, Ren S, et al. Delving 

deep into rectifiers: Surpassing human-level performance on imagenet 

classification[C]//Proceedings of the IEEE international conference on 

computer vision. 2015: 1026-1034.

[5] Glorot X, Bengio Y. Understanding the 

difficulty of training deep feedforward neural networks[C]//Proceedings 

of the Thirteenth International Conference on Artificial Intelligence 

and Statistics. 2010: 249-256.

[6] He K, Zhang X, Ren S, et al. Deep 

residual learning for image recognition[C]//Proceedings of the IEEE 

conference on computer vision and pattern recognition. 2016: 770-778.

[7] Goodfellow I J, Warde-Farley D, Mirza M, et al. Maxout networks[J]. arXiv preprint arXiv:1302.4389, 2013.

[8] Klambauer G, Unterthiner T, Mayr A, et al. Self-Normalizing Neural Networks[J]. arXiv preprint arXiv:1706.02515, 2017.

[9] Ioffe S, Szegedy C. Batch 

normalization: Accelerating deep network training by reducing internal 

covariate shift[C]//International Conference on Machine Learning. 2015: 

448-456.

218.梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？

　　@李振华，

https://www.zhihu.com/question ... 43638

　　深度神经网络“容易收敛到局部最优”，很可能是一种想象，实际情况是，我们可能从来没有找到过“局部最优”，更别说全局最优了。

　　很多人都有一种看法，就是“局部最优是神经网络优化的主要难点”。这来源于一维优化问题的直观想象。在单变量的情形下，优化问题最直观的困难就是有很多局部极值，如

[![218.1_.png](https://ask.julyedu.com/uploads/questions/20171123/b20f027b692cdc462ba8d1b41d3c5476.png)](https://ask.julyedu.com/uploads/questions/20171123/b20f027b692cdc462ba8d1b41d3c5476.png)

　　人们直观的想象，高维的时候这样的局部极值会更多，指数级的增加，于是优化到全局最优就更难了。然而单变量到多变量一个重要差异是，单变量的时候，Hessian矩阵只有一个特征值，于是无论这个特征值的符号正负，一个临界点都是局部极值。但是在多变量的时候，Hessian有多个不同的特征值，这时候各个特征值就可能会有更复杂的分布，如有正有负的不定型和有多个退化特征值（零特征值）的半定型

[![218.2_.png](https://ask.julyedu.com/uploads/questions/20171123/224c2dfefb787f97530385f423563f87.png)](https://ask.julyedu.com/uploads/questions/20171123/224c2dfefb787f97530385f423563f87.png)

　　在后两种情况下，是很难找到局部极值的，更别说全局最优了。

　　现在看来，神经网络的训练的困难主要是鞍点的问题。在实际中，我们很可能也从来没有真的遇到过局部极值。Bengio组这篇文章Eigenvalues of the Hessian in Deep Learning（

https://arxiv.org/abs/1611.07476

）里面的实验研究给出以下的结论：

• Training stops at a point that has a small gradient. The norm of the gradient is not zero, therefore it does not, technically speaking, converge to a critical point.

• There are still negative eigenvalues even when they are small in magnitude.

　　另一方面，一个好消息是，即使有局部极值，具有较差的loss的局部极值的吸引域也是很小的Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes。（

https://arxiv.org/abs/1706.10239

）

For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima.

　　所以，很可能我们实际上是在“什么也没找到”的情况下就停止了训练，然后拿到测试集上试试，“咦，效果还不错”。

　　补充说明，这些都是实验研究结果。理论方面，各种假设下，深度神经网络的Landscape 的鞍点数目指数增加，而具有较差loss的局部极值非常少。

219.EM算法、HMM、CRF

　　这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。 

（1）EM算法 

　　EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。 

　　注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。 

（2）HMM算法 

　　隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。 

马尔科夫三个基本问题：

概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法

学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。

预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）

（3）条件随机场CRF 

　　给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。 

　　之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。

（4）HMM和CRF对比 

　　其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。 

220.CNN常用的几个模型

[![220.png](https://ask.julyedu.com/uploads/questions/20171123/999e821be7c35d7e7759a165542b00ed.png)](https://ask.julyedu.com/uploads/questions/20171123/999e821be7c35d7e7759a165542b00ed.png)



221.带核的SVM为什么能分类非线性问题？

 

　　核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积 

222.常用核函数及核函数的条件：

 

　　核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。

RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。

线性核：主要用于线性可分的情况

多项式核

223.Boosting和Bagging

（1）随机森林 

　　随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：

　　1）Boostrap从袋内有放回的抽取样本值

　　2）每次随机抽取一定数量的特征（通常为sqr(n)）。 

　　分类问题：采用Bagging投票的方式选择类别频次最高的 

　　回归问题：直接取每颗树结果的平均值。

[![2.png](https://ask.julyedu.com/uploads/questions/20171124/6495d979ad335a2aa04dea59e560f787.png)](https://ask.julyedu.com/uploads/questions/20171124/6495d979ad335a2aa04dea59e560f787.png)

（2）Boosting之AdaBoost 

　　Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。 

（3）Boosting之GBDT 

　　将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。 

　　注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。 

（4）Xgboost 

这个工具主要有以下几个特点：

支持线性分类器

可以自定义损失函数，并且可以用二阶偏导

加入了正则化项：叶节点数、每个叶节点输出score的L2-norm

支持特征抽样

在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。

224.逻辑回归相关问题

（1）公式推导一定要会

（2）逻辑回归的基本概念 

　　这个最好从广义线性模型的角度分析，逻辑回归是假设y服从Bernoulli分布。

（3）L1-norm和L2-norm 

　　其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。 

　　但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。

（4）LR和SVM对比 

　　首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss

[![1.png](https://ask.julyedu.com/uploads/questions/20171124/534bb2a80189eff4a558a4d79c946e05.png)](https://ask.julyedu.com/uploads/questions/20171124/534bb2a80189eff4a558a4d79c946e05.png)

　　其次，两者都是线性模型。 

　　最后，SVM只考虑支持向量（也就是和分类相关的少数点） 

（5）LR和随机森林区别 

　　随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。 

（6）常用的优化方法 

　　逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。 

　　一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 

　　二阶方法：牛顿法、拟牛顿法： 

　　这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。 

拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

225.用贝叶斯机率说明Dropout的原理

@许韩，来源：

https://zhuanlan.zhihu.com/p/25005808

参见论文：Dropout as a Bayesian Approximation: Insights and Applications

（http://mlg.eng.cam.ac.uk/yarin ... n.pdf）



226.为什么很多做人脸的Paper会最后加入一个Local Connected Conv？

　　@许韩，来源：

https://zhuanlan.zhihu.com/p/25005808

　　以FaceBook DeepFace 为例：

[![226.png](https://ask.julyedu.com/uploads/questions/20171128/bbb9ce8f90ad5a6073a94954a10bf6f6.png)](https://ask.julyedu.com/uploads/questions/20171128/bbb9ce8f90ad5a6073a94954a10bf6f6.png)

　　DeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。

227.什么事共线性, 跟过拟合有什么关联?

　　@抽象猴，来源：

https://www.zhihu.com/question ... 04190

　　共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。

　　共线性会造成冗余，导致过拟合。

　　解决方法：排除变量的相关性／加入权重正则。

228.为什么网络够深(Neurons 足够多)的时候，总是可以避开较差Local Optima？

　　参见：The Loss Surfaces of Multilayer Networks（

https://arxiv.org/pdf/1412.0233.pdf

）

229.机器学习中的正负样本

　　在分类问题中，这个问题相对好理解一点，比如人脸识别中的例子，正样本很好理解，就是人脸的图片，负样本的选取就与问题场景相关，具体而言，如果你要进行教室中学生的人脸识别，那么负样本就是教室的窗子、墙等等，也就是说，不能是与你要研究的问题毫不相关的乱七八糟的场景图片，这样的负样本并没有意义。负样本可以根据背景生成，有时候不需要寻找额外的负样本。一般3000-10000的正样本需要5，000,000-100,000,000的负样本来学习，在互金领域一般在入模前将正负比例通过采样的方法调整到3:1-5:1。

230.机器学习中，有哪些特征选择的工程方法？

　　数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已

　　1. 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；

　　2. 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；

　　3.通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*；

　　4. 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；

　　5.通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。

　　6.通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。



  **231.在一个n维的空间中， 最好的检测outlier(离群点)的方法是：**
A. 作正态分布概率图
B. 作盒形图
C. 马氏距离
D. 作散点图
答案：C
马氏距离是基于卡方分布的，度量多元outlier离群点的统计方法。更多请详见：[http://eurekastatistics.com/us ... iers/](http://eurekastatistics.com/using-mahalanobis-distance-to-find-outliers/)和”[http://blog.csdn.net/v_july_v/ ... 03674](http://blog.csdn.net/v_july_v/article/details/8203674)“。

**232.对数几率回归（logistics regression）和一般回归分析有什么区别？**
A. 对数几率回归是设计用来预测事件可能性的
B. 对数几率回归可以用来度量模型拟合程度
C. 对数几率回归可以用来估计回归系数
D. 以上所有
答案：D
A: 对数几率回归其实是设计用来解决分类问题的
B: 对数几率回归可以用来检验模型对数据的拟合度
C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。

**233.bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）**
A. 有放回地从总共M个特征中抽样m个特征
B. 无放回地从总共M个特征中抽样m个特征
C. 有放回地从总共N个样本中抽样n个样本
D. 无放回地从总共N个样本中抽样n个样本
答案：C

**234.“过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是：**
A. 对的
B. 错的
答案：B
我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）

**235.对于k折交叉验证, 以下对k的说法正确的是 :**
A. k越大, 不一定越好, 选择大的k会加大评估时间
B. 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)
C. 在选择k时, 要最小化数据集之间的方差
D. 以上所有
答案：D
k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.
如果不明白bias和variance的概念, 务必参考下面链接:
Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning
Understanding the Bias-Variance Tradeoff



236.回归模型中存在多重共线性, 你如何解决这个问题？

\1. 去除这两个共线性变量

\2. 我们可以先去除一个共线性变量

\3. 计算VIF(方差膨胀因子), 采取相应措施

\4. 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归.

以下哪些是对的:

A. 1

B. 2

C. 2和3

D. 2, 3和4

答案: D

解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高.

我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。

237.模型的高bias是什么意思, 我们如何降低它 ?

A. 在特征空间中减少特征

B. 在特征空间中增加特征

C. 增加数据点

D. B和C

E. 以上所有

答案: B

bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !

238.训练决策树模型, 属性节点的分裂, 具有最大信息增益的图是下图的哪一个:



[![238.png](https://ask.julyedu.com/uploads/questions/20171129/133785374f4d1db05c5d60ad4d88856a.png)](https://ask.julyedu.com/uploads/questions/20171129/133785374f4d1db05c5d60ad4d88856a.png)

A. Outlook

B. Humidity

C. Windy

D. Temperature

答案: A信息增益, 增加平均子集纯度, 详细研究, 请戳下面链接:

A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)

Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio

239.对于信息增益, 决策树分裂节点, 下面说法正确的是:

 

\1. 纯度高的节点需要更多的信息去区分

\2. 信息增益可以用”1比特-熵”获得

\3. 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的

A. 1

B. 2

C.2和3

D. 所有以上

答案: C

详细研究, 请戳下面链接:

A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)

Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio

240. 如果SVM模型欠拟合, 以下方法哪些可以改进模型 :

 

A. 增大惩罚参数C的值

B. 减小惩罚参数C的值

C. 减小核系数(gamma参数)

答案: A

如果SVM模型欠拟合, 我们可以调高参数C的值, 使得模型复杂度上升.LibSVM中，SVM的目标函数是：

[![240.png](https://ask.julyedu.com/uploads/questions/20171129/407d5122a43d22a989c488a70ea67732.png)](https://ask.julyedu.com/uploads/questions/20171129/407d5122a43d22a989c488a70ea67732.png)

而, gamma参数是你选择径向基函数作为kernel后,该函数自带的一个参数.隐含地决定了数据映射到新的特征空间后的分布.

gamma参数与C参数无关. gamma参数越高, 模型越复杂. 



241.下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是 :



[![1.jpg](https://ask.julyedu.com/uploads/questions/20171130/4cc64b9fd5d285fbbd496b41dc50b2bd.jpg)](https://ask.julyedu.com/uploads/questions/20171130/4cc64b9fd5d285fbbd496b41dc50b2bd.jpg)

A. g1 > g2 > g3

B. g1 = g2 = g3

C. g1 < g2 < g3

D. g1 >= g2 >= g3

E. g1 <= g2 <= g3

答案: C

242.假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 :

 

\1. 模型分类的召回率会降低或不变

2 模型分类的召回率会升高

\3. 模型分类准确率会升高或不变

\4. 模型分类准确率会降低

A. 1

B. 2

C.1和3

D. 2和4

E. 以上都不是

答案: C

这篇文章讲述了阈值对准确率和召回率影响 :

Confidence Splitting Criterions Can Improve Precision And Recall in Random Forest Classifiers

243.”点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是 :

 

A. 模型预测准确率已经很高了, 我们不需要做什么了

B. 模型预测准确率不高, 我们需要做点什么改进模型

C. 无法下结论

D. 以上都不对

答案: B

99%的预测准确率可能说明, 你预测的没有点进去的人很准确 (因为有99%的人是不会点进去的, 这很好预测). 不能说明你的模型对点进去的人预测准确, 所以, 对于这样的非平衡数据集, 我们要把注意力放在小部分的数据上, 即那些点击进去的人.

详细参考: 

https://www.analyticsvidhya.co ... lems/

244.使用k=1的knn算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少 :



[![14.png](https://ask.julyedu.com/uploads/questions/20171130/384f522c52e26e00f751fdc4bdcb9bf6.png)](https://ask.julyedu.com/uploads/questions/20171130/384f522c52e26e00f751fdc4bdcb9bf6.png)

A. 0%

B. 100%

C. 0% 到 100%

D. 以上都不是

答案: B

knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100%

245.我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以 :

 

A. 增加树的深度

B. 增加学习率 (learning rate)

C. 减少树的深度

D. 减少树的数量

答案: C

A.增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.

B.决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)

D.决策树只有一棵树, 不是随机森林.



246.对于神经网络的说法, 下面正确的是 : 
1. 增加神经网络层数, 可能会增加测试数据集的分类错误率
2. 减少神经网络层数, 总是能减小测试数据集的分类错误率
3. 增加神经网络层数, 总是能减小训练数据集的分类错误率
  A. 1
  B. 1 和 3
  C. 1 和 2
  D. 2
  答案: A
  深度神经网络的成功, 已经证明, 增加神经网络层数, 可以增加模型范化能力, 即, 训练数据集和测试数据集都表现得更好. 但更多的层数, 也不一定能保证有更好的表现（https://arxiv.org/pdf/1512.03385v1.pdf）. 所以, 不能绝对地说层数多的好坏, 只能选A

247.假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？
A. 设C=1
B. 设C=0
C. 设C=无穷大
D. 以上都不对
答案: C
C无穷大保证了所有的线性不可分都是可以忍受的.

248.训练完SVM模型后, 不是支持向量的那些样本我们可以丢掉, 也可以继续分类:
A. 正确
B. 错误
答案: A
SVM模型中, 真正影响决策边界的是支持向量

249.以下哪些算法, 可以用神经网络去构造: 
1. KNN
2. 线性回归
3. 对数几率回归
  A. 1和 2
  B. 2 和 3
  C. 1, 2 和 3
  D. 以上都不是
  答案: B
4. KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙
5. 最简单的神经网络, 感知器, 其实就是线性回归的训练
6. 我们可以用一层的神经网络构造对数几率回归

250.请选择下面可以应用隐马尔科夫(HMM)模型的选项: 
A. 基因序列数据集
B. 电影浏览数据集
C. 股票市场数据集
D. 所有以上
答案: D
只要是和时间序列问题有关的 , 都可以试试HMM



251.我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 : 
A. 我们随机抽取一些样本, 在这些少量样本之上训练
B. 我们可以试用在线机器学习算法
C. 我们应用PCA算法降维, 减少特征数
D. B 和 C
E. A 和 B
F. 以上所有
答案: F

252.我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 :
1. 使用前向特征选择方法
2. 使用后向特征排除方法
3. 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征.
4. 查看相关性表, 去除相关性最高的一些特征
  A. 1 和 2
  B. 2, 3和4
  C. 1, 2和4
  D. All

答案: D
1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法
2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法.
3.用相关性的度量去删除多余特征, 也是一个好方法
所有D是正确的

253.对于随机森林和GradientBoosting Trees, 下面说法正确的是:
1.在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的.
2.这两个模型都使用随机特征子集, 来生成许多单个的树.
3.我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的, GradientBoosting Trees训练模型的表现总是比随机森林好
A. 2
B. 1 and 2
C. 1, 3 and 4
D. 2 and 4

答案: A
1.随机森林是基于bagging的, 而Gradient Boosting trees是基于boosting的, 所有说反了,在随机森林的单个树中, 树和树之间是没有依赖的, 而GradientBoosting Trees中的单个树之间是有依赖关系.
2.这两个模型都使用随机特征子集, 来生成许多单个的树.
所有A是正确的

254.对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :
A. 正确的
B. 错误的
答案: B.
这个说法是错误的, 首先, “不依赖”和”不相关”是两回事, 其次, 转化过的特征, 也可能是相关的

255.对于PCA说法正确的是 :
1. 我们必须在使用PCA前规范化数据
2. 我们应该选择使得模型有最大variance的主成分
3. 我们应该选择使得模型有最小variance的主成分
4. 我们可以使用PCA在低维度上做数据可视化
  A. 1, 2 and 4
  B. 2 and 4
  C. 3 and 4
  D. 1 and 3
  E. 1, 3 and 4

答案: A
1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).
2）我们总是应该选择使得模型有最大variance的主成分
3）有时在低维度上左图是需要PCA的降维帮助的



256.对于下图, 最好的主成分选择是多少 ?

[![1.png](https://ask.julyedu.com/uploads/questions/20171207/f54f5826ea2071aea34598bab656a0f5.png)](https://ask.julyedu.com/uploads/questions/20171207/f54f5826ea2071aea34598bab656a0f5.png)

A. 7

B. 30

C. 35

D. 不确定

答案: B

主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。

257.数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是 :

A. 单个模型之间有高相关性

B. 单个模型之间有低相关性

C. 在集成学习中使用“平均权重”而不是“投票”会比较好

D. 单个模型都是用的一个算法

答案: B

详细请参考下面文章:

Basics of Ensemble Learning Explained in Simple English（

https://www.analyticsvidhya.co ... ning/

）

Kaggle Ensemble Guide（

http://mlwave.com/kaggle-ensembling-guide/

）

5 Easy questions on Ensemble Modeling everyone should know（

https://www.analyticsvidhya.co ... ling/

）

258.在有监督学习中， 我们如何使用聚类方法？

 

\1. 我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习

\2. 我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习

\3. 在进行监督学习之前， 我们不能新建聚类类别

\4. 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习

A. 2 和 4

B. 1 和 2

C. 3 和 4

D. 1 和 3

答案: B

我们可以为每个聚类构建不同的模型， 提高预测准确率。

“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。

所以B是正确的

259.以下说法正确的是 :

\1. 一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的

\2. 如果增加模型复杂度， 那么模型的测试错误率总是会降低

\3. 如果增加模型复杂度， 那么模型的训练错误率总是会降低

\4. 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习

A. 1

B. 2

C. 3

D. 1 and 3

答案: C

考的是过拟合和欠拟合的问题。

260.对应GradientBoosting tree算法， 以下说法正确的是 :

\1. 当增加最小样本分裂个数，我们可以抵制过拟合

\2. 当增加最小样本分裂个数，会导致过拟合

\3. 当我们减少训练单个学习器的样本个数，我们可以降低variance

\4. 当我们减少训练单个学习器的样本个数，我们可以降低bias

A. 2 和 4

B. 2 和 3

C. 1 和 3

D. 1 和 4

答案: C

最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。

第二点是靠bias和variance概念的。



271.SVM、LR、决策树的对比。
模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝
损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失
数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感
数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核

272.什么是ill-condition病态问题？
训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。

273.简述KNN最近邻分类算法的过程？
1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；
2. 对上面所有的距离值进行排序；
3. 选前k个最小距离的样本；
4. 根据这k个样本的标签进行投票，得到最后的分类类别；

274.常用的聚类划分方式有哪些？列举代表算法。
1. 基于划分的聚类:K-means，k-medoids，CLARANS。
2. 基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。
3. 基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。
4. 基于网格的方法：STING，WaveCluster。
5. 基于模型的聚类：EM,SOM，COBWEB。

275.下面对集成学习模型中的弱学习者描述错误的是？
A.他们经常不会过拟合
B.他们通常带有高偏差，所以其并不能解决复杂学习问题
C.他们通常会过拟合
答案：C，弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。

  **276.下面哪个/些选项对 K 折交叉验证的描述是正确的？**
1.增大 K 将导致交叉验证结果时需要更多的时间
2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心
3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量
A. 1 和 2
B. 2 和 3
C. 1 和 3
D. 1、2 和 3
答案（D)：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。

**277.最出名的降维算法是 PAC 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？**
A. X_projected_PCA 在最近邻空间能得到解释
B. X_projected_tSNE 在最近邻空间能得到解释
C. 两个都在最近邻空间能得到解释
D. 两个都不能在最近邻空间得到解释
答案（B）：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。

**278.给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？**
A. D1= C1, D2 < C2, D3 > C3
B. D1 = C1, D2 > C2, D3 > C3
C. D1 = C1, D2 > C2, D3 < C3
D. D1 = C1, D2 < C2, D3 < C3
E. D1 = C1, D2 = C2, D3 = C3
答案（E）：特征之间的相关性系数不会因为特征加或减去一个数而改变。

**279.为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？**
A. 将数据转换成零均值
B. 将数据转换成零中位数
C. 无法做到
答案（A）：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。

**280.假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。**
注意：所有其他超参数是相同的，所有其他因子不受影响。
1.深度为 4 时将有高偏差和低方差
2.深度为 4 时将有低偏差和低方差
A. 只有 1
B. 只有 2
C. 1 和 2
D. 没有一个
答案（A)：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。

  **281.在 k-均值算法中，以下哪个选项可用于获得全局最小？**
A. 尝试为不同的质心（centroid）初始化运行算法
B. 调整迭代的次数
C. 找到集群的最佳数量
D. 以上所有
答案（D）：所有都可以用来调试以找到全局最小。

**282.你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？**
A. 第一个 w2 成了 0，接着 w1 也成了 0
B. 第一个 w1 成了 0，接着 w2 也成了 0
C. w1 和 w2 同时成了 0
D. 即使在 C 成为大值之后，w1 和 w2 都不能成 0
答案（B）：通过观察图像我们发现，即使只使用 x2，我们也能高效执行分类。因此一开始 w1 将成 0；当正则化参数不断增加时，w2 也会越来越接近 0。

**283.假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。**
A.如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它。
B.对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大。
C.log-loss 越低，模型越好
D.以上都是
答案为（D）

**284.下面哪个选项中哪一项属于确定性算法？**
A.PCA
B.K-Means
C. 以上都不是
答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。

**285.特征向量的归一化方法有哪些？**
线性函数转换，表达式如下：
y=(x-MinValue)/(MaxValue-MinValue)
对数函数转换，表达式如下：
y=log10 (x)
反余切函数转换 ，表达式如下：
y=arctan(x)*2/PI
减去均值，除以方差：
y=(x-means)/ variance









**331题、哪些机器学习算法不需要做归一化处理？**

解析：

概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。

我理解归一化和标准化主要是为了使计算更方便 比如两个变量的量纲不同 可能一个的数值远大于另一个那么他们同时作为变量的时候 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能 量纲也需要调整 所以我估计lr 和 knn 保准话一下应该有好处。

至于其他的算法 我也觉得如果变量量纲差距很大的话 先标准化一下会有好处。

引用自@管博士一般我习惯说树形模型，这里说的概率模型可能是差不多的意思。引用自@寒小阳

**332题、对于树形结构为什么不需要归一化？**

解析：

数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。

另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。

**333题、在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别**

解析：

欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,...,xn) 和 y = (y1,...,yn) 之间的距离为：

[![1.jpg](https://ask.julyedu.com/uploads/questions/20180814/5e9ce9538de279b81b1b9b7dc2d24d59.jpg)](https://ask.julyedu.com/uploads/questions/20180814/5e9ce9538de279b81b1b9b7dc2d24d59.jpg)

欧氏距离虽然很有用，但也有明显的缺点。它将样品的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。例如，在教育研究中，经常遇到对人的分析和判别，个体的不同属性对于区分个体有着不同的重要性。因此，欧氏距离适用于向量各分量的度量标准统一的情况。

曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：

[![2.jpg](https://ask.julyedu.com/uploads/questions/20180814/52b12dc7e15ddb30f4a2b59c2459ae55.jpg)](https://ask.julyedu.com/uploads/questions/20180814/52b12dc7e15ddb30f4a2b59c2459ae55.jpg)

要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。当坐标轴变动时，点间的距离就会不同。

通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。

曼哈顿距离和欧式距离一般用途不同，无相互替代性。

另，关于各种距离的比较参看《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：

http://blog.csdn.net/v_july_v/ ... 03674

）。

**334题、数据归一化（或者标准化，注意归一化和标准化不同）的原因**

解析：

要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。

有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。

有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。

本题解析来源：@我愛大泡泡，链接：

http://blog.csdn.net/woaidapao ... 06273

补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。

**335题、请简要说说一个完整机器学习项目的流程**

解析：

1、抽象成数学问题

明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。

这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。

2、获取数据

数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。

数据要有代表性，否则必然会过拟合。

而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。

而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

3、特征预处理与特征选择

良好的数据要能够提取出良好的特征才能真正发挥效力。

特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。

筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

4、训练模型与调优

直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。

5、模型诊断

如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。

过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。

误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……

诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

6、模型融合

一般来说，模型融合后都能使得效果有一定提升。而且效果很好。

工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

7、上线运行

这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。

这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。

故，基于此，七月在线每一期ML算法班都特此增加特征工程、模型调优等相关课。比如，这里有个公开课视频《特征处理与特征选择》。

引用自：@寒小阳、@龙心尘

**336题、逻辑斯特回归为什么要对特征进行离散化。**

解析：

在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：

1. 离散特征的增加和减少都很容易，易于模型的快速迭代；
2. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
3. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
4. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
5. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
6. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
7. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

本题解析来源：@严林，链接：

https://www.zhihu.com/question/31989952

相关课程

**337题、简单介绍下LR**

解析：

@rickjin：把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。

原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。

虽然逻辑斯蒂回归姓回归，不过其实它的真实身份是二分类器。先弄清楚一个概念：线性分类器。

给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。

如果用x表示数据点，用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为（ wT中的T代表转置）：

[![1.jpg](https://ask.julyedu.com/uploads/questions/20180818/46bfc830c30f16c609703fae88e03f2d.jpg)](https://ask.julyedu.com/uploads/questions/20180818/46bfc830c30f16c609703fae88e03f2d.jpg)

可能有读者对类别取1或-1有疑问，事实上，这个1或-1的分类标准起源于logistic回归。

Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。

假设函数

[![2.jpg](https://ask.julyedu.com/uploads/questions/20180818/c5af636d4169a5dab9fa833d95708d76.jpg)](https://ask.julyedu.com/uploads/questions/20180818/c5af636d4169a5dab9fa833d95708d76.jpg)

其中x是n维特征向量，函数g就是logistic函数。

而

[![3.jpg](https://ask.julyedu.com/uploads/questions/20180818/937851080113293d5db5e33af3bd18d7.jpg)](https://ask.julyedu.com/uploads/questions/20180818/937851080113293d5db5e33af3bd18d7.jpg)

的图像是

[![4.jpg](https://ask.julyedu.com/uploads/questions/20180818/c909067c0da92467f215b7ea72ea63f1.jpg)](https://ask.julyedu.com/uploads/questions/20180818/c909067c0da92467f215b7ea72ea63f1.jpg)

可以看到，将无穷映射到了(0,1)。

而假设函数就是特征属于y=1的概率。

[![5.jpg](https://ask.julyedu.com/uploads/questions/20180818/5c0fab8d549169d87e4f9157ba89a0da.jpg)](https://ask.julyedu.com/uploads/questions/20180818/5c0fab8d549169d87e4f9157ba89a0da.jpg)

从而，当我们要判别一个新来的特征属于哪个类时，只需求即可，若大于0.5就是y=1的类，反之属于y=0类。

更多可以参考下这几篇文章：

1 Logistic Regression 的前世今生（理论篇）：

http://blog.csdn.net/cyh_24/ar ... 59055

2 机器学习算法与Python实践之（七）逻辑回归：

http://blog.csdn.net/zouxy09/a ... 19673

**338题、overfitting怎么解决**

解析：

overfitting就是过拟合, 其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集, 对训练集外的数据却不work, 这称之为泛化(generalization)性能不好。泛化性能是训练的效果评价中的首要目标，没有良好的泛化，就等于南辕北辙, 一切都是无用功。

[![6.jpg](https://ask.julyedu.com/uploads/questions/20180818/126755f41e0c6f3d4523874d78d82827.jpg)](https://ask.julyedu.com/uploads/questions/20180818/126755f41e0c6f3d4523874d78d82827.jpg)

过拟合是泛化的反面，好比乡下快活的刘姥姥进了大观园会各种不适应，但受过良好教育的林黛玉进贾府就不会大惊小怪。实际训练中, 降低过拟合的办法一般如下：

正则化(Regularization)

L2正则化：目标函数中增加所有权重w参数的平方之和, 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候, 拟合函数需要顾忌每一个点, 最终形成的拟合函数波动很大, 在某些很小的区间里, 函数值的变化很剧烈, 也就是某些w非常大. 为此, L2正则化的加入就惩罚了权重变大的趋势.

L1正则化：目标函数中增加所有权重w参数的绝对值之和, 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0, 奔向零的速度不如L1给力了). 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。

随机失活(dropout)

在训练的运行的时候，让神经元以超参数p的概率被激活(也就是1-p的概率被设置为0), 每个w因此随机参与, 使得任意w都不是不可或缺的, 效果类似于数量巨大的模型集成。

逐层归一化(batch normalization)

这个方法给每层的输出都做一次归一化(网络上相当于加了一个线性变换层), 使得下一层的输入接近高斯分布. 这个方法相当于下一层的w训练时避免了其输入以偏概全, 因而泛化效果非常好.

提前终止(early stopping)

理论上可能的局部极小值数量随参数的数量呈指数增长, 到达某个精确的最小值是不良泛化的一个来源. 实践表明, 追求细粒度极小值具有较高的泛化误差。

这是直观的，因为我们通常会希望我们的误差函数是平滑的, 精确的最小值处所见相应误差曲面具有高度不规则性, 而我们的泛化要求减少精确度去获得平滑最小值, 所以很多训练方法都提出了提前终止策略.

典型的方法是根据交叉叉验证提前终止: 若每次训练前, 将训练数据划分为若干份, 取一份为测试集, 其他为训练集, 每次训练完立即拿此次选中的测试集自测. 因为每份都有一次机会当测试集, 所以此方法称之为交叉验证. 交叉验证的错误率最小时可以认为泛化性能最好, 这时候训练错误率虽然还在继续下降, 但也得终止继续训练了.

@AntZ

339题、LR和SVM的联系与区别

解析：

联系：

1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）

2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。

区别：

1、LR是参数模型，SVM是非参数模型。

2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。

3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。

4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。

5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

本题解析来源：@朝阳在望

http://blog.csdn.net/timcompp/ ... 37986

340题

什么是熵

解析：

从名字上来看，熵给人一种很玄乎，不知道是啥的感觉。其实，熵的定义很简单，即用来表示随机变量的不确定性。之所以给人玄乎的感觉，大概是因为为何要取这样的名字，以及怎么用。

熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。

熵的引入

事实上，熵的英文原文为entropy，最初由德国物理学家鲁道夫·克劳修斯提出，其表达式为：

[![8.jpg](https://ask.julyedu.com/uploads/questions/20180818/f7e9f941931f68bea3746dfaf563abd9.jpg)](https://ask.julyedu.com/uploads/questions/20180818/f7e9f941931f68bea3746dfaf563abd9.jpg)

它表示一个系系统在不受外部干扰时，其内部最稳定的状态。后来一中国学者翻译entropy时，考虑到entropy是能量Q跟温度T的商，且跟火有关，便把entropy形象的翻译成“熵”。

我们知道，任何粒子的常态都是随机运动，也就是"无序运动"，如果让粒子呈现"有序化"，必须耗费能量。所以，温度（热能）可以被看作"有序化"的一种度量，而"熵"可以看作是"无序化"的度量。

如果没有外部能量输入，封闭系统趋向越来越混乱（熵越来越大）。比如，如果房间无人打扫，不可能越来越干净（有序化），只可能越来越乱（无序化）。而要让一个系统变得更有序，必须有外部能量的输入。

1948年，香农Claude E. Shannon引入信息（熵），将其定义为离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。

更多请查看《最大熵模型中的数学推导》（链接：

http://blog.csdn.net/v_july_v/ ... 08465

）。

[![7.jpg](https://ask.julyedu.com/uploads/questions/20180818/6fc280eff8a0c13c72f5029b56209c1d.jpg)](https://ask.julyedu.com/uploads/questions/20180818/6fc280eff8a0c13c72f5029b56209c1d.jpg)

341题

说说梯度下降法

解析：

1、什么是梯度下降法

经常在机器学习中的优化问题中看到一个算法，即梯度下降法，那到底什么是梯度下降法呢？

维基百科给出的定义是梯度下降法（Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。

额，问题又来了，什么是梯度？为了避免各种复杂的说辞，咱们可以这样简单理解，在单变量的实值函数的情况，梯度就是导数，或者，对于一个线性函数，也就是线的斜率。

1.1 梯度下降法示例

举个形象的例子吧，比如当我们要做一个房屋价值的评估系统，那都有哪些因素决定或影响房屋的价值呢？比如说面积、房子的大小（几室几厅）、地段、朝向等等，这些影响房屋价值的变量被称为特征(feature)。在这里，为了简单，我们假定房屋只由一个变量影响，那就是房屋的面积。

假设有一个房屋销售的数据如下：

面积(m^2) 销售价钱（万元）

123 250

150 320

87 160

102 220

… …

插句题外话，顺便吐下槽，这套房屋价格数据在五年前可能还能买到帝都5环左右的房子，但现在只能买到二线城市的房屋了。

我们可以做出一个图，x轴是房屋的面积。y轴是房屋的售价，如下：

[![1.png](https://ask.julyedu.com/uploads/questions/20180821/57067174eceacfedf835c5111d18c688.png)](https://ask.julyedu.com/uploads/questions/20180821/57067174eceacfedf835c5111d18c688.png)

如果来了一个新的房子/面积，假设在房屋销售价格的记录中没有的，我们怎么办呢？

我们可以用一条曲线去尽量准的拟合这些数据，然后如果有新的输入面积，我们可以在将曲线上这个点对应的值返回。如果用一条直线去拟合房屋价格数据，可能如下图这个样子：

[![2.png](https://ask.julyedu.com/uploads/questions/20180821/e72dc830f4bffe4d47fccd7dd8a5e0af.png)](https://ask.julyedu.com/uploads/questions/20180821/e72dc830f4bffe4d47fccd7dd8a5e0af.png)

而图中绿色的点就是我们想要预测的点。

而图中绿色的点就是我们想要预测的点。

为了数学建模，首先给出一些概念和常用的符号。

房屋销售记录表 – 训练集(training set)或者训练数据(training data), 是我们流程中的输入数据，一般称为x

房屋销售价钱 – 输出数据，一般称为y

拟合的函数（或者称为假设或者模型），一般写做 y = h(x)

训练数据的条目数(#training set), 一条训练数据是由一对输入数据和输出数据组成的

输入数据的维度(特征的个数，#features)，n

然后便是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。

[![3.png](https://ask.julyedu.com/uploads/questions/20180821/571a822ae56a320ac5a36a0d6f9fb0cd.png)](https://ask.julyedu.com/uploads/questions/20180821/571a822ae56a320ac5a36a0d6f9fb0cd.png)

我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向等等，我们可以做出一个估计函数：

[![4.png](https://ask.julyedu.com/uploads/questions/20180821/23b6200533721abaa6aec56b701d81b1.png)](https://ask.julyedu.com/uploads/questions/20180821/23b6200533721abaa6aec56b701d81b1.png)

θ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。

如果我们令X0 = 1，就可以用向量的方式来表示了：

[![5.png](https://ask.julyedu.com/uploads/questions/20180821/c216f4bf027cc0c1281914e9ed5861f7.png)](https://ask.julyedu.com/uploads/questions/20180821/c216f4bf027cc0c1281914e9ed5861f7.png)

我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个进行评估的函数称为损失函数（loss function），描述h函数不好的程度，这里我们称这个函数为J函数。

[![6.png](https://ask.julyedu.com/uploads/questions/20180821/505c420021b5b922764b40aea37e8f6d.png)](https://ask.julyedu.com/uploads/questions/20180821/505c420021b5b922764b40aea37e8f6d.png)

换言之，我们把对x(i)的估计值与真实值y(i)差的平方和作为损失函数，前面乘上的系数1/2是为了方便求导（且在求导的时候，这个系数会消掉）。

如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，另外一种就是梯度下降法。

1.2 梯度下降算法流程

梯度下降法的算法流程如下：

1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。

2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。

为了描述的更清楚，给出下面的图：

[![7.png](https://ask.julyedu.com/uploads/questions/20180821/36829708d8586f826cea12bfde1c38ea.png)](https://ask.julyedu.com/uploads/questions/20180821/36829708d8586f826cea12bfde1c38ea.png)

这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低，也就是达到深蓝色的部分（让误差/损失最小嘛）。θ0，θ1表示θ向量的两个维度。

在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。

然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如下图所示，算法的结束将是在θ下降到无法继续下降为止。

[![8.png](https://ask.julyedu.com/uploads/questions/20180821/a0c0c9ba29e85088673ab625e6cdcca2.png)](https://ask.julyedu.com/uploads/questions/20180821/a0c0c9ba29e85088673ab625e6cdcca2.png)

当然，可能梯度下降的最终点并非是全局最小点，即也可能是一个局部最小点，如下图所示：

[![9.png](https://ask.julyedu.com/uploads/questions/20180821/c28c815f824749a865d180e532e7309e.png)](https://ask.julyedu.com/uploads/questions/20180821/c28c815f824749a865d180e532e7309e.png)

上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。

下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J：

[![10.png](https://ask.julyedu.com/uploads/questions/20180821/b1cfcfd8b72d0108ab3b52760bc59ad1.png)](https://ask.julyedu.com/uploads/questions/20180821/b1cfcfd8b72d0108ab3b52760bc59ad1.png)

下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。

[![11.png](https://ask.julyedu.com/uploads/questions/20180821/8710bf81b11b6fd84e7a82c4b027590b.png)](https://ask.julyedu.com/uploads/questions/20180821/8710bf81b11b6fd84e7a82c4b027590b.png)

一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。

用更简单的数学语言进行描述步骤2）是这样的：

[![12.png](https://ask.julyedu.com/uploads/questions/20180821/cb5fe17dd4437fb89aaf3648fd5ed37c.png)](https://ask.julyedu.com/uploads/questions/20180821/cb5fe17dd4437fb89aaf3648fd5ed37c.png)

本题解析来源：@LeftNotEasy，链接：

http://www.cnblogs.com/LeftNot ... .html

342

牛顿法和梯度下降法有什么不同？

解析：

牛顿法（Newton's method）

牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

具体步骤：

首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f ' (x0)（这里f ' 表示函数 f 的导数）。

然后我们计算穿过点(x0,f(x0))并且斜率为f '(x0)的直线和x轴的交点的x坐标，也就是求如下方程的解：

[![13.png](https://ask.julyedu.com/uploads/questions/20180821/20b4f747198c5bbef437d4db21faa098.png)](https://ask.julyedu.com/uploads/questions/20180821/20b4f747198c5bbef437d4db21faa098.png)

我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f (x) = 0的解。

因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示：

[![14.png](https://ask.julyedu.com/uploads/questions/20180821/34704cf1fa9dc8078970b5522a244dcd.png)](https://ask.julyedu.com/uploads/questions/20180821/34704cf1fa9dc8078970b5522a244dcd.png)

已经证明，如果f'是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值x0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果f'(x)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示：

[![15.gif](https://ask.julyedu.com/uploads/questions/20180821/026753df964446e39eee156fbc9f74a2.gif)](https://ask.julyedu.com/uploads/questions/20180821/026753df964446e39eee156fbc9f74a2.gif)

关于牛顿法和梯度下降法的效率对比：

a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。

b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

[![16.png](https://ask.julyedu.com/uploads/questions/20180821/a38b666aea17efbe1356a74363cf9a5d.png)](https://ask.julyedu.com/uploads/questions/20180821/a38b666aea17efbe1356a74363cf9a5d.png)

注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。

牛顿法的优缺点总结：

优点：二阶收敛，收敛速度快；

缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

本题解析来源：@wtq1993，链接：

http://blog.csdn.net/wtq1993/a ... 07040

343题

熵、联合熵、条件熵、相对熵、互信息的定义

解析：

为了更好的理解，需要了解的概率必备知识有：

大写字母X表示随机变量，小写字母x表示随机变量X的某个具体的取值；

P(X)表示随机变量X的概率分布，P(X,Y)表示随机变量X、Y的联合概率分布，P(Y|X)表示已知随机变量X的情况下随机变量Y的条件概率分布；

p(X = x)表示随机变量X取某个具体值的概率，简记为p(x)；

p(X = x, Y = y) 表示联合概率，简记为p(x,y)，p(Y = y|X = x)表示条件概率，简记为p(y|x)，且有：p(x,y) = p(x) * p(y|x)。

熵：如果一个随机变量X的可能取值为X = {x1, x2,…, xk}，其概率分布为P(X = xi) = pi（i = 1,2, ..., n），则随机变量X的熵定义为：

[![17.jpg](https://ask.julyedu.com/uploads/questions/20180821/8b703dcf5b639ec843dd2085dad34777.jpg)](https://ask.julyedu.com/uploads/questions/20180821/8b703dcf5b639ec843dd2085dad34777.jpg)

把最前面的负号放到最后，便成了：

[![18.png](https://ask.julyedu.com/uploads/questions/20180821/439f37f1850162dbaa2025ef20d8835f.png)](https://ask.julyedu.com/uploads/questions/20180821/439f37f1850162dbaa2025ef20d8835f.png)

上面两个熵的公式，无论用哪个都行，而且两者等价，一个意思（这两个公式在下文中都会用到）。

联合熵：两个随机变量X，Y的联合分布，可以形成联合熵Joint Entropy，用H(X,Y)表示。

条件熵：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。

且有此式子成立：H(Y|X) = H(X,Y) – H(X)，整个式子表示(X,Y)发生所包含的熵减去X单独发生包含的熵。至于怎么得来的请看推导：

[![19.jpg](https://ask.julyedu.com/uploads/questions/20180821/c3ec41d83e43627bd4f607467e6fdc49.jpg)](https://ask.julyedu.com/uploads/questions/20180821/c3ec41d83e43627bd4f607467e6fdc49.jpg)

简单解释下上面的推导过程。整个式子共6行，其中

第二行推到第三行的依据是边缘分布p(x)等于联合分布p(x,y)的和；

第三行推到第四行的依据是把公因子logp(x)乘进去，然后把x,y写在一起；

第四行推到第五行的依据是：因为两个sigma都有p(x,y)，故提取公因子p(x,y)放到外边，然后把里边的-（log p(x,y) - log p(x)）写成- log (p(x,y)/p(x) ) ；

第五行推到第六行的依据是：p(x,y) = p(x) * p(y|x)，故p(x,y) / p(x) = p(y|x)。

相对熵：又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等。设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是：

[![20.jpg](https://ask.julyedu.com/uploads/questions/20180821/11acd2d881f545b9988c9b812a95bdab.jpg)](https://ask.julyedu.com/uploads/questions/20180821/11acd2d881f545b9988c9b812a95bdab.jpg)

在一定程度上，相对熵可以度量两个随机变量的“距离”，且有D(p||q) ≠D(q||p)。另外，值得一提的是，D(p||q)是必然大于等于0的。

互信息：两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，用I(X,Y)表示：

[![21.jpg](https://ask.julyedu.com/uploads/questions/20180821/53b542991d27fc88797f4183cc936ba1.jpg)](https://ask.julyedu.com/uploads/questions/20180821/53b542991d27fc88797f4183cc936ba1.jpg)

且有I(X,Y)=D(P(X,Y) || P(X)P(Y))。下面，咱们来计算下H(Y)-I(X,Y)的结果，如下：

[![22.jpg](https://ask.julyedu.com/uploads/questions/20180821/e1e44435b0a737aec7529649332948a6.jpg)](https://ask.julyedu.com/uploads/questions/20180821/e1e44435b0a737aec7529649332948a6.jpg)

通过上面的计算过程，我们发现竟然有H(Y)-I(X,Y) = H(Y|X)。故通过条件熵的定义，有：H(Y|X) = H(X,Y) - H(X)，而根据互信息定义展开得到H(Y|X) = H(Y) - I(X,Y)，把前者跟后者结合起来，便有I(X,Y)= H(X) + H(Y) - H(X,Y)，此结论被多数文献作为互信息的定义。更多请查看《最大熵模型中的数学推导》（链接：

http://blog.csdn.net/v_july_v/ ... 08465

）。

344

说说你知道的核函数

解析：

通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：

多项式核

[![23.png](https://ask.julyedu.com/uploads/questions/20180821/7b7b5e5bc48a0a769d42f9efaae2b3c9.png)](https://ask.julyedu.com/uploads/questions/20180821/7b7b5e5bc48a0a769d42f9efaae2b3c9.png)

显然刚才我们举的例子是这里多项式核的一个特例（R = 1，d = 2）。虽然比较麻烦，而且没有必要，不过这个核所对应的映射实际上是可以写出来的，该空间的维度是

[![24.png](https://ask.julyedu.com/uploads/questions/20180821/48bf43eacb79af6b8779c3bae8db5883.png)](https://ask.julyedu.com/uploads/questions/20180821/48bf43eacb79af6b8779c3bae8db5883.png)

其中 m 是原始空间的维度。

高斯核

[![25.png](https://ask.julyedu.com/uploads/questions/20180821/72f766948e98bfdcb3db4dfc95a8e8f0.png)](https://ask.julyedu.com/uploads/questions/20180821/72f766948e98bfdcb3db4dfc95a8e8f0.png)

这个核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。

不过，如果 σ 选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；

反过来，如果 σ 选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。

不过，总的来说，通过调控参数 σ ，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间：

[![26.png](https://ask.julyedu.com/uploads/questions/20180821/15a385fa500c9ef1aff136bd6d9d670c.png)](https://ask.julyedu.com/uploads/questions/20180821/15a385fa500c9ef1aff136bd6d9d670c.png)

线性核

[![27.png](https://ask.julyedu.com/uploads/questions/20180821/112693dd73cc4e32e8381eff72211b91.png)](https://ask.julyedu.com/uploads/questions/20180821/112693dd73cc4e32e8381eff72211b91.png)

这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了(意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)。

345题

什么是拟牛顿法（Quasi-Newton Methods）？

解析：

拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。

另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

具体步骤：

拟牛顿法的基本思想如下。首先构造目标函数在当前迭代xk的二次模型：

[![28.png](https://ask.julyedu.com/uploads/questions/20180821/926165ccecb953d2598f482bbb7039cf.png)](https://ask.julyedu.com/uploads/questions/20180821/926165ccecb953d2598f482bbb7039cf.png)

这里Bk是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：

[![29.png](https://ask.julyedu.com/uploads/questions/20180821/3c261a64a0947d9ecac2ecbc8513e734.png)](https://ask.julyedu.com/uploads/questions/20180821/3c261a64a0947d9ecac2ecbc8513e734.png)

其中我们要求步长ak 满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hessian矩阵Bk 代替真实的Hessian矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk 的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型：

[![30.png](https://ask.julyedu.com/uploads/questions/20180821/2f66decc12c98a8cbee852cfb97d583f.png)](https://ask.julyedu.com/uploads/questions/20180821/2f66decc12c98a8cbee852cfb97d583f.png)

我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求

[![31.png](https://ask.julyedu.com/uploads/questions/20180821/328c98ebd6301de8eb0147e2f12fd764.png)](https://ask.julyedu.com/uploads/questions/20180821/328c98ebd6301de8eb0147e2f12fd764.png)

从而得到

[![32.png](https://ask.julyedu.com/uploads/questions/20180821/4d993ce5652d72b51618c1454aa7d6fc.png)](https://ask.julyedu.com/uploads/questions/20180821/4d993ce5652d72b51618c1454aa7d6fc.png)

这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。

本题解析来源：@wtq1993，链接：

http://blog.csdn.net/wtq1993/a ... 07040

346题

kmeans的复杂度？

解析：

[![1.png](https://ask.julyedu.com/uploads/questions/20180821/b4d5a6389292467a281d507479a18ecf.png)](https://ask.julyedu.com/uploads/questions/20180821/b4d5a6389292467a281d507479a18ecf.png)

时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数

空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数

347题

请说说随机梯度下降法的问题和挑战？

解析：

[![2.png](https://ask.julyedu.com/uploads/questions/20180821/28b48b2bf003c84796e70ddcf5a5f683.png)](https://ask.julyedu.com/uploads/questions/20180821/28b48b2bf003c84796e70ddcf5a5f683.png)

[![3.png](https://ask.julyedu.com/uploads/questions/20180821/661bda1bcf94c5b146d725d2bc9ec177.png)](https://ask.julyedu.com/uploads/questions/20180821/661bda1bcf94c5b146d725d2bc9ec177.png)

[![4.png](https://ask.julyedu.com/uploads/questions/20180821/328618dcad685f4069033b26012d2411.png)](https://ask.julyedu.com/uploads/questions/20180821/328618dcad685f4069033b26012d2411.png)

[![5.png](https://ask.julyedu.com/uploads/questions/20180821/b59ff1123b86e32938512a69b3e03811.png)](https://ask.julyedu.com/uploads/questions/20180821/b59ff1123b86e32938512a69b3e03811.png)

348题

说说共轭梯度法？

解析：

共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。

下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：

[![6.png](https://ask.julyedu.com/uploads/questions/20180821/ce5ac73cab500c06413222876d4b6c5d.png)](https://ask.julyedu.com/uploads/questions/20180821/ce5ac73cab500c06413222876d4b6c5d.png)

注：绿色为梯度下降法，红色代表共轭梯度法

本题解析来源： @wtq1993，链接：

http://blog.csdn.net/wtq1993/a ... 07040

349题

对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法？

解析：

[![7.png](https://ask.julyedu.com/uploads/questions/20180821/fdfe0acfee7f516b6666c1cd9d72e66c.png)](https://ask.julyedu.com/uploads/questions/20180821/fdfe0acfee7f516b6666c1cd9d72e66c.png)

没有免费的午餐定理：

对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。

也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。

但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。

本题解析来源：@抽象猴，链接：

https://www.zhihu.com/question ... 04190

350题

什么是最大熵

解析：

熵是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0。如果没有外界干扰，随机变量总是趋向于无序，在经过足够时间的稳定演化，它应该能够达到的最大程度的熵。

为了准确的估计随机变量的状态，我们一般习惯性最大化熵，认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。换言之，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，其原则是承认已知事物（知识），且对未知事物不做任何假设，没有任何偏见。

例如，投掷一个骰子，如果问"每个面朝上的概率分别是多少"，你会说是等概率，即各点出现的概率均为1/6。因为对这个"一无所知"的色子，什么都不确定，而假定它每一个朝上概率均等则是最合理的做法。从投资的角度来看，这是风险最小的做法，而从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。

3.1 无偏原则

下面再举个大多数有关最大熵模型的文章中都喜欢举的一个例子。

例如，一篇文章中出现了“学习”这个词，那这个词是主语、谓语、还是宾语呢？换言之，已知“学习”可能是动词，也可能是名词，故“学习”可以被标为主语、谓语、宾语、定语等等。

令x1表示“学习”被标为名词， x2表示“学习”被标为动词。

令y1表示“学习”被标为主语， y2表示被标为谓语， y3表示宾语， y4表示定语。

且这些概率值加起来的和必为1，即

[![8.jpg](https://ask.julyedu.com/uploads/questions/20180821/4efe0584271428101083f9c08a2039ac.jpg)](https://ask.julyedu.com/uploads/questions/20180821/4efe0584271428101083f9c08a2039ac.jpg)

则根据无偏原则，认为这个分布中取各个值的概率是相等的，故得到：

[![9.jpg](https://ask.julyedu.com/uploads/questions/20180821/17d245ff0c01212990c2991fc671644a.jpg)](https://ask.julyedu.com/uploads/questions/20180821/17d245ff0c01212990c2991fc671644a.jpg)

因为没有任何的先验知识，所以这种判断是合理的。如果有了一定的先验知识呢？

即进一步，若已知：“学习”被标为定语的可能性很小，只有0.05，即

[![10.jpg](https://ask.julyedu.com/uploads/questions/20180821/1bc6a1a212acbc5a9f73a2f82d35ed25.jpg)](https://ask.julyedu.com/uploads/questions/20180821/1bc6a1a212acbc5a9f73a2f82d35ed25.jpg)

剩下的依然根据无偏原则，可得：

[![11.jpg](https://ask.julyedu.com/uploads/questions/20180821/b443e68dc2c4fabd08cbfd5d810eaa62.jpg)](https://ask.julyedu.com/uploads/questions/20180821/b443e68dc2c4fabd08cbfd5d810eaa62.jpg)

再进一步，当“学习”被标作名词x1的时候，它被标作谓语y2的概率为0.95，即

[![12.jpg](https://ask.julyedu.com/uploads/questions/20180821/a3f6bef1c701c45426195539672c6915.jpg)](https://ask.julyedu.com/uploads/questions/20180821/a3f6bef1c701c45426195539672c6915.jpg)

此时仍然需要坚持无偏见原则，使得概率分布尽量平均。但怎么样才能得到尽量无偏见的分布？

实践经验和理论计算都告诉我们，在完全无约束状态下，均匀分布等价于熵最大（有约束的情况下，不一定是概率相等的均匀分布。 比如，给定均值和方差，熵最大的分布就变成了正态分布 ）。

于是，问题便转化为了：计算X和Y的分布，使得H(Y|X)达到最大值，并且满足下述条件：

[![13.jpg](https://ask.julyedu.com/uploads/questions/20180821/dfebf2e4767df4cd996bd410811126b5.jpg)](https://ask.julyedu.com/uploads/questions/20180821/dfebf2e4767df4cd996bd410811126b5.jpg)

因此，也就引出了最大熵模型的本质，它要解决的问题就是已知X，计算Y的概率，且尽可能让Y的概率最大（实践中，X可能是某单词的上下文信息，Y是该单词翻译成me，I，us、we的各自概率），从而根据已有信息，尽可能最准确的推测未知信息，这就是最大熵模型所要解决的问题。

相当于已知X，计算Y的最大可能的概率，转换成公式，便是要最大化下述式子H(Y|X)：

[![14.jpg](https://ask.julyedu.com/uploads/questions/20180821/6b200ff37ed1106c928a978e50d3d258.jpg)](https://ask.julyedu.com/uploads/questions/20180821/6b200ff37ed1106c928a978e50d3d258.jpg)

且满足以下4个约束条件：

[![15.jpg](https://ask.julyedu.com/uploads/questions/20180821/003d15c37a4ff3c8c8279a0c288b99dc.jpg)](https://ask.julyedu.com/uploads/questions/20180821/003d15c37a4ff3c8c8279a0c288b99dc.jpg)

351题

LR与线性回归的区别与联系

解析：

LR工业上一般指Logistic Regression(逻辑回归)而不是Linear Regression(线性回归). LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的f*(1-f)形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。

引用自：@AntZ

个人感觉逻辑回归和线性回归首先都是广义的线性回归，

其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，

另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

引用自：@nishizhen

逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

引用自：@乖乖癞皮狗

352题

简单说下有监督学习和无监督学习的区别

解析：

有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）

无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)

353题

请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么？

解析：

集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权).

决策树属于最常用的学习器, 其学习过程是从根建立树, 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂, CART决策树用基尼指数计算最优分裂, xgboost决策树使用二阶泰勒展开系数计算最优分裂.

下面所提到的学习器都是决策树:

Bagging方法:

学习器间不存在强依赖关系, 学习器可并行训练生成, 集成方式一般为投票;

Random Forest属于Bagging的代表, 放回抽样, 每个学习器随机选择部分特征去优化;

Boosting方法:

学习器之间存在强依赖关系、必须串行生成, 集成方式为加权和;

Adaboost属于Boosting, 采用指数损失函数替代原本分类任务的0/1损失函数;

GBDT属于Boosting的优秀代表, 对函数残差近似值进行梯度下降, 用CART回归树做学习器, 集成为回归模型;

xgboost属于Boosting的集大成者, 对函数残差近似值进行梯度下降, 迭代时利用了二阶梯度信息, 集成模型可分类也可回归. 由于它可在特征粒度上并行计算, 结构风险和工程实现都做了很多优化, 泛化, 性能和扩展性都比GBDT要好。

关于决策树，这里有篇《决策树算法》（链接：

http://blog.csdn.net/v_july_v/ ... 77684

）。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文"Adaptive Boosting"（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。

引用自：@AntZ

xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：

1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数

2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性

3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的

引用自：@Xijun LI

354题

为什么xgboost要用泰勒展开，优势在哪里？

解析：

xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。

引用自：@AntZ

355题

协方差和相关性有什么区别？

解析：

相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20180822/f360a7de905ac46c2fa6bae9b31ee4ee.jpg)](https://ask.julyedu.com/uploads/questions/20180822/f360a7de905ac46c2fa6bae9b31ee4ee.jpg)

为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。

[![2.jpg](https://ask.julyedu.com/uploads/questions/20180822/a6a7e6b23ae1972af841bd006f54106a.jpg)](https://ask.julyedu.com/uploads/questions/20180822/a6a7e6b23ae1972af841bd006f54106a.jpg)

356、xgboost如何寻找最优特征？是有放回还是无放回的呢？

解析：

xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据, 从而记忆了每个特征对在模型训练时的重要性 -- 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序.

xgboost属于boosting集成学习方法, 样本是不放回的, 因而每轮计算样本不重复. 另一方面, xgboost支持子采样, 也就是每轮计算可以不使用全部样本, 以减少过拟合. 进一步地, xgboost 还有列采样, 每轮计算按百分比随机采样一部分特征, 既提高计算速度又减少过拟合。

357、谈谈判别式模型和生成式模型？

解析：

判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。

生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。

由生成模型可以得到判别模型，但由判别模型得不到生成模型。

常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场

常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机

358、线性分类器与非线性分类器的区别以及优劣

解析：

线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。

线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。

非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。

常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归

常见的非线性分类器：决策树、RF、GBDT、多层感知机

SVM两种都有（看线性核还是高斯核）

引用自@伟祺

359、L1和L2的区别

解析：

L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。

比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|.

简单总结一下就是：

L1范数: 为x向量各个元素绝对值之和。

L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或者Frobenius范数

Lp范数: 为x向量各个元素绝对值p次方和的1/p次方.

在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征，即L1范数可以使权值稀疏，方便特征提取。

L2范数可以防止过拟合，提升模型的泛化能力。

L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？看导数一个是1一个是w便知, 在靠进零附近, L1以匀速下降到零, 而L2则完全停下来了. 这说明L1是将不重要的特征(或者说, 重要性不在一个数量级上)尽快剔除, L2则是把特征贡献尽量压缩最小但不至于为零. 两者一起作用, 就是把重要性在一个数量级(重要性最高的)的那些特征一起平等共事(简言之, 不养闲人也不要超人)。

360、L1和L2正则先验分别服从什么分布

解析：

面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。

引用自：@齐同学

先验就是优化的起跑线, 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。

对参数引入高斯正态先验分布相当于L2正则化, 这个大家都熟悉：

[![1.jpg](https://ask.julyedu.com/uploads/questions/20180823/9391a07c899b77b60e8f122ba431127d.jpg)](https://ask.julyedu.com/uploads/questions/20180823/9391a07c899b77b60e8f122ba431127d.jpg)

[![2.jpg](https://ask.julyedu.com/uploads/questions/20180823/f6cf85212c213e05f6eef57a3a34cc87.jpg)](https://ask.julyedu.com/uploads/questions/20180823/f6cf85212c213e05f6eef57a3a34cc87.jpg)

对参数引入拉普拉斯先验等价于 L1正则化, 如下图：

[![3.jpg](https://ask.julyedu.com/uploads/questions/20180823/987a8ceee41bb970a3e9b48c390467cb.jpg)](https://ask.julyedu.com/uploads/questions/20180823/987a8ceee41bb970a3e9b48c390467cb.jpg)

[![4.jpg](https://ask.julyedu.com/uploads/questions/20180823/10eb6cc49c2a77c770c27de6151d39d4.jpg)](https://ask.julyedu.com/uploads/questions/20180823/10eb6cc49c2a77c770c27de6151d39d4.jpg)

从上面两图可以看出, L2先验趋向零周围, L1先验趋向零本身。



361、简单介绍下logistics回归？

解析：

Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。

假设函数

[![1.png](https://ask.julyedu.com/uploads/questions/20180823/be1df1ff7371a111449d4c89a264efea.png)](https://ask.julyedu.com/uploads/questions/20180823/be1df1ff7371a111449d4c89a264efea.png)

其中x是n维特征向量，函数g就是logistic函数。

而

[![2.png](https://ask.julyedu.com/uploads/questions/20180823/cc1396c4090ef0d1ffdadf1549e6031b.png)](https://ask.julyedu.com/uploads/questions/20180823/cc1396c4090ef0d1ffdadf1549e6031b.png)

的图像是

[![3.png](https://ask.julyedu.com/uploads/questions/20180823/7f72211bcd35b0635cc322d93278c0f9.png)](https://ask.julyedu.com/uploads/questions/20180823/7f72211bcd35b0635cc322d93278c0f9.png)

可以看到，将无穷映射到了(0,1)。

而假设函数就是特征属于y=1的概率。

[![4.png](https://ask.julyedu.com/uploads/questions/20180823/d6240d3f04056d15bbb9db3ed4bf6e0a.png)](https://ask.julyedu.com/uploads/questions/20180823/d6240d3f04056d15bbb9db3ed4bf6e0a.png)

从而，当我们要判别一个新来的特征属于哪个类时，只需求 hθ(x) 即可，若 hθ(x) 大于0.5就是y=1的类，反之属于y=0类。

[![01.png](https://ask.julyedu.com/uploads/questions/20180823/f9f4b9b13261ff3eeb23835d6e1a3254.png)](https://ask.julyedu.com/uploads/questions/20180823/f9f4b9b13261ff3eeb23835d6e1a3254.png)

362、说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式。

解析：

给定一个训练数据集T={(x1,y1), (x2,y2)…(xN,yN)}，其中实例 x∈ X

而实例空间

[![5.png](https://ask.julyedu.com/uploads/questions/20180823/6e3ed7beea63578bcff3c36de4a5df6d.png)](https://ask.julyedu.com/uploads/questions/20180823/6e3ed7beea63578bcff3c36de4a5df6d.png)

，yi属于标记集合{-1,+1}，Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。

Adaboost的算法流程如下：

步骤1. 首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N。

[![6.png](https://ask.julyedu.com/uploads/questions/20180823/3aa760a1e8ae2380d719de3f78f186e4.png)](https://ask.julyedu.com/uploads/questions/20180823/3aa760a1e8ae2380d719de3f78f186e4.png)

步骤2. 进行多轮迭代，用m = 1,2, ..., M表示迭代的第多少轮

a. 使用具有权值分布Dm的训练数据集学习，得到基本分类器（选取让误差率最低的阈值来设计基本分类器）：

[![7.png](https://ask.julyedu.com/uploads/questions/20180823/9525553c19f5c5fff287a40b46be1fba.png)](https://ask.julyedu.com/uploads/questions/20180823/9525553c19f5c5fff287a40b46be1fba.png)

b. 计算Gm(x)在训练数据集上的分类误差率

[![8.png](https://ask.julyedu.com/uploads/questions/20180823/52b1a04c6e2f95ae74f38f5c07a56d76.png)](https://ask.julyedu.com/uploads/questions/20180823/52b1a04c6e2f95ae74f38f5c07a56d76.png)

由上述式子可知，Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。

c. 计算Gm(x)的系数，am表示Gm(x)在最终分类器中的重要程度（目的：得到基本分类器在最终分类器中所占的权重）：

[![9.png](https://ask.julyedu.com/uploads/questions/20180823/970afa734bc2977d9529ac529a1d0f7d.png)](https://ask.julyedu.com/uploads/questions/20180823/970afa734bc2977d9529ac529a1d0f7d.png)

由上述式子可知，em <= 1/2时，am >= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。

d. 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代

[![10.png](https://ask.julyedu.com/uploads/questions/20180823/27992a816983be21963bfdfa34d77234.png)](https://ask.julyedu.com/uploads/questions/20180823/27992a816983be21963bfdfa34d77234.png)

使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上。

其中，Zm是规范化因子，使得Dm+1成为一个概率分布：

[![11.png](https://ask.julyedu.com/uploads/questions/20180823/6415370a639d8ed60132d6dc77120428.png)](https://ask.julyedu.com/uploads/questions/20180823/6415370a639d8ed60132d6dc77120428.png)

步骤3. 组合各个弱分类器

[![12.png](https://ask.julyedu.com/uploads/questions/20180823/4d60dc3b66d2e8f607a4542beebf52ff.png)](https://ask.julyedu.com/uploads/questions/20180823/4d60dc3b66d2e8f607a4542beebf52ff.png)

从而得到最终分类器，如下：

[![13.png](https://ask.julyedu.com/uploads/questions/20180823/68ec5dee6f7957a409c998d75ff7f4a6.png)](https://ask.julyedu.com/uploads/questions/20180823/68ec5dee6f7957a409c998d75ff7f4a6.png)

更多请查看此文：《Adaboost 算法的原理与推导》（链接：

http://blog.csdn.net/v_july_v/ ... 18799

）。

363 、经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：

[![14.png](https://ask.julyedu.com/uploads/questions/20180823/77f88eb638cb04d0cce3f511776363d3.png)](https://ask.julyedu.com/uploads/questions/20180823/77f88eb638cb04d0cce3f511776363d3.png)

这叫做拼写检查。根据谷歌一员工写的文章显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。

解析：

用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做c（代表correct），拼写错误的情况记做w（代表wrong），那么"拼写检查"要做的事情就是：在发生w的情况下，试图推断出c。换言之：已知w，然后在若干个备选方案中，找出可能性最大的那个c，也就是求

[![15.png](https://ask.julyedu.com/uploads/questions/20180823/f37d15655e036a26e482822b3c474b5b.png)](https://ask.julyedu.com/uploads/questions/20180823/f37d15655e036a26e482822b3c474b5b.png)

的最大值。

而根据贝叶斯定理，有：

[![16.png](https://ask.julyedu.com/uploads/questions/20180823/d85dac148d6df51fd6922391f601e6f4.png)](https://ask.julyedu.com/uploads/questions/20180823/d85dac148d6df51fd6922391f601e6f4.png)

由于对于所有备选的c来说，对应的都是同一个w，所以它们的P(w)是相同的，因此我们只要最大化

[![17.png](https://ask.julyedu.com/uploads/questions/20180823/2875ba45cca8d6f1adb3e82ddb7976ba.png)](https://ask.julyedu.com/uploads/questions/20180823/2875ba45cca8d6f1adb3e82ddb7976ba.png)

即可。其中：

P(c)表示某个正确的词的出现"概率"，它可以用"频率"代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P(c)就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。

P(w|c)表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见博客中的这篇文章。

所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。具体的计算过程及此方法的缺陷请参见这里。

364、为什么朴素贝叶斯如此“朴素”？

解析：

因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是"很简单很天真"地假设样本特征彼此独立. 这个假设现实中基本上不存在, 但特征相关性很小的实际情况还是很多的, 所以这个模型仍然能够工作得很好。

引用自：@AntZ

**365、请大致对比下plsa和LDA的区别**

解析：

pLSA中，主题分布和词分布确定后，以一定的概率

[![17.png](https://ask.julyedu.com/uploads/questions/20180823/2875ba45cca8d6f1adb3e82ddb7976ba.png)](https://ask.julyedu.com/uploads/questions/20180823/2875ba45cca8d6f1adb3e82ddb7976ba.png)

分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布时，最终用EM算法（极大似然估计思想）求解出了两个未知但固定的参数的值：∅kj （由

[![18.png](https://ask.julyedu.com/uploads/questions/20180823/ed6a693f82ea6b5cf1ba168cf8141913.png)](https://ask.julyedu.com/uploads/questions/20180823/ed6a693f82ea6b5cf1ba168cf8141913.png)

转换而来）和 ∅ik

（由

[![19.png](https://ask.julyedu.com/uploads/questions/20180823/9561a981226167864d9e091a11d2b994.png)](https://ask.julyedu.com/uploads/questions/20180823/9561a981226167864d9e091a11d2b994.png)

转换而来）。

文档d产生主题z的概率，主题z产生单词w的概率都是两个固定的值。

举个文档d产生主题z的例子。给定一篇文档d，主题分布是一定的，比如{ P(zi|d), i = 1,2,3 }可能就是{0.4,0.5,0.1}，表示z1、z2、z3，这3个主题被文档d选中的概率都是个固定的值：P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，如下图所示（图截取自沈博PPT上）：

[![20.jpg](https://ask.julyedu.com/uploads/questions/20180823/68af5c3563f6e3695028bd6b405a6682.jpg)](https://ask.julyedu.com/uploads/questions/20180823/68af5c3563f6e3695028bd6b405a6682.jpg)

但在贝叶斯框架下的LDA中，我们不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。但一篇文档总得对应一个主题分布和一个词分布吧，怎么办呢？LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。

文档d产生主题z（准确的说，其实是Dirichlet先验为文档d生成主题分布Θ，然后根据主题分布Θ产生主题z）的概率，主题z产生单词w的概率都不再是某两个确定的值，而是随机变量。

还是再次举下文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布{ P(zi|d), i = 1,2,3 }可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，可能是P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，也有可能是P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6等等，而主题分布到底是哪个取值集合我们不确定（为什么？这就是贝叶斯派的核心思想，把未知参数当作是随机变量，不再认为是某一个确定的值），但其先验分布是dirichlet 分布，所以可以从无穷多个主题分布中按照dirichlet 先验随机抽取出某个主题分布出来。如下图所示（图截取自沈博PPT上）：

[![21.png](https://ask.julyedu.com/uploads/questions/20180823/d1e6a6d72e8e450af72bf6f0d24cacc4.png)](https://ask.julyedu.com/uploads/questions/20180823/d1e6a6d72e8e450af72bf6f0d24cacc4.png)

换言之，LDA在pLSA的基础上给这两参数（

[![17.png](https://ask.julyedu.com/uploads/questions/20180823/2875ba45cca8d6f1adb3e82ddb7976ba.png)](https://ask.julyedu.com/uploads/questions/20180823/2875ba45cca8d6f1adb3e82ddb7976ba.png)

）加了两个先验分布的参数（贝叶斯化）：一个主题分布的先验分布Dirichlet分布 α

，和一个词语分布的先验分布Dirichlet分布 β

。综上，LDA真的只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布，只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。

更多请参见：《通俗理解LDA主题模型》（链接：

http://blog.csdn.net/v_july_v/ ... 09515

）。

366、请简要说说EM算法

解析：

有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步：

　　E步：选取一组参数，求出在该参数下隐含变量的条件概率值；

　　M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。

　　重复上面2步直至收敛。

　　公式如下所示：

[![1.png](https://ask.julyedu.com/uploads/questions/20180824/c4fed8aa509e7ca23f818d73799f5ab4.png)](https://ask.julyedu.com/uploads/questions/20180824/c4fed8aa509e7ca23f818d73799f5ab4.png)

M步公式中下界函数的推导过程：

[![2.png](https://ask.julyedu.com/uploads/questions/20180824/37edc4ac0334d15e1bb3ba37e11ab524.png)](https://ask.julyedu.com/uploads/questions/20180824/37edc4ac0334d15e1bb3ba37e11ab524.png)

EM算法一个常见的例子就是GMM模型，每个样本都有可能由k个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。

GMM的E步公式如下（计算每个样本对应每个高斯的概率）：

[![3.png](https://ask.julyedu.com/uploads/questions/20180824/9d64df375062942df137714098aa7da0.png)](https://ask.julyedu.com/uploads/questions/20180824/9d64df375062942df137714098aa7da0.png)

更具体的计算公式为：

[![4.png](https://ask.julyedu.com/uploads/questions/20180824/bf0508f1aa123dcd0599bff008aa8b6a.png)](https://ask.julyedu.com/uploads/questions/20180824/bf0508f1aa123dcd0599bff008aa8b6a.png)

M步公式如下（计算每个高斯的比重，均值，方差这3个参数）：

[![5.png](https://ask.julyedu.com/uploads/questions/20180824/2f21834228f5907e7ef106c2753b65cf.png)](https://ask.julyedu.com/uploads/questions/20180824/2f21834228f5907e7ef106c2753b65cf.png)

本题解析来源：@tornadomeet，链接：

http://www.cnblogs.com/tornadomeet/p/3395593.html

367、KNN中的K如何选取的？

解析：

关于什么是KNN，可以查看此文：《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：

http://blog.csdn.net/v_july_v/ ... 03674

）。KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：

如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；

如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。

K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。

在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。

368、防止过拟合的方法

解析：

过拟合的原因是算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。 

处理方法：

1 早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练

2 数据集扩增：原有数据增加、原有数据加随机噪声、重采样

3 正则化，正则化可以限制模型的复杂度

4 交叉验证

5 特征选择/特征降维

6 创建一个验证集是最基本的防止过拟合的方法。我们最终训练得到的模型目标是要在验证集上面有好的表现，而不训练集

369、什么最小二乘法？

解析：

我们口头中经常说：一般来说，平均来说。如平均来说，不吸烟的健康优于吸烟者，之所以要加“平均”二字，是因为凡事皆有例外，总存在某个特别的人他吸烟但由于经常锻炼所以他的健康状况可能会优于他身边不吸烟的朋友。而最小二乘法的一个最简单的例子便是算术平均。

最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。用函数表示为：

[![6.png](https://ask.julyedu.com/uploads/questions/20180824/2ec8619c2ed5d639335d4536706e5a2f.png)](https://ask.julyedu.com/uploads/questions/20180824/2ec8619c2ed5d639335d4536706e5a2f.png)

使误差「所谓误差，当然是观察值与实际真实值的差量」平方和达到最小以寻求估计值的方法，就叫做最小二乘法，用最小二乘法得到的估计，叫做最小二乘估计。当然，取平方和作为目标函数只是众多可取的方法之一。

最小二乘法的一般形式可表示为：

[![7.png](https://ask.julyedu.com/uploads/questions/20180824/3a1eb4fddd4d421058bd9a5ad02fcffc.png)](https://ask.julyedu.com/uploads/questions/20180824/3a1eb4fddd4d421058bd9a5ad02fcffc.png)

有效的最小二乘法是勒让德在 1805 年发表的，基本思想就是认为测量中有误差，所以所有方程的累积误差为

[![8.jpeg](https://ask.julyedu.com/uploads/questions/20180824/57f1bc5b17b4ac3f2689a3e882198af5.jpeg)](https://ask.julyedu.com/uploads/questions/20180824/57f1bc5b17b4ac3f2689a3e882198af5.jpeg)

我们求解出导致累积误差最小的参数即可：

[![9.jpeg](https://ask.julyedu.com/uploads/questions/20180824/58eea09c601ed08248141dd65f3c2c19.jpeg)](https://ask.julyedu.com/uploads/questions/20180824/58eea09c601ed08248141dd65f3c2c19.jpeg)

勒让德在论文中对最小二乘法的优良性做了几点说明：

1.最小二乘使得误差平方和最小，并在各个方程的误差之间建立了一种平衡，从而防止某一个极端误差取得支配地位

2.计算中只要求偏导后求解线性方程组，计算过程明确便捷

3.最小二乘可以导出算术平均值作为估计值

对于最后一点，从统计学的角度来看是很重要的一个性质。推理如下：假设真值为θ, x1,⋯,xn为n次测量值, 每次测量的误差为ei=xi−θ，按最小二乘法，误差累积为

[![10.jpeg](https://ask.julyedu.com/uploads/questions/20180824/55a1956c397081b17364057450032034.jpeg)](https://ask.julyedu.com/uploads/questions/20180824/55a1956c397081b17364057450032034.jpeg)

求解θ使L(θ)达到最小，正好是算术平均

[![11.jpeg](https://ask.julyedu.com/uploads/questions/20180824/aafa61e020bd4696b1dfa564b3509360.jpeg)](https://ask.julyedu.com/uploads/questions/20180824/aafa61e020bd4696b1dfa564b3509360.jpeg)

由于算术平均是一个历经考验的方法，而以上的推理说明，算术平均是最小二乘的一个特例，所以从另一个角度说明了最小二乘方法的优良性，使我们对最小二乘法更加有信心。

最小二乘法发表之后很快得到了大家的认可接受，并迅速的在数据分析实践中被广泛使用。不过历史上又有人把最小二乘法的发明归功于高斯，这又是怎么一回事呢。高斯在1809年也发表了最小二乘法，并且声称自己已经使用这个方法多年。高斯发明了小行星定位的数学方法，并在数据分析中使用最小二乘方法进行计算，准确的预测了谷神星的位置。

对了，最小二乘法跟SVM有什么联系呢？请参见《支持向量机通俗导论（理解SVM的三层境界）》(链接：

http://blog.csdn.net/v_july_v/ ... 24837

）。

370、简单说说贝叶斯定理

解析：

在引出贝叶斯定理之前，先学习几个定义：

条件概率（又称后验概率）就是事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。

比如，在同一个样本空间Ω中的事件或者子集A与B，如果随机从Ω中选出的一个元素属于B，那么这个随机选择的元素还属于A的概率就定义为在B的前提下A的条件概率，所以：P(A|B) = |A∩B|/|B|，接着分子、分母都除以|Ω|得到

[![12.jpeg](https://ask.julyedu.com/uploads/questions/20180824/b47ca1f6516edd6ee8f51edc7e841e7f.jpeg)](https://ask.julyedu.com/uploads/questions/20180824/b47ca1f6516edd6ee8f51edc7e841e7f.jpeg)

联合概率表示两个事件共同发生的概率。A与B的联合概率表示为

[![13.png](https://ask.julyedu.com/uploads/questions/20180824/d9086ea846df3ff5bac2b09ac115211c.png)](https://ask.julyedu.com/uploads/questions/20180824/d9086ea846df3ff5bac2b09ac115211c.png)

和

[![14.png](https://ask.julyedu.com/uploads/questions/20180824/6b801612916a1eb1e7de2cd4a3d215d2.png)](https://ask.julyedu.com/uploads/questions/20180824/6b801612916a1eb1e7de2cd4a3d215d2.png)

边缘概率（又称先验概率）是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)。 

接着，考虑一个问题：P(A|B)是在B发生的情况下A发生的可能性。

首先，事件B发生之前，我们对事件A的发生有一个基本的概率判断，称为A的先验概率，用P(A)表示；

其次，事件B发生之后，我们对事件A的发生概率重新评估，称为A的后验概率，用P(A|B)表示；

类似的，事件A发生之前，我们对事件B的发生有一个基本的概率判断，称为B的先验概率，用P(B)表示；

同样，事件A发生之后，我们对事件B的发生概率重新评估，称为B的后验概率，用P(B|A)表示。

贝叶斯定理便是基于下述贝叶斯公式：

[![15.jpeg](https://ask.julyedu.com/uploads/questions/20180824/4231e5281b74e1d1e810eb285f885db1.jpeg)](https://ask.julyedu.com/uploads/questions/20180824/4231e5281b74e1d1e810eb285f885db1.jpeg)

上述公式的推导其实非常简单，就是从条件概率推出。

根据条件概率的定义，在事件B发生的条件下事件A发生的概率是

[![16.png](https://ask.julyedu.com/uploads/questions/20180824/219ac346198bc76141c4b5ac86a623ce.png)](https://ask.julyedu.com/uploads/questions/20180824/219ac346198bc76141c4b5ac86a623ce.png)

同样地，在事件A发生的条件下事件B发生的概率

[![17.png](https://ask.julyedu.com/uploads/questions/20180824/7938c8b25c9e9fc42395e39ab84f4cf4.png)](https://ask.julyedu.com/uploads/questions/20180824/7938c8b25c9e9fc42395e39ab84f4cf4.png)

整理与合并上述两个方程式，便可以得到：

[![18.png](https://ask.julyedu.com/uploads/questions/20180824/cc1d304261234bfecb30401c1130b751.png)](https://ask.julyedu.com/uploads/questions/20180824/cc1d304261234bfecb30401c1130b751.png)

接着，上式两边同除以P(B)，若P(B)是非零的，我们便可以得到贝叶斯定理的公式表达式：

[![19.png](https://ask.julyedu.com/uploads/questions/20180824/07c9a1de392a52174b04323fbc385ba5.png)](https://ask.julyedu.com/uploads/questions/20180824/07c9a1de392a52174b04323fbc385ba5.png)

所以，贝叶斯公式可以直接根据条件概率的定义直接推出。即因为P(A,B) = P(A)P(B|A) = P(B)P(A|B)，所以P(A|B) = P(A)P(B|A) / P(B)。更多请参见此文：《从贝叶斯方法谈到贝叶斯网络》（链接：

http://blog.csdn.net/v_july_v/ ... 84699

）。

371、标准化与归一化的区别？

解析：

简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。

归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。

关于什么是归一化，请参见：

https://www.julyedu.com/questi ... /1011

372、随机森林如何处理缺失值？

解析：

方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。

方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似12。

373、随机森林如何评估特征重要性？

解析：

衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：

1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。

2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。

374、优化Kmeans？

解析：

使用kd树或者ball tree

将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。

375、KMeans初始类簇中心点的选取。

解析：

k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。

1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心
2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
4. 重复2和3直到k个聚类中心被选出来
5. 利用这k个初始的聚类中心来运行标准的k-means算法

376、解释对偶的概念。

解析：

一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

377、如何进行特征选择？

解析：

特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解

常见的特征选择方式：

1. 去除方差较小的特征
2. 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。
3. 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。
4. 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。

378、衡量分类器的好坏？

解析：

　这里首先要知道TP、FN（真的判成假的）、FP（假的判成真）、TN四种（可以画一个表格）。

几种常用的指标：

精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）

召回率 recall = TP/(TP+FN) = TP/ P

F1值： 2/F1 = 1/recall + 1/precision

ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N

更详细请点击：

https://siyaozhang.github.io/2 ... 7%258

解析来源：@我愛大泡泡，链接：

http://blog.csdn.net/woaidapao ... 06273

379、机器学习和统计里面的auc的物理意义是啥？

解析：

auc是评价模型好坏的常见指标之一，详见：

https://www.zhihu.com/question/39840928

380、数据预处理

解析：

1. 缺失值，填充缺失值fillna：

i. 离散：None,

ii. 连续：均值。

iii. 缺失值太多，则直接去除该列

1. 连续值：离散化。有的模型（如决策树）需要离散值
2. 对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作
3. 皮尔逊相关系数，去除高度相关的列

381、观察增益gain, alpha和gamma越大，增益越小？

解析：

xgboost寻找分割点的标准是最大化gain. 考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中计算Gain按最大值找出最佳的分割点。它的计算公式分为四项, 可以由正则化项参数调整(lamda为叶子权重平方和的系数, gama为叶子数量):

第一项是假设分割的左孩子的权重分数, 第二项为右孩子, 第三项为不分割总体分数, 最后一项为引入一个节点的复杂度损失

由公式可知, gama越大gain越小, lamda越大, gain可能小也可能大.

原问题是alpha而不是lambda, 这里paper上没有提到, xgboost实现上有这个参数. 上面是我从paper上理解的答案,下面是搜索到的:

https://zhidao.baidu.com/quest ... 3Dgbk

lambda[默认1]权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。11、alpha[默认1]权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。

gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。

引用自@AntZ

382、什麽造成梯度消失问题?

解析：

Yes you should understand backdrop－Andrej Karpathy

How does the ReLu solve the vanishing gradient problem?

神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。

梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。

[![1.jpeg](https://ask.julyedu.com/uploads/questions/20180829/acc8766286039a6e01cdc7d6eadef108.jpeg)](https://ask.julyedu.com/uploads/questions/20180829/acc8766286039a6e01cdc7d6eadef108.jpeg)

解析来源：@许韩，链接：

https://www.zhihu.com/question ... 04190

简而言之，就是sigmoid函数f(x)的导数为f(x)*(1-f(x))， 因为f(x)的输出在0-1之间，所以随着深度的增加，从顶端传过来的导数每次都乘以两个小于1的数，很快就变得特别特别小。

引用自@张雨石

383、简单说说特征工程。

解析：

首先，大多数机器学习从业者主要在公司做什么呢？不是做数学推导，也不是发明多高大上的算法，而是做特征工程，如下图所示（图来自：

http://www.julyedu.com/video/play/18

）

[![2.png](https://ask.julyedu.com/uploads/questions/20180829/48ce66aa9a7de176d12811d92eb30aca.png)](https://ask.julyedu.com/uploads/questions/20180829/48ce66aa9a7de176d12811d92eb30aca.png)

进一步，特征工程主要是做如下工作（图来自机器学习第九期第五次课 特征工程）

[![3.png](https://ask.julyedu.com/uploads/questions/20180829/01a8d65675521fbb8471a4a1ba7a91db.png)](https://ask.julyedu.com/uploads/questions/20180829/01a8d65675521fbb8471a4a1ba7a91db.png)

384、你知道有哪些数据处理和特征工程的处理？

解析：

[![4.png](https://ask.julyedu.com/uploads/questions/20180829/a88893c6742891ead8eceffce3e0502f.png)](https://ask.julyedu.com/uploads/questions/20180829/a88893c6742891ead8eceffce3e0502f.png)

更多请查看此课程《机器学习工程师 第八期 [六大阶段、层层深入]》第7次课 特征工程。

385、准备机器学习面试应该了解哪些理论知识？

解析：

[![5.png](https://ask.julyedu.com/uploads/questions/20180829/258ec036dd5775b53671f8bb20b27904.png)](https://ask.julyedu.com/uploads/questions/20180829/258ec036dd5775b53671f8bb20b27904.png)

看下来，这些问题的答案基本都在本BAT机器学习面试1000题系列里了。



  **386、数据不平衡问题**

解析：

这主要是由于数据分布不平衡造成的。解决方法如下：

采样，对小样本加噪声采样，对大样本进行下采样

数据生成，利用已知样本生成新的样本

进行特殊的加权，如在Adaboost中或者SVM中

采用对不平衡数据集不敏感的算法

改变评价标准：用AUC/ROC来进行评价

采用Bagging/Boosting/ensemble等方法

在设计模型的时候考虑数据的先验分布

**387、特征比数据量还大时，选择什么样的分类器？**

解析：

线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。

来源：

[http://blog.sina.com.cn/s/blog ... .html](http://blog.sina.com.cn/s/blog_178bcad000102x70r.html)

**388、常见的分类算法有哪些？**

解析：

SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯

**389、常见的监督学习算法有哪些？**

解析：

感知机、svm、人工神经网络、决策树、逻辑回归

**390、说说常见的优化算法及其优缺点？**

解析：

温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。

简言之

1）随机梯度下降

优点：可以一定程度上解决局部最优解的问题

缺点：收敛速度较慢

2）批量梯度下降

优点：容易陷入局部最优解

缺点：收敛速度较快

3）mini_batch梯度下降

综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。

4）牛顿法

牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。

5）拟牛顿法

拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

具体而言

从每个batch的数据来区分

梯度下降：每次使用全部数据集进行训练

优点：得到的是最优解

缺点：运行速度慢，内存可能不够

随机梯度下降：每次使用一个数据进行训练

优点：训练速度快，无内存问题

缺点：容易震荡，可能达不到最优解

Mini-batch梯度下降

优点：训练速度快，无内存问题，震荡较少

缺点：可能达不到最优解

从优化方法上来分：

随机梯度下降（SGD）

缺点

选择合适的learningrate比较难

对于所有的参数使用同样的learning rate

容易收敛到局部最优

可能困在saddle point

SGD+Momentum

优点：

积累动量，加速训练

局部极值附近震荡时，由于动量，跳出陷阱

梯度方向发生变化时，动量缓解动荡。

Nesterov Mementum

与Mementum类似，优点：

避免前进太快

提高灵敏度

AdaGrad

优点：

控制学习率，每一个分量有各自不同的学习率

适合稀疏数据

缺点

依赖一个全局学习率

学习率设置太大，其影响过于敏感

后期，调整学习率的分母积累的太大，导致学习率很低，提前结束训练。

RMSProp

优点：

解决了后期提前结束的问题。

缺点：

依然依赖全局学习率

Adam

Adagrad和RMSProp的合体

优点：

结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点

为不同的参数计算不同的自适应学习率

也适用于大多非凸优化 -适用于大数据集和高维空间

牛顿法

牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难

拟牛顿法

拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。  

391、特征向量的归一化方法有哪些？

解析：

线性函数转换，表达式如下：

y=(x-MinValue)/(MaxValue-MinValue)

对数函数转换，表达式如下：

y=log10 (x)

反余切函数转换 ，表达式如下：

y=arctan(x)*2/PI

减去均值，除以方差：

y=(x-means)/ variance

392、RF与GBDT之间的区别与联系？

解析：

1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。

2）不同点：

a 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成

b 组成随机森林的树可以并行生成，而GBDT是串行生成

c 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和

d 随机森林对异常值不敏感，而GBDT对异常值比较敏感

e 随机森林是减少模型的方差，而GBDT是减少模型的偏差

f 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化

393、试证明样本空间中任意点X到超平面(w, b)的距离为式(6.2).

解析：

[![1.jpeg](https://ask.julyedu.com/uploads/questions/20180831/baaea0f45faaee402ae0ae73cd5dd18b.jpeg)](https://ask.julyedu.com/uploads/questions/20180831/baaea0f45faaee402ae0ae73cd5dd18b.jpeg)

从网上下载或自己编程实现一一个卷积神经网络,并在手写字符识别数据 MNIST上进行实验测试

394、请比较下EM算法、HMM、CRF

解析：

这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。

（1）EM算法

　　EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。

　　注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。

（2）HMM算法

　　隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。

马尔科夫三个基本问题：

概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法

学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。

预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）

（3）条件随机场CRF

　　给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。

　　之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。

（4）HMM和CRF对比

　　其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。

395、带核的SVM为什么能分类非线性问题？

解析：

核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面, 如图:

[![2.png](https://ask.julyedu.com/uploads/questions/20180831/7820c71f797ec75b02de3241214d738a.png)](https://ask.julyedu.com/uploads/questions/20180831/7820c71f797ec75b02de3241214d738a.png)

其分类结果也视为低维空间的非线性分类结果, 因而带核的SVM就能分类非线性问题。

396、请说说常用核函数及核函数的条件

解析：

我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。SVM关键是选取核函数的类型，常用核函数主要有线性内核，多项式内核，径向基内核（RBF），sigmoid核。

线性核函数

[![1.png](https://ask.julyedu.com/uploads/questions/20180903/9bb46b2cc8014aa41c870718be852e0f.png)](https://ask.julyedu.com/uploads/questions/20180903/9bb46b2cc8014aa41c870718be852e0f.png)

线性核，主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的

多项式核函数

[![2.png](https://ask.julyedu.com/uploads/questions/20180903/f4fdaf0ce2cf0c414cf63ba63c8056bb.png)](https://ask.julyedu.com/uploads/questions/20180903/f4fdaf0ce2cf0c414cf63ba63c8056bb.png)

多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。

高斯（RBF）核函数

[![3.png](https://ask.julyedu.com/uploads/questions/20180903/bacae9eeea66f7b1662033b6cf5cff34.png)](https://ask.julyedu.com/uploads/questions/20180903/bacae9eeea66f7b1662033b6cf5cff34.png)

高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数。

sigmoid核函数

[![4.png](https://ask.julyedu.com/uploads/questions/20180903/1af16caacd21f2b983cab23416d25b74.png)](https://ask.julyedu.com/uploads/questions/20180903/1af16caacd21f2b983cab23416d25b74.png)

采用sigmoid核函数，支持向量机实现的就是一种多层神经网络。

因此，在选用核函数的时候，如果我们对我们的数据有一定的先验知识，就利用先验来选择符合数据分布的核函数；如果不知道的话，通常使用交叉验证的方法，来试用不同的核函数，误差最下的即为效果最好的核函数，或者也可以将多个核函数结合起来，形成混合核函数。

在吴恩达的课上，也曾经给出过一系列的选择核函数的方法：

如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；

如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；

如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。

397、请具体说说Boosting和Bagging的区别

解析：

（1） Bagging之随机森林

　　随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：

　　1）Boostrap从袋内有放回的抽取样本值

　　2）每次随机抽取一定数量的特征（通常为sqr(n)）。

　　分类问题：采用Bagging投票的方式选择类别频次最高的

　　回归问题：直接取每颗树结果的平均值。

常见参数误差分析优点缺点

1、树最大深度

2、树的个数

3、节点上的最小样本数

4、特征数(sqr(n))oob(out-of-bag)

将各个树的未采样样本作为预测样本统计误差作为误分率可以并行计算

不需要特征选择

可以总结出特征重要性

可以处理缺失数据

不需要额外设计测试集在回归上不能输出连续结果

（2）Boosting之AdaBoost

Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。

（3）Boosting之GBDT

将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。

　　注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。

（4）Boosting之Xgboost

这个工具主要有以下几个特点：

支持线性分类器

可以自定义损失函数，并且可以用二阶偏导

加入了正则化项：叶节点数、每个叶节点输出score的L2-norm

支持特征抽样

在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。

398、逻辑回归相关问题

解析：

（1）公式推导一定要会

（2）逻辑回归的基本概念

　　这个最好从广义线性模型的角度分析，逻辑回归是假设y服从Bernoulli分布。

（3）L1-norm和L2-norm

　　其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。

　　但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。

（4）LR和SVM对比

　　首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss。

　　其次，两者都是线性模型。

　　最后，SVM只考虑支持向量（也就是和分类相关的少数点）

（5）LR和随机森林区别

　　随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。

（6）常用的优化方法

　　逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。

　　一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。

　　二阶方法：牛顿法、拟牛顿法：

　　这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。

缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。

拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

399、什么是共线性, 跟过拟合有什么关联?

解析：

共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。

共线性会造成冗余，导致过拟合。

解决方法：排除变量的相关性／加入权重正则。

本题解析来源：@抽象猴，链接：

https://www.zhihu.com/question ... 04190

400、用贝叶斯机率说明Dropout的原理

解析：

回想一下使用Bagging学习,我们定义 k 个不同的模型,从训练集有替换采样 构造 k 个不同的数据集,然后在训练集上训练模型 i。

Dropout的目标是在指数 级数量的神经网络上近似这个过程。Dropout训练与Bagging训练不太一样。在Bagging的情况下,所有模型是独立 的。

在Dropout的情况下,模型是共享参数的,其中每个模型继承的父神经网络参 数的不同子集。参数共享使得在有限可用的内存下代表指数数量的模型变得可能。 在Bagging的情况下,每一个模型在其相应训练集上训练到收敛。

在Dropout的情况下,通常大部分模型都没有显式地被训练,通常该模型很大,以致到宇宙毁灭都不 能采样所有可能的子网络。取而代之的是,可能的子网络的一小部分训练单个步骤,参数共享导致剩余的子网络能有好的参数设定。

http://bi.dataguru.cn/article-10459-1.html

401、对于维度极低的特征，选择线性还是非线性分类器？

解析：

非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。

1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。

402、请问怎么处理特征向量的缺失值

解析：

一方面，缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。

另一方面缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:

1) 把NaN直接作为一个特征，假设用0表示；

2) 用均值填充；

3) 用随机森林等算法预测填充。

403、SVM、LR、决策树的对比。

解析：

模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝

损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失

数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感

数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核

404、什么是ill-condition病态问题？

解析：

训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。

405、简述KNN最近邻分类算法的过程？

解析：

1. 计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；
2. 对上面所有的距离值进行排序；
3. 选前 k 个最小距离的样本；
4. 根据这 k 个样本的标签进行投票，得到最后的分类类别；

406、常用的聚类划分方式有哪些？列举代表算法。

解析：

1. 基于划分的聚类:K-means，k-medoids，CLARANS。
2. 基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。
3. 基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。
4. 基于网格的方法：STING，WaveCluster。
5. 基于模型的聚类：EM,SOM，COBWEB。

407、什么是偏差与方差？

解析：

泛化误差可以分解成偏差的平方加上方差加上噪声。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。

偏差：

[![1.png](https://ask.julyedu.com/uploads/questions/20180905/5adc10f2706189920970e0e8eefbf049.png)](https://ask.julyedu.com/uploads/questions/20180905/5adc10f2706189920970e0e8eefbf049.png)

方差：

[![2.png](https://ask.julyedu.com/uploads/questions/20180905/fb75f69eebcf0d8a2ac2e9b65d36002d.png)](https://ask.julyedu.com/uploads/questions/20180905/fb75f69eebcf0d8a2ac2e9b65d36002d.png)

408、解决bias和Variance问题的方法是什么？

解析：

High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征

High Variance解决方案：agging、简化模型、降维

具体而言

高偏差, 可以用boosting模型, 对预测残差进行优化, 直接降低了偏差. 也可以用高模型容量的复杂模型(比如非线性模型, 深度神经网络), 更多的特征, 来增加对样本的拟合度.

高方差, 一般使用平均值法, 比如bagging, 或者模型简化/降维方法, 来降低方差.

高偏差和高方差都是不好的, 我们应该加以避免. 但是它们又是此消彼长的关系, 所以必须权衡考虑. 一般情况下, 交叉验证训练可以取得比较好的平衡:

将原始样本均分成K组, 将每组样本分别做一次验证集,其余的K-1组子集数据作为训练集,这样会得到K个模型, 这K个模型可以并发训练以加速. 用这K个模型最终的验证集的分类准确率的平均数作为此K-CV下分类器的性能指标. K一般大于等于3, 而K-CV 的实验共需要建立 k 个models，并计算 k 次 test sets 的平均预测正确率。

在实作上，k 要够大才能使各回合中的 训练样本数够多，一般而言 k=10 (作为一个经验参数)算是相当足够了。

409、采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？

解析：

用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。

410、xgboost怎么给特征评分？

解析：

在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。

```python
# feature importance
print(model.feature_importances_)
plot pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)
pyplot.show() ==========
plot feature importance
plot_importance(model)
pyplot.show()
```

411、什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？

解析：

bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为袋外数据oob（out of bag）,它可以用于取代测试集误差估计方法。

袋外数据(oob)误差的计算方法如下：

对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类,因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O;这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。

412、推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到

解析：

根据贝叶斯公式P(c|d)=（P(c)P(d|c)/P(d)）

这里，分母P(d)不必计算，因为对于每个类都是相等的。 分子中，P(c)是每个类别的先验概率，可以从训练集直接统计，P(d|c)根据独立性假设，可以写成如下 P(d|c)=￥P(wi|c)（￥符号表示对d中每个词i在c类下概率的连乘），P(wi|c)也可以从训练集直接统计得到。 至此，对未知类别的d进行分类时，类别为c=argmaxP(c)￥P(wi|c)。

413、请写出你了解的机器学习特征工程操作，以及它的意义

解析：

特征工程包括数据与特征处理、特征选择和降纬三部分。

数据与特征处理包括：

1.数据选择、清洗、采样

- 数据格式化；
- 数据清洗，填充缺失值、去掉脏数据，将不可信的样本丢掉，缺省值极多的字段考虑不用；
- 采样：针对正负样本不平衡的情况，当正样本远大于负样本时，且量都很大时，使用下采样，量不大时，可采集更多的数据或oversampling或修改损失函数；采样过程中可利用分层抽样保持不同类别数据的比例。

2.不同类型数据的特征处理

- 数值型：幅度调整/归一化、log等变化、统计值（例如max、min、mean、std）、离散化、分桶等
- 类别型：one-hot编码等
- 时间型： 提取出连续值的持续时间和间隔时间；提取出离散值的“年”、“月”、“日”、“一年中哪个星期/季度”、“一周中的星期几”、“工作日/周末”等信息
- 文本型：使用If-idf特征
- 统计型：加减平均、分位线、次序、比例

意义：

- 对数据进行预处理，可提高数据质量，提高挖掘质量。对数据进行清洗可填充缺失值、光滑噪声数据，识别和删除离群点数据，保证数据的一致性；
- 使用正确的采样方法可解决因数据不平衡带来的预测偏差；
- 对不同的数据类型进行不同的特征处理有助于提高特征的可用性，例如对数值型数据进行归一化可将数据转化到统一量纲下；对类别型数据，可用one-hot编码方法将类别数据数字化，数字化特征之后可更用来计算距离、相似性等；可从时间型数据当中提取中更多的时间特征，例如年、月和日等，这些特征对于业务场景以及模型的预测往往有很大的帮助。统计型特征处理有助于从业务场景中挖掘更丰富的信息。

特征选择包括：

1.Filter

使用方差、Pearson相关系数、互信息等方法过滤特征，评估单个特征和结果值之间的相关程度，留下Top相关的特征部分。

2.Wrapper

可利用“递归特征删除算法”，把特征选择看做一个特征子集搜索问题，筛选各种特征子集，用模型评估效果。

3.Embedded

可利用正则化方式选择特征，使用带惩罚项的基模型，除了选择出特征外，同时也进行了降纬。

意义：

-剔除对结果预测不大的特征，减小冗余，选择有意义的特征输入模型，提高计算性能。

降纬：

方法：主成分分析法（PCA）和线性判别分析（LDA）

意义：

通过PCA或LDA方法，将较高纬度样本空间映射到较低维度的样本空间，从而达到降纬的目的，减少模型的训练时间，提高模型的计算性能。

414、请写出你对VC维的理解和认识

解析：

VC维是模型的复杂程度，模型假设空间越大，VC维越高。某种程度上说，VC维给机器学习可学性提供了理论支撑。

1. 测试集合的loss是否和训练集合的loss接近？VC维越小，理论越接近，越不容易overfitting。
2. 训练集合的loss是否足够小？VC维越大，loss理论越小，越不容易underfitting。

我们对模型添加的正则项可以对模型复杂度(VC维)进行控制，平衡这两个部分。

415、怎么理解“机器学习的各种模型与他们各自的损失函数一一对应？”

解析：

寒：首先你要明确 超参数 和 参数 的差别，超参数通常是你为了定义模型，需要提前敲定的东西(比如多项式拟合的最高次数，svm选择的核函数)，参数是你确定了超参数(比如用最高3次的多项式回归)，学习到的参数(比如多项式回归的系数)

另外可以把机器学习视作 表达 + 优化，其中表达的部分，各种模型会有各种不同的形态(线性回归 逻辑回归 SVM 树模型)，但是确定了用某个模型(比如逻辑回归)去解决问题，你需要知道当前模型要达到更好的效果，优化方向在哪，这个时候就要借助损失函数了。

下面就是一个小例子，一样的打分函数，选用不同的loss function会变成不同的模型

[![1.jpeg](https://ask.julyedu.com/uploads/questions/20180906/470931a53938f3d23222fc25d8486cfd.jpeg)](https://ask.julyedu.com/uploads/questions/20180906/470931a53938f3d23222fc25d8486cfd.jpeg)

图取自 

http://cs231n.github.io/linear-classify/

有一个我汉化的版本 

https://blog.csdn.net/han_xiao

 ... 99583

更深入的内容欢迎查阅更多机器学习的资料，或者参与机器学习相关的课程，来讨论

**416、给你一个有1000列和1百万行的训练数据集。这个数据集是基于分类问题的。**

经理要求你来降低该数据集的维度以减少模型计算时间。你的机器内存有限。你会怎么做？（你可以自由做各种实际操作假设）

解析：

答：你的面试官应该非常了解很难在有限的内存上处理高维的数据。以下是你可以使用的处理方法：

1.由于我们的RAM很小，首先要关闭机器上正在运行的其他程序，包括网页浏览器，以确保大部分内存可以使用。

2.我们可以随机采样数据集。这意味着，我们可以创建一个较小的数据集，比如有1000个变量和30万行，然后做计算。

3.为了降低维度，我们可以把数值变量和分类变量分开，同时删掉相关联的变量。对于数值变量，我们将使用相关性分析。对于分类变量，我们可以用卡方检验。

4.另外，我们还可以使用PCA（主成分分析），并挑选可以解释在数据集中有最大偏差的成分。

5.利用在线学习算法，如VowpalWabbit（在Python中可用）是一个可能的选择。

6.利用Stochastic GradientDescent（随机梯度下降）法建立线性模型也很有帮助。

7.我们也可以用我们对业务的理解来估计各预测变量对响应变量的影响大小。但是，这是一个主观的方法，如果没有找出有用的预测变量可能会导致信息的显著丢失。

注意：对于第4和第5点，请务必阅读有关在线学习算法和随机梯度下降法的内容。这些是高阶方法。

**417、问2：在PCA中有必要做旋转变换吗？**

如果有必要，为什么？如果你没有旋转变换那些成分，会发生什么情况？

解析：

答：是的，旋转（正交）是必要的，因为它把由主成分捕获的方差之间的差异最大化。这使得主成分更容易解释。但是不要忘记我们做PCA的目的是选择更少的主成分（与特征变量个数相较而言），那些选上的主成分能够解释数据集中最大方差。

通过做旋转，各主成分的相对位置不发生变化，它只能改变点的实际坐标。如果我们没有旋转主成分，PCA的效果会减弱，那样我们会不得不选择更多个主成分来解释数据集里的方差。

注意：对PCA（主成分分析）需要了解更多。

**418.给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？**

解析：

答：这个问题给了你足够的提示来开始思考！由于数据分布在中位数附近，让我们先假设这是一个正态分布。

我们知道，在一个正态分布中，约有68％的数据位于跟平均数（或众数、中位数）1个标准差范围内的，那样剩下的约32%的数据是不受影响的。

因此，约有32%的数据将不受到缺失值的影响。

**419、给你一个癌症检测的数据集。你已经建好了分类模型，取得了96％的精度。为什么你还是不满意你的模型性能？你可以做些什么呢？**

解析：

答：如果你分析过足够多的数据集，你应该可以判断出来癌症检测结果是不平衡数据。在不平衡数据集中，精度不应该被用来作为衡量模型的标准，因为96％（按给定的）可能只有正确预测多数分类，但我们感兴趣是那些少数分类（4％），是那些被诊断出癌症的人。

因此，为了评价模型的性能，应该用灵敏度（真阳性率），特异性（真阴性率），F值用来确定这个分类器的“聪明”程度。如果在那4%的数据上表现不好，我们可以采取以下步骤：

1.我们可以使用欠采样、过采样或SMOTE让数据平衡。

2.我们可以通过概率验证和利用AUC-ROC曲线找到最佳阀值来调整预测阀值。

3.我们可以给分类分配权重，那样较少的分类获得较大的权重。

4.我们还可以使用异常检测。

注意：要更多地了解不平衡分类

**420、为什么朴素贝叶斯如此“朴素”？**

解析：

答：朴素贝叶斯太‘朴素’了，因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的。

  **421、解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？**

解析：

先验概率就是因变量（二分法）在数据集中的比例。这是在你没有任何进一步的信息的时候，是对分类能做出的最接近的猜测。

例如，在一个数据集中，因变量是二进制的（1和0）。例如，1（垃圾邮件）的比例为70％和0（非垃圾邮件）的为30％。因此，我们可以估算出任何新的电子邮件有70％的概率被归类为垃圾邮件。

似然估计是在其他一些变量的给定的情况下，一个观测值被分类为1的概率。例如，“FREE”这个词在以前的垃圾邮件使用的概率就是似然估计。边际似然估计就是，“FREE”这个词在任何消息中使用的概率

**422、你正在一个时间序列数据集上工作。经理要求你建立一个高精度的模型。你开始用决策树算法，因为你知道它在所有类型数据上的表现都不错。**

后来，你尝试了时间序列回归模型，并得到了比决策树模型更高的精度。

这种情况会发生吗？为什么？

解析：

众所周知，时间序列数据有线性关系。另一方面，决策树算法是已知的检测非线性交互最好的算法。

为什么决策树没能提供好的预测的原因是它不能像回归模型一样做到对线性关系的那么好的映射。

因此，我们知道了如果我们有一个满足线性假设的数据集，一个线性回归模型能提供强大的预测。

**423、给你分配了一个新的项目，是关于帮助食品配送公司节省更多的钱。问题是，公司的送餐队伍没办法准时送餐。结果就是他们的客户很不高兴。**

最后为了使客户高兴，他们只好以免餐费了事。哪个机器学习算法能拯救他们？

解析：

你的大脑里可能已经开始闪现各种机器学习的算法。但是等等！这样的提问方式只是来测试你的机器学习基础。这不是一个机器学习的问题，而是一个路径优化问题。

机器学习问题由三样东西组成：

1.模式已经存在。

2.不能用数学方法解决（指数方程都不行）。

3.有相关的数据。

**424、你意识到你的模型受到低偏差和高方差问题的困扰。应该使用哪种算法来解决问题呢？为什么？**

解析：

低偏差意味着模型的预测值接近实际值。换句话说，该模型有足够的灵活性，以模仿训练数据的分布。貌似很好，但是别忘了，一个灵活的模型没有泛化能力。这意味着，当这个模型用在对一个未曾见过的数据集进行测试的时候，它会令人很失望。

在这种情况下，我们可以使用bagging算法（如随机森林），以解决高方差问题。bagging算法把数据集分成重复随机取样形成的子集。然后，这些样本利用单个学习算法生成一组模型。接着，利用投票（分类）或平均（回归）把模型预测结合在一起。

另外，为了应对大方差，我们可以：

1.使用正则化技术，惩罚更高的模型系数，从而降低了模型的复杂性。

2.使用可变重要性图表中的前n个特征。可以用于当一个算法在数据集中的所有变量里很难寻找到有意义信号的时候。

**425、给你一个数据集。该数据集包含很多变量，你知道其中一些是高度相关的。**

经理要求你用PCA。你会先去掉相关的变量吗？为什么？

解析：

答：你可能会说不，但是这有可能是不对的。丢弃相关变量会对PCA有实质性的影响，因为有相关变量的存在，由特定成分解释的方差被放大。

例如：在一个数据集有3个变量，其中有2个是相关的。如果在该数据集上用PCA，第一主成分的方差会是与其不相关变量的差异的两倍。此外，加入相关的变量使PCA错误地提高那些变量的重要性，这是有误导性的。  

  **426、花了几个小时后，现在你急于建一个高精度的模型。结果，你建了5 个GBM （Gradient Boosted Models），想着boosting算法会显示魔力。**

不幸的是，没有一个模型比基准模型表现得更好。最后，你决定将这些模型结合到一起。

尽管众所周知，结合模型通常精度高，但你就很不幸运。你到底错在哪里？

解析：

答：据我们所知，组合的学习模型是基于合并弱的学习模型来创造一个强大的学习模型的想法。但是，只有当各模型之间没有相关性的时候组合起来后才比较强大。由于我们已经试了5个 GBM，但没有提高精度，表明这些模型是相关的。

具有相关性的模型的问题是，所有的模型提供相同的信息。例如：如果模型1把User1122归类为 1，模型2和模型3很有可能会做有同样分类，即使它的实际值应该是0，因此，只有弱相关的模型结合起来才会表现更好。

**427、KNN和KMEANS聚类（kmeans clustering）有什么不同？**

解析：

答：不要被它们的名字里的“K”误导。

你应该知道，这两种算法之间的根本区别是，KMEANS本质上是无监督学习而KNN是监督学习。KMEANS是聚类算法。KNN是分类（或回归）算法。

KMEAN算法把一个数据集分割成簇，使得形成的簇是同构的，每个簇里的点相互靠近。该算法试图维持这些簇之间有足够的可分离性。由于无监督的性质，这些簇没有任何标签。NN算法尝试基于其k（可以是任何数目）个周围邻居来对未标记的观察进行分类。它也被称为懒惰学习法，因为它涉及最小的模型训练。因此，它不用训练数据对未看见的数据集进行泛化。

**428、真阳性率和召回有什么关系？写出方程式。**

解析：

答：真阳性率=召回。是的，它们有相同的公式（TP / TP + FN）。

注意：要了解更多关于估值矩阵的知识。

**429、你建了一个多元回归模型。你的模型R2为并不如你设想的好。为了改进，你去掉截距项，模型R的平方从0.3变为0.8。 这是否可能？怎样才能达到这个结果？**

解析：

答：是的，这有可能。我们需要了解截距项在回归模型里的意义。截距项显示模型预测没有任何自变量，比如平均预测。公式R² = 1 – ∑(y – y´)²/∑(y – ymean)²中的y´是预测值。

当有截距项时，R²值评估的是你的模型基于均值模型的表现。在没有截距项（ymean）时，当分母很大时，该模型就没有这样的估值效果了，∑(y – y´)²/∑(y – ymean)²式的值会变得比实际的小，而R2会比实际值大。

**430、在分析了你的模型后，经理告诉你，你的模型有多重共线性。**

你会如何验证他说的是真的？在不丢失任何信息的情况下，你还能建立一个更好的模型吗？

解析：

答：要检查多重共线性，我们可以创建一个相关矩阵，用以识别和除去那些具有75％以上相关性（决定阈值是主观的）的变量。此外，我们可以计算VIF（方差膨胀因子）来检查多重共线性的存在。

VIF值<= 4表明没有多重共线性，而值> = 10意味着严重的多重共线性。

此外，我们还可以用容差作为多重共线性的指标。但是，删除相关的变量可能会导致信息的丢失。为了留住这些变量，我们可以使用惩罚回归模型，如Ridge和Lasso回归。

我们还可以在相关变量里添加一些随机噪声，使得变量变得彼此不同。但是，增加噪音可能会影响预测的准确度，因此应谨慎使用这种方法。  



  **441、你会在时间序列数据集上使用什么交叉验证技术？是用k倍或LOOCV？**

解析：

答：都不是。对于时间序列问题，k倍可能会很麻烦，因为第4年或第5年的一些模式有可能跟第3年的不同，而对数据集的重复采样会将分离这些趋势，我们可能最终是对过去几年的验证，这就不对了。

相反，我们可以采用如下所示的5倍正向链接策略：

fold 1 : training [1], test [2]

fold 2 : training [1 2], test [3]

fold 3 : training [1 2 3], test [4]

fold 4 : training [1 2 3 4], test [5]

fold 5 : training [1 2 3 4 5], test [6]

1，2，3，4，5，6代表的是年份。

**442、给你一个缺失值多于30%的数据集？比方说，在50个变量中，有8个变量的缺失值都多于30%。你对此如何处理？**

解析：

答：我们可以用下面的方法来处理：

1.把缺失值分成单独的一类，这些缺失值说不定会包含一些趋势信息。

2.我们可以毫无顾忌地删除它们。

3.或者，我们可以用目标变量来检查它们的分布，如果发现任何模式，我们将保留那些缺失值并给它们一个新的分类，同时删除其他缺失值。

**443、“买了这个的客户，也买了......”亚马逊的建议是哪种算法的结果？**

解析：

答：这种推荐引擎的基本想法来自于协同过滤。

协同过滤算法考虑用于推荐项目的“用户行为”。它们利用的是其他用户的购买行为和针对商品的交易历史记录、评分、选择和购买信息。针对商品的其他用户的行为和偏好用来推荐项目（商品）给新用户。在这种情况下，项目（商品）的特征是未知的。

注意：了解更多关于推荐系统的知识。

**444、你怎么理解第一类和第二类错误？**

解析：

答：第一类错误是当原假设为真时，我们却拒绝了它，也被称为“假阳性”。第二类错误是当原假设为是假时，我们接受了它，也被称为“假阴性”。

在混淆矩阵里，我们可以说，当我们把一个值归为阳性（1）但其实它是阴性（0）时，发生第一类错误。而当我们把一个值归为阴性（0）但其实它是阳性（1）时，发生了第二类错误。

**445、当你在解决一个分类问题时，出于验证的目的，你已经将训练集随机抽样地分成训练集和验证集。你对你的模型能在未看见的数据上有好的表现非常有信心，因为你的验证精度高。但是，在得到很差的精度后，你大失所望。什么地方出了错？**

解析：

答：在做分类问题时，我们应该使用分层抽样而不是随机抽样。随机抽样不考虑目标类别的比例。相反，分层抽样有助于保持目标变量在所得分布样本中的分布。  

446、在应用机器学习算法之前纠正和清理数据的步骤是什么？

解析：

1.将数据导入

2.看数据：重点看元数据，即对字段解释、数据来源等信息；导入数据后，提取部分数据进行查看

3.缺失值清洗

- 根据需要对缺失值进行处理，可以删除数据或填充数据
- 重新取数：如果某些非常重要的字段缺失，需要和负责采集数据的人沟通，是否可以再获得

4.数据格式清洗：统一数据的时间、日期、全半角等显示格式

5.逻辑错误的数据

- 重复的数据
- 不合理的值

6.不一致错误的处理：指对矛盾内容的修正，最常见的如身份证号和出生年月日不对应

不同业务中数据清洗的任务略有不同，比如数据有不同来源的话，数据格式清洗和不一致错误的处理就尤为突出。数据预处理是数据类岗位工作内容中重要的部分。

447、在 K-Means 中如何拾取 k？

解析：

K-Means 算法的最大缺点是不能自动选择分类数 k，常见的确定 k 的方法有：

- 根据先验知识来确定
- k=2N ，N 为样本数
- 拐点法：把聚类结果的 F-test 值对聚类个数的曲线画出来，选择图中的拐点
- 基于信息准则判断，如果模型有似然函数，则可以用 BIC、DIC 来进行决策

具体的 k 的选择往往和业务联系紧密，如希望能将用户进行分类，就有先验的分类要求

448、如何理解模型的过拟合与欠拟合，以及如何解决？

解析：

欠拟合（underfiting / high bias）

训练误差和验证误差都很大，这种情况称为欠拟合。出现欠拟合的原因是模型尚未学习到数据的真实结构。因此，模拟在训练集和验证集上的性能都很差。

解决办法

1 做特征工程，添加跟多的特征项。如果欠拟合是由于特征项不够，没有足够的信息支持模型做判断。

2 增加模型复杂度。如果模型太简单，不能够应对复杂的任务。可以使用更复杂的模型，减小正则化系数。具体来说可以使用核函数，集成学习方法。

3 集成学习方法boosting（如GBDT）能有效解决high bias

过拟合（overfiting / high variance）

模型在训练集上表现很好，但是在验证集上却不能保持准确，也就是模型泛化能力很差。这种情况很可能是模型过拟合。

造成原因主要有以下几种：

1 训练数据集样本单一，样本不足。如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型。

2 训练数据中噪声干扰过大。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。

3 模型过于复杂。模型太复杂，已经能够死记硬背记录下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。

针对过拟合的上述原因，对应的预防和解决办法如下：

1 在训练和建立模型的时候，从相对简单的模型开始，不要一开始就把特征做的非常多，模型参数跳的非常复杂。

2 增加样本，要覆盖全部的数据类型。数据经过清洗之后再进行模型训练，防止噪声数据干扰模型。

3 正则化。在模型算法中添加惩罚函数来防止过拟合。常见的有L1，L2正则化。

4 集成学习方法bagging(如随机森林）能有效防止过拟合

5 减少特征个数(不是太推荐)

注意：降维不能解决过拟合。降维只是减小了特征的维度，并没有减小特征所有的信息。

449、以下哪种方法属于判别式模型(discriminative model)（ ）

A、隐马模型(HMM)

B、朴素贝叶斯

C、LDA

D、支持向量机

正确答案是：D

解析：

已知输入变量x，判别模型(discriminative model)通过求解条件概率分布P(y|x)或者直接计算y的值来预测y。生成模型（generative model）通过对观测值和标注数据计算联合概率分布P(x,y)来达到判定估算y的目的。

常见的判别模型有线性回归（Linear Regression）,逻辑回归（Logistic Regression）,支持向量机（SVM）, 传统神经网络（Traditional Neural Networks）,线性判别分析（Linear Discriminative Analysis），条件随机场（Conditional Random Field）；常见的生成模型有朴素贝叶斯（Naive Bayes）, 隐马尔科夫模型（HMM）,贝叶斯网络（Bayesian Networks）和隐含狄利克雷分布（Latent Dirichlet Allocation）。

A选项的隐马尔科夫模型和 B选项的朴素贝叶斯属于生成模型。C选项的LDA，如果是指Linear Discriminative Analysis，那么属于判别模型，如果是指 Latent Dirichlet Allocation，那么属于生成模型。D选项的支持向量机属于判别模型。

450、以P(w)表示词条w的概率，假设已知P（南京）=0.8，P（市长）=0.6，P（江大桥）=0.4：P（南京市）=0.3，P（长江大桥）=0.5：如果假设前后两个词的出现是独立的，那么分词结果就是（ ）

A、南京市*长江*大桥

B、南京*市长*江大桥

C、南京市长*江大桥

D、南京市*长江大桥

正确答案是： B

解析：

该题考察的是最大概率分词，其基本思想是：一个待切分的汉字串可能包含多种分词结果，将其中概率最大的作为该字串的分词结果。若某候选词在训练语料中未出现，其概率为0。

A分词结果的概率为P(A)=P(南京市)*P(长江)*P(大桥)，由于“长江”未在语料中出现，所以P(长江)=0，从而P(A)=0;

同理可以算出B, C, D分词结果的概率分别是：

P(B)=P(南京)*P(市长)*P(江大桥)=0.8*0.6*0.4=0.192；

P(C)=P(南京市长)*P(江大桥)=0*0.4=0；

P(D)=P(南京市)*P(长江大桥)=0.3*0.5=0.15。

因为P(B)最大，所以为正确的分词结果。

451、基于统计的分词方法为（ ）

A、正向量最大匹配法

B、逆向量最大匹配法

C、最少切分

D、条件随机场

正确答案：D

解析：

中文分词的基本方法可以分为基于语法规则的方法、基于词典的方法和基于统计的方法。

基于语法规则的分词法基本思想是在分词的同时进行句法、语义分析, 利用句法信息和语义信息来进行词性标注, 以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂, 基于语法和规则的分词法所能达到的精确度远远还不能令人满意, 目前这种分词系统应用较少。

在基于词典的方法中，可以进一步分为最大匹配法，最大概率法，最短路径法等。最大匹配法指的是按照一定顺序选取字符串中的若干个字当做一个词，去词典中查找。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分。最大概率法指的是一个待切分的汉字串可能包含多种分词结果，将其中概率最大的那个作为该字串的分词结果。最短路径法指的是在词图上选择一条词数最少的路径。

基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合，相邻的字同时出现的次数越多, 就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。常用的方法有HMM（隐马尔科夫模型），MAXENT（最大熵模型），MEMM（最大熵隐马尔科夫模型），CRF（条件随机场）。

本题中，基于统计的方法为条件随机场。ABC三个选项为基于词典的方法。

452、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）

A、特征灵活

B、速度快

C、可容纳较多上下文信息

D、全局最优

正确答案：B

解析：

HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。

CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。

453、 隐马尔可夫模型（HMM），设其观察值

空间为

[![1.jpg](https://ask.julyedu.com/uploads/questions/20180919/97ea56088eb35052f9dff7b2b0f307c7.jpg)](https://ask.julyedu.com/uploads/questions/20180919/97ea56088eb35052f9dff7b2b0f307c7.jpg)

状态空间为

[![2.jpg](https://ask.julyedu.com/uploads/questions/20180919/aabf3f0c910680c29cac3cfbb8056b78.jpg)](https://ask.julyedu.com/uploads/questions/20180919/aabf3f0c910680c29cac3cfbb8056b78.jpg)

如果用维特比算法(Viterbi algorithm)进行解码，时间复杂度为（ ）

A、O(NK)

B、O(NK^2)

C、O(N^2K)

D、以上都不是

正确答案是：D

解析：

[![3.jpg](https://ask.julyedu.com/uploads/questions/20180919/9bae74a4ca8cf3f207cf8e960ef1167d.jpg)](https://ask.julyedu.com/uploads/questions/20180919/9bae74a4ca8cf3f207cf8e960ef1167d.jpg)

454、在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（ ）（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）

A、Accuracy:(TP+TN)/all

B、F-value:2*recall*precision/(recall+precision)

C、G-mean:sqrt(precision*recall)

D、AUC:ROC曲线下面积

正确答案是：A

解析：

对于分类器，主要的评价指标有precision，recall，F-score，以及ROC曲线等。

在二分类问题中，我们主要关注的是测试集的正样本能否正确分类。当样本不均衡时，比如样本中负样本数量远远多于正样本，此时如果负样本能够全部正确分类，而正样本只能部分正确分类，那么(TP+TN)可以得到很高的值，也就是Accuracy是个较大的值，但是正样本并没有取得良好的分类效果。因此A选项是不合理的。在样本不均衡时，可以采用BCD选项方法来评价。

455、下面关于ID3算法中说法错误的是（ ）

A、ID3算法要求特征必须离散化

B、信息增益可以用熵，而不是GINI系数来计算

C、选取信息增益最大的特征，作为树的根节点

D、ID3算法是一个二叉树模型

正确答案是：D

解析：

ID3算法（IterativeDichotomiser3迭代二叉树3代）是一个由RossQuinlan发明的用于决策树的算法。可以归纳为以下几点：

使用所有没有使用的属性并计算与之相关的样本熵值

选取其中熵值最小的属性

生成包含该属性的节点

D3算法对数据的要求：

1)所有属性必须为离散量；

2)所有的训练例的所有属性必须有一个明确的值；

3)相同的因素必须得到相同的结论且训练例必须唯一。

456、如下表是用户是否使用某产品的调查结果（ ）

 

请计算年龄、地区、学历、收入中对用户是否使用调查产品信息增益最大的属性。（

[![1.png](https://ask.julyedu.com/uploads/questions/20180920/f003549e2d8178e295e9965ba18844a0.png)](https://ask.julyedu.com/uploads/questions/20180920/f003549e2d8178e295e9965ba18844a0.png)

）

[![2.png](https://ask.julyedu.com/uploads/questions/20180920/6eb9f86d115b168fb1d443af93c96ee8.png)](https://ask.julyedu.com/uploads/questions/20180920/6eb9f86d115b168fb1d443af93c96ee8.png)

A、年龄

B、地区

C、学历

D、收入

正确答案是：C

解析：

信息增益最大，也就是分类以后信息最少，熵最小。没有划分时，原始数据熵为

[![3.png](https://ask.julyedu.com/uploads/questions/20180920/e5fd4a6b1dd2d635269015c705af6d1f.png)](https://ask.julyedu.com/uploads/questions/20180920/e5fd4a6b1dd2d635269015c705af6d1f.png)

，如果按照年龄进行划分，划分后的熵为

[![4.png](https://ask.julyedu.com/uploads/questions/20180920/20ab0ceb6c65f8c321e60a966ad41038.png)](https://ask.julyedu.com/uploads/questions/20180920/20ab0ceb6c65f8c321e60a966ad41038.png)

，分别按照熵的方法计算出划分以后的熵值，可以发现按照学历划分以后，熵为0，其他选项都大于0。因此，信息增益最大的属性是学历。

如果不进行计算，可以由观察得出，按照学历划分以后，所有的用户都能正确分类，此时熵最小，信息增益最大。如果按照其他属性分类，都出现了错分的情况，对应的熵大于0。

457、在其它条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（ ）

A、增加训练集数量

B、减少神经网络隐藏层节点数

C、删除稀疏的特征

D、SVM算法中使用高斯核/RBF核代替

正确答案是：D

解析：

机器学习中发生过拟合的主要原因有：

（1）使用过于复杂的模型；

（2）数据噪声较大；

（3）训练数据少。

由此对应的降低过拟合的方法有：

（1）简化模型假设，或者使用惩罚项限制模型复杂度；

（2）进行数据清洗，减少噪声；

（3）收集更多训练数据。

本题中，A对应于增加训练数据，B为简化模型假设，C为数据清洗。D选项中，高斯核的使用增加了模型复杂度，容易引起过拟合。选择合适的核函数以及软边缘参数C就是训练SVM的重要因素。一般来讲，核函数越复杂，模型越偏向于过拟合；C越大模型越偏向于过拟合，反之则拟合不足。

458、如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（ ）

A、无偏的，有效的

B、无偏的，非有效的

C、有偏的，有效的

D、有偏的，非有效的

正确答案是：B

解析：

OLS即普通最小二乘法。由高斯—马尔可夫定理，在给定经典线性回归的假定下，最小二乘估计量是具有最小方差的线性无偏估计量。根据证明过程可知，随机误差中存在异方差性不会影响其无偏性，而有效性证明中涉及同方差性，即异方差会影响参数OLS估计量的有效性。

459、一个二进制源X发出符号集为{-1,1}，经过离散无记忆信道传输，由于信道中噪音的存在，接收端Y收到符号集为{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求条件熵H(Y|X)（ ）

A、0.2375

B、0.3275

C、0.5273

D、0.5372

正确答案是：A

解析：

由H(Y|X)= -∑P(X,Y)logP(Y|X)= -∑P(Y|X)P(X)logP(Y|X)，将(y=-1,x=-1), (y=0,x=-1), (y=1,x=1), (y=0,x=1)四种情况带入公式求和，得到H(Y|X)≈-(-0.01938-0.03495-0.07028-0.11289)=0.2375。

460、Fisher线性判别函数的求解过程是将M维特征矢量投影在（ ）中进行求解。

A、M-1维空间

B、一维空间

C、三维空间

D、二维空间

正确答案是： B

解析：

Fisher线性判别函数是将多维空间中的特征矢量投影到一条直线上，也就是把维数压缩到一维。寻找这条最优直线的准则是Fisher准则：两类样本在一维空间的投影满足类内尽可能密集，类间尽可能分开，也就是投影后两类样本均值之差尽可能大，类内部方差尽可能小。一般而言，对于数据分布近似高斯分布的情况，Fisher线性判别准则能够得到很好的分类效果。

  **461、类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是（ ）**

A、势函数法

B、基于二次准则的H-K算法

C、伪逆法

D、感知器算法

正确答案是：D

解析：

线性分类器的设计就是利用训练样本集建立线性判别函数式，也就是寻找最优的权向量的过程。求解权重的过程就是训练过程，训练方法的共同点是，先给出准则函数，再寻找是准则函数趋于极值的优化方法。ABC方法都可以得到线性不可分情况下分类问题近似解。感知器可以解决线性可分的问题，但当样本线性不可分时，感知器算法不会收敛。

**462、下列哪个不属于CRF模型对于HMM和MEMM模型的优势**

A、特征灵活

B、速度快

C、可容纳较多上下文信息

D、全局最优

正确答案是： B

解析：

HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。

CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。

**463、Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）**

A、各类别的先验概率P(C)是相等的

B、以0为均值，sqr(2)/2为标准差的正态分布

C、特征变量X的各个维度是类别条件独立随机变量

D、P(X|C)是高斯分布

正确答案是：C

解析：

朴素贝叶斯的基本假设就是每个变量相互独立。

**464、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计（）**

A、EM算法

B、维特比算法

C、前向后向算法

D、极大似然估计

正确答案是：D

解析：

EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法

维特比算法： 用动态规划解决HMM的预测问题，不是参数估计

前向后向算法：用来算概率

极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数

注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。

题目解析参考自：@BlackEyes_SGC

**465、假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中不正确的是？**

A、模型效果相比无重复特征的情况下精确度会降低

B、如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样

C、当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题

正确答案是： B

解析：

朴素贝叶斯的条件就是每个变量相互独立。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。

此外，若高度相关的特征在模型中引入两次, 这样增加了这一特征的重要性, 则它的性能因数据包含高度相关的特征而下降。正确做法是评估特征的相关矩阵，并移除那些高度相关的特征。  

  **466、以下哪些方法不可以直接来对文本分类？**

A、Kmeans

B、决策树

C、支持向量机

D、KNN

正确答案是：A

解析：

Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。

**467、已知一组数据的协方差矩阵P,下面关于主分量说法错误的是（）**

A、主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小

B、在经主分量分解后,协方差矩阵成为对角矩阵

C、主分量分析就是K-L变换

D、主分量是通过求协方差矩阵的特征值得到

正确答案是：C

解析：

K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。

解析参考自：@BlackEyes_SGC

**468、关于logit 回归和SVM 不正确的是（ ）**

A、Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。

B、Logit回归的输出就是样本属于正类别的几率，可以计算出概率。

C、SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。

D、SVM可以通过正则化系数控制模型的复杂度，避免过拟合。

正确答案是：A

解析：

Logit回归目标函数是最小化后验概率，Logit回归可以用于预测事件发生概率的大小，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。

**469、以下不属于影响聚类算法结果的主要因素有（）**

A、已知类别的样本质量

B、分类准则

C、特征选取

D、模式相似性测度

正确答案是：A

解析：

都已知了，就不必再进行聚类了。

**470、模式识别中，不属于马式距离较之于欧式距离的优点的是（ ）**

A、平移不变性

B、尺度不变性

C、考虑了模式的分布

正确答案是：A  

  **471、影响基本K-均值算法的主要因素有（）**

A、样本输入顺序

B、模式相似性测度

C、聚类准则

正确答案是： B

**472、在统计模式分类问题中，当先验概率未知时，可以使用（）**

A、最小损失准则

B、最小最大损失准则

C、最小误判概率准则

正确答案是： B

**473、如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（ ）**

A、已知类别样本质量

B、分类准则

C、量纲

正确答案是： B

**474、以下属于欧式距离特性的有（）**

A、旋转不变性

B、尺度缩放不变性

C、不受量纲影响的特性

正确答案是：A

**475、以下( )不属于线性分类器最佳准则？**

A、感知准则函数

B、贝叶斯分类

C、支持向量机

D、Fisher准则

正确答案是： B

解析：

线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。

支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）

Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。

根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>  

  **476、一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：**

A、二分类问题

B、多分类问题

C、层次聚类问题

D、k-中心点聚类问题

E、回归问题

F、结构分析问题

正确答案是： B

解析：

二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。

层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其 他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。

K-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。

回归分析：处理变量之间具有相关性的一种统计方法，这里的狱警、小偷、送餐员、其他之间并没有什 么直接关系。

结构分析： 结构分析法是在统计分组的基础上，计算各组成部分所占比重，进而分析某一总体现象的内部结构特征、总体的性质、总体内部结构依时间推移而表现出的变化规律性的统计方法。结构分析法的基本表现形式，就是计算结构指标。这里也行不通。

多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>

**477、关于 logit 回归和 SVM 不正确的是（）**

A、Logit回归目标函数是最小化后验概率

B、Logit回归可以用于预测事件发生概率的大小

C、SVM目标是结构风险最小化

D、SVM可以有效避免模型过拟合

正确答案是：A

解析：

A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误

B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确

C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。

D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>

**478、有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是( )**

A、2x+y=4

B、x+2y=5

C、x+2y=3

D、2x-y=0

正确答案是：C

解析：

这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。斜率是两点连线的斜率的负倒数-1/((-1-3)/(0-2)) = -1/2, 可得y=-(1/2)x + c, 过中点((0+2)/2, (-1+3)/2) = (1, 1), 可得c=3/2, 故选C.

**479、下面有关分类算法的准确率，召回率，F1 值的描述，错误的是？**

A、准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率

B、召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率

C、正确率、召回率和 F 值取值都在0和1之间，数值越接近0，查准率或查全率就越高

D、为了解决准确率和召回率冲突问题，引入了F1分数

正确答案是：C

解析：

对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：

TP——将正类预测为正类数

FN——将正类预测为负类数

FP——将负类预测为正类数

TN——将负类预测为负类数

由此：

精准率定义为：P = TP / (TP + FP)

召回率定义为：R = TP / (TP + FN)

F1值定义为： F1 = 2 P R / (P + R)

精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。

**480、以下几种模型方法属于判别式模型(Discriminative Model)的有( )**

1)混合高斯模型

2)条件随机场模型

3)区分度训练

4)隐马尔科夫模型

A、2,3

B、3,4

C、1,4

D、1,2

正确答案是：A

解析：

常见的判别式模型有：

Logistic regression（logistical 回归）

Linear discriminant analysis（线性判别分析）

Supportvector machines（支持向量机）

Boosting（集成学习）

Conditional random fields（条件随机场）

Linear regression（线性回归）

Neural networks（神经网络）

常见的生成式模型有:

Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）

Hidden Markov model（隐马尔可夫）

NaiveBayes（朴素贝叶斯）

AODE（平均单依赖估计）

Latent Dirichlet allocation（LDA主题模型）

Restricted Boltzmann Machine（限制波兹曼机）

生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>  

  **481、Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）**

A、各类别的先验概率P(C)是相等的

B、以0为均值，sqr(2)/2为标准差的正态分布

C、特征变量X的各个维度是类别条件独立随机变量

D、P(X|C)是高斯分布

正确答案是：C

解析：

朴素贝叶斯的条件就是每个变量相互独立。

来源@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>

**482、关于支持向量机SVM,下列说法错误的是（）**

A、L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力

B、Hinge 损失函数，作用是最小化经验分类错误

C、分类间隔为1/||w||，||w||代表向量的模

D、当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习

正确答案是：C

解析：

A正确。考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。

B正确。

C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。

D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和ai∗yi∗xi，a变小使得w变小，因此间隔2/||w||变大

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>

**483、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计()**

A、EM算法

B、维特比算法

C、前向后向算法

D、极大似然估计

正确答案是：D

解析：

EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法

维特比算法： 用动态规划解决HMM的预测问题，不是参数估计

前向后向算法：用来算概率

极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数

注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>

**484、在Logistic Regression 中,如果同时加入L1和L2范数,不会产生什么效果（）**

A、以做特征选择,并在一定程度上防止过拟合

B、能解决维度灾难问题

C、能加快计算速度

D、可以获得更准确的结果

正确答案是：D

解析：

Ｌ１范数具有系数解的特性，但是要注意的是，Ｌ１没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。所以能加快计算速度和缓解维数灾难.

在代价函数后面加上正则项，Ｌ１即是Ｌｏｓｓｏ回归，Ｌ２是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。

对于机器学习中的范数规则化，也就是L0,L1,L2范数的详细解答，请参阅《范数规则化》（链接：[http://blog.csdn.net/zouxy09/a ... 1995/](http://blog.csdn.net/zouxy09/article/details/24971995/)）。

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>

**485、机器学习中L1正则化和L2正则化的区别是？**

A、使用L1可以得到稀疏的权值

B、使用L1可以得到平滑的权值

C、使用L2可以得到稀疏的权值

正确答案是：A

解析：

L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0.

L2主要功能是为了防止过拟合，当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。

L1正则化/Lasso

L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。

L2正则化/Ridge regression

L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。

对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。

可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。

因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>  

**486、位势函数法的积累势函数K(x)的作用相当于Bayes判决中的（）**

A、后验概率

B、先验概率

C、类概率密度

D、类概率密度与先验概率的和

正确答案是：A

解析：

具体的，势函数详解请看——《势函数法》。

来源：@刘炫320，链接：<http://blog.csdn.net/column/details/16442.html>

**487、隐马尔可夫模型三个基本问题以及相应的算法说法错误的是（ ）**

A、评估—前向后向算法

B、解码—维特比算法

C、学习—Baum-Welch算法

D、学习—前向后向算法

正确答案是：D

解析：

评估问题，可以使用前向算法、后向算法、前向后向算法。

**488、在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题？**

A、增加训练集量

B、减少神经网络隐藏层节点数

C、删除稀疏的特征

D、SVM算法中使用高斯核/RBF核代替线性核

正确答案是：D

解析：

一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。

B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合

D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。

本题题目及解析来源：@刘炫320

链接：<http://blog.csdn.net/column/details/16442.html>

**489、下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。**

A、AR模型

B、MA模型

C、ARMA模型

D、GARCH模型

正确答案是：D

解析：

AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。

MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。

ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。

GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。

本题题目及解析来源：@刘炫320

链接：<http://blog.csdn.net/column/details/16442.html>

**490、以下说法中错误的是（）**

A、SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性

B、在adaboost算法中，所有被分错样本的权重更新比例不相同

C、boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重

D、给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的

正确答案是：C

解析：

A 软间隔分类器对噪声是有鲁棒性的。

B 请参考[http://blog.csdn.net/v_july_v/ ... 18799](http://blog.csdn.net/v_july_v/article/details/40718799)

C boosting是根据分类器正确率确定权重，bagging不是。

D 训练集变大会提高模型鲁棒性。

491、你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？

[![1.png](https://ask.julyedu.com/uploads/questions/20180930/48f9821ca12ae0afbea719634b363969.png)](https://ask.julyedu.com/uploads/questions/20180930/48f9821ca12ae0afbea719634b363969.png)

A、第一个 w2 成了 0，接着 w1 也成了 0

B、第一个 w1 成了 0，接着 w2 也成了 0

C、w1 和 w2 同时成了 0

D、即使在 C 成为大值之后，w1 和 w2 都不能成 0

正确答案是：C

解析：

答案是C。L1正则化的函数如下图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。

492、在 k-均值算法中，以下哪个选项可用于获得全局最小？

A、尝试为不同的质心（centroid）初始化运行算法

B、调整迭代的次数

C、找到集群的最佳数量

D、以上所有

正确答案是：D

解析：

答案（D）：所有都可以用来调试以找到全局最小。

493、假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。

A、如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它

B、对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大

C、log-loss 越低，模型越好

D、以上都是

正确答案是：D

494、下面哪个选项中哪一项属于确定性算法？

A、PCA

B、K-Means

C、以上都不是

正确答案是：A

解析：

答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。

495、两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？

A、正确

B、错误

正确答案是：A

解析：

答案为（A）：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。

496、下面哪个/些超参数的增加可能会造成随机森林数据过拟合？

A、树的数量

B、树的深度 

C、学习速率

正确答案是： B

解析：

答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。

497、下列哪个不属于常用的文本分类的特征选择算法？

A、卡方检验值

B、互信息

C、信息增益

D、主成分分析

正确答案是：D

解析：

常采用特征选择方法。常见的六种特征选择方法： 1）DF(Document Frequency) 文档频率 DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性 2）MI(Mutual Information) 互信息法 互信息法用于衡量特征词与文档类别直接的信息量。 如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。 相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。 3）(Information Gain) 信息增益法 通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。 4）CHI(Chi-square) 卡方检验法 利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的 如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。 5）WLLR(Weighted Log Likelihood Ration)加权对数似然 6）WFO（Weighted Frequency and Odds）加权频率和可能性 本题解析来源：

http://blog.csdn.net/ztf312/ar ... 90099

498、机器学习中做特征选择时，可能用到的方法有？

A、卡方

B、信息增益

C、平均互信息

D、期望交叉熵

E、以上都有

正确答案是：E

499、下列方法中，不可以用于特征降维的方法包括

A、主成分分析PCA

B、线性判别分析LDA

C、深度学习SparseAutoEncoder

D、矩阵奇异值分解SVD

正确答案是：C

解析：

特征降维方法主要有： PCA，LLE，Isomap SVD和PCA类似，也可以看成一种降维方法 LDA:线性判别分析，可用于降维 AutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出 L2组成，中间则是权重连接。Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别 进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。 Autoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse 惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。 结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。 

[![2.png](https://ask.julyedu.com/uploads/questions/20181009/f15288ef6e9280e0e031f8617b9272fb.png)](https://ask.julyedu.com/uploads/questions/20181009/f15288ef6e9280e0e031f8617b9272fb.png)

500、下列哪些不特别适合用来对高维数据进行降维

A、LASSO

B、主成分分析法

C、聚类分析

D、小波分析法

E、线性判别法

F、拉普拉斯特征映射

正确答案是：C

解析：

lasso通过参数缩减达到降维的目的； pca就不用说了 线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维； 小波分析有一些变换的操作降低其他干扰可以看做是降维 拉普拉斯请看这个

http://f.dataguru.cn/thread-287243-1-1.html

  **501、下列属于无监督学习的是**

A、k-means

B、SVM

C、最大熵

D、CRF

正确答案是：A

解析：

正确答案：A

A是聚类，属于无监督学习。BC是分类，属于监督学习。至于D是序列化标注，也是有监督学习。

**502、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）**

A、特征灵活

B、速度快

C、可容纳较多上下文信息

D、全局最优

正确答案是： B

解析：

CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢

CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较

同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较

CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较

**503、以下哪个是常见的时间序列算法模型**

A、RSI

B、MACD

C、ARMA

D、KDJ

正确答案是：C

解析：

自回归滑动平均模型(ARMA)

其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。

其他三项都不是一个层次的。

A.相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .

B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .

D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 .

**504、下列不是SVM核函数的是**

A、多项式核函数

B、logistic核函数

C、径向基核函数

D、Sigmoid核函数

正确答案是： B

解析：

@刘炫320，本题题目及解析来源：<http://blog.csdn.net/column/details/16442.html>

SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。

核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：

(1)线性核函数

K ( x , x i ) = x ⋅ x i

(2)多项式核

K ( x , x i ) = ( ( x ⋅ x i ) 1 ) d

(3)径向基核（RBF）

K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 )

Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。

(4)傅里叶核

K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) q 2 )

(5)样条核

K ( x , x i ) = B 2 n 1 ( x − x i )

(6)Sigmoid核函数

K ( x , x i ) = tanh ( κ ( x , x i ) − δ )

采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。

核函数的选择

在选取核函数解决实际问题时，通常采用的方法有：

一是利用专家的先验知识预先选定核函数；

二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．

三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．

**505、解决隐马模型中预测问题的算法是**

A、前向算法

B、后向算法

C、Baum-Welch算法

D、维特比算法

正确答案是：D

解析：

@刘炫320，本题题目及解析来源：<http://blog.csdn.net/column/details/16442.html>

A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。

C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；

D：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。  

506、一般，k-NN最近邻方法在（）的情况下效果较好

A、样本较多但典型性不好

B、样本较少但典型性好

C、样本呈团状分布

D、样本呈链状分布

正确答案是： B

解析：

K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B

样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。

507、在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）

A、作正态分布概率图

B、作盒形图

C、马氏距离

D、作散点图

正确答案是：C

解析：

马氏距离是基于卡方分布的，度量多元outlier离群点的统计方法。

有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为：

[![1.png](https://ask.julyedu.com/uploads/questions/20181011/9c0e64ed9d221551ec424ae71cf81a9e.png)](https://ask.julyedu.com/uploads/questions/20181011/9c0e64ed9d221551ec424ae71cf81a9e.png)

（协方差矩阵中每个元素是各个矢量元素之间的协方差Cov(X,Y)，Cov(X,Y) = E{ [X-E(X)] [Y-E(Y)]}，其中E为数学期望）

而其中向量Xi与Xj之间的马氏距离定义为：

[![2.png](https://ask.julyedu.com/uploads/questions/20181011/80d223ecff79844882f42c01386a2f2b.png)](https://ask.julyedu.com/uploads/questions/20181011/80d223ecff79844882f42c01386a2f2b.png)

若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：

[![3.png](https://ask.julyedu.com/uploads/questions/20181011/3b541c42a2911d3a7af4375856b4203b.png)](https://ask.julyedu.com/uploads/questions/20181011/3b541c42a2911d3a7af4375856b4203b.png)

也就是欧氏距离了。　　

若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。

(2)马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。

508、对数几率回归（logistics regression）和一般回归分析有什么区别？

A、对数几率回归是设计用来预测事件可能性的

B、对数几率回归可以用来度量模型拟合程度

C、对数几率回归可以用来估计回归系数

D、以上所有

正确答案是：D

解析：

A: 对数几率回归其实是设计用来解决分类问题的

B: 对数几率回归可以用来检验模型对数据的拟合度

C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。

509、bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）

A、有放回地从总共M个特征中抽样m个特征

B、无放回地从总共M个特征中抽样m个特征

C、有放回地从总共N个样本中抽样n个样本

D、无放回地从总共N个样本中抽样n个样本

正确答案是：C

解析：

boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例.

510、“过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）

A、对的

B、错的

正确答案是： B

解析：

我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）

https://en.wikipedia.org/wiki/ ... index

511、对于k折交叉验证, 以下对k的说法正确的是（）

A、k越大, 不一定越好, 选择大的k会加大评估时间

B、选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)

C、在选择k时, 要最小化数据集之间的方差

D、以上所有

正确答案是：D

解析：

k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.

如果不明白bias和variance的概念, 务必参考下面链接:

Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning

http://machinelearningmastery. ... ning/

Understanding the Bias-Variance Tradeoff

http://scott.fortmann-roe.com/ ... .html

512、回归模型中存在多重共线性, 你如何解决这个问题？

1 去除这两个共线性变量

2 我们可以先去除一个共线性变量

3 计算VIF(方差膨胀因子), 采取相应措施

4 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归

A、1

B、2

C、2和3

D、2, 3和4

正确答案是：D

解析：

解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高.

我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。

513、模型的高bias是什么意思, 我们如何降低它 ？

A、在特征空间中减少特征

B、在特征空间中增加特征

C、增加数据点

D、B和C

E、以上所有

正确答案是： B

解析：

bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !

514、训练决策树模型, 属性节点的分裂, 具有最大信息增益的图是下图的哪一个（）

[![1.png](https://ask.julyedu.com/uploads/questions/20181015/9f958aa63206f3dda31159df92e1dda6.png)](https://ask.julyedu.com/uploads/questions/20181015/9f958aa63206f3dda31159df92e1dda6.png)

A、Outlook

B、Humidity

C、Windy

D、Temperature

正确答案是：A

解析：

A信息增益, 增加平均子集纯度, 详细研究, 请戳看相关论文:

A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)

Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio

515、对于信息增益, 决策树分裂节点, 下面说法正确的是（）

1 纯度高的节点需要更多的信息去区分

2 信息增益可以用”1比特-熵”获得

3 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的

A、1

B、2

C、2和3

D、所有以上

正确答案是：C

解析：

C

详细研究, 请看相关论文:

A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)

Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio

516、下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是

[![2.png](https://ask.julyedu.com/uploads/questions/20181015/39ff55ab7a432b520312f89675bc37bc.png)](https://ask.julyedu.com/uploads/questions/20181015/39ff55ab7a432b520312f89675bc37bc.png)

A、g1 > g2 > g3

B、g1 = g2 = g3

C、g1 < g2 < g3

D、g1 >= g2 >= g3E. g1 <= g2 <= g3

正确答案是：C

解析：

所谓径向基函数 (Radial Basis Function 简称 RBF), 就是某种沿径向对称的标量函数。 通常定义为空间中任一点x到某一中心点xc之间欧氏距离的单调函数 , 可记作 k(||x-xc||), 其作用往往是局部的 , 即当x远离xc时函数取值很小。最常用的径向基函数是高斯核函数 ,形式为 k(||x-xc||)=exp{- ||x-xc||^2/(2*σ^2) } 其中xc为核函数中心,σ为函数的宽度参数 , 控制了函数的径向作用范围。由radial basis: exp(-gamma*|u-v|^2)可知, gamma越小, 模型越简单, 平滑度越好, 分类边界越不容易过拟合, 所以选C。

517、假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 :

1 模型分类的召回率会降低或不变

2 模型分类的召回率会升高

3 模型分类准确率会升高或不变

4 模型分类准确率会降低

A、1

B、2

C、1和3

D、2和4

E、以上都不是

正确答案是：A

解析：

精确率, 准确率和召回率是广泛用于信息检索和统计学分类领域的度量值，用来评价结果的质量。下图可以帮助理解和记忆它们之间的关系, 其中精确率(precision)和准确率(accuracy)都是关于预测效果的描述. 召回率是关于预测样本的描述。

精确率表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP), 也就是P = TP / (TP + FP)。

准确率表示的是预测的正负样本有多少是真实的正和负, 预测正确的数量占全部预测数量的比例, 也就是A = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / 全部样本。

召回率表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN), 也就是R = TP / (TP + FN)。

精确率和召回率二者计算方法其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。

提高分界阈值大于0.5, 则预测为正的样本数要降低, 相当于把图中圆圈变小, 按下图则可计算

[![3.png](https://ask.julyedu.com/uploads/questions/20181015/6d6dac93410419965f52a58f2a82e0f4.png)](https://ask.julyedu.com/uploads/questions/20181015/6d6dac93410419965f52a58f2a82e0f4.png)

召回率的分子变小分母不变, 所以召回率会变小或不变;

精确率的分子分母同步变化, 所以精确率的变化不能确定;

准确率的分子为圆内绿色加圆外右侧矩形面积所围样本, 两者之和变化不能确定; 分母为矩形所含全部样本不变化, 所以准确率的变化不能确定;

综上, 所以选A。

518、“点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是

A、模型预测准确率已经很高了, 我们不需要做什么了

B、模型预测准确率不高, 我们需要做点什么改进模型

C、无法下结论

D、以上都不对

正确答案是：C

解析：

如寒老师所说，类别不均衡的情况下，不要用准确率做分类评估指标，因为全判断为不会点，准确率也是99%，但是这个分类器一点用都没有。

详细可以参考这篇文章：

https://www.analyticsvidhya.co ... lems/

519、使用k=1的knn算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少：

[![4.png](https://ask.julyedu.com/uploads/questions/20181015/68097672a34966c6e23e14dd45b3b7f4.png)](https://ask.julyedu.com/uploads/questions/20181015/68097672a34966c6e23e14dd45b3b7f4.png)

A、0%

B、100%

C、0%到100

D、以上都不是

正确答案是： B

解析：

knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100%。

520、我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以

A、增加树的深度

B、增加学习率 (learning rate)

C、减少树的深度

D、减少树的数量

正确答案是：C

解析：

增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.

决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)

决策树只有一棵树, 不是随机森林。

521、假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？

A、设C=1

B、设C=0

C、设C=无穷大

D、以上都不对

正确答案是：C

解析：

答案: C

C无穷大保证了所有的线性不可分都是可以忍受的

522、以下哪些算法, 可以用神经网络去构造:

1. KNN
2. 线性回归
3. 对数几率回归

A、1和 2

B、2 和 3

C、1, 2 和 3

D、以上都不是

正确答案是： B

解析：

答案: B

1. KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙
2. 最简单的神经网络, 感知器, 其实就是线性回归的训练
3. 我们可以用一层的神经网络构造对数几率回归

523、请选择下面可以应用隐马尔科夫(HMM)模型的选项

A、基因序列数据集

B、电影浏览数据集

C、股票市场数据集

D、所有以上

正确答案是：D

解析：

答案: D

只要是和时间序列问题有关的 , 都可以试试HMM.

524、我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 :

A、我们随机抽取一些样本, 在这些少量样本之上训练

B、我们可以试用在线机器学习算法

C、我们应用PCA算法降维, 减少特征数

D、B 和 C

E、A 和 B

F、以上所有

正确答案是：F

解析：

样本数过多, 或者特征数过多, 而不能单机完成训练, 可以用小批量样本训练, 或者在线累计式训练, 或者主成分PCA降维方式减少特征数量再进行训练.

525、我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 :

1. 使用前向特征选择方法
2. 使用后向特征排除方法
3. 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征.
4. 查看相关性表, 去除相关性最高的一些特征

A、1 和 2

B、2, 3和4

C、1, 2和4

D、All

正确答案是：D

解析：

答案: D

1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法

2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法.

3.用相关性的度量去删除多余特征, 也是一个好方法

所以D是正确的

526、对于随机森林和GradientBoosting Trees, 下面说法正确的是:

1 在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的

2 这两个模型都使用随机特征子集, 来生成许多单个的树

3 我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的

4 GradientBoosting Trees训练模型的表现总是比随机森林好

A、2

B、1 and 2

C、1, 3 and 4

D、2 and 4

正确答案是：A

解析：

答案: A

1 随机森林是基于bagging的, 在随机森林的单个树中, 树和树之间是没有依赖的。

2 Gradient Boosting trees是基于boosting的，且GradientBoosting Trees中的单个树之间是有依赖关系。

3 这两个模型都使用随机特征子集, 来生成许多单个的树。

所以题干中只有第二点是正确的，选A。

更多详情请参见《通俗理解kaggle比赛大杀器xgboost》：

https://blog.csdn.net/v_JULY_v ... 10574

，循序渐进，先后理解：决策树、CBDT、xgboost。

527、对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :

A、正确的

B、错误的

正确答案是： B

解析：

答案: B.

这个说法是错误的, 首先, “不依赖”和”不相关”是两回事, 其次, 转化过的特征, 也可能是相关的

528、对于PCA说法正确的是 :

1. 我们必须在使用PCA前规范化数据
2. 我们应该选择使得模型有最大variance的主成分
3. 我们应该选择使得模型有最小variance的主成分
4. 我们可以使用PCA在低维度上做数据可视化

A、1, 2 and 4

B、2 and 4

C、3 and 4

D、1 and 3

E、1, 3 and 4

正确答案是：A

解析：

答案: A

1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).

2）我们总是应该选择使得模型有最大variance的主成分

3）有时在低维度上左图是需要PCA的降维帮助的

529、对于下图, 最好的主成分选择是多少 ?

[![1.png](https://ask.julyedu.com/uploads/questions/20181017/f721fabecf354d8d148aef5f50e7abcf.png)](https://ask.julyedu.com/uploads/questions/20181017/f721fabecf354d8d148aef5f50e7abcf.png)

A、7

B、30

C、35

D、Can’t Say

正确答案是： B

解析：

答案: B

主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。

530、数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是

A、单个模型之间有高相关性

B、单个模型之间有低相关性

C、在集成学习中使用“平均权重”而不是“投票”会比较好

D、单个模型都是用的一个算法

正确答案是： B

解析：

答案: B

详细请参考下面文章:

Basics of Ensemble Learning Explained in Simple English

Kaggle Ensemble Guide

5 Easy questions on Ensemble Modeling everyone should know

531、在有监督学习中， 我们如何使用聚类方法？

1. 我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习
2. 我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习
3. 在进行监督学习之前， 我们不能新建聚类类别
4. 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习

A、2 和 4

B、1 和 2

C、3 和 4

D、 1 和 3

正确答案是： B

解析：

答案: B

我们可以为每个聚类构建不同的模型， 提高预测准确率。

“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。

所以B是正确的

532、以下说法正确的是

1. 一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的
2. 如果增加模型复杂度， 那么模型的测试错误率总是会降低
3. 如果增加模型复杂度， 那么模型的训练错误率总是会降低
4. 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习

A、1

B、2

C、3

D、2和3

E、都错

正确答案是：E

解析：

答案:E

1的模型中, 如果负样本占比非常大,也会有很高的准确率, 对正样本的分类不一定很好;

4的模型中, “类别id”可以作为一个特征项去训练, 这样会有效地总结了数据特征。

533、对应GradientBoosting tree算法， 以下说法正确的是:

1. 当增加最小样本分裂个数，我们可以抵制过拟合
2. 当增加最小样本分裂个数，会导致过拟合
3. 当我们减少训练单个学习器的样本个数，我们可以降低variance
4. 当我们减少训练单个学习器的样本个数，我们可以降低bias

A、2 和 4

B、2 和 3

C、1 和 3

D、1 和 4

正确答案是：C

解析：

答案: C

最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。

第二点是靠bias和variance概念的。

534、以下哪个图是KNN算法的训练边界

[![2.jpeg](https://ask.julyedu.com/uploads/questions/20181018/a385b6b6add74820b873df51a02df923.jpeg)](https://ask.julyedu.com/uploads/questions/20181018/a385b6b6add74820b873df51a02df923.jpeg)

A、B

B、A

C、D

D、C

E、都不是

正确答案是： B

解析：

答案: B

KNN算法肯定不是线性的边界， 所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。

535、如果一个训练好的模型在测试集上有100%的准确率， 这是不是意味着在一个新的数据集上，也会有同样好的表现？

A、是的，这说明这个模型的范化能力已经足以支持新的数据集合了

B、不对，依然后其他因素模型没有考虑到，比如噪音数据

正确答案是： B

解析：

答案: B

没有一个模型是可以总是适应新数据的。我们不可能可到100%准确率。

536、下面的交叉验证方法

i. 有放回的Bootstrap方法

ii. 留一个测试样本的交叉验证

iii. 5折交叉验证

iv. 重复两次的5折教程验证

当样本是1000时，下面执行时间的顺序，正确的是

A、i > ii > iii > iv

B、ii > iv > iii > i

C、iv > i > ii > iii

D、ii > iii > iv > i

正确答案是： B

解析：

答案: B

Boostrap方法是传统地随机抽样，验证一次的验证方法，只需要训练1次模型，所以时间最少。

留一个测试样本的交叉验证，需要n次训练过程（n是样本个数），这里，要训练1000个模型。

5折交叉验证需要训练5个模型。

重复2次的5折交叉验证，需要训练10个模型。

所有B是正确的

537、变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？

1. 多个变量其实有相同的用处
2. 变量对于模型的解释有多大作用
3. 特征携带的信息
4. 交叉验证

A、1 和 4

B、1, 2 和 3

C、1,3 和 4

D

以上所有

正确答案是：C

解析：

答案: C

注意， 这题的题眼是考虑模型效率，所以不要考虑选项2.

538、对于线性回归模型，包括附加变量在内，以下的可能正确的是 :

1. R-Squared 和 Adjusted R-squared都是递增的
2. R-Squared 是常量的，Adjusted R-squared是递增的
3. R-Squared 是递减的， Adjusted R-squared 也是递减的
4. R-Squared 是递减的， Adjusted R-squared是递增的

A、1 和 2

B、1 和 3

C、2 和 4

D、以上都不是

正确答案是：D

解析：

答案: D

R-squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-squared有R-squared 和 predicted R-squared 所没有的问题。

每次你为模型加入预测器，R-squared递增或不变.

539、对于下面三个模型的训练情况， 下面说法正确的是:

[![3.jpeg](https://ask.julyedu.com/uploads/questions/20181018/4654d3bfd36faf47ca99b6ff072d8f7c.jpeg)](https://ask.julyedu.com/uploads/questions/20181018/4654d3bfd36faf47ca99b6ff072d8f7c.jpeg)

1. 第一张图的训练错误与其余两张图相比，是最大的
2. 最后一张图的训练效果最好，因为训练错误最小
3. 第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型
4. 第三张图相对前两张图过拟合了
5. 三个图表现一样，因为我们还没有测试数据集

A、1 和 3

B、1 和 3

C、1, 3 和 4

D、5

正确答案是：C

解析：

答案: C

最后一张过拟合, 训练错误最小, 第一张相反, 训练错误就是最大了. 所以1是对的;

仅仅训练错误最小往往说明过拟合, 所以2错, 4对;

第二张图平衡了拟合和过拟合, 所以3对;

540、对于线性回归，我们应该有以下哪些假设？

1. 找到离群点很重要, 因为线性回归对离群点很敏感
2. 线性回归要求所有变量必须符合正态分布
3. 线性回归假设数据没有多重线性相关性

A、1 和 2

B、2 和 3

C、1,2 和 3

D、以上都不是

正确答案是：D

解析：

答案: D

第1个假设, 离群点要着重考虑, 第一点是对的

第2个假设, 正态分布不是必须的. 当然, 如果是正态分布, 训练效果会更好

第3个假设, 有少量的多重线性相关性也是可以的, 但是我们要尽量避免

541、当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论:

1. Var1和Var2是非常相关的
2. 因为Var1和Var2是非常相关的, 我们可以去除其中一个
3. Var3和Var1的1.23相关系数是不可能的

A、1 and 3

B、1 and 2

C、1,2 and 3

D、1

正确答案是：C

解析：

答案: C

相关性系数范围应该是 [-1,1]

一般地, 如果相关系数大于0.7或者小于-0.7, 是高相关的.

Var1和Var2相关系数是接近负1, 所以这是多重线性相关, 我们可以考虑去除其中一个.

所以1, 2, 3个结论都是对的, 选C.

542、如果在一个高度非线性并且复杂的一些变量中, 一个树模型可能比一般的回归模型效果更好. 这是（）

A、对的

B、错的

正确答案是：A

解析：

答案: A

543、下面对集成学习模型中的弱学习者描述错误的是？

A、他们经常不会过拟合

B、他们通常带有高偏差，所以其并不能解决复杂学习问题

C、他们通常会过拟合

正确答案是：C

解析：

答案：C，弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。

544、下面哪个/些选项对 K 折交叉验证的描述是正确的？

1.增大 K 将导致交叉验证结果时需要更多的时间

2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心

3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量

A、1 和 2

B、2 和 3

C、1 和 3

D、1、2 和 3

正确答案是：D

解析：

答案（D)：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。

545、最出名的降维算法是 PCA 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？

A、X_projected_PCA 在最近邻空间能得到解释

B、X_projected_tSNE 在最近邻空间能得到解释

C、两个都在最近邻空间能得到解释

D、两个都不能在最近邻空间得到解释

正确答案是： B

解析：

答案（B）：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。

  **546、给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？**

A、D1= C1, D2 < C2, D3 > C3

B、D1 = C1, D2 > C2, D3 > C3

C、D1 = C1, D2 > C2, D3 < C3

D、D1 = C1, D2 < C2, D3 < C3

E、D1 = C1, D2 = C2, D3 = C3

正确答案是：E

解析：

答案（E）：特征之间的相关性系数不会因为特征加或减去一个数而改变。

**547、为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？**

A、将数据转换成零均值

B、将数据转换成零中位数

C、无法做到

正确答案是：A

解析：

答案（A）：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据*均值归 0。

**548、假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。**

注意：所有其他超参数是相同的，所有其他因子不受影响。

1.深度为 4 时将有高偏差和低方差

2.深度为 4 时将有低偏差和低方差

A、只有 1

B、只有 2

C、1 和 2

D、没有一个

正确答案是：A

解析：

答案（A)：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。

**549、在以下不同的场景中,使用的分析方法不正确的有**

A、根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级

B、根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式

C、用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫

D、根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女

正确答案是： B

解析：

解析：

预测消费更合适的算法是用回归模型来做。而不是聚类算法。

**550、以下对k-means聚类算法解释正确的是**

A、能自动识别类的个数,随即挑选初始点为中心点计算

B、能自动识别类的个数,不是随即挑选初始点为中心点计算

C、不能自动识别类的个数,随即挑选初始点为中心点计算

D、不能自动识别类的个数,不是随即挑选初始点为中心点计算

正确答案是：C

解析：

（1）适当选择c个类的初始中心；

（2）在第k次迭代中，对任意一个样本，求其到c个中心的距离，将该样本归到距离最短的中心所在的类；

（3）利用均值等方法更新该类的中心值；

（4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。

以上是KMeans（C均值）算法的具体步骤，可以看出需要选择类别数量，但初次选择是随机的，最终的聚类中心是不断迭代稳定以后的聚类中心。所以答案选C。  

551、（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（）

A、Accuracy:(TP+TN)/all

B、F-value:2*recall*precision/(recall+precision)

C、G-mean:sqrt(precision*recall)

D、AUC:曲线下面积

正确答案是：A

解析：

题目提到测试集正例和负例数量不均衡，那么假设正例数量很少占10%，负例数量占大部分90%。

而且算法能正确识别所有负例，但正例只有一半能正确判别。

那么TP=0.05×all,TN=0.9×all，Accuracy=95%。

虽然Accuracy很高，precision是100%,但正例recall只有50%

552、下列选项中,识别模式与其他不⼀样的是

A、⽤户年龄分布判断:少年、青年、中年、⽼年

B、医⽣给病⼈诊断发病类型

C、投递员分拣信件

D、消费者类型判断:⾼消费、⼀般消息、低消费

E、出⾏方式判断:步⾏、骑车、坐车

F、商家对商品分级

正确答案是：E

解析：

解析：

E属于预测问题，其他的选项属于分类问题

553、在大规模的语料中，挖掘词的相关性是一个重要的问题。以下哪一个信息不能用于确定两个词的相关性。

A、互信息

B、最大熵

C、卡方检验

D、最大似然比

正确答案是： B

解析：

解析：

最大熵代表了整体分布的信息，通常具有最大熵的分布作为该随机变量的分布，不能体现两个词的相关性，但是卡方是检验两类事务发生的相关性。所以选B【正解】

554、基于统计的分词方法为（）

A、正向最大匹配法

B、逆向最大匹配法

C、最少切分

D、条件随机场

正确答案是：D

解析：

第一类是基于语法和规则的分词法。其基本思想就是在分词的同时进行句法、语义分析,利用句法信息和语义信息来进行词性标注,以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂,基于语法和规则的分词法所能达到的精确度远远还不能令人满意,目前这种分词系统还处在试验阶段。

第二类是机械式分词法（即基于词典）。机械分词的原理是将文档中的字符串与词典中的词条进行逐一匹配,如果词典中找到某个字符串,则匹配成功,可以切分,否则不予切分。基于词典的机械分词法,实现简单,实用性强,但机械分词法的最大的缺点就是词典的完备性不能得到保证。据统计,用一个含有70000个词的词典去切分含有15000个词的语料库,仍然有30%以上的词条没有被分出来,也就是说有4500个词没有在词典中登录。

第三类是基于统计的方法。基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合,相邻的字同时出现的次数越多,就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。

555、在下面的图像中，哪一个是多元共线（multi-collinear）特征？

[![1.png](https://ask.julyedu.com/uploads/questions/20181024/cc1483b7cf4343a098cd1186e199e4cc.png)](https://ask.julyedu.com/uploads/questions/20181024/cc1483b7cf4343a098cd1186e199e4cc.png)

A、图 1 中的特征

B、图 2 中的特征

C、图 3 中的特征

D、图 1、2 中的特征

E、图 2、3 中的特征

F、图 1、3 中的特征

正确答案是：D

解析：

答案为（D）：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。

556、线性回归的基本假设不包括哪个？

A、随机误差项是一个期望值为0的随机变量

B、对于解释变量的所有观测值，随机误差项有相同的方差

C、随机误差项彼此相关

D、解释变量是确定性变量不是随机变量，与随机误差项之间相互独立

E、随机误差项服从正态分布

正确答案是：C

557、下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是错误的？

A、类型 1 通常称之为假正类，类型 2 通常称之为假负类

B、类型 2 通常称之为假正类，类型 1 通常称之为假负类

C、类型 1 错误通常在其是正确的情况下拒绝假设而出现

正确答案是： B

解析：

在统计学假设测试中，I 类错误即错误地拒绝了正确的假设即假正类错误，II 类错误通常指错误地接受了错误的假设即假负类错误。

558、给线性回归模型添加一个不重要的特征可能会造成？

A、增加 R-square

B、减少 R-square

正确答案是：A

解析：

答案为（A）：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。

R-square定义如下:

[![2.png](https://ask.julyedu.com/uploads/questions/20181026/efb7c31b452e987abfd5e5361ce83251.png)](https://ask.julyedu.com/uploads/questions/20181026/efb7c31b452e987abfd5e5361ce83251.png)

在给特征空间添加了一个特征后，分子会增加一个残差平方项, 分母会增加一个均值差平方项, 前者一般小于后者, 所以不论特征是重要还是不重要，R-square 通常会增加。

559、关于 ARMA 、 AR 、 MA 模型的功率谱，下列说法正确的是（ ）

A、MA模型是同一个全通滤波器产生的

B、MA模型在极点接近单位圆时，MA谱是一个深谷

C、AR模型在零点接近单位圆时，AR谱是一个尖峰

D、RMA谱既有尖峰又有深谷

正确答案是：D

560、符号集 a 、 b 、 c 、 d ，它们相互独立，相应概率为 1/2 、 1/4 、 1/8/ 、 1/16 ，其中包含信息量最小的符号是（ ）

A、a

B、b

C、c

D、d

正确答案是：A

解析：

因为消息出现的概率越小，则消息中所包含的信息量就越大。因此选a,同理d信息量最大。

  **561、下列哪个不属于常用的文本分类的特征选择算法？**

A、卡方检验值

B、互信息

C、信息增益

D、主成分分析

正确答案是：D

解析：

主成分分析是特征转换算法（特征抽取），而不是特征选择

**562、在数据清理中，下面哪个不是处理缺失值的方法?**

A、估算

B、整例删除

C、变量删除

D、成对删除

正确答案是：D

解析：

数据清理中，处理缺失值的方法有两种：

一、删除法：

1）删除观察样本

2）删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除

3）使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析

4）改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差

二、查补法：均值插补、回归插补、抽样填补等

**563、统计模式分类问题中，当先验概率未知时，可以使用（）**

A、最小最大损失准则

B、最小误判概率准则

C、最小损失准则

D、N-P判决

正确答案是：A

解析：

A. 考虑p(wi)变化的条件下，是风险最小

B. 最小误判概率准则， 就是判断p(w1|x)和p(w2|x)哪个大，x为特征向量，w1和w2为两分类，根据贝叶斯公式，需要用到先验知识

C. 最小损失准则，在B的基础之上，还要求出p(w1|x)和p(w2|x)的期望损失，因为B需要先验概率，所以C也需要先验概率

D. N-P判决，即限定一类错误率条件下使另一类错误率为最小的两类别决策，即在一类错误率固定的条件下，求另一类错误率的极小值的问题，直接计算p(x|w1)和p(x|w2)的比值，不需要用到贝叶斯公式_

**564、决策树的父节点和子节点的熵的大小关系是什么？**

A、A. 决策树的父节点更大

B、B. 子节点的熵更大

C、C. 两者相等

D、D. 根据具体情况而定

正确答案是：D

解析：

正确答案：D。

假设一个父节点有2正3负样本，进一步分裂情况1：两个叶节点（2正，3负）；情况2：两个叶节点（1正1负，1正2负）。分别看下情况1和情况2，分裂前后确实都有信息增益，但是两种情况里不是每一个叶节点都比父节点的熵小。

**565、语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用（ ）**

A、平滑

B、去噪

C、随机插值

D、增加白噪音

正确答案是：A

解析：

A，拉普拉斯平滑假设，将分子和分母各加上一个常数项。  

566、逻辑回归与多元回归分析有哪些不同？

A、逻辑回归预测某事件发生的概率

B、逻辑回归有较高的拟合效果

C、逻辑回归回归系数的评估

D、以上全选

正确答案是：D

解析：

答案：D

逻辑回归是用于分类问题，我们能计算出一个事件/样本的概率；一般来说，逻辑回归对测试数据有着较好的拟合效果；建立逻辑回归模型后，我们可以观察回归系数类标签(正类和负类)与独立变量的的关系。

567、"过拟合是有监督学习的挑战，而不是无监督学习"以上说法是否正确：

A、正确

B、错误

正确答案是： B

解析：

答案：B

我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数。

568、中文同义词替换时，常用到Word2Vec，以下说法错误的是

A、Word2Vec基于概率统计

B、Word2Vec结果符合当前预料环境

C、Word2Vec得到的都是语义上的同义词

D、Word2Vec受限于训练语料的数量和质量

正确答案是：C

解析：

Word2vec，为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。

训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系。该向量为神经网络之隐藏。

Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。

569、假定你用一个线性SVM分类器求解二类分类问题，如下图所示，这些用红色圆圈起来的点表示支持向量

[![1.png](https://ask.julyedu.com/uploads/questions/20181030/38acb2fc6f7d398dbe79222f15ae655b.png)](https://ask.julyedu.com/uploads/questions/20181030/38acb2fc6f7d398dbe79222f15ae655b.png)

如果移除这些圈起来的数据，决策边界（即分离超平面）是否会发生改变？

A、Yes

B、No

正确答案是： B

解析：

从数据的分布来看，移除那三个数据，决策边界不会受影响。

570、如果将数据中除圈起来的三个点以外的其他数据全部移除，那么决策边界是否会改变？

[![2.png](https://ask.julyedu.com/uploads/questions/20181030/c6d15a198d3f811c78b681f6ae55d5a2.png)](https://ask.julyedu.com/uploads/questions/20181030/c6d15a198d3f811c78b681f6ae55d5a2.png)

A、会

B、不会

正确答案是： B

解析：

决策边界只会被支持向量影响，跟其他点无关。

  **571、关于SVM泛化误差描述正确的是**

A、超平面与支持向量之间距离

B、SVM对未知数据的预测能力

C、SVM的误差阈值

正确答案是： B

解析：

统计学中的泛化误差是指对模型对未知数据的预测能力。

**572、以下关于硬间隔hard margin描述正确的是**

A、SVM允许分类存在微小误差

B、SVM允许分类是有大量误差

正确答案是：A

解析：

硬间隔意味着SVM在分类时很严格，在训练集上表现尽可能好，有可能会造成过拟合。

**573、训练SVM的最小时间复杂度为O(n2)，那么一下哪种数据集不适合用SVM?**

A、大数据集

B、小数据集

C、中等大小数据集

D、和数据集大小无关

正确答案是：A

解析：

有明确分类边界的数据集最适合SVM

**574、SVM的效率依赖于**

A、核函数的选择

B、核参数

C、软间隔参数

D、以上所有

正确答案是：D

解析：

SVM的效率依赖于以上三个基本要求，它能够提高效率，降低误差和过拟合

**575、支持向量是那些最接近决策平面的数据点**

A、对

B、错

正确答案是：A

解析：

支持向量就在间隔边界上  

  **576、SVM在下列那种情况下表现糟糕**

A、线性可分数据

B、清洗过的数据

C、含噪声数据与重叠数据点

正确答案是：C

解析：

当数据中含有噪声数据与重叠的点时，要画出干净利落且无误分类的超平面很难

**577、假定你使用了一个很大γ值的RBF核，这意味着：**

A、模型将考虑使用远离超平面的点建模

B、模型仅使用接近超平面的点来建模

C、模型不会被点到超平面的距离所影响

D、以上都不正确

正确答案是： B

解析：

SVM调参中的γ衡量距离超平面远近的点的影响。

对于较小的γ，模型受到严格约束，会考虑训练集中的所有点，而没有真正获取到数据的模式、对于较大的γ，模型能很好地学习到模型。

**578、SVM中的代价参数表示：**

A、交叉验证的次数

B、使用的核

C、误分类与模型复杂性之间的平衡

D、以上均不是

正确答案是：C

解析：

代价参数决定着SVM能够在多大程度上适配训练数据。

如果你想要一个平稳的决策平面，代价会比较低；如果你要将更多的数据正确分类，代价会比较高。可以简单的理解为误分类的代价。

**579、假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。**

当你使用较大的C（C趋于无穷），则：

A、仍然能正确分类数据

B、不能正确分类

C、不确定

D、以上均不正确

正确答案是：A

解析：

采用更大的C，误分类点的惩罚就更大，因此决策边界将尽可能完美地分类数据。

**580、如果我使用数据集的全部特征并且能够达到100%的准确率，但在测试集上仅能达到70%左右，这说明：**

A、欠拟合

B、模型很棒

C、过拟合

正确答案是：C

解析：

如果在训练集上模型很轻易就能达到100%准确率，就要检查是否发生过拟合。  

  **581、假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。**

如果使用较小的C（C趋于0），则：

A、误分类

B、正确分类

C、不确定

D、以上均不正确

正确答案是：A

解析：

分类器会最大化大多数点之间的间隔，少数点会误分类，因为惩罚太小了。

**582、下面哪个属于SVM应用**

A、文本和超文本分类

B、图像分类

C、新文章聚类

D、以上均是

正确答案是：D

解析：

SVM广泛应用于实际问题中，包括回归，聚类，手写数字识别等。

**583、假设你训练SVM后，得到一个线性决策边界，你认为该模型欠拟合。在下次迭代训练模型时，应该考虑：**

A、增加训练数据

B、减少训练数据

C、计算更多变量

D、减少特征

正确答案是：C

解析：

由于是欠拟合，最好的选择是创造更多特征带入模型训练。

**584、假设你训练SVM后，得到一个线性决策边界，你认为该模型欠拟合。假如你想修改SVM的参数，同样达到模型不会欠拟合的效果，应该怎么做？**

A、增大参数C

B、减小参数C

C、改变C并不起作用

D、以上均不正确

正确答案是：A

解析：

增大参数C会得到正则化模型

**585、SVM中使用高斯核函数之前通常会进行特征归一化，以下关于特征归一化描述不正确的是？**

A、经过特征正则化得到的新特征优于旧特征

B、特征归一化无法处理类别变量

C、SVM中使用高斯核函数时，特征归一化总是有用的

正确答案是：C

解析：

非万能。  

  **586、假设现在只有两个类，这种情况下SVM需要训练几次？**

A、1

B、2

C、3

D、4

正确答案是：A

解析：

两个类训练1次就可以了

**587、假设你训练了一个基于线性核的SVM，多项式阶数为2，在训练集和测试集上准确率都为100%。如果增加模型复杂度或核函数的多项式阶数，将会发生什么？**

A、导致过拟合

B、导致欠拟合

C、无影响，因为模型已达100%准确率

D、以上均不正确

正确答案是：A

解析：

增加模型复杂度会导致过拟合

**588、想象一下，机器学习中有1000个输入特征和1个目标特征，必须根据输入特征和目标特征之间的关系选择100个最重要的特征。你认为这是减少维数的例子吗？**

A、是

B、不是

正确答案是：A

**589、判断：没有必要有一个用于应用维数降低算法的目标变量。**

A、真

B、假

正确答案是：A

解析：

LDA是有监督降维算法的一个例子。

**590、在数据集中有4个变量，如A，B，C和D.执行了以下操作：**

步骤1：使用上述变量创建另外两个变量，即E = A + 3 * B和F = B + 5 * C + D。

步骤2：然后只使用变量E和F建立了一个随机森林模型。

上述步骤可以表示降维方法吗？

A、真

B、假

正确答案是：A

解析：

因为步骤1可以用于将数据表示为2个较低的维度。  

  **591、以下哪种技术对于减少数据集的维度会更好？**

A、删除缺少值太多的列

B、删除数据差异较大的列

C、删除不同数据趋势的列

D、都不是

正确答案是：A

解析：

如果列的缺失值太多（例如99％），那么可以删除这些列。

**592、判断：降维算法是减少构建模型所需计算时间的方法之一。**

A、真

B、假

正确答案是：A

解析：

降低数据维数将花费更少的时间来训练模型。

**593、以下哪种算法不能用于降低数据的维数？**

A、t-SNE

B、PCA

C、LDA

D、都不是

正确答案是：D

解析：

所有算法都是降维算法的例子。

**594、判断：PCA可用于在较小维度上投影和可视化数据。**

A、真

B、假

正确答案是：A

解析：

有时绘制较小维数据非常有用，可以使用前两个主要分量，然后使用散点图可视化数据。

**595、最常用的降维算法是PCA，以下哪项是关于PCA的？**

1.PCA是一种无监督的方法

2.它搜索数据具有最大差异的方向

3.主成分的最大数量<=特征能数量

4.所有主成分彼此正交

A、2、3和4

B、1、2和3

C、1、2和4

D、以上所有

正确答案是：D  

  **596、假设使用维数降低作为预处理技术，使用PCA将数据减少到k维度。然后使用这些PCA预测作为特征，以下哪个声明是正确的？**

A、更高的“k”意味着更正则化

B、更高的“k”意味着较少的正则化

C、都不对

正确答案是： B

解析：

较高的k导致较少的平滑，因此能够保留更多的数据特征，从而减少正则化。

**597、在相同的机器上运行并设置最小的计算能力，以下哪种情况下t-SNE比PCA降维效果更好？**

A、具有1百万项300个特征的数据集

B、具有100000项310个特征的数据集

C、具有10,000项8个特征的数据集

D、具有10,000项200个特征的数据集

正确答案是：C

解析：

t-SNE具有二次时空复杂度。

**598、对于t-SNE代价函数，以下陈述中的哪一个正确？**

A、本质上是不对称的

B、本质上是对称的

C、与SNE的代价函数相同

正确答案是： B

解析：

SNE代价函数是不对称的，这使得使用梯度下降难以收敛。对称是SNE和t-SNE代价函数之间的主要区别之一。

**599、想像正在处理文本数据，使用单词嵌入（Word2vec）表示使用的单词。在单词嵌入中，最终会有1000维。现在想减小这个高维数据的维度，这样相似的词应该在最邻近的空间中具有相似的含义。在这种情况下，您最有可能选择以下哪种算法？**

A、t-SNE

B、PCA

C、LDA

D、都不是

正确答案是：A

解析：

t-SNE代表t分布随机相邻嵌入，它考虑最近的邻居来减少数据。

**600、判断：t-SNE学习非参数映射。**

A、真

B、假

正确答案是：A

解析：

t-SNE学习非参数映射，这意味着它不会学习将数据从输入空间映射到地图的显式函数。  

  **601、以下对于t-SNE和PCA的陈述中哪个是正确的？**
A、t-SNE是线性的，而PCA是非线性的
B、t-SNE和PCA都是线性的
C、t-SNE和PCA都是非线性的
D、t-SNE是非线性的，而PCA是线性的

正确答案是：D

**602、在t-SNE算法中，可以调整以下哪些超参数？**
A、维度数量
B、平稳测量有效数量的邻居
C、最大迭代次数
D、以上所有

正确答案是：D
解析：
选项中的所有超参数都可以调整。

**603、与PCA相比，t-SNE的以下说明哪个正确？** 
A、数据巨大（大小）时，t-SNE可能无法产生更好的结果。
B、无论数据的大小如何，T-NSE总是产生更好的结果。
C、对于较小尺寸的数据，PCA总是比t-SNE更好。
D、都不是

正确答案是：A

**604、Xi和Xj是较高维度表示中的两个不同点，其中Yi和Yj是较低维度中的Xi和Xj的表示。**
1.数据点Xi与数据点Xj的相似度是条件概率p（j | i）。
2.数据点Yi与数据点Yj的相似度是条件概率q（j | i）。
对于在较低维度空间中的Xi和Xj的完美表示，以下哪一项必须是正确的？
A、p（j | i）= 0，q（j | i）= 1
B、p（j | i）
C、p（j | i）= q（j | i）
D、P（j | i）> q（j | i）

正确答案是：C
解析：
两点的相似性的条件概率必须相等，因为点之间的相似性必须在高维和低维中保持不变，以使它们成为完美的表示。

**605、对于投影数据为(( √2)，(0)，(√2))。现在如果在二维空间中重建，并将它们视为原始数据点的重建，那么重建误差是多少？**
A、0％
B、10％ 
C、30％
D、40％

正确答案是：A
解析：
重建误差为0，因为所有三个点完全位于第一个主要分量的方向上或者计算重建;  

606、LDA的以下哪项是正确的？

[![1.png](https://ask.julyedu.com/uploads/questions/20181109/1cafec87a74bebe5e6c384ef8d9a9ff1.png)](https://ask.julyedu.com/uploads/questions/20181109/1cafec87a74bebe5e6c384ef8d9a9ff1.png)

A、LDA旨在最大化之间类别的距离，并最小化类内之间的距离

B、LDA旨在最小化类别和类内之间的距离

C、LDA旨在最大化类内之间的距离，并最小化类别之间的距离

D、LDA旨在最大化类别和类内之间的距离

正确答案是：A

607、LDA的思想是找到最能区分两类别之间的线，下图中哪个是好的投影？

[![2.jpeg](https://ask.julyedu.com/uploads/questions/20181109/96ad69e34ff0499dfc0dd4fc6bc64752.jpeg)](https://ask.julyedu.com/uploads/questions/20181109/96ad69e34ff0499dfc0dd4fc6bc64752.jpeg)

A、LD1

B、LD2

C、两者

D、都不是

正确答案是：A

608、以下哪种情况LDA会失败？

A、如果有辨识性的信息不是平均值，而是数据的方差

B、如果有辨识性的信息是平均值，而不是数据方差

C、如果有辨识性的信息是数据的均值和方差

D、都不是

正确答案是：A

609、PCA和LDA的以下比较哪些是正确的？

1. LDA和PCA都是线性变换技术
2. LDA是有监督的，而PCA是无监督的
3. PCA最大化数据的方差，而LDA最大化不同类之间的分离

A、1和2

B、1和3

C、只有3

D、1、2和3

正确答案是：D

610、PCA是一种很好的技术，因为它很容易理解并通常用于数据降维。获得特征值λ1≥λ2≥•••≥λN并画图。

[![4.jpeg](https://ask.julyedu.com/uploads/questions/20181109/406c66e3c4b0f014648638c7d176e73f.jpeg)](https://ask.julyedu.com/uploads/questions/20181109/406c66e3c4b0f014648638c7d176e73f.jpeg)

看看f(M)（贡献率）如何随着M而增加，并且在M = D处获得最大值1，给定两图：

[![3.jpeg](https://ask.julyedu.com/uploads/questions/20181109/6e6c758335672af3ef38fa337d47f57e.jpeg)](https://ask.julyedu.com/uploads/questions/20181109/6e6c758335672af3ef38fa337d47f57e.jpeg)

上述哪个图表显示PCA的性能更好？其中M是主要分量，D是特征的总数。

A、左图

B、右图

正确答案是：A

解析：

如果f（M）渐近线快速到达1，则PCA是好的；如果第一个特征值较大且其余较小，则会发生这种情况。如果所有特征值大致相等，PCA是坏的。

611题

以下哪个选项是真的？

A、LDA明确地尝试对数据类别之间的差异进行建模，而PCA没有。

B、两者都试图模拟数据类之间的差异。

C、PCA明确地试图对数据类别之间的差异进行建模，而LDA没有。

D、两者都不试图模拟数据类之间的差异。

正确答案是：A

612题

应用PCA后，以下哪项可以是前两个主成分？

1.(0.5,0.5,0.5,0.5)和(0.71,0.71,0,0)

1. (0.5,0.5,0.5,0.5)和(0,0，-0.71,0.71)
2. (0.5,0.5,0.5,0.5)和(0.5,0.5,-0.5,-0.5)
3. (0.5,0.5,0.5,0.5)和(-0.5,-0.5,0.5,0.5)

A、1和2

B、1和3

C、2和4

D、3和4

正确答案是：D

解析：

对于前两个选择，两个向量不是正交的。

613题

以下哪一项给出了逻辑回归与LDA之间的差异？

1. 如果类别分离好，逻辑回归的参数估计可能不稳定。
2. 如果样本量小，并且每个类的特征分布是正常的。在这种情况下，线性判别分析比逻辑回归更稳定。

A、1

B、2

C、1和2

D、都不是

正确答案是：C

614题

在PCA中会考虑以下哪个偏差？

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181122/bd6a40fd664a80c4165f6129ecc9f441.jpg)](https://ask.julyedu.com/uploads/questions/20181122/bd6a40fd664a80c4165f6129ecc9f441.jpg)

A、垂直偏移

B、正交偏移

C、两者

D、都不是

正确答案是： B

解析：

总是将残差视为垂直偏移，正交偏移在PCA的情况下是有用的。

615题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181122/d0d85765325a8418bd133c05bb5c4ed9.jpg)](https://ask.julyedu.com/uploads/questions/20181122/d0d85765325a8418bd133c05bb5c4ed9.jpg)

上图中哪一个显示了决策边界过度拟合训练数据？

A、A

B、B

C、C

D、这些都没有

正确答案是：C

解析：

答案：C由于在图3中，决策边界不平滑，表明其过度拟合数据。

616题

假设正在处理10类分类问题，并且想知道LDA最多可以产生几个判别向量。以下哪个是正确答案？

A、20

B、9

C、21

D、11

正确答案是： B

解析：

LDA最多产生c-1个判别向量。

617题

给定的数据集包括“胡佛塔”和其他一些塔的图像。现在要使用PCA（特征脸）和最近邻方法来构建一个分类器，可以预测新图像是否显示“胡佛塔”。该图给出了输入的训练图像样本

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181122/579416f049cad2ac1d931e3fc2b7fc66.jpg)](https://ask.julyedu.com/uploads/questions/20181122/579416f049cad2ac1d931e3fc2b7fc66.jpg)

为了从“特征脸”算法获得合理的性能，这些图像将需要什么预处理步骤？ 

1.将塔对准图像中相同的位置。

1. 将所有图像缩放或裁剪为相同的大小。

A、1

B、2

C、1和2

D、都不是

正确答案是：C

618题

下图中主成分的最佳数量是多少？

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181122/6a68644bb4570827f58274e4ff293b08.jpg)](https://ask.julyedu.com/uploads/questions/20181122/6a68644bb4570827f58274e4ff293b08.jpg)

A、7

B、30

C、40

D、不知道

正确答案是： B

解析：

可以在上图中看到，主成分的数量为30时以最小的数量得到最大的方差。

619题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

[![3.jpg](https://ask.julyedu.com/uploads/questions/20181122/710e7913ed6a875273e4b02ff31cd44e.jpg)](https://ask.julyedu.com/uploads/questions/20181122/710e7913ed6a875273e4b02ff31cd44e.jpg)

正则化项惩罚度最高的是？

A、A

B、B

C、C

D、都具有相同的正则化

正确答案是：A

解析：

答案：A因为正则化意味着更多的罚值和图A所示的较简单的决策界限。

620题

下图显示了三个逻辑回归模型的AUC-ROC曲线。不同的颜色表示不同超参数值的曲线。以下哪个AUC-ROC会给出最佳结果？

[![4.jpg](https://ask.julyedu.com/uploads/questions/20181122/8d83c8c1f5efe80e25dc434b321668cd.jpg)](https://ask.julyedu.com/uploads/questions/20181122/8d83c8c1f5efe80e25dc434b321668cd.jpg)

A、黄色

B、粉红色

C、黑色

D、都相同

正确答案是：A

解析：

答案：A最佳分类是曲线下区域面积最大者，而黄线在曲线下面积最大。

  621题

**如果对相同的数据进行逻辑回归，将花费更少的时间，并给出比较相似的精度（也可能不一样），怎么办？假设在庞大的数据集上使用Logistic回归模型。可能遇到一个问题，Logistic回归需要很长时间才能训练。**

A、降低学习率，减少迭代次数

B、降低学习率，增加迭代次数

C、提高学习率，增加迭代次数

D、增加学习率，减少迭代次数

正确答案是：D

解析：

答案：D如果在训练时减少迭代次数，就能花费更少的时间获得相同的精度，但需要增加学习率。

622题

**Logistic regression（逻辑回归）是一种监督式机器学习算法吗？**

A、是

B、否

正确答案是：A

解析：

当然，Logistic regression是一种监督式学习算法，因为它使用真假标签进行测试。 测试模型时，监督式学习算法应具有输入变量（x）和目标变量（Y）。

623题

**Logistic Regression主要用于回归吗？**

A、是

B、否

正确答案是： B

解析：

逻辑回归是一种分类算法，不要因为名称将其混淆。

624题

**是否能用神经网络算法设计逻辑回归算法？**

A、是

B、否

正确答案是：A

解析：

是的，神经网络是一种通用逼近器，因此能够实现线性回归算法。

625题

**是否可以对三分问题应用逻辑回归算法？**

A、是

B、否

正确答案是：A

解析：

当然可以对三分问题应用逻辑回归，只需在逻辑回归中使用One Vs all方法。  

626题

以下哪种方法能最佳地适应逻辑回归中的数据？

A、Least Square Error

B、Maximum Likelihood

C、Jaccard distance

D、Both A and B

正确答案是： B

解析：

Logistic Regression使用可能的最大似然估值来测试逻辑回归过程。

627题

在逻辑回归输出与目标对比的情况下，以下评估指标中哪一项不适用？

A、AUC-ROC

B、准确度

C、Logloss

D、均方误差

正确答案是：D

解析：

因为Logistic Regression是一个分类算法，所以它的输出不能是实时值，所以均方误差不能用于评估它。

628题

如下逻辑回归图显示了3种不同学习速率值的代价函数和迭代次数之间的关系（不同的颜色在不同的学习速率下显示不同的曲线）。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181122/0ca372ef8e81d898328e87ab4537babd.jpg)](https://ask.julyedu.com/uploads/questions/20181122/0ca372ef8e81d898328e87ab4537babd.jpg)

为了参考而保存图表后，忘记其中不同学习速率的值。现在需要曲线的倾斜率值之间的关系。以下哪一个是正确的？ 

注：

1.蓝色的学习率是L1

2.红色的学习率是L2

3.绿色学习率为lL3

A、L1> L2> L3

B、L1 = L2 = L3

C、L1

D、都不是

正确答案是：C

解析：

答案：C如果学习速率低下，代价函数将缓慢下降，学习速度过高，则其代价函数会迅速下降。

629题

分析逻辑回归表现的一个良好的方法是AIC，它与线性回归中的R平方相似。有关AIC，以下哪项是正确的？

A、具有最小AIC值的模型更好

B、具有最大AIC值的模型更好

C、视情况而定

D、以上都不是

正确答案是：A

解析：

AIC信息准则即Akaike information criterion，是衡量统计模型拟合优良性的一种标准，由于它为日本统计学家赤池弘次创立和发展的，因此又称赤池信息量准则。 

考虑到AIC=2k-2In(L) ，所以一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。

综上，我们一般选择逻辑回归中最少的AIC作为最佳模型。有关更多信息，请参阅此来源：www4.ncsu.edu/~shu3/Presentation/AIC.pdf

630题

在训练逻辑回归之前需要对特征进行标准化。

A、是

B、否

正确答案是： B

解析：

逻辑回归不需要标准化。功能标准化的主要目标是帮助优化技术组合。

631题

选择Logistic回归中的One-Vs-All方法中的哪个选项是真实的。

A、我们需要在n类分类问题中适合n个模型

B我们需要适合n-1个模型来分类为n个类

C、我们需要只适合1个模型来分类为n个类

D、这些都没有

正确答案是：A

解析：

如果存在n个类，那么n个单独的逻辑回归必须与之相适应，其中每个类的概率由剩余类的概率之和确定。

632题

使用以下哪种算法进行变量选择？

A、LASSO

B、Ridge

C、两者

D、都不是

正确答案是：A

解析：

使用Lasso的情况下，我们采用绝对罚函数，在增加Lasso中罚值后，变量的一些系数可能变为零。

633题

以下是两种不同的对数模型，分别为β0和β1。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181122/d132cbc207aed5b82bcc769893446a36.jpg)](https://ask.julyedu.com/uploads/questions/20181122/d132cbc207aed5b82bcc769893446a36.jpg)

对于两种对数模型（绿色，黑色）的β0和β1值，下列哪一项是正确的？注： Y =β0+β1* X。其中β0是截距，β1是系数。

A、绿色的β1大于黑色

B、绿色的β1小于黑色

C、两种颜色的β1相同

D、不能说

正确答案是： B

解析：

β0和β1：β0= 0，β1= 1为X1颜色（黑色），β0= 0，β1= -1为X4颜色（绿色）

634题

逻辑回归的以下模型：P（y = 1 | x，w）= g（w0 + w1x）其中g（z）是逻辑函数。在上述等式中，通过改变参数w可以得到的P（y = 1 | x; w）被视为x的函数。

A、（0，inf）

B、（-inf，0）

C、（0,1）

D、（-inf，inf）

正确答案是：C

解析：

对于从-∞到+∞的实数范围内的x的值。逻辑函数将给出（0,1）的输出。

635题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181122/6683c5b2f6dd79dfd29c4a5699915411.jpg)](https://ask.julyedu.com/uploads/questions/20181122/6683c5b2f6dd79dfd29c4a5699915411.jpg)

上图中哪一个显示了决策边界过度拟合训练数据？

A、A

B、B

C、C

D、这些都没有

正确答案是：C

解析：

由于在图3中，决策边界不平滑，表明其过度拟合数据。

636题

逻辑回归的以下模型：P（y = 1 | x，w）= g（w0 + w1x）其中g（z）是逻辑函数。在上述等式中，通过改变参数w可以得到的P（y = 1 | x; w）被视为x的函数。在上面的问题中，你认为哪个函数会产生（0,1）之间的p？

A、逻辑函数

B、对数似然函数

C、两者的复合函数

D、都不会

正确答案是：A

解析：

对于从-∞到+∞的实数范围内的x的值。逻辑函数将给出（0,1）的输出。

637题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181123/0dd856b9ed37462d8f38b9977d327d6f.jpg)](https://ask.julyedu.com/uploads/questions/20181123/0dd856b9ed37462d8f38b9977d327d6f.jpg)

根据可视化后的结果，能得出什么结论？

1.与第二和第三图相比，第一幅图中的训练误差最大

2.该回归问题的最佳模型是最后（第三个）图，因为它具有最小的训练误差（零）

3.第二个模型比第一个和第三个更强，它在不可见数据中表现最好

4.与第一种和第二种相比，第三种模型过度拟合了

5.所有的模型执行起来都一样，因为没有看到测试数据。

A、1和3

B、1和3

C、1,3和4

D、5

正确答案是：C

解析：

图中趋势像是自变量X的二次趋势。更高次方的多项式（右图）可能对训练中的数据群具有超高的精度，但预计在测试数据集上将会严重失败。但是在左图中可以测试最大错误值，因为适合训练数据

638题

下面是三个散点图（A，B，C，从左到右）和和手绘的逻辑回归决策边界。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181123/0dd856b9ed37462d8f38b9977d327d6f.jpg)](https://ask.julyedu.com/uploads/questions/20181123/0dd856b9ed37462d8f38b9977d327d6f.jpg)

假设上述决策边界是针对不同的正则化（regularization）值生成的。那么其中哪一个显示最大正则化？

A、A

B、B

C、C

D、都具有相同的正则化

正确答案是：A

解析：

因为正则化意味着更多的罚值和图A所示的较简单的决策界限。

639题

下图显示了三个逻辑回归模型的AUC-ROC曲线。不同的颜色表示不同超参数值的曲线。以下哪个AUC-ROC会给出最佳结果？

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181123/3c2a388daa8284b3532a9cd134604eee.jpg)](https://ask.julyedu.com/uploads/questions/20181123/3c2a388daa8284b3532a9cd134604eee.jpg)

A、黄色

B、粉红色

C、黑色

D、都相同

正确答案是：A

解析：

最佳分类是曲线下区域面积最大者，而黄线在曲线下面积最大。

640题

假设你在测试逻辑回归分类器，设函数H为

[![3.jpg](https://ask.julyedu.com/uploads/questions/20181123/ff178fe058ea74e366f6eef9922ae71b.jpg)](https://ask.julyedu.com/uploads/questions/20181123/ff178fe058ea74e366f6eef9922ae71b.jpg)

下图中的哪一个代表上述分类器给出的决策边界？

A

[![4.jpg](https://ask.julyedu.com/uploads/questions/20181123/0be4d0ee9a48d332e9097a637635d779.jpg)](https://ask.julyedu.com/uploads/questions/20181123/0be4d0ee9a48d332e9097a637635d779.jpg)

B

[![5.jpg](https://ask.julyedu.com/uploads/questions/20181123/f41f1b82b8036c44097b4e68feb3967a.jpg)](https://ask.julyedu.com/uploads/questions/20181123/f41f1b82b8036c44097b4e68feb3967a.jpg)

C

[![6.jpg](https://ask.julyedu.com/uploads/questions/20181123/9ffb94ae9ff2259d3c7f648f149fe6e4.jpg)](https://ask.julyedu.com/uploads/questions/20181123/9ffb94ae9ff2259d3c7f648f149fe6e4.jpg)

正确答案是： B

解析：

选项B正确。虽然我们的式子由选项A和选项B所示的y = g（-6 + x2）表示，但是选项B才是正确的答案，因为当将x2 = 6的值放在等式中时，要使y = g（0）就意味着y = 0.5将在线上，如果你将x2的值增加到大于6，你会得到负值，所以输出将是区域y = 0。

641题

所谓几率，是指发生概率和不发生概率的比值。所以，抛掷一枚正常硬币，正面朝上的几率（odds）为多少？

A、0.5

B、1

C、都不是

正确答案是： B

解析：

几率（odds）是事件发生不发生概率的比率，正面朝上概率为1/2和反面朝上的概率都为1/2，所以几率为1。

642题

Logit函数（给定为l（x））是几率函数的对数。域x = [0,1]中logit函数的范围是多少？

A、（ - ∞，∞）

B、（0,1）

C、（0，∞）

D、（ - ∞，0）

正确答案是：A

解析：

为了与目标相适应，几率函数具有将值从0到1的概率函数变换成值在0和∞之间的等效函数的优点。当我们采用几率函数的自然对数时，我们便能范围是-∞到∞的值。

643题

如果对相同的数据进行逻辑回归，将花费更少的时间，并给出比较相似的精度（也可能不一样），怎么办？（假设在庞大的数据集上使用Logistic回归模型。可能遇到一个问题，Logistic回归需要很长时间才能训练。）

A、降低学习率，减少迭代次数

B、降低学习率，增加迭代次数

C、提高学习率，增加迭代次数

D、增加学习率，减少迭代次数

正确答案是：D

解析：

如果在训练时减少迭代次数，就能花费更少的时间获得相同的精度，但需要增加学习率。

644题

以下哪些选项为真？

A、线性回归误差值必须正态分布，但是在Logistic回归的情况下，情况并非如此

B、逻辑回归误差值必须正态分布，但是在线性回归的情况下，情况并非如此

C、线性回归和逻辑回归误差值都必须正态分布

D、线性回归和逻辑回归误差值都不能正态分布

正确答案是：A

解析：

只有A是真的。请参考教程 czep.net/stat/mlelr.pdf

645题

以下哪个图像显示y = 1的代价函数？以下是两类分类问题的逻辑回归（Y轴损失函数和x轴对数概率）的损失函数。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181127/13ca59965bc6eceed98439f2b00e420f.jpg)](https://ask.julyedu.com/uploads/questions/20181127/13ca59965bc6eceed98439f2b00e420f.jpg)

注：Y是目标类

A、A

B、B

C、两者

D、这些都没有

正确答案是：A

解析：

A正确，因为损失函数随着对数概率的增加而减小

  **646题**
以下不属于影响聚类算法结果的主要因素有

A 已知类别的样本质量

B 分类准则

C 特征选取

D 模式相似性测度

正确答案是：A

解析：
都已知了，就不必再进行聚类了。

**647题**
2、模式识别中，不属于马式距离较之于欧式距离的优点的是

A 平移不变性

B 尺度不变性

C 考虑了模式的分布

正确答案是：A

**648题**
3、影响基本K-均值算法的主要因素有

A 样本输入顺序

B 模式相似性测度

C 聚类准则

正确答案是：B

**649题**
4、在统计模式分类问题中，当先验概率未知时，可以使用

A 最小损失准则

B 最小最大损失准则

C 最小误判概率准则

正确答案是：B

**650题**
5、如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有

A已知类别样本质量

B分类准则

C量纲

正确答案是：B





656题

选择Logistic回归中的One-Vs-All方法中的哪个选项是真实的。

A、我们需要在n类分类问题中适合n个模型

B、我们需要适合n-1个模型来分类为n个类

C、我们需要只适合1个模型来分类为n个类

D、这些都没有

正确答案是：A

解析：

答案：A如果存在n个类，那么n个单独的逻辑回归必须与之相适应，其中每个类的概率由剩余类的概率之和确定。

657题

假设有一个如下定义的神经网络：

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181128/c0ff08ad3dcfc5bd64d1792b4eec396c.jpg)](https://ask.julyedu.com/uploads/questions/20181128/c0ff08ad3dcfc5bd64d1792b4eec396c.jpg)

如果我们去掉ReLU层，这个神经网络仍能够处理非线性函数，这种说法是：

A、正确的

B、错误的

正确答案是： B

658题

假定特征 F1 可以取特定值：A、B、C、D、E 和 F，其代表着学生在大学所获得的评分。在下面说法中哪一项是正确的？

A、特征 F1 是名义变量（nominal variable）的一个实例。

B、特征 F1 是有序变量（ordinal variable）的一个实例。

C、该特征并不属于以上的分类。

D、以上说法都正确。

正确答案是： B

解析：

答案为（B）：有序变量是一种在类别上有某些顺序的变量。例如，等级 A 就要比等级 B 所代表的成绩好一些。

659题

下面哪个选项中哪一项属于确定性算法？

A、PCA

B、K-Means

C、以上都不是

正确答案是：A

解析：

答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。

660题

两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。

A、正确

B、错误

正确答案是：A

解析：

答案为（A）：Y=X2，请注意他们不仅仅相关联，同时一个还是另一个的函数。尽管如此，他们的相关性系数还是为 0，因为这两个变量的关联是正交的，而相关性系数就是检测这种关联。详情查看：

https://en.wikipedia.org/wiki/Anscomb

e's_quartet

661题

下面哪一项对梯度下降（GD）和随机梯度下降（SGD）的描述是正确的？

1 在 GD 和 SGD 中，每一次迭代中都是更新一组参数以最小化损失函数。

2 在 SGD 中，每一次迭代都需要遍历训练集中的所有样本以更新一次参数。

3 在 GD 中，每一次迭代需要使用整个训练集或子训练集的数据更新一个参数。

A、只有 1

B、只有 2

C、只有 3

D、都正确

正确答案是：A

解析：

答案为（A）：在随机梯度下降中，每一次迭代选择的批量是由数据集中的随机样本所组成，但在梯度下降，每一次迭代需要使用整个训练数据集。

662题

下面哪个/些超参数的增加可能会造成随机森林数据过拟合？

1 树的数量 

2 树的深度

3 学习速率

A、只有 1

B、只有 2

C、只有 3

D、都正确

正确答案是： B

解析：

答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率在随机森林中并不是超参数。增加树的数量可能会造成欠拟合。

663题

假如你在「Analytics Vidhya」工作，并且想开发一个能预测文章评论次数的机器学习算法。你的分析的特征是基于如作者姓名、作者在 Analytics Vidhya 写过的总文章数量等等。那么在这样一个算法中，你会选择哪一个评价度量标准？

1 均方误差

2 精确度

3 F1 分数

A、 只有 1

B、只有 2

C、只有 3

正确答案是：A

解析：

答案为（A）：你可以把文章评论数看作连续型的目标变量，因此该问题可以划分到回归问题。因此均方误差就可以作为损失函数的度量标准。

664题

给定以下三个图表（从上往下依次为1，2，3）， 哪一个选项对以这三个图表的描述是正确的？

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181130/2cff99d27016ac02393683d9b0a1bb2a.jpg)](https://ask.julyedu.com/uploads/questions/20181130/2cff99d27016ac02393683d9b0a1bb2a.jpg)

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181130/6594b4317f6340bc08c85d3425404178.jpg)](https://ask.julyedu.com/uploads/questions/20181130/6594b4317f6340bc08c85d3425404178.jpg)

[![3.jpg](https://ask.julyedu.com/uploads/questions/20181130/f037c400ca1e802c8524aae87f5e758c.jpg)](https://ask.julyedu.com/uploads/questions/20181130/f037c400ca1e802c8524aae87f5e758c.jpg)

A、1 是 tanh，2 是 ReLU，3 是 SIGMOID 激活函数

B、1 是 SIGMOID，2 是 ReLU，3 是 tanh 激活函数

C、1 是 ReLU，2 是 tanh，3 是 SIGMOID 激活函数

D、1 是 tanh，2 是 SIGMOID，3 是 ReLU 激活函数

正确答案是：D

解析：

答案为（D）：因为 SIGMOID 函数的取值范围是 [0,1]，tanh 函数的取值范围是 [-1,1]，RELU 函数的取值范围是 [0,infinity]。

665题

以下是目标变量在训练集上的 8 个实际值 [0,0,0,1,1,1,1,1]，目标变量的熵是所少？

A、-(5/8 log(5/8) + 3/8 log(3/8))

B、5/8 log(5/8) + 3/8 log(3/8)

C、3/8 log(5/8) + 5/8 log(3/8)

D、5/8 log(3/8) – 3/8 log(5/8)

正确答案是：A

666题

假定你正在处理类属特征，并且没有查看分类变量在测试集中的分布。现在你想将 one hot encoding（OHE）应用到类属特征中。那么在训练集中将 OHE 应用到分类变量可能要面临的困难是什么？

A、分类变量所有的类别没有全部出现在测试集中

B、类别的频率分布在训练集和测试集是不同的

C、训练集和测试集通常会有一样的分布

D、A 和 B 都正确

正确答案是：D

解析：

答案为（D）：A、B 项都正确，如果类别在测试集中出现，但没有在训练集中出现，OHE 将会不能进行编码类别，这将是应用 OHE 的主要困难。选项 B 同样也是正确的，在应用 OHE 时，如果训练集和测试集的频率分布不相同，我们需要多加小心。

667题

Skip gram 模型是在 Word2vec 算法中为词嵌入而设计的最优模型。以下哪一项描绘了 Skip gram 模型？

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181203/cda1c7a20cf1ec420bceb164f7a88d8f.jpg)](https://ask.julyedu.com/uploads/questions/20181203/cda1c7a20cf1ec420bceb164f7a88d8f.jpg)

A、A

B、B

C、A和B

D、以上都不是

正确答案是： B

解析：

答案为（B）：这两个模型都是在 Word2vec 算法中所使用的。模型 A 代表着 CBOW，模型 B 代表着 Skip gram。

668题

假定你在神经网络中的隐藏层中使用激活函数 X。在特定神经元给定任意输入，你会得到输出「-0.0001」。X 可能是以下哪一个激活函数？

A、ReLU

B、tanh

C、SIGMOID

D、以上都不是

正确答案是： B

解析：

答案为（B）：该激活函数可能是 tanh，因为该函数的取值范围是 (-1,1)。

669题

对数损失度量函数可以取负值。

A、对

B、错

正确答案是： B

解析：

答案为（B）：对数损失函数不可能取负值。

670题

下面哪个/些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是正确的？

类型 1 通常称之为假正类，类型 2 通常称之为假负类。 

类型 2 通常称之为假正类，类型 1 通常称之为假负类。 

类型 1 错误通常在其是正确的情况下拒绝假设而出现。

A、只有 1

B、只有 2

C、只有 3

D、1和3

正确答案是：D

解析：

答案为（E）：在统计学假设测试中，I 类错误即错误地拒绝了正确的假设（即假正类错误），II 类错误通常指错误地接受了错误的假设（即假负类错误）。

671题

假定你想将高维数据映射到低维数据中，那么最出名的降维算法是 PCA 和 t-SNE。现在你将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？

A、X_projected_PCA 在最近邻空间能得到解释

B、X_projected_tSNE 在最近邻空间能得到解释

C、两个都在最近邻空间能得到解释

D、两个都不能在最近邻空间得到解释

正确答案是： B

解析：

答案为（B）：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。

672题

给定下面两个特征的三个散点图（从左到右依次为图 1、2、3）

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181204/61db48e03ec5d7bca6cbc09364017e01.jpg)](https://ask.julyedu.com/uploads/questions/20181204/61db48e03ec5d7bca6cbc09364017e01.jpg)

在上面的图像中，哪一个是多元共线（multi-collinear）特征？

A、图 1 中的特征

B、图 2 中的特征

C、图 3 中的特征

D、图 1、2 中的特征

正确答案是：D

解析：

答案为（D）：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。

673题

在先前问题中，假定你已经鉴别了多元共线特征。那么下一步你可能的操作是什么？

1 移除两个共线变量

2 不移除两个变量，而是移除一个

3 移除相关变量可能会导致信息损失。为了保留这些变量，我们可以使用带罚项的回归模型（如 ridge 或 lasso regression）。

A、只有 1

B、只有 2

C、只有 3

D、2 或 3

正确答案是：D

解析：

答案为（D）：因为移除两个变量会损失一切信息，所以我们只能移除一个特征，或者也可以使用正则化算法（如 L1 和 L2）。

674题

给线性回归模型添加一个不重要的特征可能会造成：

1 增加 R-square

2 减少 R-square

A、只有 1 是对的

B、只有 2 是对的

C、1 或 2 是对的

D、都不对

正确答案是：A

解析：

答案为（A）：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。

675题

假设给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？

A、D1= C1, D2 < C2, D3 > C3

B、D1 = C1, D2 > C2, D3 > C3

C、D1 = C1, D2 > C2, D3 < C3

D、D1 = C1, D2 < C2, D3 < C3

E、D1 = C1, D2 = C2, D3 = C3

正确答案是：E

解析：

答案为（E）：特征之间的相关性系数不会因为特征加或减去一个数而改变。









681题

假设存在一个黑箱算法，其输入为有多个观察（t1, t2, t3,…….. tn）的训练数据和一个新的观察（q1）。该黑箱算法输出 q1 的最近邻 ti 及其对应的类别标签 ci。你可以将这个黑箱算法看作是一个 1-NN（1-最近邻）我们不使用 1-NN 黑箱，而是使用 j-NN(j>1) 算法作为黑箱。为了使用 j-NN 寻找 k-NN，下面哪个选项是正确的？

A、 j 必须是 k 的一个合适的因子

B、j>k

C、不能办到

正确答案是：A

解析：

用 1NN 实现 KNN 的话，每次找到最近邻，然后把这项从数据中取出来，重新运行 1NN 算法，这样重复 K 次，就行了。所以，少找多的话，少一定要是多的因子。

682题

有以下 7 副散点图（从左到右分别编号为 1-7），你需要比较每个散点图的变量之间的皮尔逊相关系数。下面正确的比较顺序是？

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181206/6ea57e13e4fc8c0767dba0d5b74cab83.jpg)](https://ask.julyedu.com/uploads/questions/20181206/6ea57e13e4fc8c0767dba0d5b74cab83.jpg)

1 1<2<3<4 

2 1>2>3 > 4 

3 7<6<5<4 

4 7>6>5>4

A、1 和 3

B、2 和 3

C、1 和 4

D、2 和 4

正确答案是： B

683题

你可以使用不同的标准评估二元分类问题的表现，例如准确率、log-loss、F-Score。让我们假设你使用 log-loss 函数作为评估标准。下面这些选项，哪个／些是对作为评估标准的 log-loss 的正确解释。

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181206/472e15680cb5b138576945842e31f212.jpg)](https://ask.julyedu.com/uploads/questions/20181206/472e15680cb5b138576945842e31f212.jpg)

1、如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它。 

2、对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大。 

3、log-loss 越低，模型越好。

A、1 和 3

B、2 和 3

C、1 和 2

D、1、2、3

正确答案是：D

684题

假设你被给到以下数据，你想要在给定的两个类别中使用 logistic 回归模型对它进行分类。

[![3.jpg](https://ask.julyedu.com/uploads/questions/20181206/7c6d194b839918c5acb6585211bd6bc8.jpg)](https://ask.julyedu.com/uploads/questions/20181206/7c6d194b839918c5acb6585211bd6bc8.jpg)

你正在使用带有 L1 正则化的 logistic 回归，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。

[![4.jpg](https://ask.julyedu.com/uploads/questions/20181206/fd10c3565acdc745526a5b4a45111e05.jpg)](https://ask.julyedu.com/uploads/questions/20181206/fd10c3565acdc745526a5b4a45111e05.jpg)

当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？

A、第一个 w2 成了 0，接着 w1 也成了 0

B、第一个 w1 成了 0，接着 w2 也成了 0

C、w1 和 w2 同时成了 0

D、即使在 C 成为大值之后，w1 和 w2 都不能成 0

正确答案是： B

解析：

答案（B）：通过观察图像我们发现，即使只使用 x2，我们也能高效执行分类。因此一开始 w1 将成 0；当正则化参数不断增加时，w2 也会越来越接近 0。

685题

假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。注意：所有其他超参数是相同的，所有其他因子不受影响。

1 深度为 4 时将有高偏差和低方差 

2 深度为 4 时将有低偏差和低方差

A、只有 1

B、只有 2

C、1 和 2

D、没有一个

正确答案是：A

解析：

答案（A)：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。

686题

在 k-均值算法中，以下哪个选项可用于获得全局最小？

1 尝试为不同的质心（centroid）初始化运行算法

2 调整迭代的次数 

3 找到集群的最佳数量

A、2 和 3

B、1 和 3

C、1 和 2

D、以上所有

正确答案是：D

解析：

答案（D）：所有都可以用来调试以找到全局最小。

687题

假设你正在做一个项目，它是一个二元分类问题。你在数据集上训练一个模型，并在验证数据集上得到混淆矩阵。基于上述混淆矩阵，下面哪个选项会给你正确的预测。

1 精确度是~0.91 

2 错误分类率是~0.91 

3 假正率（False correct classification）是~0.95 

4 真正率（True positive rate）是~0.95

A、1 和 3

B、2 和 4

C、1 和 4

D、2 和 3

正确答案是：C

解析：

答案（C）：精确度（正确分类）是 (50+100)/165，约等于 0.91。真正率是你正确预测正分类的次数，因此真正率将是 100/105 = 0.95，也被称作敏感度或召回。

688题

对于下面的超参数来说，更高的值对于决策树算法更好吗？

1 用于拆分的样本量 

2 树深 

3 树叶样本

A、1 和 2

B、2 和 3

C、1 和 3

D、1、2 和 3

E、无法分辨

正确答案是：E

解析：

答案（E）：对于选项 A、B、C 来说，如果你增加参数的值，性能并不一定会提升。例如，如果我们有一个非常高的树深值，结果树可能会过拟合数据，并且也不会泛化。另一方面，如果我们有一个非常低的值，结果树也许与数据欠拟合。因此我们不能确定更高的值对于决策树算法就更好。

689题

想象一下，你有一个 28x28 的图片，并使用输入深度为 3 和输出深度为 8 在上面运行一个 3x3 的卷积神经网络。注意，步幅padding是1，你正在使用相同的填充（padding）。当使用给定的参数时，输出特征图的尺寸是多少？

A、28 宽、28 高、8 深

B、13 宽、13 高、8 深

C、28 宽、13 高、8 深

D、13 宽、28 高、8 深

正确答案是：A

解析：

答案（A）

计算输出尺寸的公式是：输出尺寸=(N – F)/S + 1。其中，N 是输入尺寸，F 是过滤器尺寸，S 是步幅。更多可阅读这篇文章（链接：

https://www.analyticsvidhya.co ... orks/

）获得更多了解。

690题

假设，我们正在 SVM 算法中为 C（惩罚参数）的不同值进行视觉化绘图。由于某些原因，我们忘记了使用视觉化标注 C 值。这个时候，下面的哪个选项在 rbf 内核的情况下最好地解释了下图（1、2、3 从左到右，图 1 的 C 值 是 C 1，图 2 的 C 值 是 C 2，图 3 的 C 值 是 C 3）中的 C 值。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181207/5c257f9bb9b29b9df82246b6a4925227.jpg)](https://ask.julyedu.com/uploads/questions/20181207/5c257f9bb9b29b9df82246b6a4925227.jpg)

A、C1 = C2 = C3

B、C1 > C2 > C3

C、C1 < C2 < C3

D、没有一个

正确答案是：C

解析：

答案 (C)：错误项的惩罚参数 C。它也控制平滑决策边界和训练点正确分类之间的权衡。对于 C 的大值，优化会选择一个较小边距的超平面。更多内容：

https://www.analyticsvidhya.co ... code/

691题

假设有如下一组输入并输出一个实数的数据，则线性回归（Y = bX+c）的留一法交叉验证均方差为？

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181211/9d22cd7d69f32d7b003a4c6a77254d2c.jpg)](https://ask.julyedu.com/uploads/questions/20181211/9d22cd7d69f32d7b003a4c6a77254d2c.jpg)

A、10/27

B、20/27

C、50/27

D、49/27

正确答案是：D

解析：

我们需要计算每个交叉验证点的残差，拟合后得到两点连线和一点用于交叉验证。

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181211/57949ac03d11b27c2c4839d617f0f8e0.jpg)](https://ask.julyedu.com/uploads/questions/20181211/57949ac03d11b27c2c4839d617f0f8e0.jpg)

留一法交叉验证均方差为(2^2 +(2/3)^2 +1^2) /3 = 49/27

692题

下列哪一项关于极大似然估计（MLE）的说法是正确的？

1.MLE并不总是存在

2.MLE一直存在

3.如果MLE存在，它可能不特异

4.如果MLE存在，它一定是特异的

A、1和4

B、2和3

C、1和3

D、2和4

正确答案是：C

解析：

MLE可能不是一个转折点，即它可能不是一个似然函数的一阶导数消失的点

[![3.jpg](https://ask.julyedu.com/uploads/questions/20181211/e6249e76da1200b2ad78dfa8759dab59.jpg)](https://ask.julyedu.com/uploads/questions/20181211/e6249e76da1200b2ad78dfa8759dab59.jpg)

MLE可能并不特异

[![4.jpg](https://ask.julyedu.com/uploads/questions/20181211/75b5f0f95b1fe4bbae2eb129ca3b559e.jpg)](https://ask.julyedu.com/uploads/questions/20181211/75b5f0f95b1fe4bbae2eb129ca3b559e.jpg)

693题

假设线性回归模型完美拟合训练数据（即训练误差为零），则下列哪项是正确的？

A、测试误差一定为零

B、测试误差一定不为零

C、以上都不对

正确答案是：C

解析：

如果测试数据无干扰，则测试误差可能为零。换言之，如果测试数据是训练数据的典型代表，测试误差即为零，但这种情况并不总是出现。

694题

在线性回归问题中，我们用“R方”来衡量拟合的好坏。在线性回归模型中增加特征值并再训练同一模型。下列哪一项是正确的？

A、如果R方上升，则该变量是显著的

B、如果R方下降，则该变量不显著

C、单单R方不能反映变量重要性，不能就此得出正确结论

D、都不正确

正确答案是：C

解析：

单单R方不能表示变量显著性，因为每次加入一个特征值，R方都会上升或维持不变。但在“调整R方”的情况下这也有误（如果特征值显著的话，调整R方会上升）。

695题

下列关于回归分析中的残差表述正确的是

A、残差的平均值总为零

B、残差的平均值总小于零

C、残差的平均值总大于零

D、残差没有此类规律

正确答案是：A

解析：

回归的残差之和一定为零，故而平均值也为零





701题

一个人年龄和健康之间的相关系数是-1.09，据此可以得出：

A、年龄是健康预测的好的参考量

B、年龄是健康预测的不好的参考量

C、都不对

正确答案是：C

解析：

相关系数的范围是[-1,1]，-1.09 是不可能的。

702题

下列哪个坐标用于最小二乘拟合？假设水平轴为自变量，垂直轴为因变量。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181213/bfdc3c43380cc4fad32caad8e36c83d3.jpg)](https://ask.julyedu.com/uploads/questions/20181213/bfdc3c43380cc4fad32caad8e36c83d3.jpg)

A、垂直坐标

B、正交坐标

C、都可以，视情况而定

D、都不对

正确答案是：A

解析：

一般将残差想作垂直坐标，正交坐标在PCA的例子中很有用

703题

假设我们有由三次多项式回归产生的数据（三次多项式完美契合数据），下列说法哪些是对的？

1.简单线性回归有高偏差和低方差

2.简单线性回归有低偏差和高方差

3.三次多项式有低偏差和高方差

4.三次多项式有低偏差和低方差

A、1

B、1和3

C、1和4

D、2和3

正确答案是：C

解析：

如果选择比3更高的次数来拟合，则会过拟合，因为模型会变得更复杂。如果选择比3低的次数，模型会变得简单，这样会有高偏差和低方差。但是在三次多项式拟合的情况下，偏差和方差都是低的。

704题

假设你在训练一个线性回归模型，以下哪项是正确的？

1.数据越少越易过拟合

2.假设区间小则易过拟合

A、都是错的

B、1是错的，2是对的

C、1是对的，2是错的

D、都是对的

正确答案是：C

解析：

1.小训练数据集更容易找到过拟合训练数据的假设，对于泛化能力，小数据集很难训练处泛化能力强的学习器。

2.从偏差和方差的权衡中可以看出，假设区间小，偏差更大，方差更小。所以在小假设区间的情况下，不太可能找到欠拟合数据的假设。

705题

假设我们用Lasso回归拟合一个有100个特征值(X1,X2…X100)的数据集，现在，我们重新调节其中一个值，将它乘10（将它视作X1），并再次拟合同一规则化参数。下列哪一项正确？

A、X1很可能被模型排除

B、X1很可能被包含在模型内

C、很难说

D、都不对

正确答案是： B

解析：

大特征值=⇒小相关系数=⇒更少lasso penalty =⇒更可能被保留



706题

关于Ridge和Lasso回归在特征值选择上的方法，一下哪项正确？

A、Ridge回归使用特征值的子集选择

B、Lasso回归使用特征值的子集选择

C、二者都使用特征值的子集选择

D、以上都不正确

正确答案是： B

解析：

Ridge回归在最终模型中用到了所有自变量，然而Lasso回归可被用于特征值选择，因为相关系数可以为零。

707题

在线性回归模型中增加一个变量，下列哪一项是正确的？

1.R方和调整R方都上升

2.R方上升，调整R方下降

3.R方和调整R方都下降

4.R方下降，调整R方上升

A、1和2

B、1和3

C、2和4

D、以上都不对

正确答案是：A

解析：

每次加入特征值，R方总是上升或维持不变。但调整R方并非如此，当它上升时，特征值是显著的。

708题

下图显示了对相同训练数据的三种不同拟合模型（蓝线标出），从中可以得出什么结论？

1.同第二第三个模型相比，第一个模型的训练误差更大

2.该回归问题的最佳模型是第三个，因为它有最小的训练误差

3.第二个模型比第一、第三个鲁棒性更好，因为它在处理不可见数据方面表现更好

4.相比第一、第二个模型，第三个模型过拟合了数据

5.因为我们尚未看到测试数据，所以所有模型表现一致

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181214/fdd7c4a56ff66eaab130df598ff58a80.jpg)](https://ask.julyedu.com/uploads/questions/20181214/fdd7c4a56ff66eaab130df598ff58a80.jpg)

A、1和3

B、1和2

C、1、3和4

D、只有5

正确答案是：C

解析：

数据的趋势看起来像以X 为自变量的二项式。更高的次数（最右边的图）的多项式对于训练数据可能具有更高的准确性，但在测试集上毫无疑问的惨败。在最左面一张图中，由于数据欠拟合，将会得到最大训练误差。

709题

下列哪项可以评价回归模型？

1.R方

2.调整R方

3.F统计量

4.RMSE/MSE/MAE

A、2和4

B、1和2

C、2,3和4

D、以上所有

正确答案是：D

解析：

以上这些都是评价回归模型的指标

710题

我们可以通过一种叫“正规方程”的分析方法来计算线性回归的相关系数，下列关于“正规方程”哪一项是正确的？

1.我们不必选择学习比率

2.当特征值数量很大时会很慢

3.不需要迭代

A、1和2

B、1和3

C、2和3

D、1,2和3

正确答案是：D

解析：

正规方程可替代梯度下降来计算相关系数，参考这篇文章获得更多关于正规方程的知识：

http://eli.thegreenplace.net/2 ... sion/

711题

Y的预期值是关于变量X(X1,X2….Xn)的线性函数，回归线定义为Y = β0 + β1 X1 + β2 X2……+ βn Xn，下列陈述哪项正确？

1、如果Xi的变化量为 ∆Xi，其它为常量，则Y的变化量为βi ∆Xi，常量βi可以为正数或负数

2、βi 的值都是一样的，除非是其它X的βi

3、X对Y预期值的总影响为每个分影响之和提示：特征值间相互独立，互不干扰

A、1和2

B、1和3

C、2和3

D、1,2和3

正确答案是：D

解析：

Y的预期值是关于X的线性函数，这表示： 

如果Xi的变化量为 ∆Xi，其它变量不变，Y的预期值随β i ∆X i而变化，β i可以为正数或负数 

βi 的值都是一样的，除非是其它X的βi 

X对Y预期值的总影响为每个分影响之和 

Y的未知变化独立于随机变量（特别之处，当随机变量为时间序列时，Y与随机变量不是自动关联的） 

它们的方差一致（同方差性） 

它们一般是分散的。

712题

为了评价一个简单线性回归模型（单自变量），需要多少个参数？

A、1

B、2

C、不确定

正确答案是： B

解析：

在简单线性回归模型中，有一个自变量，需要两个参数（Y=a+bX）

713题

下图展示了两条对随机生成的数据的回归拟合线（A和B），请探究A，B的各自的残差之和

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181218/c308bb2f7ce1306fb7aa48d80d33aa26.jpg)](https://ask.julyedu.com/uploads/questions/20181218/c308bb2f7ce1306fb7aa48d80d33aa26.jpg)

提示： 

1.两张图的横纵轴大小一致

2.X轴是自变量，Y 轴是因变量下列对A，B各自残差和的陈述哪项正确？

A、A比B高

B、A比B低

C、两者相同

D、以上都不对

正确答案是：C

解析：

残差之和总为零

714题

若两个变量相关，它们之间一定有线性关系吗？

A、是

B、否

正确答案是： B

解析：

不是必要条件，二者可以没有线性关系

715题

相关变量的相关系数可以为零，对吗？

A、是

B、否

正确答案是：A

716题

假设对数据提供一个逻辑回归模型，得到训练精度X和测试精度Y。在数据中加入新的特征值，则下列哪一项是正确的？ 

提示：其余参数是一样的

A、训练精度总是下降

B、训练精度总是上升或不变

C、测试精度总是下降

D、测试精度总是上升或不变

正确答案是： B

解析：

向模型中加入更多特征值会提高训练精度，低偏差；如果特征值是显著的，测试精度会上升

717题

下图显示了由X预测Y的回归线，图上的值展示了每个预期的离差，请据此计算SSE（残差平方和）

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181218/61649c656b88c87fcc842dfbc42e8908.jpg)](https://ask.julyedu.com/uploads/questions/20181218/61649c656b88c87fcc842dfbc42e8908.jpg)

A、3.02

B、0.75

C、1.01

D、以上都不对

正确答案是：A

解析：

SSE是预估误差的平方之和，所以SSE = (-.2)^2 + (.4)^2 + (-.8)^2 + (1.3)^2 + (-.7)^2 = 3.02

718题

众所周知，身高体重呈正相关。忽略图表大小（变量被标准化了）下列两张图哪张更像描绘身高（X轴）体重（Y轴）的图表？

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181218/7bc1280228a5da1783ec0e1455717a0e.jpg)](https://ask.julyedu.com/uploads/questions/20181218/7bc1280228a5da1783ec0e1455717a0e.jpg)

A、图2

B、图1

C、两张都是

D、无法确定

正确答案是：A

解析：

图2很明显更好的展现了身高体重之间的联系，个体身高更高，体积就越大，体重就相应越大，所以预期身高体重是正相关的。右图是正相关而左图是负相关。

719题

假设X公司的员工收入分布中位数为$35,000，25%和75%比例处的数值为$21,000 和$53,000。收入$1会被认为是异常值吗？

A、是

B、否

C、需要更多信息

D、以上都不对

正确答案是：C

720题

关于回归和相关，下列哪项是正确的？ 

提示：y是因变量，x是自变量

A、在两者中，x、y关系都是对称的

B、在两者中，x、y关系都是不对称的

C、x、y在相关情况下不对称，在回归中对称

D、x、y在相关情况下对称，在回归中不对称

正确答案是：D

解析：

1.相关是衡量两个变量线性联系的统计度量，对待x、y是对称的

2.回归是用于根据x预测y，其关系不对称

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181218/0f6ef3283db7ba85ea711e4f680cb2dd.jpg)](https://ask.julyedu.com/uploads/questions/20181218/0f6ef3283db7ba85ea711e4f680cb2dd.jpg)

726题

关于Ridge回归，下列哪项正确？

1.lambda为0时，模型作用类似于线性回归模型

2.lambda为0时，模型作用与线性回归模型不相像

3.当lambda趋向无穷，会得到非常小，趋近0的相关系数 

4.当lambda趋向无穷，会得到非常大，趋近无穷的相关系数

A、1和3

B、1和4

C、2和3

D、2和4

正确答案是：A

解析：

当lambda为0时我们得到了最小的最小二乘解；当lambda趋近无穷时，会得到非常小、趋近0的相关系数。

727题

下列三张残差图，哪张与其它相比是最糟糕的模型？

[![1.jpg](https://ask.julyedu.com/uploads/questions/20181219/5cdb9efe3f2169f7795aa23f678f72bb.jpg)](https://ask.julyedu.com/uploads/questions/20181219/5cdb9efe3f2169f7795aa23f678f72bb.jpg)

提示： 

1.所有残差都被标准化

2.这些图是关于预期值和残差的

A、1

B、2

C、3

D、1和2

正确答案是：C

解析：

预期值和残差之间应该没有任何关系，若果有则说明模型未能完美捕获数据信息。

728题

下列哪一种回归方法的相关系数没有闭式解？

A、Ridge回归

B、Lasso回归

C、Ridge回归 and Lasso回归

D、两者都不是

正确答案是： B

解析：

Lasso不允许闭式解，L1-penalty使解为非线性的，所以需要近似解。参考链接获得更多关于闭式解的知识：

http://statweb.stanford.edu/~t ... n.pdf

729题

参考如下数据集，移除哪一个黑点将会对回归拟合线（黑虚线所示）产生最大影响

[![2.jpg](https://ask.julyedu.com/uploads/questions/20181219/d97c0e20177b139a1b145d4aa4552174.jpg)](https://ask.julyedu.com/uploads/questions/20181219/d97c0e20177b139a1b145d4aa4552174.jpg)

A、a

B、b

C、c

D、d

正确答案是：D

解析：

线性回归对数据中的异常值敏感，虽然C也是给定数据区间内的异常值，但它离回归拟合线很近，所以不会造成太多影响

730题

在简单线性回归模型中（单自变量），如果改变输入变量1单元，输出变量会变化多少？

A、1单元

B、无变化

C、截距值

D、斜率值

正确答案是：D

解析：

简单线性回归公式为Y=a+bx，如果给x增加1，y就变成了a+b(x+1)，即y增加了b

731题

逻辑回归是输出结果落在[0,1]区间内，下列哪个函数用于转换概率，使其落入[0,1]？

A、Sigmoid

B、Mode

C、Square

D、Probit

正确答案是：A

解析：

Sigmoid函数用于转换输出结果，使之落在逻辑回归区间[0,1]内

732题

考虑线性回归和逻辑回归中的重量/相关系数，关于cost函数的偏导，下列哪一项是正确的？

A、都不一样

B、都一样

C、无法确定

D、以上都不对

正确答案是： B

解析：

参考这个链接：

http://feature-space.com/2011/ ... tive/

733题

假设使用逻辑回归模型处理n元分类问题，可以用到One-vs-rest方法，则下列哪一项是正确的？

A、在n元分类问题中，需要拟合n个模型

B、为了分类为n类，需要拟合n-1个模型

C、为了分类为n类，只需要拟合1个模型

D、都不正确

正确答案是：A

解析：

如果有n类，就有n个分散的逻辑回归需要拟合，每一类的概率都是基于其余类来预测的。以三类分类(-1,0,1)为例，需要训练三个逻辑回归分类器：1. -1 vs 0 and 12.0 vs -1 and 13.1 vs 0 and -1

734题

下图是两种有不同β0 和β1值的逻辑回归模型，下列关于两种逻辑回归模型中β0 和β1的叙述哪项是正确的？

[![1.jpg](https://ask.julyedu.com/uploads/questions/20190102/a5249188d4810851d2038578def25930.jpg)](https://ask.julyedu.com/uploads/questions/20190102/a5249188d4810851d2038578def25930.jpg)

提示：Y = β0 β1*X，β0为截距，β1是斜率

A、绿线的β1比黑线的大

B、绿线的β1比黑线的小

C、两个模型的β1是一样的

D、无法得出结论

正确答案是： B

解析：

β0和β1: β0 = 0, β1 = 1是黑线的情况；β0 = 0, β1 = −1是绿线的情况我们希望你能发现这个测试和提供的解决方法有趣而实用。这个测试注重回归的理论知识和它的多种技巧。我们试着通过这篇文章解释你们的所有疑惑，如果你发现了任何错误和遗漏，或者你有好的建议，请和我们联系。

735题

下列哪一个项对欠拟合和过拟合的权衡影响最大？

A、多项式次数

B、是否通过矩阵倒置或梯度下降来学习权重

C、使用常数项

正确答案是：A

解析：

答案：A选择正确的多项式次数在回归拟合中扮演重要角色，如果选择的次数太高，过拟合的可能性将大大提高。

  **736题**

假设有如下一组输入并输出一个实数的数据，则线性回归（Y = bX+c）的留一法交叉验证均方差为？

A、10/27

B、20/27

C、50/27

D、49/27

正确答案是：D

解析：

我们需要计算每个交叉验证点的残差，拟合后得到两点连线和一点用于交叉验证。

留一法交叉验证均方差为(2^2 +(2/3)^2 +1^2) /3 = 49/27。

**737题**

下列哪一项关于极大似然估计（MLE）的说法是正确的？

1.MLE并不总是存在

2.MLE一直存在 

3.如果MLE存在，它可能不特异 

4.如果MLE存在，它一定是特异的

A、1 and 4

B、2 and 3

C、1 and 3

D、2 and 4

正确答案是：C

解析：

答案：CMLE可能不是一个转折点，即它可能不是一个似然函数的一阶导数消失的点。MLE可能并不特异

**738题**

假设线性回归模型完美拟合训练数据（即训练误差为零），则下列哪项是正确的？

A、测试误差一定为零

B、测试误差一定不为零

C、以上都不对

正确答案是：C

解析：

答案：C如果测试数据无干扰，则测试误差可能为零。换言之，如果测试数据是训练数据的典型代表，测试误差即为零，但这种情况并不总是出现。

**739题**

在线性回归问题中，我们用R方“R-squared”来衡量拟合的好坏。在线性回归模型中增加特征值并再训练同一模型。下列哪一项是正确的？

A、如果R方上升，则该变量是显著的

B、如果R方下降，则该变量不显著

C、单单R方不能反映变量重要性，不能就此得出正确结论

D、都不正确

正确答案是：C

解析：

答案：C单单R方不能表示变量显著性，因为每次加入一个特征值，R方都会上升或维持不变。但在“调整R方”的情况下这也有误（如果特征值显著的话，调整R方会上升）。

**740题**

下列关于回归分析中的残差表述正确的是：

A、残差的平均值总为零

B、残差的平均值总小于零

C、残差的平均值总大于零

D、残差没有此类规律

正确答案是：C

解析：

答案：C残差总是为正的，BD不正确.因此回归的残差之和一般情况总是大于为零，故而平均值也大于零。因为模型很少情况下做到完全拟合，A不正确。

  **741题**

下列哪一项说明了X，Y之间的较强关系？

A、相关系数为0.9

B、Beta系数为0的空假设的p-value是0.0001

C、Beta系数为0的空假设的t统计量是30

D、都不对

正确答案是：A

解析：

答案：A变量间的相关系数为0说明了变量间的较强关系；另一方面，p-value和t统计量仅仅衡量了非零联系的证据有多强。在数据足够多的情况下，哪怕弱影响都可能是显著的。

**742题**

在导出线性回归的参数时，我们做出下列哪种假定？

1.因变量y和自变量x的真实关系是线性的

2.模型误差是统计独立的

3.误差通常服从一个平均值为零，标准差恒定的分布 

4.自变量x是非随机的，无错的

A、1,2 and 3

B、1,3 and 4

C、1 and 3

D、以上都对

正确答案是：D

答案：D当导出回归参数时，我们做出以上全部4种假设，缺少任何一种，模型都会出错。

**743题**

为了检验连续变量x，y之间的线性关系，下列哪种图最合适？

A、散点图

B、条形图

C、直方图

D、都不对

正确答案是：A

解析：

答案：A为了检验连续变量的线性关系，散点图是最好的选择，可以看出一个变量如何关于另一个变量变化。散点图反映两个定量变量之间的关系。

年龄是健康预测的不好的参考量

**744题**

下列哪种方法被用于预测因变量？

1.线性回归

2.逻辑回归

A、1和

B、1

C、2

D、都不是

正确答案是： B

解析：

逻辑回归是用于分类问题的，Regression term is misleading here。

**745题**

一个人年龄和健康之间的相关系数是-1.09，据此可以得出：

A、年龄是健康预测的好的参考量

B、年龄是健康预测的不好的参考量

C、都不对

正确答案是：C

解析：

答案：C相关系数的范围是[-1,1]，-1.09 是不可能的。

746题

下列哪个坐标用于最小二乘拟合？假设水平轴为自变量，垂直轴为因变量。

[![1.jpg](https://ask.julyedu.com/uploads/questions/20190104/3c5934c1e954d17a4781ca18104b0160.jpg)](https://ask.julyedu.com/uploads/questions/20190104/3c5934c1e954d17a4781ca18104b0160.jpg)

A、垂直坐标

B、正交坐标

C、都可以，视情况而定

D、都不对

正确答案是：A

解析：

答案：A一般将残差想作垂直坐标，正交坐标在PCA的例子中很有用

747题

假设你在训练一个线性回归模型，以下哪项是正确的？

1.数据越少越易过拟合

2.假设区间小则易过拟合

A、都是错的

B、1错2对

C、1对2错

D、都是对的

正确答案是：C

解析：

1.小训练数据集更容易找到过拟合训练数据的假设，对于泛化能力，小数据集很难训练处泛化能力强的学习器。2.从偏差和方差的权衡中可以看出，假设区间小，偏差更大，方差更小。所以在小假设区间的情况下，不太可能找到欠拟合数据的假设。

748题

假设我们用Lasso回归拟合一个有100个特征值(X1,X2…X100)的数据集，现在，我们重新调节其中一个值，将它乘10（将它视作X1），并再次拟合同一规则化参数。下列哪一项正确？

A、X1很可能被模型排除

B、X1很可能被包含在模型内

C、很难说

D、都不对

正确答案是： B

解析：

答案：B大特征值= 小相关系数= 更少lasso penalty = 更可能被保留

749题

下图显示了对相同训练数据的三种不同拟合模型（蓝线标出），从中可以得出什么结论？

[![2.jpg](https://ask.julyedu.com/uploads/questions/20190104/5eeb6ede65cbc3542da7659197bd21db.jpg)](https://ask.julyedu.com/uploads/questions/20190104/5eeb6ede65cbc3542da7659197bd21db.jpg)

1.同第二第三个模型相比，第一个模型的训练误差更大

2.该回归问题的最佳模型是第三个，因为它有最小的训练误差 

3.第二个模型比第一、第三个鲁棒性更好，因为它在处理不可见数据方面表现更好 

4.相比第一、第二个模型，第三个模型过拟合了数据 

5.因为我们尚未看到测试数据，所以所有模型表现一致

A、1和3

B、1和2

C、1,3和4

D、只有5

正确答案是：C

解析：

答案C数据的趋势看起来像以X 为自变量的二项式。更高的次数（最右边的图）的多项式对于训练数据可能具有更高的准确性，但在测试集上毫无疑问的惨败。在最左面一张图中，由于数据欠拟合，将会得到最大训练误差。

750题

下列哪项可以评价回归模型？

1.R方R Squared

2.调整R方 

3.F统计量 

4.RMSE/MSE/MAE

A、2和4

B、1和2

C、2,3和4

D、以上所有

正确答案是：D

解析：

答案：D以上这些都是评价回归模型的指标



















































































 





