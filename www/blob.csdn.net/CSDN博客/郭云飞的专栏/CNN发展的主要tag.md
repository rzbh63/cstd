
# CNN发展的主要tag - 郭云飞的专栏 - CSDN博客


2017年09月30日 17:28:57[guoyunfei20](https://me.csdn.net/guoyunfei20)阅读数：280个人分类：[机器学习																](https://blog.csdn.net/guoyunfei20/article/category/7178819)


**一、AlexNet之前的世界**
![](https://img-blog.csdn.net/20171031133648966?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
上图所示是刘昕博士总结的[CNN结构演化的历史](https://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&mid=2650324619&idx=1&sn=ca1aed9e42d8f020d0971e62148e13be&scene=1&srcid=0503De6zpYN01gagUvn0Ht8D#wechat_redirect)，起点是神经认知机模型，此时已经出现了卷积结构，经典的LeNet诞生于1998年。然而之后CNN的锋芒开始被SVM等手工设计的特征盖过。随着ReLU和dropout的提出，以及GPU和大数据带来的历史机遇，CNN在2012年迎来了历史突破–AlexNet。
CNN的演化路径可以总结为以下几个方向：

从LeNet到AlexNet
进化之路一：网络结构加深
进化之路二：加强卷积功能
进化之路三：从分类到检测
进化之路四：新增功能模块
AlexNet 之所以能够成功，深度学习之所以能够重回历史舞台，原因在于：
非线性激活函数：ReLU
防止过拟合的方法：Dropout，Data augmentation
大数据训练：百万级ImageNet图像数据
其他：GPU实现，LRN（Local Responce Normalization）的使用
以上部分重点致敬AlexNet所做的贡献！接下来逐一介绍。
**二、AlexNet之后的世界**
![](https://img-blog.csdn.net/20170930181727201?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
=======================================================
**AlexNet：**
一切都从这里开始（尽管有些人会说是Yann LeCun 1998年发表的那篇论文才真正开启了一个时代）。这篇论文，题目叫做*“ImageNet Classification with Deep Convolutional Networks”*，迄今被引用*6184*次，被业内普遍视为行业最重要的论文之一。Alex Krizhevsky、Ilya Sutskever和 Geoffrey Hinton创造了一个“大型的深度卷积神经网络”，赢得了*2012 ILSVRC*（2012年ImageNet 大规模视觉识别挑战赛）。这个比赛被誉为计算机视觉的年度奥林匹克竞赛，全世界的团队相聚一堂，看看是哪家的视觉模型表现最为出色。2012年是*CNN*首次实现Top 5误差率15.4%的一年（Top 5误差率是指给定一张图像，其标签不在模型认为最有可能的5个结果中的几率），当时的次优项误差率为26.2%。这个表现不用说震惊了整个计算机视觉界。可以说，是自那时起，CNN才成了家喻户晓的名字。
论文中，作者讨论了网络的架构（名为*AlexNet*）。相比现代架构，他们使用了一种相对简单的布局，整个网络由5层卷积层组成，最大池化层、退出层（dropout layer）和3层全卷积层。网络能够对1000种潜在类别进行分类。
**要点：**
>> 使用ImageNet数据训练网络，ImageNet数据库含有1500多万个带标记的图像，超过2.2万个类别。
>> 使用*ReLU*代替传统正切函数引入非线性（ReLU比传统正切函数快几倍，缩短训练时间）。
>> 使用了图像转化（image translation）、水平反射（horizontal reflection）和补丁提取（patch extraction）这些**数据增强技术**。
>> 用**dropout**层应对训练数据过拟合的问题。
>> 使用批处理随机梯度下降训练模型，注明动量衰减值和权重衰减值。
>> 使用两台GTX 580 GPU，训练了5到6天

=======================================================
**ZF Net：**
2012年AlexNet出尽了风头，**ILSVRC 2013**就有一大批CNN模型冒了出来。2013年的冠军是纽约大学Matthew Zeiler 和 Rob Fergus设计的网络*ZF Net*，错误率**11.2%**。ZF Net模型更像是AlexNet架构的微调优化版，但还是提出了有关优化性能的一些关键想法。还有一个原因，这篇论文写得非常好，论文作者花了大量时间阐释有关卷积神经网络的直观概念，展示了将滤波器和权重可视化的正确方法。
在这篇题为*“Visualizing and Understanding Convolutional Neural Networks”*的论文中，Zeiler和Fergus从大数据和GPU计算力让人们重拾对CNN的兴趣讲起，讨论了研究人员对模型内在机制知之甚少，一针见血地指出“发展更好的模型实际上是不断试错的过程”。虽然我们现在要比3年前知道得多一些了，但论文所提出的问题至今仍然存在！这篇论文的主要贡献在于提出了一个比AlexNet稍微好一些的模型并给出了细节，还提供了一些制作可视化特征图值得借鉴的方法。
![](https://img-blog.csdn.net/20170930174221289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
**要点：**

1. 除了一些小的修改，整体架构非常类似AlexNet。
2. AlexNet训练用了1500万张图片，而ZFNet只用了130万张。
3. AlexNet在第一层中使用了大小为11×11的滤波器，而ZF使用的滤波器大小为7x7，整体处理速度也有所减慢。做此修改的原因是，对于输入数据来说，第一层卷积层有助于保留大量的原始象素信息。11×11的滤波器漏掉了大量相关信息，特别是因为这是第一层卷积层。
4. 随着网络增大，使用的滤波器数量增多。
5. 利用ReLU的激活函数，将交叉熵代价函数作为误差函数，使用批处理随机梯度下降进行训练。
6. 使用一台GTX 580 GPU训练了12天。
7. 开发可视化技术“解卷积网络”（Deconvolutional Network），有助于检查不同的特征激活和其对输入空间关系。名字之所以称为“deconvnet”，是因为它将特征映射到像素（与卷积层恰好相反）。

=======================================================
**VGG Net：**
简单、有深度，这就是2014年错误率**7.3%**的模型VGG Net（不是ILSVRC 2014冠军）。牛津大学的Karen Simonyan 和 Andrew Zisserman Main Points创造了一个19层的CNN，**严格使用3x3**的过滤器（stride =1，pad= 1）和**2x2 max pooling**层（stride =2）。
**要点：**

这里使用3x3的滤波器和AlexNet在第一层使用11x11的滤波器和ZF Net 7x7的滤波器作用完全不同。作者认为两个3x3的卷积层组合可以实现5x5的有效感受野。这就在保持滤波器尺寸较小的同时模拟了大型滤波器，减少了参数。此外，有两个卷积层就能够使用两层ReLU。
3卷积层具有7x7的有效感受野。
每个max pool层后滤波器的数量增加一倍。进一步加强了缩小空间尺寸，但保持深度增长的想法。
图像分类和定位任务都运作良好。
使用Caffe工具包建模。
训练中使用scale jittering的数据增强技术。
每层卷积层后使用ReLU层和批处理梯度下降训练。
使用4台英伟达Titan Black GPU训练了两到三周。

=======================================================
**GoogLeNet：**
GoogLeNet是一个**22**层的卷积神经网络，在2014年的ILSVRC2014上凭借*6.7%*的错误率进入Top 5。据我所知，这是第一个真正不使用通用方法的卷积神经网络架构，传统的卷积神经网络的方法是**简单堆叠卷积层**，然后把各层以序列结构堆积起来。论文的作者也强调，这种新的模型重点考虑了内存和能量消耗。这一点很重要，我自己也会经常忽略：把所有的层都堆叠、增加大量的滤波器，在计算和内存上消耗很大，过拟合的风险也会增加。
GoogLeNet 是第一个引入了“CNN 各层不需要一直都按顺序堆叠”这一概念的模型。用Inception模型，作者展示了一个具有创造性的层次机构，能带来性能和计算效率的提升。这篇论文确实为接下来几年可能会见到的令人惊叹的架构打下了基础。
![](https://img-blog.csdn.net/20170930175624621?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

=======================================================
**微软 ResNet：**
想象一个深度CNN架构，再深、再深、再深，估计都还没有 ILSVRC 2015 冠军，微软的152层ResNet架构深。除了在层数上面创纪录，ResNet 的错误率也低得惊人，达到了3.6%，人类都大约在5%~10%的水平。
只有3.6％的误差率，这应该足以说服你。ResNet模型是目前最好的CNN架构，而且是残差学习理念的一大创新。从2012年起，错误率逐年下降，我怀疑到ILSVRC2016，是否还会一直下降。我相信，我们现在堆放更多层将不会实现性能的大幅提升。我们必须要创造新的架构。
=======================================================
**区域 CNN：R-CNN（2013年）、Fast R-CNN（2015年）、Faster R-CNN（2015年）：**
一些人可能会认为，R-CNN的出现比此前任何关于新的网络架构的论文都有影响力。第一篇关于R-CNN的论文被引用了超过1600次。Ross Girshick 和他在UC Berkeley 的团队在机器视觉上取得了最有影响力的进步。正如他们的文章所写， Fast R-CNN 和 Faster R-CNN能够让模型变得更快，更好地适应现代的物体识别任务。
R-CNN的目标是解决物体识别的难题。在获得特定的一张图像后， 我们希望能够绘制图像中所有物体的边缘。这一过程可以分为两个组成部分，一个是区域建议，另一个是分类。
论文的作者强调，任何分类不可知区域的建议方法都应该适用。Selective Search专用于RCNN。Selective Search 的作用是聚合2000个不同的区域，这些区域有最高的可能性会包含一个物体。在我们设计出一系列的区域建议之后，这些建议被汇合到一个图像大小的区域，能被填入到经过训练的CNN（论文中的例子是AlexNet），能为每一个区域提取出一个对应的特征。这个向量随后被用于作为一个线性SVM的输入，SVM经过了每一种类型和输出分类训练。向量还可以被填入到一个有边界的回归区域，获得最精准的一致性。
**Fast R-CNN**
原始模型得到了改进，主要有三个原因：训练需要多个步骤，这在计算上成本过高，而且速度很慢。Fast R-CNN通过从根本上在不同的建议中分析卷积层的计算，同时打乱生成区域建议的顺利以及运行CNN，能够快速地解决问题。
**Faster R-CNN**
Faster R-CNN的工作是克服R-CNN和 Fast R-CNN所展示出来的，在训练管道上的复杂性。作者在最后一个卷积层上引入了一个区域建议网络（RPN）。这一网络能够只看最后一层的特征就产出区域建议。从这一层面上来说，相同的R-CNN管道可用。

=======================================================
生成对抗网络：
按照Yann LeCun的说法，生成对抗网络可能就是深度学习下一个大突破。假设有两个模型，一个生成模型，一个判别模型。判别模型的任务是决定某幅图像是真实的（来自数据库），还是机器生成的，而生成模型的任务则是生成能够骗过判别模型的图像。这两个模型彼此就形成了“对抗”，发展下去最终会达到一个平衡，生成器生成的图像与真实的图像没有区别，判别器无法区分两者。
听上去很简单，然而这是只有在理解了“数据内在表征”之后才能建立的模型，你能够训练网络理解真实图像和机器生成的图像之间的区别。因此，这个模型也可以被用于CNN中做特征提取。此外，你还能用生成对抗模型制作以假乱真的图片。

=======================================================
生成图像描述：
把CNN和RNN结合在一起会发生什么？Andrej Karpathy 和李飞飞写的这篇论文探讨了结合CNN和双向RNN生成不同图像区域的自然语言描述问题。简单说，这个模型能够接收一张图片，然后输出

很神奇吧。传统CNN，训练数据中每幅图像都有单一的一个标记。这篇论文描述的模型则是每幅图像都带有一句话（或图说）。这种标记被称为弱标记，使用这种训练数据，一个深度神经网络“推断句子中的部分与其描述的区域之间的潜在对齐（latent alignment）”，另一个神经网络将图像作为输入，生成文本的描述。

=======================================================
空间转换器网络：
本文是谷歌DeepMind的一个团队在一年前写的。这篇论文的主要贡献是介绍了空间变换器（Spatial Transformer）模块。基本思路是，这个模块会转变输入图像，使随后的层可以更轻松地进行分类。作者试图在图像到达特定层前改变图像，而不是更改主CNN架构本身。该模块希望纠正两件事：姿势标准化（场景中物体倾斜或缩放）和空间注意力（在密集的图像中将注意力集中到正确的物体）。对于传统的CNN，如果你想使你的模型对于不同规格和旋转的图像都保持不变，那你需要大量的训练样本来使模型学习。让我们来看看这个模块是如何帮助解决这一问题。
传统CNN模型中，处理空间不变性的是maxpooling层。其原因是，一旦我们知道某个特定特性还是起始输入量（有高激活值），它的确切位置就没有它对其他特性的相对位置重要，其他功能一样重要。这个新的空间变换器是动态的，它会对每个输入图像产生不同的行为（不同的扭曲/变形）。这不仅仅是像传统 maxpool 那样简单和预定义。让我们来看看这个模块是如何工作的。该模块包括：
一个本地化网络，会吸收输入量，并输出应施加的空间变换的参数。参数可以是6维仿射变换。
采样网格，这是由卷曲规则网格和定位网络中创建的仿射变换（theta）共同产生的。
一个采样器，其目的是执行输入功能图的翘曲。
**为什么重要？**
CNN的改进不一定要到通过网络架构的大改变来实现。我们不需要创建下一个ResNet或者 Inception 模型。本文实现了对输入图像进行仿射变换的简单的想法，以使模型对平移，缩放和旋转保持不变。


















