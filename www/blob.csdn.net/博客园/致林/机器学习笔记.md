
# 机器学习笔记 - 致林 - 博客园






# [机器学习笔记](https://www.cnblogs.com/bincoding/p/8178911.html)
**梯度下降**：值越大，函数变化越快。
[梯度下降（Gradient Descent）小结](http://www.cnblogs.com/pinard/p/5970503.html)
**激活函数**：Relu函数是一个**单调递增**函数，用来解决线形问题。
[ReLu(Rectified Linear Units)激活函数](http://www.cnblogs.com/neopenx/p/4453161.html)
**池化和全连接**：
[tensorflow 1.0 学习：池化层（pooling)和全连接层(dense)](http://www.cnblogs.com/denny402/p/6933172.html)
[caffe学习笔记31-理解全连接层](http://blog.csdn.net/yiliang_/article/details/60468530)
**SoftMax**：解决多分类问题
[Softmax回归](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)
[Softmax分类函数](https://www.jianshu.com/p/8eb17fa41164)
**均差**：数据是否平均分布
**方差**：数据是否聚合
**卷积神经网络**：
[技术向：一文读懂卷积神经网络CNN](http://www.cnblogs.com/nsnow/p/4562308.html)





