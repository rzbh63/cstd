
# 理解熵和交叉熵 - SIGAI_CSDN的博客 - CSDN博客
# [SIGAI_CSDN的博客](https://blog.csdn.net/sigai_csdn)


[博客首页](https://blog.csdn.net/SIGAI_CSDN)
[关于我们](https://me.csdn.net/SIGAI_CSDN)

置顶2019年01月19日 16:55:27[SIGAI_csdn](https://me.csdn.net/SIGAI_CSDN)阅读数：397标签：[机器学习																](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)[SIGAI																](https://so.csdn.net/so/search/s.do?q=SIGAI&t=blog)[交叉熵																](https://so.csdn.net/so/search/s.do?q=交叉熵&t=blog)[熵																](https://so.csdn.net/so/search/s.do?q=熵&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=交叉熵&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=SIGAI&t=blog)个人分类：[机器学习																](https://blog.csdn.net/SIGAI_CSDN/article/category/7772121)[人工智能																](https://blog.csdn.net/SIGAI_CSDN/article/category/7772122)[AI																](https://blog.csdn.net/SIGAI_CSDN/article/category/7772124)[SIGAI																](https://blog.csdn.net/SIGAI_CSDN/article/category/8625732)[信息论																](https://blog.csdn.net/SIGAI_CSDN/article/category/8652251)[香农																](https://blog.csdn.net/SIGAI_CSDN/article/category/8652252)[
							](https://blog.csdn.net/SIGAI_CSDN/article/category/8652251)
[
																								](https://blog.csdn.net/SIGAI_CSDN/article/category/8625732)
[
				](https://blog.csdn.net/SIGAI_CSDN/article/category/7772124)
[
			](https://blog.csdn.net/SIGAI_CSDN/article/category/7772124)
[
		](https://blog.csdn.net/SIGAI_CSDN/article/category/7772122)
[
	](https://blog.csdn.net/SIGAI_CSDN/article/category/7772121)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190130235507782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=,size_16,color_FFFFFF,t_70)
##### 作者简介：SIGAI人工智能平台
##### PDF下载地址：
##### [http://www.tensorinfinity.com/paper_98.html](http://www.tensorinfinity.com/paper_98.html)
熵、交叉熵是机器学习中常用的概念，也是信息论中的重要概念。它应用广泛，尤其是在深度学习中。本文对交叉熵进行系统的、深入浅出的介绍。文章中的内容在已经出版的《机器学习与应用》（清华大学出版社，雷明著）中有详细的介绍。
#### 熵
在介绍交叉熵之前首先介绍熵（entropy）的概念。熵是信息论中最基本、最核心的一个概念，它衡量了一个概率分布的随机程度，或者说包含的信息量的大小。
首先来看离散型随机变量。考虑随机变量取某一个特定值时包含的信息量的大小。假设随机变量取值为$x$，对应的概率为$p(x)$。直观来看，取这个值的可能性越小，而它又发生了，则包含的信息量就越大。因此如果定义一个函数$h(x)$来描述随机变量取值为的信息量的大小的话，则$h(x)$应该是$p(x)$的单调减函数。例如，一年之内人类登陆火星，包含的信息量显然比广州明天要下雨大，因为前者的概率明显小于后者。
满足单调递减要求的函数太多了，我们该选择哪个函数呢？接着考虑。假设有两个相互独立的随机变量，它们的取值分别为和，取该值的概率为$p(x)$和$p(y)$。根据随机变量的独立性，它们的联合概率为
$$
p(x,y)=p(x)p(y)
$$
由于这两个随机变量是相互独立的，因此它们各自取某一值时包含的信息量应该是两个随机变量分别取这些值的时候包含的信息量之和
$$
h(x,y)=h(x)+h(y)
$$
这要求$h(x)$能把$p(x)$的乘法转化为加法，在数学上，满足此要求的是对数函数。因此，可以把信息量定义为
$$
h(x)=-\log p(x)
$$
这个对数的底数是多少并没有太大关系，根据换底公式，最后计算出来的结果就差了一个倍数，信息论中通常以$2$为底，在机器学习中通常以$e$为底，在后面的计算中为了方便起见我们用$10$为底。需要强调的对数函数前面加上了负号，这是因为对数函数是增函数，而我们要求$h(x)$是$p(x)$的减函数。另外，由于$0\leq p(x)\leq 1$，因此$\log p(x)&lt; 0$，加上负号之后刚好可以保证这个信息量为正。
上面只是考虑了随机变量取某一个值时包含的信息量，而随机变量的取值是随机的，有各种可能，那又怎么计算它取所有各种取值时所包含的信息量呢？既然随机变量取值有各种情况，而且取每个值有一个概率，那我们计算它取各个值时的信息量的均值即数学期望即可，这个信息量的均值，就是熵。
对于离散型随机变量，熵定义为
$$
\begin{aligned}
H(p)&amp;=E_{x}\left [ -\log p(x) \right ] \\ 
&amp;= -\sum_{i}p_{i}\log p_{i}
\end{aligned}
$$
这里约定$pi =p(x_i)$。
下面用实际例子来说明离散型随机变量熵的计算。对于下表定义的概率分布
|x|1|2|3|4|
|---|---|---|---|---|
|p|0.25|1.25|0.25|1.25|
它的熵为
$$
\begin{aligned}
H(p) &amp;=-0.25\times \log 0.25-0.25\times \log 0.25-0.25\times \log 0.25-0.25\times \log 0.25 \\ 
 &amp;= \log 4\\
&amp;=0.6
\end{aligned}
$$
再来看另外一个概率分布
|x|1|2|3|4|
|---|---|---|---|---|
|p|0.9|0.05|0.02|0.03|
它的熵为
$$
\begin{aligned}
H(p) &amp;=-0.9\times \log 0.9-0.05\times \log 0.05-0.02\times \log 0.02-0.03\times \log 0.03 \\ 
&amp;= 0.041+0.065+0.034+0.046\\
&amp;=0.186
\end{aligned}
$$
从上面两个结果可以看出一个现象。第一个概率分布非常均匀，随机变量取每个值的概率相等；第二个不均匀，以极大的概率取值为$1$，取值为$2-4$的概率非常小。第一个概率分布的熵明显的大于第二个概率分布，即随机变量越均匀（随机），熵越大，反之越小。
下面考虑连续型随机变量。对于连续型随机变量，熵（微分熵）定义为
$$
H(p)=-\int_{-\infty }^{+\infty }p(x)\log p(x)dx
$$
这里将求和换成了广义积分。
根据熵的定义，随机变量取各个值的概率相等（均匀分布）时有极大值，在取某一个值的概率为$1$，取其他所有值的概率为$0$时有极小值（此时随机变量退化成某一必然事件或者说确定的变量）。下面证明这一结论。
对于离散型随机变量，熵定义的是如下函数
$$
H(p)=-\sum_{i=1}^{n}x_{i}\log x_{i}
$$
其中$x_i$为随机变量取第$i$个值的概率。约束条件为
$$
\begin{aligned}
 \sum_{i=1}^{n}x_{i}=1 \\ 
 x_{i}\geq 0 
\end{aligned}
$$
由于对数函数的定义域是非负的，因此可以去掉不等式约束。构造拉格朗日乘子函数
$$
L(x,\lambda )=-\sum_{i=1}^{n}x_{i}\log x_{i}+\lambda \left ( \sum_{i=1}^{n}x_{i}-1 \right )
$$
对$x_i$求偏导数并令其为$0$，可以得到
$$
\frac{\partial L}{\partial x_{i}}=-\log x_{i}-1+\lambda =0
$$
这意味着在极值点处所有的$x_i$必须相等。对$\lambda$求偏导数并令其为$0$，可以得到
$$
\sum_{i=1}^{n}x_{i}=1
$$
因此当$x_i = 1/n$时函数取得极值。此时熵的值为
$$
\begin{aligned}
 H(p)&amp;= -\sum_{i=1}^{n}\frac{1}{n}\log\frac{1}{n}\\ 
 &amp;= \log n
\end{aligned}
$$
进一步的可以证明该值是极大值。熵的二阶偏导数为
$$
\begin{aligned}
\frac{\partial ^{2}H}{\partial x_{i}^{2}} &amp;= -1/x_{i}\\ 
\frac{\partial ^{2}H}{\partial x_{i}\partial x_{j}}&amp;= 0, j\neq i
\end{aligned}
$$
因此Hessian矩阵为
$$
\begin{bmatrix}
-1/x_{1} &amp; ... &amp; 0\\ 
... &amp; .. &amp; ...\\ 
0 &amp; ... &amp; -1/x_{n}
\end{bmatrix}
$$
由于$x_i &gt; 0$，该矩阵负定，熵是凹函数，有极大值。因此当$x_i =1/n$时熵有极大值。如果定义
$$
0\log 0=0
$$
显然它与下面的极限是一致的
$$
\lim_{x \to 0} xlogx=0
$$
则当某一个$x_i = 1$，其他$x_j = 0$,$j\neq i$的时熵有极小值$0$
$$
H
$$
$$
\begin{aligned}
H &amp;=0\log0+....+1\log1+...+0\log0 \\ 
 &amp;= 0
\end{aligned}
$$
除此情况之外，只要满足$ 0< x_{i}< 1$，则$\log x_{i}< 0$，因此
$$
-x_{i}\log x_{i}&gt; 0
$$
上面这些结果说明熵是非负的，当且仅当随机变量取某一值的概率为$1$，取其他值的概率为$0$时熵有极小值$0$。此时随机变量退化成普通的变量，取值固定。而当随机变量取所有值的概率相等时即均匀分布时熵有极大值。故熵的范围为
$$
0\leq H(p)\leq \log n
$$
下面举例说明熵在机器学习中的应用，以决策树为例。用于对分类问题，决策树在训练每个非叶子节点时要寻找最佳分裂，将样本进行划分成尽可能纯的子集。此时熵的作用是度量数据集的“纯度”值。样本集$D$的熵不纯度定义为
$$
E(D)=-\sum_{i}p_{i}\;log_{2}\;p_{i}
$$
当样本只属于某一类时熵有最小值，当样本均匀的分布于所有类中时熵有最大值。找到一个分裂让熵最小化，它就是最佳分裂。
#### 交叉熵
交叉熵的定义与熵类似，不同的是定义在两个概率分布而不是一个概率分布之上。对于离散型随机变量，交叉熵定义为
$$
H(p,g)=-\sum_{x}p(x)\log q(x)
$$
其中$x$为离散型随机变量，$p(x)$和$q(x)$是它的两个概率分布。交叉熵衡量了两个概率分布的差异。其值越大，两个概率分布相差越大；其值越小，则两个概率分布的差异越小。
下面通过实际例子来说明交叉熵的计算。对于下表的两个概率分布
|x|1|2|3|4|
|---|---|---|---|---|
|p|0.4|0.4|0.4|0.4|
|q|0.4|0.4|0.4|0.4|
其交叉熵为
$$
\begin{aligned}
 H(p,q) &amp;= -0.4 \times \log 0.4 -0.4 \times \log 0.4 -0.1 \times \log 0.1 -0.1 \times \log0.1 \\ 
 &amp;=0.318+0.2 \\ 
 &amp;= 0.518
\end{aligned}
$$
对于下表的两个概率分布
|x|1|2|3|4|
|---|---|---|---|---|
|p|0.4|0.4|0.1|0.1|
|q|0.1|0.1|0.4|0.4|
其交叉熵为
$$
\begin{aligned}
 H(p,q) &amp;= -0.1 \times \log 0.4 -0.1 \times \log 0.4 -0.4 \times \log 0.1 -0.4 \times \log0.1 \\ 
 &amp;=0.08+0.8 \\ 
 &amp;= 0.88
\end{aligned}
$$
第一个表格中两个概率分布完全相等，第二个则差异很大。第二个的熵比第一个大。后面我们会证明这一结论。
对于连续型概率分布，交叉熵定义为
$$
\int _x p(x)\log q(x)dx = E_p\left [  -\log q \right ]
$$
如果两个概率分布完全相等，则交叉熵退化成熵。
可以证明，当两个分布相等的时候，交叉熵有极小值。假设第一个概率分布固定即$p(x)$为常数，此时交叉熵为如下形式的函数
$$
H(x) = -\sum_{i=1}^{n}a_i \log x_i
$$
约束条件为
$$
-\sum_{i=1}^{n} x_i =1
$$
构造拉格朗日乘子函数
$$
L(x,\lambda) = -\sum_{i=1}^{n}a_i \log x_i + \lambda(\sum_{i=1}^{n}x_i-1)
$$
对所有变量求偏导数，并令偏导数为0，有
$$
\begin{aligned}
-\frac{a_i}{x_i}+\lambda &amp;= 0\\ 
 \sum_{i=1}^n x_i&amp;=1 \\ 
 \sum_{i=1}^n a_i&amp;=1 
\end{aligned}
$$
最后可以解得
$$
λ
$$
$$
x
$$
$$
i
$$
$$
\begin{aligned}
\lambda &amp;= 1\\ 
x_i&amp;=a_i
\end{aligned}
$$
交叉熵函数的Hessian矩阵为：
$$
\begin{bmatrix}
a_1/x_1^2 &amp; \cdots  &amp; 0 \\ 
 \cdots &amp; \cdots &amp; \cdots\\ 
0 &amp; \cdots &amp; a_n/x_n^2
\end{bmatrix}
$$
该矩阵正定，因此交叉熵损失函数是凸函数，上面的极值点是极小值点。
#### 用于logistic回归
交叉熵在机器学习中用得更多，通常用于充当目标函数，以最小化两个概率分布的差异。下面介绍交叉熵在logistic回归中的应用。logistic的预测函数为
$$
h(x) = \frac{1}{1+\exp(-w^TX)}
$$
样本属于正样本的概率为
$$
p(y=1|x)=h(x)
$$
属于负样本的概率为
$$
p(y=0|x)=1-h(x)
$$
其中$y$为类别标签，取值为$1$或者$0$，分别对应正负样本。
训练样本集为$(x_i,  y_i)，i=1,\cdots, l$,$x_i$为特征向量，$y_i$为类别标签，取值为$1$或$0$。给定$w$参数和样本特征向量$X$，样本属于每个类的概率可以统一写成如下形式
$$
p(y|x,w)=(h(x))^y(1-h(x))^{1-y}
$$
令$y$为$1$和$0$，上式分别等于样本属于正负样本的概率。由于训练样本之间相互独立，训练样本集的似然函数为
$$
L(w) = \prod _{i=1}^lp(y_i|x_i,w)= \prod _{i=1}^l(h(x_i)^{y_i}(1-h(x_i))^{1-y_i})
$$
对似然函数取对数，得到对数似然函数为
$$
\ln L(w) = \sum _{i=1}^l(y_i \log h(x_i) + （1-y_i）\log(1-h(x_i)))
$$
这就是交叉熵的特殊情况，随机变量只取$0$和$1$两个值。要求该函数的最大值，等价于求下面函数的极小值：
$$
f(w)=-\sum_{i=1}^l(y_i\log h(x_i)+(1-y_i)\log(1-h(x_i)))
$$
目标函数的梯度为
$$
\begin{aligned}
\triangledown _wf(w) &amp;= -\sum_{i=1}^l(\frac{y_i}{h(x_i)}h(x_i)(1-h(x_i))\triangledown_w(w^Tx_i)-\frac{(1-y_i)}{1-h(x_i)}h(x_i)(1-h(x_i))\triangledown_w(w^Tx_i))\\ 
 &amp;= -\sum_{i=1}^l(y_i(1-h(x_i))x_i-(1-y_i)h(x_i)x_i) \\ 
 &amp;= -\sum_{i=1}^1(y_i-y_ih(x_i)-h(x_i)+y_ih(x_i))x_i\\ 
 &amp;= \sum_{i=1}^l(h(x_i)-y_i)x_i
\end{aligned}
$$
Hessian矩阵为
$$
\begin{aligned}
 \triangledown_w^2f(w) &amp;= \triangledown_w\sum_{i=1}^l(h(x_i)-y_i)x_i \\ 
 &amp;= \sum_{i=1}^l h(x_i)(1-h(x_i))X_i
\end{aligned}
$$
如果单个样本的特征向量为$x_i = \left [ x_{i1},\cdots,x_in \right ]^T$，矩阵$X_i$定义为
$$
X_i = \begin{bmatrix}
x_{i1}^2 &amp; \cdots &amp; x_{il}x_{in}\\ 
\cdots &amp;\cdots  &amp;\cdots \\ 
x_{in}x_i1 &amp; \cdots &amp; x^2_{in}
\end{bmatrix}
$$
此矩阵可以写成如下乘积形式
$$
X_i=x_ix_i^T
$$
对任意不为$0$的向量$X$有
$$
x^TX_ix = x^T(x_ix_i^T)x=x^Tx_ix_i^Tx= (x^Tx_i)(x_i^Tx) \geq 0
$$
从而矩阵$X_i$半正定，另外由于
$$
h(x_i)(1-h(x_i))&gt;0
$$
因此Hessian矩阵半正定，目标函数是凸函数。logistic回归求解的优化问题是不带约束条件的凸优化问题。可以证明，如果使用欧氏距离作为目标函数则无法保证目标函数是凸函数。这是使用交叉熵而不使用欧氏距离作为logistic回归的目标函数的主要原因。
#### 用于softmax回归
softmax回归是logistic回归的推广，用于解决多分类问题。给定l个训练样本$（x_i,  y_i)$，其中$x_i$为$n$维特征向量，$y_i$为类别标签，取值为$1$到$k$之间的整数。softmax回归估计一个样本属于每一类的概率
$$
h_{\theta}(x) = \frac{1}{\sum_{i=1}^{k}e^{\theta^T_ix}}
$$
模型的输出为一个$k$维的概率向量，其元素之和为$1$，每一个分量为样本属于该类的概率。使用指数函数进行变换的原因是指数函数值都大于$0$，概率值必须是非负的。分类时将样本判定为概率最大的那个类。要估计的参数为：
$$
\theta = 
\begin{bmatrix}
 \theta_1&amp; \theta_2 &amp;\cdots  &amp; \theta_k
\end{bmatrix}
$$
其中每个$\theta_i$都是一个列向量，$\theta$是一个$n\times k$的矩阵。如果将上面预测出的概率向量记为$KaTeX parse error: Expected group after '^' at position 2: y^̲\*$，即：
$$
y^* = \frac{1}{\sum_{i=1}^k e^{\theta_i^Tx}}\begin{bmatrix}
e^{\theta_1^Tx}\\ 
\cdots\\ 
e^{\theta_k^Tx}
\end{bmatrix}
$$
样本真实标签向量用one-hot编码，即如果样本是第$i$类，则向量的第个分量为$1$，其他为$0$，将这个标签向量记为$y$。仿照logistic回归的做法，样本属于每个类的概率可以统一写成：
$$
\prod_{i=1}^k(y_i^*)^{y_i}
$$
显然这个结论是成立的。因为只有一个$y_i$为$1$，其他的都为$0$，一旦$y$的取值确定，如样本为第j类样本，则上式的值为$KaTeX parse error: Expected group after '^' at position 4: y_j^̲\*$。给定一批样本，它们的似然函数为：
$$
\prod_{i=1}^l(\prod_{j=1}^k(\log \frac{\exp {\theta^T_j x_i}}{\sum_{t=1}^k\exp(\theta_t^Tx_i)}))
$$
$y_{ij}$为第$i$个训练样本标签向量的第$j$个分量。对上式取对数，得到对数似然函数为
$$
\sum_{i=1}^l\sum_{j=1}^k(y_{ij}\log \frac{\exp(\theta^T_jx_i)}{\sum_{t=1}^k\exp(\theta_t^Tx_i)})
$$
让对数似然函数取极大值等价于让下面的损失函数取极小值
$$
L(\theta) = -\sum_{i=1}^l\sum_{j=1}^k(y_{ij}\log \frac{\exp(\theta_j^Tx_i)}{\sum_{t=1}^k\exp(\theta^T_tx_i)})
$$
这就是交叉熵，同样可以证明这个损失函数是凸函数。
对单个样本的损失函数可以写成：
$$
\begin{aligned}
L(x,y,\theta) &amp;= -\sum_{j=1}^k(y_j\log\frac{\exp(\theta^T_jx)}{\sum^k_{t=1}\exp(\theta^T_tx)}) \\ 
 &amp;= -\sum_{j=1}^k(y_i(\theta^T_jx-\log(\sum^k_{t=1}\exp(\theta^T_tx))))
\end{aligned}
$$
如果样本属于第$i$类，则$y_i  = 1$，其他的分量都为$0$，上式可以简化为
$$
L(x,y,\theta) = -(\theta^T_ix-\log(\sum_{t=1}^k\exp(\theta^T_tX)))
$$
下面计算损失函数对$θ_p$的梯度。如果$i=p$，则有：
$$
\triangledown_{\theta_p}L = -（x-\frac{\exp(\theta_p^Tx)}{\sum_{t=1}^k\exp(\theta_t^Tx)}x）=x(\frac{\exp(\theta_p^Tx)}{\sum_{t=1}^k\exp(\theta_t^Tx)}-1)
$$
否则
$$
\triangledown_{\theta_p}L = -(0-\frac{\exp(\theta_p^Tx)}{\sum_{t=1}^k\exp(\theta_t^Tx)}x)
=\frac{\exp(\theta_p^Tx)}{\sum_{t=1}^k\exp(\theta_t^Tx)}x
$$
如果一个样本是第$p$类的，对单个样本的损失函数可以简化为
$$
-\sum_{j=1}^k(y_j\log\frac{\exp(\theta_j^Tx)}{\sum_{t=1}^k\exp(\theta_t^Tx)})
=-\log\frac{\exp(\theta_p^Tx)}{\sum_{t=1}^k\exp(\theta_t^Tx)}
$$
如果softmax回归预测出来的属于第$p$类的值为$1$，即与真实标签值完全吻合，此时损失函数有极小值$0$。反之，如果预测出来属于第$p$类的值为$0$，此时损失函数值为正无穷。
#### 用于神经网络
softmax回归经常被用作神经网络的最后一层，完成多分类任务，训练时采用的损失函数一般为交叉熵。在神经网络的早期，更多的使用了欧氏距离损失函数，后来对分类任务交叉熵使用的更多。对于分类问题，交叉熵一般比欧氏距离有更好的效果，可以收敛到更好的局部最优解，具体的可以参考文献[1]。此时训练样本的类别标签向量同样为one-hot编码形式，是类别取每个值的概率，神经网络输出的概率估计向量要和真实的样本标签向量接近。具体的推导和证明可以阅读《机器学习与应用》一书，与softmax回归类似，在这里不再详细讲述。除了上面列出的这些机器学习算法之外，交叉熵在AdaBoost算法、梯度提升算法中也有应用，感兴趣的读者可以阅读《机器学习与应用》。
#### 参考文献
[1] P Golik, P Doetsch, H Ney. Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison. 2013.

