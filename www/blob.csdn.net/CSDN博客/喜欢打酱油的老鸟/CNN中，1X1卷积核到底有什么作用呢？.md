
# CNN中，1X1卷积核到底有什么作用呢？ - 喜欢打酱油的老鸟 - CSDN博客


2018年09月10日 09:24:28[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：42


[https://www.cnblogs.com/DicksonJYL/p/9617328.html](https://www.cnblogs.com/DicksonJYL/p/9617328.html)
Question:
从NIN 到Googlenet mrsa net 都是用了这个，为什么呢？
发现很多网络使用了1X1卷积核，这能起到什么作用呢？另外我一直觉得，1X1卷积核就是对输入的一个比例缩放，因为1X1卷积核只有一个参数，这个核在输入上滑动，就相当于给输入数据乘以一个系数。不知道我理解的是否正确。
Answer
[ruirui_ICT]：我来说说我的理解，我认为1×1的卷积大概有两个方面的作用吧：
1.实现跨通道的交互和信息整合
2.进行卷积核通道数的降维和升维
下面详细解释一下：
1. 这一点孙琳钧童鞋讲的很清楚。1×1的卷积层（可能）引起人们的重视是在NIN的结构中，论文中林敏师兄的想法是利用MLP代替传统的线性卷积核，从而提高网络的表达能力。文中同时利用了跨通道pooling的角度解释，认为文中提出的MLP其实等价于在传统卷积核后面接cccp层，从而实现多个feature map的线性组合，实现跨通道的信息整合。而cccp层是等价于1×1卷积的，因此细看NIN的caffe实现，就是在每个传统卷积层后面接了两个cccp层（其实就是接了两个1×1的卷积层）。
2. 进行降维和升维引起人们重视的（可能）是在GoogLeNet里。对于每一个Inception模块（如下图），原始模块是左图，右图中是加入了1×1卷积进行降维的。虽然左图的卷积核都比较小，但是当输入和输出的通道数很大时，乘起来也会使得卷积核参数变的很大，而右图加入1×1卷积后可以降低输入的通道数，卷积核参数、运算复杂度也就跟着降下来了。以GoogLeNet的3a模块为例，输入的feature map是28×28×192，3a模块中1×1卷积通道为64，3×3卷积通道为128,5×5卷积通道为32，如果是左图结构，那么卷积核参数为1×1×192×64+3×3×192×128+5×5×192×32，而右图对3×3和5×5卷积层前分别加入了通道数为96和16的1×1卷积层，这样卷积核参数就变成了1×1×192×64+（1×1×192×96+3×3×96×128）+（1×1×192×16+5×5×16×32），参数大约减少到原来的三分之一。同时在并行pooling层后面加入1×1卷积层后也可以降低输出的feature map数量，左图pooling后feature map是不变的，再加卷积层得到的feature map，会使输出的feature map扩大到416，如果每个模块都这样，网络的输出会越来越大。而右图在pooling后面加了通道为32的1×1卷积，使得输出的feature map数降到了256。GoogLeNet利用1×1的卷积降维后，得到了更为紧凑的网络结构，虽然总共有22层，但是参数数量却只是8层的AlexNet的十二分之一（当然也有很大一部分原因是去掉了全连接层）。
![](https://img-blog.csdn.net/20180910092347553?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
最近大热的MSRA的ResNet同样也利用了1×1卷积，并且是在3×3卷积层的前后都使用了，不仅进行了降维，还进行了升维，使得卷积层的输入和输出的通道数都减小，参数数量进一步减少，如下图的结构。（不然真不敢想象152层的网络要怎么跑起来TAT）
![](https://img-blog.csdn.net/20180910092401744?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
Comment：
[孙琳钧 ]：对于单通道的feature map和单个卷积核之间的卷积来说，题主的理解是对的，CNN里的卷积大都是多通道的feature map和多通道的卷积核之间的操作（输入的多通道的feature map和一组卷积核做卷积求和得到一个输出的feature map），如果使用1x1的卷积核，这个操作实现的就是多个feature map的线性组合，可以实现feature map在通道个数上的变化。接在普通的卷积层的后面，配合激活函数，就可以实现network in network的结构了
[shiorioxy]：还有一个重要的功能，就是可以在保持feature map 尺寸不变（即不损失分辨率）的前提下大幅增加非线性特性，把网络做得很deep。
[月光里的阳光ysu]：对于两位在解答中说的1X1卷积核能够对多个feature map实现线性组合。我的个人理解是不是和全局平均池化类似，只不过一个是求feature map的平均值，一个是线性组合？


