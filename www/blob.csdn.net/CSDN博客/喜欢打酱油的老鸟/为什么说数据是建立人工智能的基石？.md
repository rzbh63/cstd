
# 为什么说数据是建立人工智能的基石？ - 喜欢打酱油的老鸟 - CSDN博客


2019年03月10日 11:12:23[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：67标签：[迁移学习																](https://so.csdn.net/so/search/s.do?q=迁移学习&t=blog)[数据																](https://so.csdn.net/so/search/s.do?q=数据&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=迁移学习&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)



> 我觉得人工智能就像是去建造一艘火箭飞船。你需要一个巨大的引擎和许多燃料。如果你有了一个大引擎，但燃料不够，那么肯定不能把火箭送上轨道；如果你有一个小引擎，但燃料充足，那么说不定根本就无法成功起飞。所以，构建火箭船，你必须要一个巨大的引擎和许多燃料。

> 深度学习（创建人工智能的关键流程之一）也是同样的道理，火箭引擎就是深度学习模型，而燃料就是海量数据，这样我们的算法才能应用上。——吴恩达
使用深度学习解决问题的一个常见障碍是训练模型所需的数据量。对大数据的需求是因为模型中有大量参数需要学习。
以下是几个例子展示了最近一些模型所需要的参数数量：
![为什么说数据是建立人工智能的基石？](http://p1.pstatp.com/large/pgc-image/04848ca8051a4b57b23d931791939377)

深度学习模型的详细信息
**神经网络又名深度学习是可以堆叠起来的层状结构（想想乐高）**
深度学习只不过是大型神经网络，它们可以被认为是流程图，数据从一边进来，推理或知识从另一边出来。
你可以拆分神经网络，把它拆开，从任何你喜欢的地方取出推理。你可能没有得到任何有意义的东西，但你依然可以这么做，例如Google DeepDream。
![为什么说数据是建立人工智能的基石？](http://p3.pstatp.com/large/pgc-image/38d055f377d741deb21a0aded3121a18)

**模型大小 ∝ 数据大小 ∝ 问题复杂度**
在所需的数据量和模型的大小之间有一个有趣的近乎线性的关系。 基本的推理是，你的模型应该足够大，以便捕捉数据中的关系（例如图像中的纹理和形状，文本中的语法和语音中的音素）以及问题的具体细节（例如类别数量）。模型早期的层捕捉输入的不同部分之间的高级关系（如边缘和模式）。后面的层捕捉有助于做出最终决策的信息，通常能够帮助在想要的输出间进行区分。因此，如果问题的复杂性很高（如图像分类），参数数量和所需数据体量也非常大。
![为什么说数据是建立人工智能的基石？](http://p3.pstatp.com/large/pgc-image/92730fafc85842249b1354c8cbd79c98)

AlexNet在每一步能够看到什么
**迁移学习来解围！**
在处理一个您的特定领域的问题时，通常无法找到构建这种大小模型所需的数据量。 然而，训练一个任务的模型捕获数据类型中的关系，并且可以很容易地再用于同一个领域中的不同问题。 这种技术被称为迁移学习。
![为什么说数据是建立人工智能的基石？](http://p9.pstatp.com/large/pgc-image/7708056a9b544dc8863faf0f4034b021)

![为什么说数据是建立人工智能的基石？](http://p9.pstatp.com/large/pgc-image/847aa63a1e9f4d888c219688a824c3a5)

转移学习就像没有人试图保留但却保存的最好的秘密一样。 业内人人都知道，但外界没有人知道。
![为什么说数据是建立人工智能的基石？](http://p1.pstatp.com/large/pgc-image/4bfcd7fc1c6144f3b53d78b3f4c8f924)

Google 趋势机器学习 vs 深度学习 vs 迁移学习
参考Awesome — Most Cited Deep Learning Papers（https://github.com/terryum/awesome-deep-learning-papers），看看深度学习中的顶级论文： 引用最多的深度学习论文，超过50％的论文使用某种形式的转移学习或预训练。 转移学习变得越来越适用于资源有限（数据和计算）的人们，但不幸的是，这个想法还没有得到
应有的社会化。 最需要它的人还不知道它。
如果深度学习是圣杯，数据是守门人，转移学习是关键。
通过转移学习，我们可以采用已经在大型现成数据集上训练好的预训练模型（在完全不同的任务上进行训练，输入相同但输出不同）。 然后尝试查找输出可重复使用特征的图层。 我们使用该层的输出作为输入特征来训练需要更少参数的小得多的网络。 这个较小的网络已经从预训练模型了解了数据中的模式，现在只需要了解它与你特定问题的关系。 猫咪检测模型可以被重利用于梵高作品重现的模型就是这样训练的。
![为什么说数据是建立人工智能的基石？](http://p9.pstatp.com/large/pgc-image/7708056a9b544dc8863faf0f4034b021)

![为什么说数据是建立人工智能的基石？](http://p9.pstatp.com/large/pgc-image/8fcec0a2746f4f13ad94a59427d96ff9)

使用转移学习的另一个主要优势是模型的泛化效果很好。 较大的模型倾向于过度拟合数据（即对数据进行建模而不是对潜在的现象建模），并且在对未见数据进行测试时效果不佳。 由于转移学习允许模型看到不同类型的数据，因此它更好地学习了世界的基本规则。
> 把过拟合看做是记忆而不是学习。—— James Faghmous
**由于迁移学习导致的数据减少**
假设想结束蓝黑礼服vs白金礼服的争论。你开始收集验证的蓝黑礼服和白金礼服的图像。如果想自己建立一个像上文提到的那样精确的模型（有140百万个参数）。为了训练这个模型，你需要找到120万张图像，这是一个不可能完成的任务。 所以可以试试迁移学习。
计算一下使用迁移学习解决该问题所需要的参数数量：
参数数量 = [输入大小 + 1] * [输出大小 + 1]
= [2048+1]*[1+1]~ 4098 个参数
我们看到参数数量从1.4×10⁸减少到4×10³，这是5个数量级。 所以我们要收集不到一百个连衣裙的图像，这样应该还好。唷！
如果你不耐烦，等不及要找出衣服的实际颜色，向下滚动，看看如何建立自己的礼服模型。
**· · ·**
**转移学习的分步指南——使用与情感分析相关的实例**
在这个实例中我们有72个电影评论
1、62个没有分配情绪，这些将被用于预先模型
2、8个分配了情绪，它们将被用于训练模型
3、2个分配了情绪，它们将被用于测试模型
由于我们只有8个有标记的句子（那些有感情相关的句子），我们首先直接训练模型来预测上下文。 如果我们只用8个句子训练一个模型，它会有50％的准确率（50％如同用抛硬币进行决策）。
为了解决这个问题，我们将使用转移学习，首先在62个句子上训练一个模型。 然后，我们使用第一个模型的一部分，并在其基础上训练情感分类器。 使用8个句子进行训练，并在剩下的2个句子上进行测试时，模型会产生100％的准确率。
**步骤一**
我们将训练一个对词语之间的关系进行建模的网络。将句子中的一个词语传递进去，并尝试预测该词语出现在同一个句子中。在下列的代码中嵌入的矩阵大小为vocabulary x embedding_size，其中存储了代表每个词语的向量（这里的大小为“4”）。
![为什么说数据是建立人工智能的基石？](http://p9.pstatp.com/large/pgc-image/7708056a9b544dc8863faf0f4034b021)

![为什么说数据是建立人工智能的基石？](http://p1.pstatp.com/large/pgc-image/e7cc457172504372b50383033bf5e2af)

**步骤二**
我们会对这个图标进行训练，让相同上下文中出现的词语可以获得类似的向量表征。我们会对这些句子进行预处理，移除所有停止词并标记他们。随后一次传递一个词语， 尽量缩短该词语向量与周边词语之间的距离，并扩大与上下文不包含的随机词语之间的距离。
![为什么说数据是建立人工智能的基石？](http://p9.pstatp.com/large/pgc-image/7708056a9b544dc8863faf0f4034b021)

![为什么说数据是建立人工智能的基石？](http://p1.pstatp.com/large/pgc-image/62a82c0654ca487bb8ea6b96aaa843c7)

**步骤三**
随后我们会试着预测句子索要表达的情绪。目前已经有10个（8个训练用，2个测试用）句子带有正面和负面的标签。由于上一步得到的模型已经包含从所有词语中习得的向量，并且这些向量的数值属性可以代表词语的上下文，借此可进一步简化情绪的预测。
![为什么说数据是建立人工智能的基石？](http://p9.pstatp.com/large/pgc-image/7708056a9b544dc8863faf0f4034b021)

![为什么说数据是建立人工智能的基石？](http://p1.pstatp.com/large/pgc-image/53d2e632502348e7824d294012a0d19c)

此时我们并不直接使用句子，而是将句子的向量设置为所含全部词语的平均值（这一任务实际上是通过类似LSTM的技术实现的）。句子向量将作为输入传递到网络中，输出结果为内容为正面或负面的分数。我们用到了一个隐藏的中间层，并通过带有标签的句子对模型进行训练。如你所见，虽然每次只是用了10个样本，但这个模型实现了100%的准确度。
虽然这只是个示例，但可以发现在迁移学习技术的帮助下，精确度从50%飞速提升至100%。若要查看完整范例和代码请访问下列地址：
**迁移学习的一些真实案例**
图像识别：图像增强、风格转移、对象检测、皮肤癌检测。
文字识别：Zero Shot翻译、情绪分类。
**迁移学习实现过程中的难点**
虽然可以用更少量的数据训练模型，但该技术的运用有着更高的技能要求。只需要看看上述例子中硬编码参数的数量，并设想一下要在模型训练完成前不断调整这些参数，迁移学习技术使用的难度之大可想而知。
1、迁移学习技术目前面临的问题包括：
2、找到预训练所需的大规模数据集
3、决定用来预训练的模型
4、两种模型中任何一种无法按照预期工作都将比较难以调试
5、不确定为了训练模型还需要额外准备多少数据
6、使用预训练模型时难以决定在哪里停止
7、在预训练模型的基础上，确定模型所需层和参数的数量
8、托管并提供组合后的模型
9、当出现更多数据或更好的技术后，对预训练模型进行更新
> 数据科学家难觅。找到能发现数据科学家的人其实一样困难 --Krzysztof Zawadzki
**让迁移学习变得更简单**
亲身经历过这些问题后，我们开始着手通过构建支持迁移学习技术的云端深度学习服务，并尝试通过这种简单易用的服务解决这些问题。该服务中包含一系列预训练的模型，我们已针对数百万个参数进行过训练。你只需要上传自己的数据（或在网络上搜索数据），该服务即可针对你的具体任务选择最适合的模型，在现有预训练模型的基础上建立新的NanoNet，将你的数据输入到NanoNet中进行处理。
![为什么说数据是建立人工智能的基石？](http://p3.pstatp.com/large/pgc-image/ebccccd1fc4044c8825363ae889d68e5)

NanoNets的迁移学习技术（该架构仅为基本呈现）
**构建你的首个NanoNet（图像分类）**
1、在这里选择你要处理的分类。
![为什么说数据是建立人工智能的基石？](http://p1.pstatp.com/large/pgc-image/27e830ae59d840e3bc8781ab209d7373)

2、 一键点击开始搜索网络并构建模型（你也可以上传自己的图片）。
![为什么说数据是建立人工智能的基石？](http://p3.pstatp.com/large/pgc-image/1f5c3549bc044db4a8c96a28e1186df3)

3、 解决蓝金裙子的争议（模型就绪后我们会通过简单易用的Web界面让你上传测试图片，同时还提供了不依赖特定语言的API）
![为什么说数据是建立人工智能的基石？](http://p9.pstatp.com/large/pgc-image/392230055e4d40fa97e654d2ab3dfed8)
[https://www.toutiao.com/a6665907541668528651/](https://www.toutiao.com/a6665907541668528651/)


