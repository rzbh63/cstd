
# 自然语言处理3 -- 词性标注 - 谢杨易的博客 - CSDN博客

2018年08月21日 14:35:01[谢杨易](https://me.csdn.net/u013510838)阅读数：3387


系列文章，请多关注
[Tensorflow源码解析1 – 内核架构和源码结构](https://blog.csdn.net/u013510838/article/details/84103503)
[带你深入AI（1） - 深度学习模型训练痛点及解决方法](https://blog.csdn.net/u013510838/article/details/79835563)
[自然语言处理1 – 分词](https://blog.csdn.net/u013510838/article/details/81673016)
[自然语言处理2 – jieba分词用法及原理](https://blog.csdn.net/u013510838/article/details/81738431)
[自然语言处理3 – 词性标注](https://blog.csdn.net/u013510838/article/details/81907121)
[自然语言处理4 – 句法分析](https://blog.csdn.net/u013510838/article/details/81976427)
[自然语言处理5 – 词向量](https://blog.csdn.net/u013510838/article/details/82108381)
[自然语言处理6 – 情感分析](https://blog.csdn.net/u013510838/article/details/82558797)
# 1 概述
词性标注在自然语言处理中也属于基础性的模块，为句法分析、信息抽取等工作打下基础。和分词一样，中文词性标注也存在着很多难点，比如一词多词性，未登录词处理等诸多问题。通过基于字符串匹配的字典查询算法和基于统计的词性标注算法，可以很好的解决这些问题。一般需要先将语句进行分词，然后再进行词性标注。
# 2 词性标注难点
词性作为词语基本的语法属性，是词语和语句的关键性特征。词性种类也很多，ICTCLAS 汉语词性标注集归纳的词性种类及其表示见[https://www.cnblogs.com/chenbjin/p/4341930.html](https://www.cnblogs.com/chenbjin/p/4341930.html)。词性标注中的难点主要有
相对于英文，中文缺少词形态变化，不能从词的形态来识别词性
一词多词性很常见。统计发现，一词多词性的概率高达22.5%。而且越常用的词，多词性现象越严重。比如“研究”既可以是名词（“基础性研究”），也可以是动词（“研究计算机科学”）。
词性划分标准不统一。词类划分粒度和标记符号等，目前还没有一个广泛认可的统一的标准。比如LDC标注语料中，将汉语一级词性划分为33类，而北京大学语料库则将其划分为26类。词类划分标准和标记符号的不统一，以及分词规范的含糊，都给词性标注带来了很大的困难。jieba分词采用了使用较为广泛的ICTCLAS 汉语词性标注集规范。
未登录词问题。和分词一样，未登录词的词性也是一个比较大的课题。未登录词不能通过查找字典的方式获取词性，可以采用HMM隐马尔科夫模型等基于统计的算法。
# 3 词性标注算法
和分词一样，词性标注算法也分为两大类，基于字符串匹配的字典查找算法和基于统计的算法。jieba分词就综合了两种算法，对于分词后识别出来的词语，直接从字典中查找其词性。而对于未登录词，则采用HMM隐马尔科夫模型和viterbi算法来识别。
### 3.1 基于字符串匹配的字典查找算法
先对语句进行分词，然后从字典中查找每个词语的词性，对其进行标注即可。jieba词性标注中，对于识别出来的词语，就是采用了这种方法。这种方法比较简单，通俗易懂，但是不能解决一词多词性的问题，因此存在一定的误差。
下图即为jieba分词中的词典的一部分词语。每一行对应一个词语，分为三部分，分别为词语名 词数 词性。因此分词完成后只需要在字典中查找该词语的词性即可对其完成标注。
![image.png | left | 623x276](https://cdn.nlark.com/lark/0/2018/png/9304/1534813922795-bf5431ed-5be6-4731-8335-7d5513b8e1a5.png)
### 3.2 基于统计的词性标注算法
和分词一样，我们也可以通过HMM隐马尔科夫模型来进行词性标注。观测序列即为分词后的语句，隐藏序列即为经过标注后的词性标注序列。起始概率 发射概率和转移概率和分词中的含义大同小异，可以通过大规模语料统计得到。观测序列到隐藏序列的计算可以通过viterbi算法，利用统计得到的起始概率 发射概率和转移概率来得到。得到隐藏序列后，就完成了词性标注过程。
# 4 jieba词性标注原理
jieba在分词的同时，可以进行词性标注。利用jieba.posseg模块来进行词性标注，会给出分词后每个词的词性。词性标示兼容ICTCLAS 汉语词性标注集，可查阅网站https://www.cnblogs.com/chenbjin/p/4341930.html
```python
import
```
```python
jieba
```
```python
.
```
```python
posseg
```
```python
as
```
```python
pseg
words
```
```python
=
```
```python
pseg
```
```python
.
```
```python
cut
```
```python
(
```
```python
"我爱北京天安门"
```
```python
)
```
```python
for
```
```python
word
```
```python
,
```
```python
flag
```
```python
in
```
```python
words
```
```python
:
```
```python
.
```
```python
.
```
```python
.
```
```python
print
```
```python
(
```
```python
'%s %s'
```
```python
%
```
```python
(
```
```python
word
```
```python
,
```
```python
flag
```
```python
)
```
```python
)
```
```python
.
```
```python
.
```
```python
.
```
```python
我 r
```
```python
# 代词
```
```python
爱 v
```
```python
# 动词
```
```python
北京 ns
```
```python
# 名词
```
```python
天安门 ns
```
```python
# 名词
```
下面来对pseg.cut()进行详细的分析，其主要流程为
准备工作：check字典是否初始化好，如果没有则先初始化字典。将语句转为UTF-8或者GBK。根据正则匹配，将输入文本分隔成一个个语句。
遍历语句list，对每个语句进行单独分词和词性标注。
对于未登录词，使用HMM隐马尔科夫模型处理。
### 4.1 准备工作
准备工作中做的事情和jieba分词基本一致，check字典是否初始化好，如果没有则先初始化字典。将语句转为UTF-8或者GBK。根据正则匹配，将输入文本分隔成一个个语句。代码如下。
```python
def
```
```python
__cut_internal
```
```python
(
```
```python
self
```
```python
,
```
```python
sentence
```
```python
,
```
```python
HMM
```
```python
=
```
```python
True
```
```python
)
```
```python
:
```
```python
# 如果没有字典没有初始化，则先加载字典。否则直接使用字典缓存即可。
```
```python
self
```
```python
.
```
```python
makesure_userdict_loaded
```
```python
(
```
```python
)
```
```python
# 将语句转为UTF-8或者GBK
```
```python
sentence
```
```python
=
```
```python
strdecode
```
```python
(
```
```python
sentence
```
```python
)
```
```python
# 根据正则匹配，将输入文本分隔成一个个语句。分隔符包括空格 逗号 句号等。
```
```python
blocks
```
```python
=
```
```python
re_han_internal
```
```python
.
```
```python
split
```
```python
(
```
```python
sentence
```
```python
)
```
```python
# 根据是否采用了HMM模型来进行不同方法的选择
```
```python
if
```
```python
HMM
```
```python
:
```
```python
cut_blk
```
```python
=
```
```python
self
```
```python
.
```
```python
__cut_DAG
```
```python
else
```
```python
:
```
```python
cut_blk
```
```python
=
```
```python
self
```
```python
.
```
```python
__cut_DAG_NO_HMM
```
```python
# 遍历正则匹配分隔好的语句，对每个语句进行单独的分词和词性标注
```
```python
for
```
```python
blk
```
```python
in
```
```python
blocks
```
```python
:
```
```python
if
```
```python
re_han_internal
```
```python
.
```
```python
match
```
```python
(
```
```python
blk
```
```python
)
```
```python
:
```
```python
# 分词和词性标注
```
```python
for
```
```python
word
```
```python
in
```
```python
cut_blk
```
```python
(
```
```python
blk
```
```python
)
```
```python
:
```
```python
yield
```
```python
word
```
```python
else
```
```python
:
```
```python
tmp
```
```python
=
```
```python
re_skip_internal
```
```python
.
```
```python
split
```
```python
(
```
```python
blk
```
```python
)
```
```python
for
```
```python
x
```
```python
in
```
```python
tmp
```
```python
:
```
```python
if
```
```python
re_skip_internal
```
```python
.
```
```python
match
```
```python
(
```
```python
x
```
```python
)
```
```python
:
```
```python
yield
```
```python
pair
```
```python
(
```
```python
x
```
```python
,
```
```python
'x'
```
```python
)
```
```python
else
```
```python
:
```
```python
for
```
```python
xx
```
```python
in
```
```python
x
```
```python
:
```
```python
if
```
```python
re_num
```
```python
.
```
```python
match
```
```python
(
```
```python
xx
```
```python
)
```
```python
:
```
```python
yield
```
```python
pair
```
```python
(
```
```python
xx
```
```python
,
```
```python
'm'
```
```python
)
```
```python
elif
```
```python
re_eng
```
```python
.
```
```python
match
```
```python
(
```
```python
x
```
```python
)
```
```python
:
```
```python
yield
```
```python
pair
```
```python
(
```
```python
xx
```
```python
,
```
```python
'eng'
```
```python
)
```
```python
else
```
```python
:
```
```python
yield
```
```python
pair
```
```python
(
```
```python
xx
```
```python
,
```
```python
'x'
```
```python
)
```
### 4.2 遍历语句，进行分词和词性标注
步骤和jieba分词基本一致，主体步骤如下，详细的每个步骤见[自然语言处理2jieba分词用法及原理](https://lark.alipay.com/mybank-mobile/ru1iog/ga4w8v)
得到语句的有向无环图DAG
动态规划构建Route，计算从语句末尾到语句起始，DAG中每个节点到语句结束位置的最大路径概率，以及概率最大时节点对应词语的结束位置
遍历每个节点的Route，组装词语组合。
如果词语不在字典中，也就是新词，使用HMM隐马尔科夫模型进行分割
通过yield将词语逐个返回。
```python
def
```
```python
__cut_DAG
```
```python
(
```
```python
self
```
```python
,
```
```python
sentence
```
```python
)
```
```python
:
```
```python
# 构建DAG有向无环图，得到语句分词所有可能的路径
```
```python
DAG
```
```python
=
```
```python
self
```
```python
.
```
```python
tokenizer
```
```python
.
```
```python
get_DAG
```
```python
(
```
```python
sentence
```
```python
)
```
```python
route
```
```python
=
```
```python
{
```
```python
}
```
```python
# 动态规划，计算从语句末尾到语句起始，DAG中每个节点到语句结束位置的最大路径概率，以及概率最大时节点对应词语的结束位置
```
```python
self
```
```python
.
```
```python
tokenizer
```
```python
.
```
```python
calc
```
```python
(
```
```python
sentence
```
```python
,
```
```python
DAG
```
```python
,
```
```python
route
```
```python
)
```
```python
# 遍历每个节点的Route，组装词语组合。
```
```python
x
```
```python
=
```
```python
0
```
```python
buf
```
```python
=
```
```python
''
```
```python
N
```
```python
=
```
```python
len
```
```python
(
```
```python
sentence
```
```python
)
```
```python
while
```
```python
x
```
```python
<
```
```python
N
```
```python
:
```
```python
# y表示词语的结束位置，x为词语的起始位置
```
```python
y
```
```python
=
```
```python
route
```
```python
[
```
```python
x
```
```python
]
```
```python
[
```
```python
1
```
```python
]
```
```python
+
```
```python
1
```
```python
# 从起始位置x到结束位置y，取出一个词语
```
```python
l_word
```
```python
=
```
```python
sentence
```
```python
[
```
```python
x
```
```python
:
```
```python
y
```
```python
]
```
```python
if
```
```python
y
```
```python
-
```
```python
x
```
```python
==
```
```python
1
```
```python
:
```
```python
# 单字，一个汉字构成的一个词语
```
```python
buf
```
```python
+=
```
```python
l_word
```
```python
else
```
```python
:
```
```python
# 多汉字词语
```
```python
if
```
```python
buf
```
```python
:
```
```python
if
```
```python
len
```
```python
(
```
```python
buf
```
```python
)
```
```python
==
```
```python
1
```
```python
:
```
```python
# 单字直接从字典中取出其词性。使用pair将分词和词性一起输出。
```
```python
yield
```
```python
pair
```
```python
(
```
```python
buf
```
```python
,
```
```python
self
```
```python
.
```
```python
word_tag_tab
```
```python
.
```
```python
get
```
```python
(
```
```python
buf
```
```python
,
```
```python
'x'
```
```python
)
```
```python
)
```
```python
elif
```
```python
not
```
```python
self
```
```python
.
```
```python
tokenizer
```
```python
.
```
```python
FREQ
```
```python
.
```
```python
get
```
```python
(
```
```python
buf
```
```python
)
```
```python
:
```
```python
# 词语不在字典中，也就是新词，使用HMM隐马尔科夫模型进行分割
```
```python
recognized
```
```python
=
```
```python
self
```
```python
.
```
```python
__cut_detail
```
```python
(
```
```python
buf
```
```python
)
```
```python
for
```
```python
t
```
```python
in
```
```python
recognized
```
```python
:
```
```python
yield
```
```python
t
```
```python
else
```
```python
:
```
```python
# 词语在字典中，直接查找字典并取出词性。
```
```python
for
```
```python
elem
```
```python
in
```
```python
buf
```
```python
:
```
```python
yield
```
```python
pair
```
```python
(
```
```python
elem
```
```python
,
```
```python
self
```
```python
.
```
```python
word_tag_tab
```
```python
.
```
```python
get
```
```python
(
```
```python
elem
```
```python
,
```
```python
'x'
```
```python
)
```
```python
)
```
```python
buf
```
```python
=
```
```python
''
```
```python
yield
```
```python
pair
```
```python
(
```
```python
l_word
```
```python
,
```
```python
self
```
```python
.
```
```python
word_tag_tab
```
```python
.
```
```python
get
```
```python
(
```
```python
l_word
```
```python
,
```
```python
'x'
```
```python
)
```
```python
)
```
```python
# 该节点取词完毕，跳到下一个词语的开始位置
```
```python
x
```
```python
=
```
```python
y
```
```python
# 通过yield，逐词返回上一步切分好的词语
```
```python
if
```
```python
buf
```
```python
:
```
```python
if
```
```python
len
```
```python
(
```
```python
buf
```
```python
)
```
```python
==
```
```python
1
```
```python
:
```
```python
yield
```
```python
pair
```
```python
(
```
```python
buf
```
```python
,
```
```python
self
```
```python
.
```
```python
word_tag_tab
```
```python
.
```
```python
get
```
```python
(
```
```python
buf
```
```python
,
```
```python
'x'
```
```python
)
```
```python
)
```
```python
elif
```
```python
not
```
```python
self
```
```python
.
```
```python
tokenizer
```
```python
.
```
```python
FREQ
```
```python
.
```
```python
get
```
```python
(
```
```python
buf
```
```python
)
```
```python
:
```
```python
recognized
```
```python
=
```
```python
self
```
```python
.
```
```python
__cut_detail
```
```python
(
```
```python
buf
```
```python
)
```
```python
for
```
```python
t
```
```python
in
```
```python
recognized
```
```python
:
```
```python
yield
```
```python
t
```
```python
else
```
```python
:
```
```python
for
```
```python
elem
```
```python
in
```
```python
buf
```
```python
:
```
```python
yield
```
```python
pair
```
```python
(
```
```python
elem
```
```python
,
```
```python
self
```
```python
.
```
```python
word_tag_tab
```
```python
.
```
```python
get
```
```python
(
```
```python
elem
```
```python
,
```
```python
'x'
```
```python
)
```
```python
)
```
其中word_tag_tab在初始化加载词典阶段构建得到，它使用词语为key，对应词性为value。代码如下
```python
def
```
```python
load_word_tag
```
```python
(
```
```python
self
```
```python
,
```
```python
f
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
word_tag_tab
```
```python
=
```
```python
{
```
```python
}
```
```python
f_name
```
```python
=
```
```python
resolve_filename
```
```python
(
```
```python
f
```
```python
)
```
```python
# 遍历字典的每一行。每一行对应一个词语。包含词语 词数 词性三部分
```
```python
for
```
```python
lineno
```
```python
,
```
```python
line
```
```python
in
```
```python
enumerate
```
```python
(
```
```python
f
```
```python
,
```
```python
1
```
```python
)
```
```python
:
```
```python
try
```
```python
:
```
```python
# 去除首尾空格符
```
```python
line
```
```python
=
```
```python
line
```
```python
.
```
```python
strip
```
```python
(
```
```python
)
```
```python
.
```
```python
decode
```
```python
(
```
```python
"utf-8"
```
```python
)
```
```python
if
```
```python
not
```
```python
line
```
```python
:
```
```python
continue
```
```python
# 利用空格将一行分隔为词语 词数 词性三部分
```
```python
word
```
```python
,
```
```python
_
```
```python
,
```
```python
tag
```
```python
=
```
```python
line
```
```python
.
```
```python
split
```
```python
(
```
```python
" "
```
```python
)
```
```python
# 使用词语为key，词性为value，构造Dict
```
```python
self
```
```python
.
```
```python
word_tag_tab
```
```python
[
```
```python
word
```
```python
]
```
```python
=
```
```python
tag
```
```python
except
```
```python
Exception
```
```python
:
```
```python
raise
```
```python
ValueError
```
```python
(
```
```python
'invalid POS dictionary entry in %s at Line %s: %s'
```
```python
%
```
```python
(
```
```python
f_name
```
```python
,
```
```python
lineno
```
```python
,
```
```python
line
```
```python
)
```
```python
)
```
```python
f
```
```python
.
```
```python
close
```
```python
(
```
```python
)
```
### 4.3 未登录词，HMM隐马尔科夫模型处理
和分词一样，词性标注中，也使用HMM隐马尔科夫模型来处理未登录词。通过大规模语料统计，得到起始概率 发射概率和转移概率。分别对应prob_start.py prob_emit.py和prob_trans.py三个文件，他们给出了词语在BEMS四种情况下，每种词性对应的概率。然后使用viterbi算法，利用得到的三个概率，将观测序列（分词后的语句）转化得到隐藏序列（词性标注序列）。这样就完成了未登录词的词性标注。代码如下。
```python
# 通过HMM隐马尔科夫模型获取词性标注序列，解决未登录的问题
```
```python
def
```
```python
__cut
```
```python
(
```
```python
self
```
```python
,
```
```python
sentence
```
```python
)
```
```python
:
```
```python
# 通过viterbi算法，利用三个概率，由语句观测序列，得到词性标注隐藏序列
```
```python
# prob为
```
```python
# pos_list对应每个汉字，包含分词标注BEMS和词语词性两部分。
```
```python
prob
```
```python
,
```
```python
pos_list
```
```python
=
```
```python
viterbi
```
```python
(
```
```python
sentence
```
```python
,
```
```python
char_state_tab_P
```
```python
,
```
```python
start_P
```
```python
,
```
```python
trans_P
```
```python
,
```
```python
emit_P
```
```python
)
```
```python
begin
```
```python
,
```
```python
nexti
```
```python
=
```
```python
0
```
```python
,
```
```python
0
```
```python
# 遍历语句的每个汉字，如果是E或者S时，也就是词语结束或者单字词语，则分隔得到词语和词性pair
```
```python
for
```
```python
i
```
```python
,
```
```python
char
```
```python
in
```
```python
enumerate
```
```python
(
```
```python
sentence
```
```python
)
```
```python
:
```
```python
pos
```
```python
=
```
```python
pos_list
```
```python
[
```
```python
i
```
```python
]
```
```python
[
```
```python
0
```
```python
]
```
```python
if
```
```python
pos
```
```python
==
```
```python
'B'
```
```python
:
```
```python
# B表示词语的开始
```
```python
begin
```
```python
=
```
```python
i
```
```python
elif
```
```python
pos
```
```python
==
```
```python
'E'
```
```python
:
```
```python
# E表示词语的结束，此时输出词语和他的词性
```
```python
yield
```
```python
pair
```
```python
(
```
```python
sentence
```
```python
[
```
```python
begin
```
```python
:
```
```python
i
```
```python
+
```
```python
1
```
```python
]
```
```python
,
```
```python
pos_list
```
```python
[
```
```python
i
```
```python
]
```
```python
[
```
```python
1
```
```python
]
```
```python
)
```
```python
nexti
```
```python
=
```
```python
i
```
```python
+
```
```python
1
```
```python
elif
```
```python
pos
```
```python
==
```
```python
'S'
```
```python
:
```
```python
# S表示单字词语，此时也输出词语和他的词性
```
```python
yield
```
```python
pair
```
```python
(
```
```python
char
```
```python
,
```
```python
pos_list
```
```python
[
```
```python
i
```
```python
]
```
```python
[
```
```python
1
```
```python
]
```
```python
)
```
```python
nexti
```
```python
=
```
```python
i
```
```python
+
```
```python
1
```
```python
# 一般不会走到这儿，以防万一。对剩余的所有汉字一起输出一个词语和词性。
```
```python
if
```
```python
nexti
```
```python
<
```
```python
len
```
```python
(
```
```python
sentence
```
```python
)
```
```python
:
```
```python
yield
```
```python
pair
```
```python
(
```
```python
sentence
```
```python
[
```
```python
nexti
```
```python
:
```
```python
]
```
```python
,
```
```python
pos_list
```
```python
[
```
```python
nexti
```
```python
]
```
```python
[
```
```python
1
```
```python
]
```
```python
)
```
观测序列到隐藏序列的计算，则通过viterbi算法实现。代码如下
```python
# 通过viterbi算法，由观测序列，也就是语句，来得到隐藏序列，也就是BEMS标注序列和词性标注序列
```
```python
# obs为语句，states为"BEMS"四种状态，
```
```python
# start_p为起始概率, trans_p为转移概率, emit_p为发射概率，三者通过语料训练得到
```
```python
def
```
```python
viterbi
```
```python
(
```
```python
obs
```
```python
,
```
```python
states
```
```python
,
```
```python
start_p
```
```python
,
```
```python
trans_p
```
```python
,
```
```python
emit_p
```
```python
)
```
```python
:
```
```python
V
```
```python
=
```
```python
[
```
```python
{
```
```python
}
```
```python
]
```
```python
# 每个汉字的每个BEMS状态的最大概率。
```
```python
mem_path
```
```python
=
```
```python
[
```
```python
{
```
```python
}
```
```python
]
```
```python
# 分词路径
```
```python
# 初始化每个state，states为"BEMS"
```
```python
all_states
```
```python
=
```
```python
trans_p
```
```python
.
```
```python
keys
```
```python
(
```
```python
)
```
```python
for
```
```python
y
```
```python
in
```
```python
states
```
```python
.
```
```python
get
```
```python
(
```
```python
obs
```
```python
[
```
```python
0
```
```python
]
```
```python
,
```
```python
all_states
```
```python
)
```
```python
:
```
```python
# init
```
```python
V
```
```python
[
```
```python
0
```
```python
]
```
```python
[
```
```python
y
```
```python
]
```
```python
=
```
```python
start_p
```
```python
[
```
```python
y
```
```python
]
```
```python
+
```
```python
emit_p
```
```python
[
```
```python
y
```
```python
]
```
```python
.
```
```python
get
```
```python
(
```
```python
obs
```
```python
[
```
```python
0
```
```python
]
```
```python
,
```
```python
MIN_FLOAT
```
```python
)
```
```python
mem_path
```
```python
[
```
```python
0
```
```python
]
```
```python
[
```
```python
y
```
```python
]
```
```python
=
```
```python
''
```
```python
# 逐字进行处理
```
```python
for
```
```python
t
```
```python
in
```
```python
xrange
```
```python
(
```
```python
1
```
```python
,
```
```python
len
```
```python
(
```
```python
obs
```
```python
)
```
```python
)
```
```python
:
```
```python
V
```
```python
.
```
```python
append
```
```python
(
```
```python
{
```
```python
}
```
```python
)
```
```python
mem_path
```
```python
.
```
```python
append
```
```python
(
```
```python
{
```
```python
}
```
```python
)
```
```python
#prev_states = get_top_states(V[t-1])
```
```python
prev_states
```
```python
=
```
```python
[
```
```python
x
```
```python
for
```
```python
x
```
```python
in
```
```python
mem_path
```
```python
[
```
```python
t
```
```python
-
```
```python
1
```
```python
]
```
```python
.
```
```python
keys
```
```python
(
```
```python
)
```
```python
if
```
```python
len
```
```python
(
```
```python
trans_p
```
```python
[
```
```python
x
```
```python
]
```
```python
)
```
```python
>
```
```python
0
```
```python
]
```
```python
prev_states_expect_next
```
```python
=
```
```python
set
```
```python
(
```
```python
(
```
```python
y
```
```python
for
```
```python
x
```
```python
in
```
```python
prev_states
```
```python
for
```
```python
y
```
```python
in
```
```python
trans_p
```
```python
[
```
```python
x
```
```python
]
```
```python
.
```
```python
keys
```
```python
(
```
```python
)
```
```python
)
```
```python
)
```
```python
obs_states
```
```python
=
```
```python
set
```
```python
(
```
```python
states
```
```python
.
```
```python
get
```
```python
(
```
```python
obs
```
```python
[
```
```python
t
```
```python
]
```
```python
,
```
```python
all_states
```
```python
)
```
```python
)
```
```python
&
```
```python
prev_states_expect_next
```
```python
if
```
```python
not
```
```python
obs_states
```
```python
:
```
```python
obs_states
```
```python
=
```
```python
prev_states_expect_next
```
```python
if
```
```python
prev_states_expect_next
```
```python
else
```
```python
all_states
```
```python
# 遍历每个状态
```
```python
for
```
```python
y
```
```python
in
```
```python
obs_states
```
```python
:
```
```python
# 计算前一个状态到本状态的最大概率和它的前一个状态
```
```python
prob
```
```python
,
```
```python
state
```
```python
=
```
```python
max
```
```python
(
```
```python
(
```
```python
V
```
```python
[
```
```python
t
```
```python
-
```
```python
1
```
```python
]
```
```python
[
```
```python
y0
```
```python
]
```
```python
+
```
```python
trans_p
```
```python
[
```
```python
y0
```
```python
]
```
```python
.
```
```python
get
```
```python
(
```
```python
y
```
```python
,
```
```python
MIN_INF
```
```python
)
```
```python
+
```
```python
emit_p
```
```python
[
```
```python
y
```
```python
]
```
```python
.
```
```python
get
```
```python
(
```
```python
obs
```
```python
[
```
```python
t
```
```python
]
```
```python
,
```
```python
MIN_FLOAT
```
```python
)
```
```python
,
```
```python
y0
```
```python
)
```
```python
for
```
```python
y0
```
```python
in
```
```python
prev_states
```
```python
)
```
```python
# 将该汉字下的某状态（BEMS）的最大概率记下来
```
```python
V
```
```python
[
```
```python
t
```
```python
]
```
```python
[
```
```python
y
```
```python
]
```
```python
=
```
```python
prob
```
```python
# 记录状态转换路径
```
```python
mem_path
```
```python
[
```
```python
t
```
```python
]
```
```python
[
```
```python
y
```
```python
]
```
```python
=
```
```python
state
    last
```
```python
=
```
```python
[
```
```python
(
```
```python
V
```
```python
[
```
```python
-
```
```python
1
```
```python
]
```
```python
[
```
```python
y
```
```python
]
```
```python
,
```
```python
y
```
```python
)
```
```python
for
```
```python
y
```
```python
in
```
```python
mem_path
```
```python
[
```
```python
-
```
```python
1
```
```python
]
```
```python
.
```
```python
keys
```
```python
(
```
```python
)
```
```python
]
```
```python
# if len(last)==0:
```
```python
#     print obs
```
```python
prob
```
```python
,
```
```python
state
```
```python
=
```
```python
max
```
```python
(
```
```python
last
```
```python
)
```
```python
route
```
```python
=
```
```python
[
```
```python
None
```
```python
]
```
```python
*
```
```python
len
```
```python
(
```
```python
obs
```
```python
)
```
```python
i
```
```python
=
```
```python
len
```
```python
(
```
```python
obs
```
```python
)
```
```python
-
```
```python
1
```
```python
while
```
```python
i
```
```python
>=
```
```python
0
```
```python
:
```
```python
route
```
```python
[
```
```python
i
```
```python
]
```
```python
=
```
```python
state
        state
```
```python
=
```
```python
mem_path
```
```python
[
```
```python
i
```
```python
]
```
```python
[
```
```python
state
```
```python
]
```
```python
i
```
```python
-=
```
```python
1
```
```python
return
```
```python
(
```
```python
prob
```
```python
,
```
```python
route
```
```python
)
```
# 5 总结
jieba可以在分词的同时，完成词性标注，因此标注速度可以得到保证。通过查询字典的方式获取识别词的词性，通过HMM隐马尔科夫模型来获取未登录词的词性，从而完成整个语句的词性标注。但可以看到查询字典的方式不能解决一词多词性的问题，也就是词性歧义问题。故精度上还是有所欠缺的。
系列文章，请多关注
[Tensorflow源码解析1 – 内核架构和源码结构](https://blog.csdn.net/u013510838/article/details/84103503)
[带你深入AI（1） - 深度学习模型训练痛点及解决方法](https://blog.csdn.net/u013510838/article/details/79835563)
[自然语言处理1 – 分词](https://blog.csdn.net/u013510838/article/details/81673016)
[自然语言处理2 – jieba分词用法及原理](https://blog.csdn.net/u013510838/article/details/81738431)
[自然语言处理3 – 词性标注](https://blog.csdn.net/u013510838/article/details/81907121)
[自然语言处理4 – 句法分析](https://blog.csdn.net/u013510838/article/details/81976427)
[自然语言处理5 – 词向量](https://blog.csdn.net/u013510838/article/details/82108381)
[自然语言处理6 – 情感分析](https://blog.csdn.net/u013510838/article/details/82558797)

