
# 线性回归模型原理及推导 - 喜欢打酱油的老鸟 - CSDN博客


2019年03月07日 12:55:15[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：81标签：[线性回归																](https://so.csdn.net/so/search/s.do?q=线性回归&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)


[https://www.toutiao.com/a6665460843691377160/](https://www.toutiao.com/a6665460843691377160/)
今天我们来看一个最常见的机器学习模型——线性回归（linear regression）模型。先举个例子让你明白什么是线性回归。
现在我们有房屋面积和价格的一些数据，如下图：
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/385107c4c46d4f46b026ce02c4283a03)

现在我们想知道的是，如果给一个新的房屋面积130m²，能否根据已知的数据来预测新的面积对应的价格是多少呢？这时，线性回归模型就派上用场了。
我们先画出已知数据的散点图：
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/81a4e35375b44f8e8a4a2d7b179671af)

那线性回归要做什么呢？它是模拟出一条直线，让已知的数据点尽量落在直线上或直线周围。效果如下图红线所示：
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/2b5b6e1ab5f049f9bebe2b6934566adf)

用公式表示的话，这个线性模型就是一条直线：
f(x) = wx + b （1）
其中，w为系数，b为截距。
当我们的模型确定后，如果给定一个新数据x‘，只需把x‘带入（1）式，求得的y就是预测的价格了。
在谈如何确定w和b之前，我们先来思考一个问题：假如房屋价格不仅仅受面积因素影响呢？比如还有厕所数量、房屋高度、房屋楼层数等等因素，这时我们又该如何构建我们的线性模型呢？
其实跟（1）式一样，我们添加系数和输入即可：
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/54c991ad1ff24659953366bf4448fa76)

假设我们有n个影响房屋价格的因素，对应的就是n个系数w。（2）式就是我们新的线性模型。那模型既然出来了，该怎么求**w**和b呢？在求解之前，我们应该了解到，模型预测的结果f(x)是和实际的结果y有一定出入的。也就是说，模型在对原始数据进行训练时，数据点并不一定都落在该模型上。这样就会产生误差，不同的**w**和b对应不同的线性模型，我们要找的就是误差最小的那个模型所对应的**w**、b的值。
首先第一步，我们要找一个代价函数（cost function），用它来衡量我们预测的误差，一般用平方和函数来表示：
![线性回归模型原理及推导](http://p9.pstatp.com/large/pgc-image/23c1a8a17b7d402f9b66db2db79c743a)

其中，J就是我们的代价函数，f(x)是我们的预测值，y是训练集数据的实际值，m代表数据的个数。代价函数描述的其实是预测值和实际值差的平方和的均值，那我们在前面为什么要加个1/2呢？这纯粹是为了以后求导方便，而且并不影响我们对代价函数的优化。
然后，我们将预测函数f(x)做一个变形，写成向量乘积的形式。
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/e39387c0e3c24d269d9a90ecbd3a65ec)

则（2）式可改写为：
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/2b2bbc40ba324aa99a75dfafe656fa4b)

不难看出，f(x)可以写作**w**和**x**两个向量的乘积：
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/495d8f4f4b224221a2ed77dc1e892ddc)

进一步缩写为：
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/d0025f9c92d24736a1f2edd16fdcae73)

其中，**w**和**x**均为n+1维列向量。再将（4）式代入（3）式，可得：
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/a33ef8b40cd64886851e89e26bd65dfc)

由f(x)可知，乘积项的求和可以写为两个向量乘积的形式。要想写成向量形式，我们需要将J(**w**)做一个变形：
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/73d79f8f441f48e6bccee6791bbd6185)

展开，得到：
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/06b25d1081be4688b6e41d3a290f7950)

其中，*X*为m*(n+1)维矩阵，m是数据的样本个数，n是每一个数据的特征。**w**为参数向量，**y**为实际值向量。
OK，现在我们已经把代价函数化简完成了，但别忘了我们的目标是求当J(**w**)取最小值时，所对应**w**的值。可以证明J(**w**)为凸函数（证明过程暂略），所以当我们对J(**w**)求导时，令导数为0，J(**w**)即可取最小值，相应可求得**w**。（注意**w**为向量）
在求导之前，我们先记住几个结论，一会就直接用了：
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/4b04312e5a844d33aadd0a882341a693)

随后，我们令**k**=*X***w-y**，则J(w)可写为
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/978ebf99963745a892f037c7f220866a)

对其求导，应用结论1，得
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/c5788e4611fc4954a23d9eec035416c8)

我们先求**k'**，运用结论3，可得
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/46af7c9575be4555bd9fbaffd6f24e36)

运用结论5，将其改写为
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/cbe7c46bc8f143349c47a9bf27c3ee69)

运用结论2，继续求导可得
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/f90bcc15e43640c987c91326c005fd2f)

运用结论4、5，并化简
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/d021032e5d5a4199bb51977694c01cdd)

将**k**=*X***w-y**代入并展开，可得
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/44d40003eb4c4b12a61d5ae3a3d7828c)

到此为止，我们的求导过程就完成了。接下来，令导数为0，求得w的值即可。令J'(**w**)=0，可得
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/a351fcf2fdb446cf804e782882630417)

一般情况下，矩阵
![线性回归模型原理及推导](http://p3.pstatp.com/large/pgc-image/2e4b0fd3b52f4c4683e87898c239996c)
为可逆矩阵（不可逆的情况我们以后会讨论），即可求得**w**的解为
![线性回归模型原理及推导](http://p1.pstatp.com/large/pgc-image/78a2d9e5244c4a04bd3ab88794db5d10)

Bingo！（5）式就是我们在代价函数取最小值时，求得的参数向量**w**。在实际应用中，计算机会帮我们对（5）式求解，得到**w**的值。但当我们的数据量非常大时，可能会拖慢运行速度。这时，另一种对J(w)优化的算法——“梯度下降法”就该上场了，我们下篇再对它进行介绍。

