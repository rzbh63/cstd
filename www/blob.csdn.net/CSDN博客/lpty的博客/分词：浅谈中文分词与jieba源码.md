
# 分词：浅谈中文分词与jieba源码 - lpty的博客 - CSDN博客

2017年12月01日 20:56:14[lpty](https://me.csdn.net/sinat_33741547)阅读数：1577所属专栏：[自然语言基础](https://blog.csdn.net/column/details/22512.html)



## 一、前言
### 1、什么是中文分词？
中文文本，从形式上看是由汉字、标点符号等组成的一个字符串。由字组成词，再组成句子、文章等。那么分词，就是按照一定的规则把字符串重新组合成词序列的过程。
### 2、为什么要分词？
（1）在中文里面，词是最小的能够独立活动的有意义的语言成分
（2）英文中单词以空格作为自然分界，虽然也有短语划分的问题。但中文词没有一个形式上的分界，相对而言难度大了许多
（3）分词作为中文自然语言处理的基础工作，质量的好坏对后面的工作影响很大
### 3、分词的难点？
（1）歧义消解问题
输入待切分句子：提高人民生活水平
可以切分输出 ：提高/人民/生活/水平
或者切分输出：提/高人/民生/活水/平
可以看到，明显第二个输出为歧义切分。
（2）未登录词识别
未登录词指的是在已有的词典中，或者训练语料里面没有出现过的词，分为实体名词，专有名词及新词。
### 4、怎么分词？
（1）基于字典、词库匹配的分词
机械分词算法，将待分的字符串与一个充分大的机器词典中的词条进行匹配。
分为正向匹配和逆向匹配；最大长度匹配和最小长度匹配；单纯分词和分词与标注过程相结合的一体化方法。所以常用的有：正向最大匹配，逆向最大匹配，最少切分法。
实际应用中，将机械分词作为初分手段，再利用其他方法提高准确率。
（2）基于词频统计的分词
统计分词，是一种全切分方法。切分出待分语句中所有的词，基于训练语料词表中每个词出现的频率，运用统计模型和决策算法决定最优的切分结果。
（3）基于知识理解的分词
主要基于句法、语法分析，并结合语义分析，通过对上下文内容所提供信息的分析对词进行定界。
这类方法试图让机器具有人类的理解能力，需要使用大量的语言知识和信息，目前还处在试验阶段。
## 二、jieba分词源码解析
jieba分词，目前是python中文分词方面比较好的工具。支持精确、全模式及搜索引擎模式的分词，具体可以请看jieba文档：[https://github.com/fxsjy/jieba](https://github.com/fxsjy/jieba)
在文档中，jieba列出了工具实现的算法策略：
1）、基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)
2）、采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合
3）、对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法
接下来我们来看看，具体jieba是怎么实现这些算法的。
### 1、前缀词典
前缀词典，实际上可以认为是一个词频词典（即：../jieba/dict.txt），具体实现参见代码中的Tokenizer.FREQ字典，读取dict.txt文件，之后转化为{词：频率}的形式，如下：

```python
def gen_pfdict(self, f_name):
        lfreq = {} # 字典存储  词条:出现次数
        ltotal = 0 # 所有词条的总的出现次数
        with open(f_name, 'rb') as f: # 打开文件 dict.txt 
            for lineno, line in enumerate(f, 1): # 行号,行
                try:
                    line = line.strip().decode('utf-8') # 解码为Unicode
                    word, freq = line.split(' ')[:2] # 获得词条 及其出现次数
                    freq = int(freq)
                    lfreq[word] = freq
                    ltotal += freq
                    for ch in xrange(len(word)):# 处理word的前缀
                        wfrag = word[:ch + 1]
                        if wfrag not in lfreq: # word前缀不在lfreq则其出现频次置0 
                            lfreq[wfrag] = 0
                except ValueError:
                    raise ValueError(
                        'invalid dictionary entry in %s at Line %s: %s' % (f_name, lineno, line))
        return lfreq, ltotal
```

### 2、有向无环图DAG
DAG是基于前面得到的前缀词典进行构造的，基本思想是将待分语句进行全切分，将切分完的词语列表与前缀词典进行比较，如果这个词存在，就存下来，并用词在字符串中的位置索引进行替代，转化为{key:list[i,j…], …}的字典结构，如下：
假设有句子，去北京大学玩，则对应的DAG为，{0 : [0], 1 : [1, 2, 4], 2 : [2], 3 : [3, 4], 4 : [4], 5 : [5]} 这样一个字典。
具体意思是，0：[0]代表0位置，即‘去’在前缀词典中代表一个词，同理1 : [1, 2, 4]代表‘北’，‘北京’，‘北京大学’。
### 3、基于词频的最大切分
我们拥有所有可能出现的词及其对应的所有可能组成路径（即DAG），那么应该用什么方法去找到一条最可能的路径呢？
jieba中使用了[动态规划](https://www.zhihu.com/question/39948290)的方法，这里简单的理解就是对于一个长度为n的句子，由后往前遍历。
假设最后一个词出现的概率为N，倒数第二个词出现的概率为N-1，那么两个词在一起出现概率则为N（N-1），以此类推，直到到达第一个词，计算出所有可能路径中的最大概率路径。这里还需要理解两个名词，重叠子问题，最优子结构，最终得到最优的路径，如下：

```python
#动态规划，计算最大概率的切分组合
    def calc(self, sentence, DAG, route):
        N = len(sentence)
        route[N] = (0, 0)
         # 对概率值取对数之后的结果(可以让概率相乘的计算变成对数相加,防止相乘造成下溢)
        logtotal = log(self.total)
        # 从后往前遍历句子 反向计算最大概率
        for idx in xrange(N - 1, -1, -1):
           # 列表推倒求最大概率对数路径
           # route[idx] = max([ (概率对数，词语末字位置) for x in DAG[idx] ])
           # 以idx:(概率对数最大值，词语末字位置)键值对形式保存在route中
           # route[x+1][0] 表示 词路径[x+1,N-1]的最大概率对数,
           # [x+1][0]即表示取句子x+1位置对应元组(概率对数，词语末字位置)的概率对数
            route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -
                              logtotal + route[x + 1][0], x) for x in DAG[idx])
```

### 4、对于未登录词实现的HMM模型
#### 1、什么是HMM？
隐藏马尔科夫模型（Hidden Markov Model），关于时序的模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。具体可以参照：[52nlp HMM系列文章](http://www.52nlp.cn/hmm%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%E7%B4%A2%E5%BC%95)
#### 2、隐马尔科夫模型五元组？
隐马尔科夫模型由初始状态概率向量pi、状态转移概率矩阵A和观测概率矩阵B决定，pi和A决定状态序列I，B决定观测序列O。
#### 3、隐马尔科夫模型基本假设？
（1）齐次马尔科夫性，即假设隐藏的马尔科夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关。
（2）观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测状态无关。
#### 4、隐马尔科夫基本问题？
**（1）评估问题(概率计算问题)**
即给定观测序列 O=O1,O2,O3…Ot和模型参数λ=(A,B,pi)，怎样有效计算这一观测序列出现的概率.
(Forward-backward算法)
**（2）解码问题(预测问题)**
即给定观测序列 O=O1,O2,O3…Ot和模型参数λ=(A,B,pi)，怎样寻找满足这种观察序列意义上最优的隐含状态序列S。
(viterbi算法,近似算法)
**（3）学习问题**
即HMM的模型参数λ=(A,B,pi)未知，如何求出这3个参数以使观测序列O=O1,O2,O3…Ot的概率尽可能的大.
(极大似然估计的方法估计参数,Baum-Welch,EM算法)

#### 5、jieba中实现的模型
**（1）观测序列O**
对于每一个待分的句子，都视为一个观测序列，如：去北京大学玩，就是一个长度T为6的观测序列
**（2）状态序列I**
每一个观测序列，都对应着相同长度的状态序列。这里将汉字按SBME进行标注，分别代表single（单独成词的字）、begin（一个词语开始字）、middle（一个词语中间的字）、end（一个词语结束的字），如：
观测序列：去 / 北京大学 / 玩
状态序列：S / BMME / S
**（3）初始概率分布pi**
对应jiaba/finalseg/prob_start.py文件，如下：

```python
P={'B': -0.26268660809250016,
 'E': -3.14e+100,
 'M': -3.14e+100,
 'S': -1.4652633398537678}
```
这里可以看到，初始状态只可能是B或者S，和实际相符。
**（4）状态转移概率分布A**
对应jieba/finalseg/prob_trans.py文件，如下：

```python
P={'B': {'E': -0.510825623765990, 'M': -0.916290731874155},
 'E': {'B': -0.5897149736854513, 'S': -0.8085250474669937},
 'M': {'E': -0.33344856811948514, 'M': -1.2603623820268226},
 'S': {'B': -0.7211965654669841, 'S': -0.6658631448798212}}
```
如P[‘B’][‘E’]代表的含义就是从状态B转移到状态E的概率，由P[‘B’][‘E’] = -0.510825623765990，表示状态B的下一个状态是E的概率对数是-0.510825623765990。
**（5）观测概率分布B**
对应jieba/finalseg/prob_emit.py文件，如下：

```python
P={'B': {'\u4e00': -3.6544978750449433,
       '\u4e01': -8.125041941842026,
       '\u4e03': -7.817392401429855,
       '\u4e07': -6.3096425804013165,
       ...}
```
比如P[‘B’][‘\u4e00’]代表的含义就是’B’状态下观测的字为’\u4e00’(对应的汉字为’一’)的概率对数P[‘B’][‘\u4e00’] = -3.6544978750449433。
**（6）这些概率分布是怎么来的？**
隐马尔科夫基本问题的第三个--学习问题，但这里用到的方法是极大似然估计，具体请看：[模型的数据是怎么生成的？](https://github.com/fxsjy/jieba/issues/7)：
1）初始概率分布pi
统计所有训练样本，以状态S、B、M、E为初始状态的数量，之后分别除以训练样本总词频，就可以得到初始概率分布
2）状态转移概率分布A
同理，统计所有样本中，从状态S转移到B的出现次数，再除以S出现的总次数，便得到由S转移到B的概率分布，其他可得
3）观测概率分布B
统计训练数据中，状态为j并观测为k的频数，除以训练数据中状态j出现的次数，其他同理可得
**（7）jieba是怎么利用HMM进行切词的？**
隐马尔科夫基本的第二个--解码问题（很形象，假设中文语句是状态序列的某种形式的编码，利用模型进行解码），这里用到了维比特算法（可以认为是动态规划）
关于算法的细节部分请看源码：[结巴分词](https://github.com/fxsjy/jieba/blob/master/jieba/__init__.py)
#### 6、其他
推荐另一个作者对于jieba的注释代码，写的很详细：[结巴分词-注释版](https://github.com/ustcdane/annotated_jieba/blob/master/jieba/finalseg/__init__.py)




