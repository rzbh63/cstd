
# BP神经网络的线性本质的理解和剖析-卷积小白的随机世界 - 喜欢打酱油的老鸟 - CSDN博客


2019年04月01日 12:39:23[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：45标签：[BP神经网络																](https://so.csdn.net/so/search/s.do?q=BP神经网络&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)


[https://www.toutiao.com/a6674387399058915852/](https://www.toutiao.com/a6674387399058915852/)
在完成上篇（第三十一篇）感知机的理解文章后，本想开始梳理SVM（支持向量机），不想感知机也是深度学习神经网络的源头。于是小白根据对感知机的理解，进一步思考一个基本深度学习模型---BP（Back Propagation）神经网络，深深被其背后简单，深刻，奇妙的思想所吸引。因此，本篇以思维递进方式为文章内容推进的线索，记录下小白对BP神经网络的理解和思考，进而呈现小白所感受的简单，深刻和奇妙。
# 第三十二篇 BP神经网络的线性本质的理解和剖析
# 我们看待函数的视角
我们如何理解一个事物，关键在于我们看待事物的视角。比如函数y=x^2，我们可以看作是二维坐标轴的抛物线，也看以理解为一个正方形的面积的随着边长而增长。
**那么，我们有哪些视角来看待一个线性函数呢？**
视角1：线性函数是一条直线或一个超平面
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p1.pstatp.com/large/pgc-image/6d474536ff3d4b1fba0cbfc85968ff6f)
如上图所表示，两者都为线性函数。对于二位坐标空间，线性函数为一条直线；对于三维坐标空间，线性函数为一个二维的平面；**对于N维坐标空间，线性函数为一个N-1位的超平面；**
视角2：线性函数创造了一维的线性空间和一维的特征
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p1.pstatp.com/large/pgc-image/1d1db1af712144b9a16496dd4089ccd4)
如上图，0=Wx+b为一个直线，x是一个二维向量，实际为（x，y），如：0=3x+2y+b。我们有理解，0=Wx+b是一个平面，假如令f(x)=Wx+b，对超平面上的点有f（x）=0。
对于超平面外的点有f(x)=m 。我们可以体会：f(x)是一个线性空间，把Ｎ维的空间映射成一个一维的空间：
**ｍ是ｘ在新空间（新的一维空间）下的坐标（或者叫特征，或者理解为"通过线性函数形成对于样本x的重新标记，这个标记是m"）。我们知道这个坐标"ｍ"就是函数距离，如果函数归一化后，ｍ就是ｘ到超平面（Ｗｘ＋ｂ＝０）的几何距离。**
视角3：线性函数即感知机，也即神经元
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p1.pstatp.com/large/pgc-image/e8d2eff869484c3f9711b485f06c9988)
如上图，左边是一个神经细胞，它内部有一个感知机（线性函数），当外界刺激它时，他会把他转化为坐标，然后产生是移动（叉表示 ）或静止不动的行为（圈表示）。而右图则是一块砖，对外界的刺激不会产生任何反馈。
我们知道，左侧其实就是一个生命体（试想万亿年前地球上第一个具有神经反应的细胞产生），因为，它本身是一个感知机，可以对外界的刺激产生行为，这个感知机是一个线性函数；而右侧是一个无机体。我们可以形象的把线性神经元表示成如下形式。
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p3.pstatp.com/large/pgc-image/92fe692bd5e14dcea68a549477b2ae63)
视角四：线性函数可以无限逼近任何事物的边界
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p3.pstatp.com/large/pgc-image/40a00a2c6db1435888cce195f4f99d80)
`如上右侧的图，对于一个曲线，我们可以想象，可以用很多的切线无限的逼近。但同时，我们结合视角２看（如左图），假如我们用Ｎ条切线逼近，切线其实就是线性函数，每条切线相当于一个线性空间。这样，相当于产生一个Ｎ维的线性空间，那么：
**原来的P点在Ｎ维的线性空间下有一个对应一个Ｎ维的坐标(x1,x2,x3…xn) ，也即Ｎ维的特征，每个特征即是Ｐ点到切线的距离。**
到这里，小白有一个想法。一个感知机是一个线性函数，那么，多个感知机是多个线性函数。如上篇关于感知机的文章介绍，感知机可以学习到线性分类边界。那么：
**多个感知机组织在一起是否可以学习和逼近一个非线性分类的边界。既然感知机可以理解为神经元，那么就称它为"神经网络"。答案是可以的。**
# 神经网络建模
根据前面的介绍，其实我们可以形成如下的一个结构，来表示刚才对曲线的逼近。
**输入层：**是原始数据输入；
**中间层：**由多个神经元组成，每个神经元其实是一个线性空间，会根据输入层的输入产生一个特征，Ｎ个中间层神经元相当于Ｎ维线性空间，将产生Ｎ个特征；
**输出层：**根据中间层的产生的N个特征再进行线性映射形成对最终特征的判断（也成预测）。
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p1.pstatp.com/large/pgc-image/244f317a453f4f5abe63ca635ca5d568)
小白思考，上面的中间层以及输出层的四个神经元总体能呈现出**非线性的表达**吗？答案是否否定的。我们来推导下。假如，中间层神经元分别是(w1,b1), (w2,b2), (w3,b3)，输出层Ｗ=(c1,c2,c3)。那么，我们有如下推导：
Y=W(w1x+b1),(w2x+b2),w3x+b3)+b
=c1w1x+c1b1+c2w2x+c2b2+c3w3x+c3b3+b
=(c1w1+c2w2+c3w3)x + (c1b1+c2b2+c3b3+b)
**我们发现上述中间层和输出层两层神经元的结构，本质上只能表达一条直线，不能够表达曲线。因此，**我们需要持续改进上述模型。
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p1.pstatp.com/large/pgc-image/4d1c7bad21f948ab9b9a2378b3187c6b)
回到刚才思路，如上我们发现，对于线性逼近，我们建模了每条切线（每一个中间层的神经元），但是我们发现我们忽视了一个因素，**即切线如何过度到另外一条切线？这个因素是必要的，只有这样，多条切线才可手拉手形成对非线性边界的逼近。**
我们思考，切线的过度形成对曲线的逼近，相当于我们建模了从一个切点滑动到另一个切点。我们回忆下上篇关于"感知机"介绍的描述：
"**越靠近最优超平面的点，被误分类次数越多，被叠加的越多，其对Ｗ的影响越大"。**
而当前的切点，不仅仅是靠近最优超平面（逼近的切线），而是就在超平面（切线）上，因此，其不仅仅是影响，而是完全决定。因此，对于切点的滑动，其行为的本意是舍弃一个感知机（切线），然后决定了另一个感知机（切线）。因此，我们体会到，**当我们试图逼近曲线时，表现为切线在曲线周围滑动，这个过程可以看作，一个切线沉寂，另一个切线被激活的过程。因此，我们可以在感知机的后面加一个激活的处理，我们称之为激活函数。**于是，我们的线性神经元，加上了一个激活开关，如下：
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p3.pstatp.com/large/pgc-image/af5eac45201f4161ba4c09e72235f45f)

# 前馈神经网络
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p1.pstatp.com/large/pgc-image/1b84c81229db4d76ba86d7a4f6bdff80)
根据上面的分析，我们顺理成章，改善模型并得到了如上图神经网络模型，称之为**前馈神经网络。**这个神经网络模型有两层（隐藏层，输出层）神经元，从输输入到输出逐层映射，最终输出到Y，故名前馈。
对于前馈神经网络，有输入层，输出层，**中间层我们称之为隐藏层，或隐层。**
# BP（Back Propagation）算法
有了前馈神经网络模型，我们下一步需要思考，如何训练这个模型（也即是这个模型如何学习），我们不防借助于感知机的思想，如下图：
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p1.pstatp.com/large/pgc-image/dbed8c0edb1045099fd666fc6e612305)
从感知机的学习过程看，整个过程都聚焦于xi（神经元的输入），w（神经元输入的权重）以及yi（输出的偏差）三者的计算和调整，即根据当前的感知机的状态w和输入xi得到输出（即预测值），通过输出和样本xi的特征标记得到误差yi，yi和xi的乘积（yi*xi）即为梯度，然后沿着梯度的方向通过一定的学习速率来调整w。依葫芦画瓢，对于前馈神经网络，我们的思路是：
**计算隐藏层和输出层神经元的误差，根据误差和输入调整隐藏层和输出层的神经元的ｗ。此外，我们还需要注意一点，对于前馈神经网络，我们引入了激活函数。如下：**
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p1.pstatp.com/large/pgc-image/b9bf07daf805464e964c3ceff865d831)
如下计算输出层神经元的梯度：
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p3.pstatp.com/large/pgc-image/d99ba2815d3941998a9079b80e2988de)
我们再思考，隐藏层神经元只有预测值，而没有误差，因为，隐藏层没有样本的标记。但，我们知道，隐藏层神经元是我们创造的一个中间过程的线性感知机，那么我们其实也同时创造对应的样本标记，并且这些隐藏层神经元的样本标记通过输出层神经元再次叠加成最终的标记。**因此，我们需要找出我们创建的隐藏层神经元的误差，它是存在的。并且我们知道，输出层的误差本质上就是隐藏层的误差，他们之所以不一样，是因为所在空间不一样。**
**前者，是在输出神经元构成的多维线性空间下观察到的误差，而后者是在隐藏层神经元所构成的多维线性空间下观察到的误差。因此，我们把输出层的误差，退回到（Back Propagation）隐藏层多维空间下就会得到隐藏层各神经元的误差。**
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p3.pstatp.com/large/pgc-image/a43f2c0664bf4748b8ec8681bca8b7b6)
上图的推导，是相对于"输出层神经元的误差"的 隐层神经元输入权重调整的梯度。我们可以看到上图中的梯度是通过多个"隐层神经元误差和 隐层神经元的输入乘积"的叠加的方式呈现。于是，对于单个隐层神经元，我们得到：
![BP神经网络的线性本质的理解和剖析-卷积小白的随机世界](http://p9.pstatp.com/large/pgc-image/d350178c0a8947b6b87d8e3702c9eeeb)
这样我们就可以更新隐层神经元的权重了。
# 总结
小白体会，如果认为感知机是一个线性神经元的话，那么，BP神经网络是感知机神经元的一次进化，这个进化体现在三个方面。
1， 感知神经元增多了，进而感知更多的信息；
2， 感知神经元增多了以后，那么，就需要对多个神经元产生的特征进行总和和汇总（即再次的感知和映射）。因此，产生了分层；
3， 多个神经元之间需要激活函数作为连接体，这样，才能完备的形成对非线性边界的模拟和逼近；
而BP算法本质上也是梯度下降算法伴随着多层神经网络进化而成，其本质仍是梯度下降算法。BP后向传播可以从误差从深层（如输出层）到浅层神经元（入隐层）的逐层传播，进而"全网同步计算各神经元输入权重的梯度下降幅度"。因此，BP神经网络是局部最优的。需要其他算法的辅助来达到全局最优。

