
# 欠拟合和过拟合学习笔记 - 喜欢打酱油的老鸟 - CSDN博客


2018年09月10日 16:16:20[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：74


**欠拟合和过拟合学习笔记**
[https://www.cnblogs.com/DicksonJYL/p/9620464.html](https://www.cnblogs.com/DicksonJYL/p/9620464.html)
在建模的过程中会经常出现1.模型的效果，但是泛化能力弱，2.模型的结果很差的情况，即过拟合和欠拟合，一下是总结的学习笔记
**1.1****欠拟合**
**欠拟合现象：**模型准确率低
**欠拟合原因：**模型没有很好地捕捉到数据特征，不能够很好地拟合数据，
**欠拟合解决方法：**
1）特征项不够增加特征项。例如，'拉长时间'，‘’变量组合‘’、“特征泛化”、“相关性”等
2）添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。
3）减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。
**1.2****过拟合**
**过拟合现象：**在训练样本中表现得过于优越，导致在验证数据集以及测试数据集中表现不佳。模型的泛化能力较弱
**过拟合原因：**1.训练数据不够，即训练数据无法对整个数据进行评估，2.对模型过度训练
**解决方法：**
1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。
2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。
3）采用正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则，下面看具体的原因。
4）采用dropout方法。这个方法在神经网络里面很常用。dropout方法是ImageNet中提出的一种方法，通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作
**1.2.1 L0****、L1、L2**
**参考：(****https://blog.csdn.net/lk274857347/article/details/76887345****)**
**L0****范数**是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。这太直观了，太露骨了吧，换句话说，让参数W是稀疏的。OK，看到了“稀疏”二字，大家都应该从当下风风火火的“压缩感知”和“稀疏编码”中醒悟过来，原来用的漫山遍野的“稀疏”就是通过这玩意来实现的。但你又开始怀疑了，是这样吗？看到的papers世界中，稀疏不是都通过L1范数来实现吗？脑海里是不是到处都是||W||1影子呀！几乎是抬头不见低头见。没错，这就是这节的题目把L0和L1放在一起的原因，因为他们有着某种不寻常的关系。那我们再来看看L1范数是什么？它为什么可以实现稀疏？为什么大家都用L1范数去实现稀疏，而不是L0范数呢？
**L1****范数**是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。现在我们来分析下这个价值一个亿的问题：为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微，但这还是不够直观。这里因为我们需要和L2范数进行对比分析。所以关于L1范数的直观理解
**L2****范数**||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。这用的很多吧，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。至于过拟合是什么，上面也解释了，就是模型训练时候的误差很小，但在测试的时候误差很大，也就是我们的模型复杂到可以拟合到我们的所有训练样本了，但在实际预测新的样本的时候，糟糕的一塌糊涂。通俗的讲就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识。

