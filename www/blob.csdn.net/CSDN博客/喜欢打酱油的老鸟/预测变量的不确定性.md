
# 预测变量的不确定性 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月26日 21:21:49[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：47


[https://www.toutiao.com/a6646320437884092936/](https://www.toutiao.com/a6646320437884092936/)
2019-01-14 19:57:45
在本文中，我们考虑的情况是预测变量的值不可信，而不是目标变量的值可信。
为了简单起见，我们考虑一个带有一个数值预测器的简单线性回归问题。我们将使用正态分布对预测值中的不确定性进行建模。此依赖关系的信念网显示如下：
![预测变量的不确定性](http://p1.pstatp.com/large/pgc-image/6534c9fefc4046dcade2c90ef9126479)
这里y是观察到的目标，X是观察（但是不可信的预测器），*ξ*是预测变量的真实但未观察到的值，而*θ*是线性回归模型的参数。由于*ξ*未被观察到，我们应该将其边缘化。
![预测变量的不确定性](http://p1.pstatp.com/large/pgc-image/7287c0f9810d48c1a55a393f502f201b)
这里我们使用了关于联合概率的贝叶斯规则。概率值如下：
![预测变量的不确定性](http://p3.pstatp.com/large/pgc-image/e3116297c88e4cbcb396540d03a1c0da)
在这里，我们再次使用贝叶斯规则，并在ξ上假设平坦。我们引入了一个测量确定性τ。当它是无穷大时，我们确定预测器的值，当它很小时，不确定性很大，变量不可信。可以在数据标记期间设置此值。
当我们忽视无法观测变量ξ通过整合ξ的联合概率在整个范围 ，我们将得到以下概率分布：
![预测变量的不确定性](http://p1.pstatp.com/large/pgc-image/48449afb26b9481aa34113f943e81421)
损失函数（对于单个案例）是此概率的负对数，然后乘以σ2：
![预测变量的不确定性](http://p1.pstatp.com/large/pgc-image/580047c9f1ed402596391753ce860e83)
第一项是正则化项。事实上,在一个非常高的τ它可以近似：
![预测变量的不确定性](http://p3.pstatp.com/large/pgc-image/1509441dd03c43e19d1a0ff1f655633f)
也就是L2正则化项。有趣的是我们如何推导出基于预测不确定性的L2正则化。我们看到不确定性越高我们需要对模型进行更高的正则化。
第二项是线性回归的修正损失函数。分母的额外因素使得它成为non-quadraticθ的函数。我们将绘制不同σ和τ值的损失函数
```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
x = 1
y = 5
def loss_function(theta, sigma, tau):
 reg_dem = 1 + theta**2/tau
 return 1/2*sigma**2 * np.log(reg_dem) + (y-theta*x)**2/2/reg_dem
plt.rcParams["figure.figsize"] = (15,10)
fig, ax = plt.subplots(1, 1)
theta = np.linspace(-5, 10, 200)
ax.plot(theta, loss_function(theta, 1,1000), 'r-', lw=1, alpha=1, label='s=1,t=1000')
ax.plot(theta, loss_function(theta, 1,100), 'y-', lw=1, alpha=1, label='s=1,t=100')
ax.plot(theta, loss_function(theta, 1,10), 'g-', lw=1, alpha=1, label='s=1,t=10')
ax.plot(theta, loss_function(theta, 1,5), 'c-', lw=1, alpha=1, label='s=1,t=10')
ax.plot(theta, loss_function(theta, 1,1), 'b-', lw=1, alpha=1, label='s=1,t=1')
ax.plot(theta, loss_function(theta, 1,0.1), 'm-', lw=1, alpha=1, label='s=1,t=0.1')
plt.legend();
```
![预测变量的不确定性](http://p3.pstatp.com/large/pgc-image/f05b8fc4edfa4fd0a278ff4d40a93980)
![预测变量的不确定性](http://p3.pstatp.com/large/pgc-image/2b1f478a89dc435cbd583f2c095db5e2)
首先我们看到损失函数是非凸的，可能有两个局部极小值。
我们还看到，对于高置信度因子τ，损失函数几乎是二次曲线，最低期望值为5。但随着不确定性增加，最小转移θ值越低，并且曲线不再像抛物线那样。
处理分类特征
如果你有不确定值的分类特征，这些特征也可以用在模型中，模型必须调整以处理不确定性。如果我们使用分类特征的独热编码，似然函数（1）必须重写为：
![预测变量的不确定性](http://p1.pstatp.com/large/pgc-image/3e4f34a0bd99449a818520291cd6c3ae)
这里我们引入了参数c代替θ，每个值对应一个类别。然后cξ类别的分类参数的值是ξ值。
为了模拟预测值中的不确定性，我们需要引入一个不确定性矩阵U，其基础是条件概率：
![预测变量的不确定性](http://p1.pstatp.com/large/pgc-image/c64538fc217b470d9589bd13414a3b27)
然后，边缘化ξ，我们可以得到后验概率：
![预测变量的不确定性](http://p3.pstatp.com/large/pgc-image/9c57ea24497d4a7da37bec057d3804e4)
矩阵U的值是在标定过程中设定的。但是，对于值太多的类别，由于U的值太多，很难正确估计出U的所有值。通常需要使用近似值。这种近似的一个例子可以是对称近似：
![预测变量的不确定性](http://p3.pstatp.com/large/pgc-image/adcc505e292c4cefaa47302cd875c6b7)
这里N是类别的数量。那么概率分布可以简化为：
![预测变量的不确定性](http://p1.pstatp.com/large/pgc-image/52537c96b56c47c6b409cee67abab128)
**结论**
有些情况下，目标值可以信任，但是一个或多个预测值是不确定的。在数据标注的过程中可以识别出这些情况，并且应该在模型中需要使用不确定度来修正预测。我们使用简单的贝叶斯方法。我们证明了小的不确定性会导致L2正则化项。对于分类特征，使用了类似的方法。我们只考虑回归问题。我们也可以使用类似的技术来解决分类问题，但数学结果可能会有所不同。

