
# Python和PyTorch对比实现cross-entropy交叉熵损失函数及反向传播 - BrightLamp的博客 - CSDN博客


2018年11月13日 16:50:10[BrightLampCsdn](https://me.csdn.net/oBrightLamp)阅读数：1079所属专栏：



## 摘要
本文使用纯 Python 和 PyTorch 对比实现cross-entropy交叉熵损失函数及其反向传播.
## 相关
*原理和详细解释, 请参考文章 :*
通过案例详解cross-entropy交叉熵损失函数
*系列文章索引 :*
[https://blog.csdn.net/oBrightLamp/article/details/85067981](https://blog.csdn.net/oBrightLamp/article/details/85067981)
## 正文
## 1. 定义:
$$
E = -\sum_{i = 1}^{k}y_{i}log(s_{i})\\
$$

## 2. 梯度:
$$
\triangledown E_{(s)} =(\frac{\partial E}{\partial s_{1}},\frac{\partial E}{\partial s_{2}}, \cdots, \frac{\partial E}{\partial s_{k}}) \\
\quad \\
=( -\frac{y_1}{s_1}, -\frac{y_2}{s_2},\cdots,-\frac{y_k}{s_k})
$$

## 3. Python和PyTorch对比实现
```python
import
```
```python
torch
```
```python
import
```
```python
numpy
```
```python
as
```
```python
np
```
```python
class
```
```python
Entropy
```
```python
:
```
```python
def
```
```python
__init__
```
```python
(
```
```python
self
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
nx
```
```python
=
```
```python
None
```
```python
self
```
```python
.
```
```python
ny
```
```python
=
```
```python
None
```
```python
self
```
```python
.
```
```python
dnx
```
```python
=
```
```python
None
```
```python
def
```
```python
loss
```
```python
(
```
```python
self
```
```python
,
```
```python
nx
```
```python
,
```
```python
ny
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
nx
```
```python
=
```
```python
nx
        self
```
```python
.
```
```python
ny
```
```python
=
```
```python
ny
        loss
```
```python
=
```
```python
np
```
```python
.
```
```python
sum
```
```python
(
```
```python
-
```
```python
ny
```
```python
*
```
```python
np
```
```python
.
```
```python
log
```
```python
(
```
```python
nx
```
```python
)
```
```python
)
```
```python
return
```
```python
loss
```
```python
def
```
```python
backward
```
```python
(
```
```python
self
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
dnx
```
```python
=
```
```python
-
```
```python
self
```
```python
.
```
```python
ny
```
```python
/
```
```python
self
```
```python
.
```
```python
nx
```
```python
return
```
```python
self
```
```python
.
```
```python
dnx

np
```
```python
.
```
```python
random
```
```python
.
```
```python
seed
```
```python
(
```
```python
123
```
```python
)
```
```python
np
```
```python
.
```
```python
set_printoptions
```
```python
(
```
```python
precision
```
```python
=
```
```python
3
```
```python
,
```
```python
suppress
```
```python
=
```
```python
True
```
```python
,
```
```python
linewidth
```
```python
=
```
```python
120
```
```python
)
```
```python
entropy
```
```python
=
```
```python
Entropy
```
```python
(
```
```python
)
```
```python
x
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
random
```
```python
(
```
```python
[
```
```python
5
```
```python
,
```
```python
10
```
```python
]
```
```python
)
```
```python
y
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
random
```
```python
(
```
```python
[
```
```python
5
```
```python
,
```
```python
10
```
```python
]
```
```python
)
```
```python
x_tensor
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
x
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
y_tensor
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
y
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
loss_numpy
```
```python
=
```
```python
entropy
```
```python
.
```
```python
loss
```
```python
(
```
```python
x
```
```python
,
```
```python
y
```
```python
)
```
```python
grad_numpy
```
```python
=
```
```python
entropy
```
```python
.
```
```python
backward
```
```python
(
```
```python
)
```
```python
loss_tensor
```
```python
=
```
```python
(
```
```python
-
```
```python
y_tensor
```
```python
*
```
```python
torch
```
```python
.
```
```python
log
```
```python
(
```
```python
x_tensor
```
```python
)
```
```python
)
```
```python
.
```
```python
sum
```
```python
(
```
```python
)
```
```python
loss_tensor
```
```python
.
```
```python
backward
```
```python
(
```
```python
)
```
```python
grad_tensor
```
```python
=
```
```python
x_tensor
```
```python
.
```
```python
grad
```
```python
print
```
```python
(
```
```python
"Python Loss :"
```
```python
,
```
```python
loss_numpy
```
```python
)
```
```python
print
```
```python
(
```
```python
"PyTorch Loss :"
```
```python
,
```
```python
loss_tensor
```
```python
.
```
```python
data
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
)
```
```python
print
```
```python
(
```
```python
"\nPython dx :"
```
```python
)
```
```python
print
```
```python
(
```
```python
grad_numpy
```
```python
)
```
```python
print
```
```python
(
```
```python
"\nPyTorch dx :"
```
```python
)
```
```python
print
```
```python
(
```
```python
grad_tensor
```
```python
.
```
```python
data
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
)
```
```python
"""
输出 :
Python Loss : 22.6094161164
PyTorch Loss : 22.609416116382963
Python dx :
[[ -0.173  -2.888  -2.658  -0.989  -0.476  -0.719  -0.425  -0.995  -1.82   -1.302]
 [ -1.95   -0.804  -1.425 -11.306  -2.116  -0.113  -4.185  -1.389  -0.365  -1.076]
 [ -0.151  -1.042  -0.866  -1.184  -0.022  -1.841  -1.539  -0.696  -0.521  -1.102]
 [ -3.461  -1.596  -1.287  -0.788  -2.173  -2.695  -0.838  -0.049  -0.323  -0.793]
 [ -1.13   -8.609  -1.122  -1.838  -0.685  -2.762  -0.313  -0.405  -0.464  -0.56 ]]
 
PyTorch dx :
[[ -0.173  -2.888  -2.658  -0.989  -0.476  -0.719  -0.425  -0.995  -1.82   -1.302]
 [ -1.95   -0.804  -1.425 -11.306  -2.116  -0.113  -4.185  -1.389  -0.365  -1.076]
 [ -0.151  -1.042  -0.866  -1.184  -0.022  -1.841  -1.539  -0.696  -0.521  -1.102]
 [ -3.461  -1.596  -1.287  -0.788  -2.173  -2.695  -0.838  -0.049  -0.323  -0.793]
 [ -1.13   -8.609  -1.122  -1.838  -0.685  -2.762  -0.313  -0.405  -0.464  -0.56 ]]
"""
```

