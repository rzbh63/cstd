
# 经典论文复现 | 基于深度卷积网络的图像超分辨率算法 - Paper weekly - CSDN博客


2018年12月14日 12:52:22[Paper_weekly](https://me.csdn.net/c9Yv2cf9I06K2A9E)阅读数：287


![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgm9RFr5icmiaj0bibJxUeIGdAFHNM4G6PJEiccw293RuVnOiadQ4zcdibdJa5FFfn0ZMgpbKib4AAKD8dm2w/640)

过去几年发表于各大 AI 顶会论文提出的 400 多种算法中，公开算法代码的仅占 6%，其中三分之一的论文作者分享了测试数据，约 54% 的分享包含“伪代码”。这是今年 AAAI 会议上一个严峻的报告。人工智能这个蓬勃发展的领域正面临着实验重现的危机，就像实验重现问题过去十年来一直困扰着心理学、医学以及其他领域一样。**最根本的问题是研究人员通常不共享他们的源代码。**

可验证的知识是科学的基础，它事关理解。随着人工智能领域的发展，打破不可复现性将是必要的。为此，**PaperWeekly 联手百度 PaddlePaddle 共同发起了****本次论文有奖复现**，我们希望和来自学界、工业界的研究者一起接力，为 AI 行业带来良性循环。

作者丨张荣成
学校丨哈尔滨工业大学（深圳）
研究方向丨计算数学

**笔者本次选择复现的是汤晓鸥教授和何恺明团队发表于 2015 年的经典论文——****SRCNN****。**超分辨率技术（Super-Resolution）是指从观测到的低分辨率图像重建出相应的高分辨率图像，在监控设备、卫星图像和医学影像等领域都有重要的应用价值。在深度卷积网络的浪潮下，**本文****首次提出了基于深度卷积网络的端到端超分辨率算法。**

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgk1gWUMjXtV8OdMqv1uoXSoeMftXRibPNrxHh2NZeOz85uGQGRXK7jefbPH28ZwWHOAFWB6R3dQGtA/640)

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgk1gWUMjXtV8OdMqv1uoXSo5LmvrbRLe8ibAicDrGfX2jiceFX44xNyicqxSVfTtoeKkzibRTVtcia4t8NQ/640)

**论文复现代码：**

http://aistudio.baidu.com/aistudio/\#/projectdetail/24446

# SRCNN流程

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgk1gWUMjXtV8OdMqv1uoXSoE5mDRsCcB4TqgbzxtORv8t3M1SMJsrKxeDe593QGmy9RWa8aZiasQRQ/640)
▲SRCNN算法框架

SRCNN 将深度学习与传统稀疏编码之间的关系作为依据，将 3 层网络划分为**图像块提取**（Patch extraction and representation）、**非线性映射**（Non-linear mapping）以及最终的**重建**（Reconstruction）。

**SRCNN 具体流程如下：**

1. 先将低分辨率图像使用双三次差值放大至目标尺寸（如放大至 2 倍、3 倍、4 倍），此时仍然称放大至目标尺寸后的图像为低分辨率图像（Low-resolution image），即图中的输入（input）；

2. 将低分辨率图像输入三层卷积神经网络。举例：在论文其中一个实验相关设置，对 YCrCb 颜色空间中的 Y 通道进行重建，网络形式为 (conv1+relu1)—(conv2+relu2)—(conv3+relu3)；第一层卷积：卷积核尺寸 9×9 (f1×f1)，卷积核数目 64 (n1)，输出 64 张特征图；第二层卷积：卷积核尺寸 1×1 (f2×f2)，卷积核数目 32 (n2)，输出 32 张特征图；第三层卷积：卷积核尺寸 5×5 (f3×f3)，卷积核数目 1 (n3)，输出 1 张特征图即为最终重建高分辨率图像。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgk1gWUMjXtV8OdMqv1uoXSoMIoFq12S3KhkPVK0S90ickFrfzfbKFicfQkYib6GEb5gn6FxJNB3qkaTQ/640)

# 训练

**训练数据集**

论文中某一实验采用 91 张自然图像作为训练数据集，对训练集中的图像先使用双三次差值缩小到低分辨率尺寸，再将其放大到目标放大尺寸，最后切割成诸多 33 × 33 图像块作为训练数据，作为标签数据的则为图像中心的 21 × 21 图像块（与卷积层细节设置相关）。

**损失函数**

采用 MSE 函数作为卷积神经网络损失函数。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgk1gWUMjXtV8OdMqv1uoXSohia4XoljkKbpz0cCuUa0UPzFzATxvPiaK071Eq4Q1cDZnuZ9tYvO69xA/640)

**卷积层细节设置**

第一层卷积核 9 × 9，得到特征图尺寸为 (33-9)/1+1=25，第二层卷积核 1 × 1，得到特征图尺寸不变，第三层卷积核 5 × 5，得到特征图尺寸为 (25-5)/1+1=21。训练时得到的尺寸为 21 × 21，因此图像中心的 21 × 21 图像块作为标签数据（卷积训练时不进行 padding）。

```python
# 查看个人持久化工作区文件
```

```python
!ls /home/aistudio/work/
```

```python
# coding=utf
```
```python
-8
```

```python
import
```
```python
os
```

```python
import
```
```python
paddle.fluid
```
```python
as
```
```python
fluid
```

```python
import
```
```python
paddle.v2
```
```python
as
```
```python
paddle
```

```python
from
```
```python
PIL
```
```python
import
```
```python
Image
```

```python
import
```
```python
numpy
```
```python
as
```
```python
np
```

```python
import
```
```python
scipy.misc
```

```python
import
```
```python
scipy.ndimage
```

```python
import
```
```python
h5py
```

```python
import
```
```python
glob
```

```python
FLAGS={
```
```python
"epoch"
```
```python
:
```
```python
10
```
```python
,
```
```python
"batch_size"
```
```python
:
```
```python
128
```
```python
,
```
```python
"image_size"
```
```python
:
```
```python
33
```
```python
,
```
```python
"label_size"
```
```python
:
```
```python
21
```
```python
,
```

```python
"learning_rate"
```
```python
:
```
```python
1e-4
```
```python
,
```
```python
"c_dim"
```
```python
:
```
```python
1
```
```python
,
```
```python
"scale"
```
```python
:
```
```python
3
```
```python
,
```
```python
"stride"
```
```python
:
```
```python
14
```
```python
,
```

```python
"checkpoint_dir"
```
```python
:
```
```python
"checkpoint"
```
```python
,
```
```python
"sample_dir"
```
```python
:
```
```python
"sample"
```
```python
,
```
```python
"is_train"
```
```python
: True}
```

```python
#utils
```

```python
def read_data(path):
```

```python
with
```
```python
h5py.File(path,
```
```python
'r'
```
```python
)
```
```python
as
```
```python
hf:
```

```python
data = np.array(hf.get(
```
```python
'data'
```
```python
))
```

```python
label = np.array(hf.get(
```
```python
'label'
```
```python
))
```

```python
return
```
```python
data, label
```

```python
def preprocess(path, scale=
```
```python
3
```
```python
):
```

```python
image = imread(path, is_grayscale=True)
```

```python
label_ = modcrop(image, scale)
```

```python
label_ = label_ /
```
```python
255.
```

```python
input_ = scipy.ndimage.interpolation.zoom(label_, zoom=(
```
```python
1.
```
```python
/ scale), prefilter=False)  # 一次
```

```python
input_ = scipy.ndimage.interpolation.zoom(input_, zoom=(scale /
```
```python
1.
```
```python
), prefilter=False)  # 二次，bicubic
```

```python
return
```
```python
input_, label_
```

```python
def prepare_data(dataset):
```

```python
if
```
```python
FLAGS[
```
```python
'is_train'
```
```python
]:
```

```python
data_dir = os.path.join(os.getcwd(), dataset)
```

```python
data = glob.glob(os.path.join(data_dir,
```
```python
"*.bmp"
```
```python
))
```

```python
else
```
```python
:
```

```python
data_dir = os.path.join(os.sep, (os.path.join(os.getcwd(), dataset)),
```
```python
"Set5"
```
```python
)
```

```python
data = glob.glob(os.path.join(data_dir,
```
```python
"*.bmp"
```
```python
))
```

```python
return
```
```python
data
```

```python
def make_data(data, label):
```

```python
if
```
```python
not os.path.exists(
```
```python
'data/checkpoint'
```
```python
):
```

```python
os.makedirs(
```
```python
'data/checkpoint'
```
```python
)
```

```python
if
```
```python
FLAGS[
```
```python
'is_train'
```
```python
]:
```

```python
savepath = os.path.join(os.getcwd(),
```
```python
'data/checkpoint/train.h5'
```
```python
)
```

```python
#
```
```python
else
```
```python
:
```

```python
#     savepath = os.path.join(os.getcwd(),
```
```python
'data/checkpoint/test.h5'
```
```python
)
```

```python
with
```
```python
h5py.File(savepath,
```
```python
'w'
```
```python
)
```
```python
as
```
```python
hf:
```

```python
hf.create_dataset(
```
```python
'data'
```
```python
, data=data)
```

```python
hf.create_dataset(
```
```python
'label'
```
```python
, data=label)
```

```python
def imread(path, is_grayscale=True):
```

```python
if
```
```python
is_grayscale:
```

```python
return
```
```python
scipy.misc.imread(path, flatten=True, mode=
```
```python
'YCbCr'
```
```python
).astype(np.float)  # 将图像转灰度
```

```python
else
```
```python
:
```

```python
return
```
```python
scipy.misc.imread(path, mode=
```
```python
'YCbCr'
```
```python
).astype(np.float)  # 默认为
```
```python
false
```

```python
def modcrop(image, scale=
```
```python
3
```
```python
):
```

```python
if
```
```python
len(image.shape) ==
```
```python
3
```
```python
:  # 彩色
```
```python
800
```
```python
*
```
```python
600
```
```python
*
```
```python
3
```

```python
h, w, _ = image.shape
```

```python
h = h - np.mod(h, scale)
```

```python
w = w - np.mod(w, scale)
```

```python
image = image[
```
```python
0
```
```python
:h,
```
```python
0
```
```python
:w, :]
```

```python
else
```
```python
:  # 灰度
```
```python
800
```
```python
*
```
```python
600
```

```python
h, w = image.shape
```

```python
h = h - np.mod(h, scale)
```

```python
w = w - np.mod(w, scale)
```

```python
image = image[
```
```python
0
```
```python
:h,
```
```python
0
```
```python
:w]
```

```python
return
```
```python
image
```

```python
def input_setup(config):
```

```python
if
```
```python
config[
```
```python
'is_train'
```
```python
]:
```

```python
data = prepare_data(dataset=
```
```python
"data/data899/Train.zip_files/Train"
```
```python
)
```

```python
else
```
```python
:
```

```python
data = prepare_data(dataset=
```
```python
"Test"
```
```python
)
```

```python
sub_input_sequence = []
```

```python
sub_label_sequence = []
```

```python
padding = abs(config[
```
```python
'image_size'
```
```python
] - config[
```
```python
'label_size'
```
```python
])
```
```python
// 2  # 6 填充
```

```python
if
```
```python
config[
```
```python
'is_train'
```
```python
]:
```

```python
for
```
```python
i
```
```python
in
```
```python
range(len(data)):
```

```python
input_, label_ = preprocess(data[i], config[
```
```python
'scale'
```
```python
])  # data[i]为数据目录
```

```python
if
```
```python
len(input_.shape) ==
```
```python
3
```
```python
:
```

```python
h, w, _ = input_.shape
```

```python
else
```
```python
:
```

```python
h, w = input_.shape
```

```python
for
```
```python
x
```
```python
in
```
```python
range(
```
```python
0
```
```python
, h - config[
```
```python
'image_size'
```
```python
] +
```
```python
1
```
```python
, config[
```
```python
'stride'
```
```python
]):
```

```python
for
```
```python
y
```
```python
in
```
```python
range(
```
```python
0
```
```python
, w - config[
```
```python
'image_size'
```
```python
] +
```
```python
1
```
```python
, config[
```
```python
'stride'
```
```python
]):
```

```python
sub_input = input_[x:x + config[
```
```python
'image_size'
```
```python
],
```
```python
y
```
```python
:y + config[
```
```python
'image_size'
```
```python
]]  # [
```
```python
33
```
```python
x
```
```python
33
```
```python
]
```

```python
sub_label = label_[x + padding:x + padding + config[
```
```python
'label_size'
```
```python
],
```

```python
y + padding:y + padding + config[
```
```python
'label_size'
```
```python
]]  # [
```
```python
21
```
```python
x
```
```python
21
```
```python
]
```

```python
# Make channel value,颜色通道
```
```python
1
```

```python
sub_input = sub_input.reshape([config[
```
```python
'image_size'
```
```python
], config[
```
```python
'image_size'
```
```python
],
```
```python
1
```
```python
])
```

```python
sub_label = sub_label.reshape([config[
```
```python
'label_size'
```
```python
], config[
```
```python
'label_size'
```
```python
],
```
```python
1
```
```python
])
```

```python
sub_input_sequence.append(sub_input)
```

```python
sub_label_sequence.append(sub_label)
```

```python
arrdata = np.asarray(sub_input_sequence)  # [?,
```
```python
33
```
```python
,
```
```python
33
```
```python
,
```
```python
1
```
```python
]
```

```python
arrlabel = np.asarray(sub_label_sequence)  # [?,
```
```python
21
```
```python
,
```
```python
21
```
```python
,
```
```python
1
```
```python
]
```

```python
make_data(arrdata, arrlabel)  # 把处理好的数据进行存储，路径为checkpoint/..
```

```python
else:
```

```python
input_, label_ = preprocess(data[
```
```python
4
```
```python
], config[
```
```python
'scale'
```
```python
])
```

```python
if
```
```python
len(input_.shape) ==
```
```python
3
```
```python
:
```

```python
h, w, _ = input_.shape
```

```python
else
```
```python
:
```

```python
h, w = input_.shape
```

```python
input = input_.reshape([h, w,
```
```python
1
```
```python
])
```

```python
label = label_[
```
```python
6
```
```python
:h -
```
```python
6
```
```python
,
```
```python
6
```
```python
:w -
```
```python
6
```
```python
]
```

```python
label = label.reshape([h -
```
```python
12
```
```python
, w -
```
```python
12
```
```python
,
```
```python
1
```
```python
])
```

```python
sub_input_sequence.append(input)
```

```python
sub_label_sequence.append(label)
```

```python
input1 = np.asarray(sub_input_sequence)
```

```python
label1 = np.asarray(sub_label_sequence)
```

```python
return
```
```python
input1, label1, h, w
```

```python
def imsave(image, path):
```

```python
return
```
```python
scipy.misc.imsave(path, image)
```

```python
#train
```

```python
def reader_creator_image_and_label():
```

```python
input_setup(FLAGS)
```

```python
data_dir= os.path.join(
```
```python
'./data/{}'
```
```python
.format(FLAGS[
```
```python
'checkpoint_dir'
```
```python
]),
```
```python
"train.h5"
```
```python
)
```

```python
images,labels=read_data(data_dir)
```

```python
def reader():
```

```python
for
```
```python
i
```
```python
in
```
```python
range(len(images)):
```

```python
yield
```
```python
images, labels
```

```python
return
```
```python
reader
```

```python
def train(use_cuda, num_passes,BATCH_SIZE =
```
```python
128
```
```python
, model_save_dir=
```
```python
'../models'
```
```python
):
```

```python
if
```
```python
FLAGS[
```
```python
'is_train'
```
```python
]:
```

```python
images = fluid.layers.data(name=
```
```python
'images'
```
```python
, shape=[
```
```python
1
```
```python
, FLAGS[
```
```python
'image_size'
```
```python
], FLAGS[
```
```python
'image_size'
```
```python
]], dtype=
```
```python
'float32'
```
```python
)
```

```python
labels = fluid.layers.data(name=
```
```python
'labels'
```
```python
, shape=[
```
```python
1
```
```python
, FLAGS[
```
```python
'label_size'
```
```python
], FLAGS[
```
```python
'label_size'
```
```python
]], dtype=
```
```python
'float32'
```
```python
)
```

```python
else
```
```python
:
```

```python
_,_,FLAGS[
```
```python
'image_size'
```
```python
],FLAGS[
```
```python
'label_size'
```
```python
]=input_setup(FLAGS)
```

```python
images = fluid.layers.data(name=
```
```python
'images'
```
```python
, shape=[
```
```python
1
```
```python
, FLAGS[
```
```python
'image_size'
```
```python
], FLAGS[
```
```python
'label_size'
```
```python
]], dtype=
```
```python
'float32'
```
```python
)
```

```python
labels = fluid.layers.data(name=
```
```python
'labels'
```
```python
, shape=[
```
```python
1
```
```python
, FLAGS[
```
```python
'image_size'
```
```python
]
```
```python
-12
```
```python
, FLAGS[
```
```python
'label_size'
```
```python
]
```
```python
-12
```
```python
], dtype=
```
```python
'float32'
```
```python
)
```

```python
#feed_order=[
```
```python
'images'
```
```python
,
```
```python
'labels'
```
```python
]
```

```python
# 获取神经网络的训练结果
```

```python
predict = model(images)
```

```python
# 获取损失函数
```

```python
cost = fluid.layers.square_error_cost(input=predict, label=labels)
```

```python
# 定义平均损失函数
```

```python
avg_cost = fluid.layers.mean(cost)
```

```python
# 定义优化方法
```

```python
optimizer = fluid.optimizer.Momentum(learning_rate=
```
```python
1e-4
```
```python
,momentum=
```
```python
0.9
```
```python
)
```

```python
opts =optimizer.minimize(avg_cost)
```

```python
# 是否使用GPU
```

```python
place = fluid.CUDAPlace(
```
```python
0
```
```python
)
```
```python
if
```
```python
use_cuda
```
```python
else
```
```python
fluid.CPUPlace()
```

```python
# 初始化执行器
```

```python
exe=fluid.Executor(place)
```

```python
exe.run(fluid.default_startup_program())
```

```python
# 获取训练数据
```

```python
train_reader = paddle.batch(
```

```python
reader_creator_image_and_label(), batch_size=BATCH_SIZE)
```

```python
# 获取测试数据
```

```python
# test_reader = paddle.batch(
```

```python
#     read_data(), batch_size=BATCH_SIZE)
```

```python
#print(len(next(train_reader())))
```

```python
feeder = fluid.DataFeeder(place=place, feed_list=[images, labels])
```

```python
for
```
```python
pass_id
```
```python
in
```
```python
range(num_passes):
```

```python
for
```
```python
batch_id, data
```
```python
in
```
```python
enumerate(train_reader()):
```

```python
avg_cost_value = exe.run(fluid.default_main_program(),
```

```python
feed=feeder.feed(data),
```

```python
fetch_list=[avg_cost])
```

```python
if
```
```python
batch_id%
```
```python
100
```
```python
==
```
```python
0
```
```python
:
```

```python
print(
```
```python
"loss="
```
```python
+avg_cost_value[
```
```python
0
```
```python
])
```

```python
def model(images):
```

```python
conv1=fluid.layers.conv2d(input=images, num_filters=
```
```python
64
```
```python
, filter_size=
```
```python
9
```
```python
, act=
```
```python
'relu'
```
```python
)
```

```python
conv2=fluid.layers.conv2d(input=conv1, num_filters=
```
```python
32
```
```python
, filter_size=
```
```python
1
```
```python
,act=
```
```python
'relu'
```
```python
)
```

```python
conv3=fluid.layers.conv2d(input=conv2, num_filters=
```
```python
1
```
```python
, filter_size=
```
```python
5
```
```python
)
```

```python
return
```
```python
conv3
```

```python
if
```
```python
__name__ ==
```
```python
'__main__'
```
```python
:
```

```python
# 开始训练
```

```python
train(use_cuda=False, num_passes=
```
```python
10
```
```python
)
```

# 测试

**全卷积网络**

所用网络为全卷积网络，因此作为实际测试时，直接输入完整图像即可。

**Padding**

训练时得到的实际上是除去四周 (33-21)/2=6 像素外的图像，若直接采用训练时的设置（无 padding），得到的图像最后会减少四周各 6 像素（如插值放大后输入 512 × 512，输出 500 × 500）。

因此在测试时每一层卷积都进行了 padding（卷积核尺寸为 1 × 1的不需要进 行 padding），这样保证插值放大后输入与输出尺寸的一致性。

# 重建结果

**客观评价指标 PSNR 与 SSIM：**相比其他传统方法，SRCNN 取得更好的重建效果。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgk1gWUMjXtV8OdMqv1uoXSoURhlicTZB4FG3EyTx1q0amyIKf5aDkMs9IyfRicGLFPevQCLX9pufw2g/640)

**主观效果：**相比其他传统方法，SRCNN 重建效果更具优势。

![640](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgk1gWUMjXtV8OdMqv1uoXSoJLa6S4GqjIgll6OicKeCHF2kLL2WQ755XUV9BjTasiasQABAScwB9bFA/640)

# 关于PaddlePaddle

PaddlePaddle 在我看来与 TensorFlow 有点类似，API 都非常好用，用户群体越来越大。不过有一点小建议，就是在我使用的时候，发现数据处理的方式比较麻烦（比较菜），比如 TensorFlow中有 TFRecords，同时有自带的数据集迭代器可以很方便的导入数据，所以希望出一个官方的数据转换格式，用来读取数据，并提供迭代器方法。

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmPEF4lW0pL5weJia5y4xhJbog2pIZZ3ZCgVUDynvus6rCzNKGAAAI6R8jaXTpYPISCMicpFegVdG0g/640?)


**点击标题查看更多论文复现：**

经典论文复现 | LSGAN：最小二乘生成对抗网络
PyraNet：基于特征金字塔网络的人体姿态估计
[经典论文复现 | InfoGAN：一种无监督生成方法](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247492415&idx=1&sn=a359e72ee99555f7a2fb4e21b2ad51db&chksm=96ea3cbfa19db5a913b9ed01490df6e902abfeb36856bb5be946527a070d399ee37a3ff6c750&scene=21#wechat_redirect)
经典论文复现 | 基于标注策略的实体和关系联合抽取




![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/xuKyIMVqtF2cO2WSmiccOqL8YlIwp5Xv2cqdDp6ANbUt8yibCc1cgQQrPHLKhf73icQGHves57M2XMZLJxIhF0e7g/640?)**\#****投 稿 通 道****\#**
**让你的论文被更多人看到**

如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？**答案就是：你不认识的人。**

总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。

PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是**最新论文解读**，也可以是**学习心得**或**技术干货**。我们的目的只有一个，让知识真正流动起来。

📝**来稿标准：**
• 稿件确系个人**原创作品**，来稿需注明作者个人信息（姓名+学校/工作单位+学历/职位+研究方向）
• 如果文章并非首发，请在投稿时提醒并附上所有已发布链接
• PaperWeekly 默认每篇文章都是首发，均会添加“原创”标志

**📬 投稿邮箱：**
• 投稿邮箱：hr@paperweekly.site
• 所有文章配图，请单独在附件中发送
• 请留下即时联系方式（微信或手机），以便我们在编辑发布时和作者沟通



🔍

现在，在**「知乎」**也能找到我们了
进入知乎首页搜索**「PaperWeekly」**
点击**「关注」**订阅我们的专栏吧


**关于PaperWeekly**

PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击**「交流群」**，小助手将把你带入 PaperWeekly 的交流群里。

![640?](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/VBcD02jFhgkXb8A1kiafKxib8NXiaPMU8mQvRWVBtFNic4G5b5GDD7YdwrsCAicOc8kp5tdEOU3x7ufnleSbKkiaj5Dg/640?)
▽ 点击 |阅读原文| 收藏复现代码


