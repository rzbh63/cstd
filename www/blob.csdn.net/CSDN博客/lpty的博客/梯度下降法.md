
# 梯度下降法 - lpty的博客 - CSDN博客

2016年10月26日 00:51:07[lpty](https://me.csdn.net/sinat_33741547)阅读数：800


1.梯度下降法是什么？
梯度下降法又称为最速下降法，是求解无约束最优化问题的一种最常用的方法，简单的来说，这是一种求解函数局部极小值的方法。
首先我们先理解一下，什么叫做梯度？梯度可以理解为函数f(x)在点x处的斜率，假设我们有一个如下图所示的函数，在点A位置的斜率既是我们讲的梯度。![](http://images2015.cnblogs.com/blog/343064/201601/343064-20160131143953849-852287147.png)梯度下降法基于思想：要找到某函数的最小值，最好的方法是沿着该函数的负梯度方向探寻。由上图我们可以看到，随着A点不断往下移动，A点的斜率越来越趋于0，等到切线平行于x轴时，x的位置便得到了函数的极小值。反之若我们往正梯度的方向移动，便能得到函数的极大值，当然上图的函数估计是遥遥无期啦。
2.梯度下降法用在哪里？
在机器学习中，我们经常会用以下的方法进行模型的建立：
![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/WindowsLiveWriter/1_1270E/image_thumb_3.png)
假设我们用训练数据，通过某一种方法学习出来了一个模型，即拥有了估计函数h(x)。现在开始用这个函数对测试数据进行预测，便可以得到一组对应于输入数据的预测输出。
这时候我们想知道，学习出来的这个模型预测的结果到底可信度有多高？那可以用估计函数h(x)对数据集X进行预测，得到的预测H与实际的Y进行比较。这里引入一个损失函数的概念，损失函数越小，代表可信度越高。
若![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/WindowsLiveWriter/1_1270E/image_thumb_5.png)为线性函数，便可得到损失函数:
![](http://latex.codecogs.com/gif.latex?J%28%5Ctheta%29%3D%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%28H_%7Bi%7D-Y_%7Bi%7D%29%5E%7B2%7D)
如何调整θ使得损失函数最小，在数学上有很多种方法，而梯度下降法便是其中的一种。
3.梯度下降法怎么用？
在上面我们已经可以看到，使用损失函数，实质上把求解最适参数θ的问题变成求解损失函数最小值的问题，那便可以用梯度下降法来解决。
梯度下降法一般按以下的步骤进行：
(1)给定θ的值
(2)求当前θ位置处的梯度
(3)改变θ的值，让损失函数J(θ)按梯度下降的方向减少
(4)求得局部极小值
在前面我们求解梯度时只是简单的计算斜率，当然多元函数也是一样的方法，只是变成了求解偏导而已，不清楚请自行查阅偏导数相关资料，有如下公式：
![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/WindowsLiveWriter/1_1270E/image_thumb_12.png)
此时得到当前θ位置处的梯度，之后按照以下的方式对θ进行更新：
![image](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/WindowsLiveWriter/1_1270E/image_thumb_14.png)
到这里我们已经往梯度负方向移动了一步，但是，这一步的步长是多少呢？上面的a便是步长，这个由个人确定，步长越大移动的速度越快，但可能取得的极小值点误差越大，反之速度慢，但是精度会有相应的提升。
之后不断更新θ的值，损失函数J(θ)便能越趋于极小值，当达到一定的精度范围，便默认我们取得了极小点啦。



