
# 机器学习算法调优 - bitcarmanlee的博客 - CSDN博客


2017年08月30日 22:32:07[bitcarmanlee](https://me.csdn.net/bitcarmanlee)阅读数：547标签：[机器学习																](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)[算法调优																](https://so.csdn.net/so/search/s.do?q=算法调优&t=blog)[调参																](https://so.csdn.net/so/search/s.do?q=调参&t=blog)[GBDT																](https://so.csdn.net/so/search/s.do?q=GBDT&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=调参&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=算法调优&t=blog)个人分类：[ml algorithm																](https://blog.csdn.net/bitcarmanlee/article/category/6182356)
[
																								](https://so.csdn.net/so/search/s.do?q=算法调优&t=blog)
[
				](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)
[
			](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)

机器学习算法众多，各种算法又涉及较多参数，本文将简要介绍RF，GBDT等算法的调优经验与步骤。
## 1. BP
调优事项
1.BP对feature scaling很敏感，要先scale data。
2.经验来说，L-BFGS在小数据上收敛更快效果更好；Adam在大数据上表现很好；SGD在参数learning rate调整好的基础效果更好。
调参
1.首先隐含层个数。一般来说，如果问题线性可分，那么不用隐含层就行；对于绝大多数问题，一个隐含层的效果就很好；多个隐含层的话训练起来更困难一些（需要用到自编码器或者RBM去预训练而且训练参数增长很快）。
2.接下来确定隐含层的神经元个数，一般来说，神经元个数在输出层和输出层神经元个数之间比较好。可以通过CV确定合适的神经元个数。同时也可以先增大下神经元的个数，然后通过pruning减少神经元个数。pruning的原理是在神经网络训练的时候主动把没什么用的神经元去掉，没什么用的神经元的特征就是其连接的w很小，所以我们可以得到神经网络后看看W的权重矩阵再决定是否要删除部分神经元。
3.画出学习曲线。
4.如果学习曲线不稳定，那么减少学习率；如果学习曲线变化很慢，那么增大学习率。
5.如果学习曲线发现过拟合，那么提前终止或加大正则系数或减少模型复杂度；如果学习曲线欠拟合，那么多迭代或减少正则项系数或增大模型复杂度。
## 2.LR
参数
1.C：正则项λ的倒数，越小模型越简单。
2.learning_rate
调优事项
1.sklearn中的LogisticRegression没有使用SGD，用的是一些solver，所以没有learning_rate。
2.sklearn中SGDClassifier适合对线性模型（SVM、逻辑回归）用SGD优化，常调的参数有learning_rate和alpha，SGD的速度更快一些。
3.sklearn中有LogisticRegressionCV，可以实现更快的CV。适合调整正则项惩罚权重的算法。
调参
grid search cross validation
## 3.SVM
参数
γ越大，单个训练数据（SV）影响越小（只有靠近SV的才可以影响），只能影响到自己周围的小区域，这样可能需要更多的SV，模型复杂度升高。
C越大，模型为了不犯错，不得不变得更复杂，模型复杂度升高。
1.γ：单个训练数据（SV）影响距离大小，因为高斯核是exp(−γ||x−xn||2)，所以γ越小，单个训练数据（SV）影响越大。γ可以看做是SV影响半径的倒数。
2.C：C参数在模型的复杂度与模型的正确率之间做权衡。如果C很大，那么模型可能会变得很复杂以尽量减少分类错误，可能会过拟合；如果C很小，那么模型可能会很简单以至于在训练集上效果不好。
调优事项
1.10−3至103的log grid的调参区间便已足够
2.模型非常受γ参数的影响，当γ很大，那么SV的影响半径就很小，只能影响到它自己，这样很容易过拟合。这种情况下即使改变C也无济于事。
3.当γ很小，模型太受限了不能够捕捉到复杂的形状。每个SV的影响半径扩大以至于包含了整个训练集，这样模型就很简单，原因类似KNN中K很大。
4.γ，C的最优组合一般在分布的对角线上，选择较平滑的模型（低γ），通过增大SV的数量（高C）就可以实现很好的效果。
5.γ不可过大，否则调C也是没用的。
调参
grid search cross validation
## 4.RF
参数
RF的参数主要涉及两部分：
1.树参数：max depth、min_samples_split、min_samples_leaf、max_features
2.bagging参数：n_estimators
调优事项
1.RF模型的子模型都有较低的bias（树深到头），整体模型的目标是降低variance，所以需要增大树的数目。同时降低子模型之间的相关度有助于降低variance，所以max_feature的值一般设置成特征数目的平方根。
2.max_features默认sqrt，一般设置地小一些。减少max_features有利于减少不同树的相关度，减少最终模型的variance，但是增加了单个模型的bias。
3.max_features的选择和数据质量也有关系，如果数据质量很差的话，小的max_features可能split的时候选错了feature因此很有可能影响最终的模型。
调参
1.max_features是首要的调参参数，大概300个树的时候通过CV找到最优的max_features
2.调整max_depth
3.调整n_estimators，一般来说越大越好，调整到验证集上的精度提高不多就可。
## 5.GBDT
参数
GBDT设计到的参数主要有三部分：
1.树参数：max_depth（max_leaf_nodes）、min_samples_split（min_weight_fraction_leaf）、min_samples_leaf、max_features
2.boost参数：learning_rate、n_estimators、subsample
3.其他参数：loss、init、random_state、verbose、warm_start、presort
一般调整的参数有：
1.树参数：max_depth、min_samples_split、min_samples_leaf、max_features
2.boost参数：learning_rate、n_estimators
调优事项
1.与RF相比，GBDT不仅降低了variance（多棵树的组合），而且降低了bias（对错误的数据去训练）。其主要目的是通过树的组合降低bias。每个子模型的bias很大但是variance很小。
2.对于树的参数，RF主要调的是max_features，其目的是增加子模型间的随机性达到降低整体模型variance的作用。对于GBDT，首要目的是降低bias，所以比较关注max_depth之类的参数，通过调整增大子模型的复杂度降低子模型的bias从而降低整体模型的bias。
3.GBDT如果一直进行下去，肯定也会过拟合的，只不过基本分类器很弱，所以GBDT的抗过拟合的能力很强。
4.没有最佳的学习率，学习率越低越好，只要有足够多的树。
5.如果学习率很低，树很多，那么时间上的开销就会很大。
调优策略
讲调优策略之前，先对一些值做下初始化：
1.max_depth：一般选的小点，避免分类器太强，5-8即可。
2.min_samples_split：根据数据大小选，选择数据量的1%即可。
3.min_samples_leaf：根据数据与直觉选。
4.max_features：一般选sqrt。
s5.ubsample：一般选0.8。
每次调优的方法都是grid search，评价参数好坏的标准是cv score。
1.先对boosting参数调优，包括learning_rate和n_estimators。一般，、learning_rate值在0.05-2，n_estimators在30-80较好。如果n_estimators过大那么增大learning_rate，反之减少learning_rate。
2.对树的参数在初始化周围调优，调优顺序是max_depth、min_samples_split、min_samples_leaf、max_features、，可视计算能力分步调优或组合参数调优。
3.确定最优的参数组合，然后降低learning_rate，增加同样倍数的n_estimators，直到计算能力到达极限或验证集上模型的提升很小。
原文链接：[http://blog.younggy.com/2017/02/24/](http://blog.younggy.com/2017/02/24/)机器学习算法调优/

