
# 神经网络端到端序列学习（一） - 丁丁的博客 - CSDN博客


2016年11月26日 15:33:49[MoussaTintin](https://me.csdn.net/JackyTintin)阅读数：11756


许多重要问题都可以抽象为变长序列学习问题（sequence to sequence learning），如语音识别、机器翻译、字符识别。这类问题的特点是，1) 输入和输入都是序列（如连续值语音信号/特征、离散值的字符），2) 序列长度都不固定，3)并且输入输出序列长度没有对应关系。因此，传统的神经网络模型（DNN， CNN， RNN）不能直接以端到端的方式解决这类问题的建模和学习问题。
解决变长序列的端到端学习，目前有两种主流的思路：一种是 CTC （Connectionist Temporal Classification，连接时序分类）；另一种是 Encoder-Decoder（以下简称 En-De）的思路。CTC 最早用于手写体字符识别上[19]，并且一度是语音识别的研究热点[20-23]。这里，我们关注的是后一种思路。
变长序列学习的 En-De 方法中，本文重点关注 Google 和 Yoshua Bengio 两个团队的工作。这两个团队这个方向上研究都比较早，也分别能代表性工业界和学界的风格。
下面首先介绍 Google 的 seq2seq 模型，然后介绍 Bengio 团队的 RNNenc 模型。可以看到两种模型基本思路一致，但在具体细节上，有着显著的不同。至于 attention-base encoder-decoder，则会在将来另文讨论。
# 1. seq2seq 模型
## 1.1 模型结构
Seq2seq 模型由文章[5]为解决机器翻译问题提出，其中基本结构可以由图1概括。
![这里写图片描述](https://img-blog.csdn.net/20161126153618934)[ ](https://img-blog.csdn.net/20161126153618934)
*图1 seq2seq 模型 [5]*
[
](https://img-blog.csdn.net/20161126153618934)seq2seq 模型由 encoder（编码器） 和 decoder（解码器）两部分组成。
1.**encoder**是一个若干层的 RNN 网络。输入序列由 encoder 从左到右，依次处理，得到各层最后一个 timestep 的隐层状态。
2.**decoder**是一个与 encoder 结构完成相同的 RNN 网络。本质上，decoder 是一个 RNN 语言模型（RNNLM）[2,3]。decoder 的输入是  + ground truth，对应的目标输出是 ground truth + 。其中， 是一个特殊的符合，用来标记序列的开始和结束。
可以看到，decoder 是一个典型的输入输出一一对应的监督学习结构，因此，可以用常规的方法进行训练。与 RNNLM 唯一的不同在于，encoder 的各层 RNN 的初始状态是于 encoder 的最后 timestep 的状态复制而来。因此，encoder 是一个条件化的（conditional） RNNLM，它所依赖的条件（condition）即是由 encoder 编码而来的信息（隐层状态）。
**通过隐层状态的共享（复制），在训练过程中，decoder 的误差可以返传回 encoder，从而实现端到端的训练。**
一个由于三层 LSTM 构成的 seq2seq 模型如图2所示。
![这里写图片描述](https://img-blog.csdn.net/20161126153641841)[ ](https://img-blog.csdn.net/20161126153641841)
*图2 3层 LSTM seq2seq*
[

](https://img-blog.csdn.net/20161126153641841)
## 实验对比
[
](https://img-blog.csdn.net/20161126153641841)如果把 seq2seq 视作 conditional RNNLM，很难想象，这种略显”简陋”的模型能为我们做什么严肃的事情。也许是 deep learning 就自带三分 black art 属性。
[
](https://img-blog.csdn.net/20161126153641841)实验是在 WMT’14 英语到法语的翻译任务上进行的。具体实验设置和训练见文献[5]。值得指出的是，输入序列（source sentences）是按逆序处理的（e.g. good morning . -> . morning good），实验表明这样能够显著改善模型性能。
[
](https://img-blog.csdn.net/20161126153641841)![这里写图片描述](https://img-blog.csdn.net/20161126153703061)[ ](https://img-blog.csdn.net/20161126153703061)
*图3 seq2seq 模型性能（得分越大越好）[5]*
[
](https://img-blog.csdn.net/20161126153703061)直接用训练好的系统进行测试，结果如图3。基于 seq2seq 的模型，性能已经好于当时 attention 机制的 NTM（图3第一行，介绍 attention 模型时会讨论）（注：放这里比较不公平，两个模型参数量相差很多）。Emsemble 5个后，性能超过了基线系统。
![这里写图片描述](https://img-blog.csdn.net/20161126153724561)[ ](https://img-blog.csdn.net/20161126153724561)
*图 4 用 seq2seq 做 rescore 的性能 [5]*
[
](https://img-blog.csdn.net/20161126153724561)如果将 seq2seq 用于 rescore，可以看到，性能已经明显好于基线，虽然还是要比最好的系统差。
[
](https://img-blog.csdn.net/20161126153724561)![这里写图片描述](https://img-blog.csdn.net/20161126153738452)[ ](https://img-blog.csdn.net/20161126153738452)
*图5 encoder 隐层状态分布（PCA 降到 2 维）[5]*
[
](https://img-blog.csdn.net/20161126153738452)最后，图5展示了 encoder 得到最后的隐层状态。直觉上，encoder 确实编码到了有用的语义信息（sent2vector）。
[
](https://img-blog.csdn.net/20161126153738452)由实验结果可知，*前途是光明的，过程是曲折的*：1) seq2seq 确实能很好的解决 nontrivial 的问题（机器翻译）；2)基于 seq2seq 的系统还不足以取代翻译系（和 state-of-the-art 差距还很大）。
[
](https://img-blog.csdn.net/20161126153738452)
> 使用完全相同的模型，将平行语料数据替换为对话的文本数据，就可以训练一个数据驱动的对话系统 [6]。
[

](https://img-blog.csdn.net/20161126153738452)
## source code
[
](https://img-blog.csdn.net/20161126153738452)基于已有的 RNN 实现 seq2seq 只需要很少很少的代码量 。比如，Google 官方的 seq2seq 实现（[tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L153)）。
[

](https://img-blog.csdn.net/20161126153738452)
## Variant： image captioning
[
](https://img-blog.csdn.net/20161126153738452)以上是标准的 seq2seq 的模型。在一些应用场景下可以需要灵活的变通。比如 [7] 中的“看图说话”任务（Image captioning）（图6）。
[
](https://img-blog.csdn.net/20161126153738452)![这里写图片描述](https://img-blog.csdn.net/20161126153759327)[ ](https://img-blog.csdn.net/20161126153759327)
*图6 自动生成图像标题/描述[7]*
[
](https://img-blog.csdn.net/20161126153759327)原则上，我们可以把图像做为一个序列，直接应用 seq2seq 模型。然而，我们知道，在 CV 领域，已经发展出了许多以 CNN 为核心的网络结构及（在大量数据集上）预训练好的网络权值，这些网络在图像领域被证明是非常效的。
[
](https://img-blog.csdn.net/20161126153759327)为了利用已有成果，[7] 对 seq2seq 进行了一些合理改造。1) encoder 替换为一个在 ImageNet 上预训练好的 Inception CNN，最后一层的输出做为 encoder 的编码结果； 2) 由于 encoder-decoder 对称性的破坏，将编码结果做为 decoder RNN 的第一步输入，做为 condition 注入 decoder。
[
](https://img-blog.csdn.net/20161126153759327)![这里写图片描述](https://img-blog.csdn.net/20161126153814515)[ ](https://img-blog.csdn.net/20161126153814515)
*图7 Structure of Neural Image Caption Generator[7]*
[

](https://img-blog.csdn.net/20161126153814515)
# RNNenc 模型
[
](https://img-blog.csdn.net/20161126153814515)与 seq2seq [5] 的工作几乎同时，Bengio 团队也提出了一种思想上很类似的 Encoder-Decoder 模型[8]。
[
](https://img-blog.csdn.net/20161126153814515)但由于 Bengio 团队很快转向了性能更好的 attention-based Encoder-decoder 模型，因此这个模型的生命周期只有短短几个月。（[8] 的更重要的一点在于提出了 GRU 模型）。这里依然介绍 RNNenc 是因为，一方面可以和 seq2seq 对照，可以明显看到业界和学界研究风格的差别；另一方面，由 RNNenc 可以更自然的扩展出 attention-based encoder-decoder。
[
](https://img-blog.csdn.net/20161126153814515)![这里写图片描述](https://img-blog.csdn.net/20161126153830625)[ ](https://img-blog.csdn.net/20161126153830625)
*图8 RNNenc[8]*
[

](https://img-blog.csdn.net/20161126153830625)
## 模型结构
[
](https://img-blog.csdn.net/20161126153830625)RNNenc 模型的基本结构如图8所示。
1.**Encoder**没有采用 vanilla RNN 或 LSTM，而是新提出了一个类似 LSTM 的 RNN，称为 GRU（Gated Recurrent Unit）。经验研究表明 LSTM 和 GRU 的性能相当[23][24]。不过 GRU 少个 gate，计算量会少一些。
encoder vector是最后一层的状态，并且作为 decoder 的每一步的输入的一部分。
Decoder也是 GRU， 当前状态由上一步状态及输入（编码向量c、历史解码结果）计算得到。
因为$h_t$的计算已经包含了$y_{t-1}$和$c$，所以，通常输出的概念分布计算如下：

$$
P(y_t|y_{t-1}, y_1) = g(h_t)
$$
但为了更直接$y_{t-1}$和$c$对$y_t$的 condition, RNNenc 计算公司如下：

$$
P(y_t|y_{t-1}, y_1) = g(h_t, y_{t-1}, c)
$$
$G(\cdot)$为仿射变换。
# seq2seq vs. RNNenc
简单总结下两种模型的区别。
encoder
seq2seq 和 RNNenc 分别使用 LSTM  和 GRU，这一点无关要旨。但标准的 seq2seq 要求 encoder 和 decoder 的结构相同，RNNenc 没有这个限制。
encoded vector 的使用
seq2seq 只将编码向量（作为隐层状态或输入） 注入到 decoder 的第一时间步中；而 RNNenc（这点类似微软的工作[4]）。
直觉上，后者一种方法中， RNN 的学习负担更轻（不需要额外记忆 encoded vector），也有利于训练时误差回传到，[24]的一些实验结论似乎也支持这点。
但在工程实现，显然前者可以直接利用现有的 （高度优化的）RNN 模块。而且，[5] 利用反转输入序列的技巧，一定程度上克服可能困难。
decoder 输出
seq2seq  中 decoder 是一个标准 RNNLM 的配置，即输入为真实的序列符号。使用真实的（clean）序列，显然有利用模型的训练。但在做预谋（inference）时，由于 decoder 只能从历史解码结果（sampling）做为输入。因此，这种训练和预测时的不一致，会损害实际性能。
另一方面，RNNenc 中，在训练过程中 encoder 使用历史解码结果做输入（结果可能是错的，即输入有噪声），做到了训练和预测时的匹配，但同时增加了训练的难度。一种自然的想法，可以综合这两种策略，以提高模型鲁棒性[9][27]。
decoder 输出
两者模型的 decoder 输出都需要输出下个概率分布。seq2seq 的输出是一个标准的流程： RNN 输出 -> 仿射变换 -> softmax。而 RNNenc 却将 RNN 的输出融合c、y_{t-1}，是一个 ad hoc 的选择。

---
参考文献
1 Graves.[Generating sequences with recurrent neural networks](https://arxiv.org/abs/1308.0850).
2.Mikolov et al.[Recurrent neural network based language model](http://isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf). InterSpeech’10
3.Mikolov.[Statistical Language Models based on Neural Networks](http://deeplearning.cs.cmu.edu/pdfs/1030/Mikolov_Thesis.pdf). PhD thesis
4.Auli et al.[Joint Language and Translation Modeling with Recurrent Neural Networks](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/EMNLP2013RNNMT.pdf). EMNLP’13
5.Sutskever et al.[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215). NIPS’14
6.Vinyal et al.[A Neural Conversational Model](https://arxiv.org/abs/1506.05869). ICML’15
7.Vinyal et al.[Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555). CVPR’15
8.Cho et al.[Learning phrase representations using RNN encoder-decoder for statistical machine translation](https://arxiv.org/abs/1406.1078). EMNLP’14
9.Bengio et al.[Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks](https://arxiv.org/abs/1506.03099). NIPS’15
10.Bahdanau et al.[Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473). ICLR’15
11.Wu et al.[Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144).
12.Johnson et al.[Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation](https://arxiv.org/abs/1611.04558).
13.Chorowski et al.[End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results](https://arxiv.org/abs/1412.1602). NIPS’14
14.Chorowski et al.[Attention-Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503). NIPS’15
15.Bahdanau et al.[End-to-End Attention-based Large Vocabulary Speech Recognition](https://arxiv.org/abs/1508.04395). ICASSP’16
16.Chan et al.[Listen, attend and spell: A neural network for large vocabulary conversational speech recognition](http://mirlab.org/conference_papers/International_Conference/ICASSP%202016/pdfs/0004960.pdf). ICASSP’16
17.Xu et al.[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). ICML’15
18.Vinyals et al.[Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449). NIPS’15
19.Graves et al.[Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks](http://www.cs.toronto.edu/~graves/icml_2006.pdf). ICML’06
20.Graves et al.[Towards end-to-end speech recognition with recurrent neural networks](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf). ICML’14
21.Sak et al.[Learning Acoustic Frame Labeling for Speech Recognition with Recurrent Neural Network](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm3201415/ctc.pdf). ICASSP’15
22.Sak et al.[Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition](http://isca-speech.org/archive/interspeech_2015/papers/i15_1468.pdf). InterSpeech’15
23.Amodei et al.[Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin](http://jmlr.org/proceedings/papers/v48/amodei16.pdf). ICML’16
24.Xu et al.[Variational Autoencoder for Semi-supervised Text Classification](http://www.cil.pku.edu.cn/publications/papers/2016/XuSunDengTanAAAI2017.pdf). AAAI’17
25.Chung et al.[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555). NIPS’14
26.Jozefowicz et al.[An Empirical Exploration of Recurrent Network Architectures](http://jmlr.csail.mit.edu/proceedings/papers/v37/jozefowicz15.pdf). ICML’15
27.Lamb et al.[Professor Forcing: A New Algorithm for Training Recurrent Networks](https://arxiv.org/abs/1610.09038). NIPS’16

