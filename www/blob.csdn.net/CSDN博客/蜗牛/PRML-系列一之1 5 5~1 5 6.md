
# PRML-系列一之1.5.5~1.5.6 - 蜗牛 - CSDN博客


2015年05月14日 00:50:08[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：1021


## 推理和决策
我们已经将分类问题分成两个独立的阶段，推理阶段，我们使用训练数据来学习p（Ck| x）的模型，在随后的决策阶段，我们使用这些后验概率做出最优分类任务。另一种可能性是一起解决这两个问题，简单学习一个直接映射输入x到决策的函数。这样的函数被叫做判别函数。
事实上，我们可以找出三种不同的方法来解决决策问题，所有这些都被用于实际应用。
（a）首先对于每一个类Ck解决确定条件为类的密度p（x| Ck）的推理问题。另外分别推断类的前概率p（Ck）。然后用贝叶斯定理的形式：
![这里写图片描述](https://img-blog.csdn.net/20150513214604361)[ ](https://img-blog.csdn.net/20150513214604361)
找到类的后验概率p（Ck| x）。像往常一样，贝叶斯定理中的分母可以用分子的形式来表示，因为
![这里写图片描述](https://img-blog.csdn.net/20150513215024803)[ ](https://img-blog.csdn.net/20150513215024803)
等效地，我们可以直接给联合分布p（x，Ck）建模，然后归一化得到后验概率。找到了后验概率，我们用决策论来确定每个新输入x的类成员。明确或隐含地给输入和输出分布建模被称作生成模型，因为通过从他们里面采样，有可能在输入空间生成合成数据点。
（b）首先解决确定类后验概率p（Ck| x）的推理问题，随后用决策论给每个新x值分配一个类。直接给后验概率建模的方法被称为判别模型。
（c）找到一个函数f（x），称为判别函数，其将每个输入x直接映射到一个类标签。例如，在两类问题的情况下，f（·）可以是二进制值并f=0表示C1类，f=1代表C2类。在这种情况下，概率不发挥作用。
让我们考虑这三个备选方案的相对优点。方法（a）是最苛刻的，因为它涉及到寻找的x和Ck的联合分布。对于许多应用，x维数比较高，因此我们可能需要大量的训练集，以便能够确定以类为条件的密度到合理的精度。注意，类的先验p（Ck）常常可以从每类的部分训练集数据点简单的估计出来。然而，方法（a）的一个优点是它也允许从（1.83）确定数据p（x）的边缘密度。这对于检测概率比较低的新数据点是有用的，不过预测的精度可能比较低，这被称为异常值检测或新奇检测。
但是，如果我们只希望做分类决策，那么对于找到联合分布p（x，Ck）就显得浪费计算资源和过分苛求数据，其实我们只需要后验概率ρ（Ck| x），直接通过方法（b）来获得。以类为条件的密度可能包含很多结构，该结构对后验概率的影响非常小，如图1.27。探索生成和判别方法做机器学习的优缺点，以寻找组合它们的方法（Jebara，2004;Lasserre等人，2006）引起了大家的兴趣。
![这里写图片描述](https://img-blog.csdn.net/20150513223616630)[ ](https://img-blog.csdn.net/20150513223616630)
一个更简单的方法是（c），我们用训练数据来寻找判别函数f（x），它直接映射每个x到一类标签，从而组合推理和决策阶段成一个单一的学习问题。在图1.27，这相当于找到垂直绿线对应的x值，因为这是给出最小误分类概率的决策边界。
然而，对于选项（c），我们不能访问后验概率p（Ck| x）。但在很多情况下，计算后验概率是非常有用了，虽然我们后来是用它来做了决策。这些包括：
**最小化风险**。考虑的一个问题，在该问题中损失矩阵的元素是不是被修改（这种情况可能发生在一个金融应用中）。如果我们知道后验概率，我们可以通过适当地调整（1.81）来修改最小风险决策准则。如果我们只有一个判别函数，则任何损失矩阵的改变将需要我们回到训练数据并重新解决分类问题。
**拒绝选项**。后验概率使我们能够确定最小化误分类率，或更一般地对拒绝数据点给定部分的期望损失的拒绝标准。
**修正先验**。再次考虑我们的医用X射线问题，假设我们已经从普通民众中收集了大量的X射线图像来作为训练数据，以便建立一个自动筛选系统。由于癌症是在总人口中是比较少见的，我们可能会发现，每1000个数据中只有一个有癌症的存在。如果我们用这样一组数据来训练自适应模型，我们可能会由于小比例的癌症类而面临严重的困难。例如，一个分配每个点到正常类的分类器实现了99.9％的准确率，这将是很难避免的。此外，甚至大的数据集将包含很少对应癌症的X射线图像，所以学习算法将不被暴露于广泛的这类图像，因此泛化结果不会很好。对不同类，我们选择相等数目的样例构成的平衡数据集会让我们找到一个更精确的模型。但是，我们接下来要修正我们对训练数据做的修改。假设我们已经使用这样的修改数据集，并找到了后验概率的模型。根据贝叶斯定理（1.82），我们可以看到，后验概率正比于先验概率，这里我们可以解释为每个类中存在点的一部分。因此，我们可以简单地从我们人工平衡数据集中获得后验概率，首先除以数据集的类分数，然后乘以我们希望应用到模型中人的类分数。最后，我们需要归一化以确保新的后验概率总和为一。注意，如果我们已经学到了一个判别函数，而不是直接确定后验概率，那么该过程不能被应用。
**组合模型**。 对于复杂的应用，我们不妨将问题分成许多较小的子问题，每个子问题可以被单独的模块处理。例如，我们假定的医疗诊断问题，我们从血液测试以及X射线图像可能得到有用的信息。而不是将所有异构信息组合成一个巨大的输入空间，建立一个系统来解释X射线图像，再建一个来解释血液数据可能更高效。只要这两个模型的任一个给出类别的后验概率，我们就可以利用概率规则系统的组合输出。做到这一点的简单方法是对每个类，假设对于X射线图像（记为xI）和血液数据（记为xB）的输入分布是独立的，从而使
![这里写图片描述](https://img-blog.csdn.net/20150513234120617)[ ](https://img-blog.csdn.net/20150513234120617)
这是条件独立属性的一个例子，因为当分布是以类Ck为条件时，独立才成立。给定X射线和血液数据的后验概率由下式给出
![这里写图片描述](https://img-blog.csdn.net/20150513234433012)[ ](https://img-blog.csdn.net/20150513234433012)
因此，我们需要类的先验概率p（Ck）（我们可以轻松地从每类的部分数据点估计出来），然后我们需要归一化所得的后验概率使得他们总和为一。特别的独立性假设（1.84）是朴素贝叶斯模型的一个例子。注意，联合边缘分布p（xI，xB）在这个模型中通常不会因式分解。我们将在后面的章节中看到如何结合不要求条件独立性假设（1.84）的数据来构造模型。
[
](https://img-blog.csdn.net/20150513234433012)
## 回归的损失函数
[
](https://img-blog.csdn.net/20150513234433012)到目前为止，我们已经在分类问题的背景下讨论了决策论。我们现在转向回归问题，如前面所讨论的曲线拟合。决策阶段包括对每个输入值x选择一个t的估计值y（x）。假设这样做了，我们导致一个损失L（t，y（x））。平均或期望损失由下式给出
![这里写图片描述](https://img-blog.csdn.net/20150513235634280)[ ](https://img-blog.csdn.net/20150513235634280)
回归问题中一个常见的随时函数选择是损失的平方![这里写图片描述](https://img-blog.csdn.net/20150513235617632)[，本例中，期望损失写为： ](https://img-blog.csdn.net/20150513235617632)
![这里写图片描述](https://img-blog.csdn.net/20150513235903603)[ ](https://img-blog.csdn.net/20150513235903603)
我们的目标是选择y（x）使得最小化E[L]，如果我们假设一个非常灵活的函数y（x），我们对变量微分得到：
![这里写图片描述](https://img-blog.csdn.net/20150513235949917)[ ](https://img-blog.csdn.net/20150513235949917)
为了求出y（x），利用概率的求和和乘积规则，我们得到:
![这里写图片描述](https://img-blog.csdn.net/20150514000413529)[ ](https://img-blog.csdn.net/20150514000413529)
这是以x为条件的t的平均值，就是所谓的回归方程。这个结果图示于1.28。很容易扩展到多元目标变量**t**，此时最优解为![这里写图片描述](https://img-blog.csdn.net/20150514000916060)[。 ](https://img-blog.csdn.net/20150514000916060)
![这里写图片描述](https://img-blog.csdn.net/20150514001011827)[ ](https://img-blog.csdn.net/20150514001011827)
我们还可以用一个稍微不同的方式得到这样的结果，这也将进一步揭示了回归问题的本质。有了最佳解决方案是条件期望这个知识点后，我们可以扩大平方项为如下
![这里写图片描述](https://img-blog.csdn.net/20150514001445733)[ ](https://img-blog.csdn.net/20150514001445733)
为了保持符号整洁，我们用E[t|x]表示Et[t|x]。代入损失函数并对t执行积分，我们看到，交叉项消失了，我们获得的损失函数表达式如下：
![这里写图片描述](https://img-blog.csdn.net/20150514001750328)[ ](https://img-blog.csdn.net/20150514001750328)
我们试图确定的函数y（x）只进入了第一项，当y（x）等于E [t|x]时它是最小的，在这种情况下，这项就会消失。这是我们之前得到的简单结果，它表明最优的最小二乘预测由条件均值给出。第二项是t的方差。它代表目标数据的固有变异性，可以被看作是噪声。因为它是独立于y（x），这表示不可约的损失函数最小值。
对于分类问题，我们既可以决定适当的概率，然后用这些来做出最佳决策，或者我们可以建立直接作出决策的模型。事实上，我们可以找出三种不同的方法来解决给出的回归问题，根据复杂性递减的方式排序：
（a）首先解决确定联合密度p（x，t）的推理问题。然后归一化找到的条件密度p（t|x），最后边缘化由（1.89）给出的条件均值。
（b）首先解决确定条件密度p（t|x）的推理问题，然后边缘化（1.89）给出的条件均值。
（c）从训练数据直接求回归函数y（x）。
作为上述的分类问题，这三种方法的优缺点遵循相同的标准。
损失的平方不是回归中损失函数的唯一选择。实际上，在有些情况中，损失的平方可能导致非常差的结果，对于这种情况我们需要开发更复杂的方法。一个重要的例子是条件分布p（t| x）是多峰的，如经常在逆问题的解决方案中出现。下面我们简明介绍一个损失平方的泛化，叫Minkowski损失，它的期望如下：
![这里写图片描述](https://img-blog.csdn.net/20150514003932543)[ ](https://img-blog.csdn.net/20150514003932543)
当q=2是，就变为期望的平方损失。函数![这里写图片描述](https://img-blog.csdn.net/20150514004448833)[如图1.29。当q=2是，E[Lq]根据条件均值给出，q=1时是条件中值，以及q→0时是条件众数。 ](https://img-blog.csdn.net/20150514004448833)
![这里写图片描述](https://img-blog.csdn.net/20150514004603589)
[
						](https://img-blog.csdn.net/20150514004448833)
[
	](https://img-blog.csdn.net/20150514004448833)
[
  ](https://img-blog.csdn.net/20150514003932543)