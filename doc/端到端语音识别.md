# 端到端语音识别



# （一） 概况

## 传统方法的局限性[1]

### HMM

#### Markovian Assumption

$$
p(q_t|q_{<t})=p(q_t|q_{t-1})
$$

转移概率只跟前一个时刻有关，无法对长时依赖性建模。

#### Conditional Independence Assumption

$$
p(x_t|x_{<t}, q_{\leq t})=p(x_t|q_t)
$$

帧的生成概率只跟当前状态有关，跟历史状态和历史帧无关。

### DNN

#### alignment

DNN的声学模型用来求输出状态对应的后验概率。需要用到GMM的对齐结果，来获得每一帧的label。首先需要GMM的对齐结果比较准确，其次是本身语音的边界不好界定，这样每一帧给一个指定label本身值得商榷。

#### Conditional Independence Assumption

和HMM类似，有独立性的假设。

### Dict

词典和音素本身是handcrafted的

## End to End实现方法[2]

end to end的语音识别主要有两种方法来实现 
\- 基于CTC的训练准则 
CTC训练准则的引入抛弃了传统的HMM框架，输出的粒度可以到phone/charactor/word等。但是仍然有输出独立性的假设，使用的时候对于语言模型的依赖性比较强。 
\- 基于sequence to sequence和attention 
虽然这种方式可以学习到word之间的关系，但是毕竟训练语料的transcription有限，所以额外的语言模型使用还是比较有必要。

end to end的语音识别框架在一定程度上减少了传统方法不合实际的假设，但是也存在训练困难的问题，为了取得比较好的效果，需要的训练语料也更多；而基于传统的DNN-HMM hybrid系统的方法可以得到更稳定的性能。

## Reference

[1].End-to-End Speech Recognition Models 
[2].Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin





# （二） ctc



## History

ICML-2006. Graves et al. [1] introduced the connectionist temporal classification (CTC) objective function for phone recognition. 
ICML-2014. Graves [2] demonstrated that character-level speech transcription can be performed by a recurrent neural network with minimal preprocessing. 
Baidu. 2014 [3] DeepSpeech, 2015 [4] DeepSpeech2. 
ASRU-2015. YaJie Miao [5] presented Eesen framework. 
ASRU-2015. Google [6] extended the application of Context-Dependent (CD) LSTM trained with CTC and sMBR loss. 
ICASSP-2016. Google [7] presented a compact large vocabulary speech recognition system that can run efficiently on mobile devices, accurately and with low latency. 
NIPS-2016. Google [8] used whole words as acoustic units. 
2017, IBM [9] employed direct acoustics-to-word models.

## Reference

[1]. A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal classfification: labelling unsegmented sequence data with recurrent neural networks. In ICML, 2006. 
[2]. Graves, Alex and Jaitly, Navdeep. Towards end-to-end speech recognition with recurrent neural networks. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1764–1772, 2014. 
[3]. Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G.,Elsen, E., Prenger, R., Satheesh, S., Sengupta, S., Coates,A., et al. (2014a).Deepspeech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567. 
[4]. D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” CoRR arXiv:1512.02595, 2015. 
[5]. Yajie Miao, Mohammad Gowayyed, Florian Metze. EESEN: End-to-End Speech Recognition using Deep RNN Models and WFST-based Decoding. 2015 Automatic Speech Recognition and Understanding Workshop (ASRU 2015) 
[6]. A. Senior, H. Sak, F. de Chaumont Quitry, T. N. Sainath, and K. Rao, “Acoustic Modelling with CD-CTC-SMBR LSTM RNNS,” in ASRU, 2015 
[7]. I. McGraw, R. Prabhavalkar, R. Alvarez, M. Gonzalez Arenas, K. Rao, D. Rybach, O. Alsharif, H. Sak, A. Gruenstein, F. Beaufays, and C. Parada, “Personalized speech recognition on mobile devices,” in Proc. of ICASSP, 2016. 
[8]. H. Soltau, H. Liao, and H. Sak, “Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition,” arXiv preprint arXiv:1610.09975,2016. 
[9]. K. Audhkhasi, B. Ramabhadran, G. Saon, M. Picheny, D. Nahamoo, “Direct Acoustics-to-Word Models for English Conversational Speech Recognition” arXiv preprint arXiv:1703.07754,2017.







# （三） Sequence to Sequence and Attention

2017年04月20日 19:54:00 [xmucas](https://me.csdn.net/xmdxcsj) 阅读数：3777



## History

### encoder-decoder

2014年Kyunghyun Cho[1]提出了RNN Encoder-Decoder的网络结构，主要用在翻译上面。 
encoder将变长的输入序列映射到一个固定长度的向量，decoder将该向量进一步映射到另外一个变长的输出序列，网络结构如下图： 
![这里写图片描述](https://img-blog.csdn.net/20170420195158452?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 
encoder: 
$$
\boldsymbol h_{\langle t \rangle}=f(\boldsymbol h_{\langle t-1 \rangle}, x_t)
$$
h⟨t⟩=f(h⟨t−1⟩,xt)h⟨t⟩=f(h⟨t−1⟩,xt)

decoder:
$$
\boldsymbol h_{\langle t \rangle}=f(\boldsymbol h_{\langle t-1 \rangle}, y_{t-1}, c)\\
P(y_t|y_{t-1},...,y_1,c)=g(\boldsymbol h_{\langle t \rangle}, y_{t-1}, c)
$$
其中c是encoder最后时刻的h，f()是类似于简化版的LSTM单元，具有reset gate和update gate，以实现捕捉short-term和long-term的依赖性。



### sequence to sequence

2014年google的Ilya Sutskever[2]等人提出了sequence to sequence的学习方法来解决英文到法文的翻译问题，整体框架如下 
![这里写图片描述](https://img-blog.csdn.net/20170420195213483?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
相比于[1]，主要是网络使用LSTM，并且将输入序列进行翻转，解决了长序列性能下降的问题。

### attention

Graves[3]在2013首先在handwriting synthesis中引入attention的机制，和简单的sequence generation不同，在预测的时候，还通过soft window使用了额外的输入信息。在动态产生预测的同时，也确定了text和pen locations之间的对齐关系。 
Dzmitry Bahdanau[4]将[1]中的decoder的cc替换为了cici，即不同时刻i的输出概率的计算不再使用相同的cc。 
![这里写图片描述](https://img-blog.csdn.net/20170420195243733?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 
其中cici的计算依赖于输入的annotations$(h_1, ..., h_{T_x})$，计算公式如下： 
$$
c_i=\sum_{j=1}^{T_x}\alpha_{ij}h_j\\
\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}\\
e_{ij}=a(s_{i-1}, h_j)
$$
其中a()使用前向神经网络来表示，和encoder-decoder一起训练，也就是在学习translate的同时还需要学习alignment。

$α_{ij}$表示输出$y_i$对齐到$x_j$的概率，相当于引入了attention的机制，这在一定程度上减轻了encoder的压力，因为之前encoder需要把所有的输入信息映射到一个固定的向量c。



## Speech Application

### phone recognition

2014年.Jan Chorowski[5]将encoder-decoder和attention的网络结构应用到语音中的phone识别上面。[3]中的attention在权重分布的时候有可能将较大的权重分配到比较远的输入上面，从而达到long-distance word reordering的效果。文中对attention的分布进行了一定的限制，保证输出附近对应的输入的权重比较大，而且权重的分布随着时间往后移动，即单调性。主要有两点改进： 
1.修改attention的计算方法，引入d()来学习权重的向后移动过程 
![这里写图片描述](https://img-blog.csdn.net/20170420195328781?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
2.在loss里面增加惩罚项，We penalize any alignment that maps to inputs which were already considered for output emission

2015年.Jan Chorowski[6]在[5]的基础上进行了改进，在使用上一时刻的alignment的时候直接进行Convolution，修改softmax函数突出重点帧的影响，同时不再使用整个序列的hh，只采用特定窗口范围内的hh。

### speech recognition

[5][6]主要将attention和encoder-decoder的网络用在了phone的识别上面，2016年Dzmitry Bahdanau[7]进一步将其应用到LVCSR，输出为character，然后结合语言模型进行解码。文中提出了pooling的方法为了解决输入长度过长带来的计算复杂的问题。 
![这里写图片描述](https://img-blog.csdn.net/20170420195345844?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 
在不使用外部语言模型的情况下，比ctc方法性能有较大提升，主要得益于encoder-decoder的框架隐式的学习character之间的关系，而CTC当前时刻的输出跟上一时刻的输出是独立的，因此无法刻画输出character之间的关系 
[8]和[7]类似，也是输出到character，使用了pooling的思想使用了pyramid BLSTM网络结构来来解决输入序列过长训练困难的问题。

## Reference

[1]. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. (2014a). Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014). 
[2].Sutskever, I., Vinyals, O., and Le, Q. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NIPS 2014). 
[3].Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv:1308.0850 [cs]. 
[4].D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in ICLR, 2015. 
[5].Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end continuous speech recognition using attention-based recurrent NN: First results. arXiv:1412.1602 [cs, stat], December 2014. 
[6].Chorowski, Jan K, Bahdanau, Dzmitry, Serdyuk, Dmitriy, Cho, Kyunghyun, and Bengio, Yoshua. Attention-based models for speech recognition. In Advances in Neural Information Processing Systems, pp. 577–585, 2015. 
[7].D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio. End-to-end attention-based large vocabulary speech recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4945–4949, March 2016. doi: 10.1109/ICASSP.2016.7472618. 
[8].William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. Listen, attend and spell. arXiv preprint arXiv:1508.01211, 2015.





# （四） raw wavform

2017年04月20日 19:55:13 [xmucas](https://me.csdn.net/xmdxcsj) 阅读数：2885



现在的端到端语音识别的第一个“端”大部分还是使用人为设定的语音特征，比如FBANK/PLP，更高级的端到端语音识别输入是语音波形，输出是文字。 
近几年也有一些工作是使用神经网络（比如CNN）来学习传统的特征提取步骤，取得了跟使用传统的语音特征相当的结果，当前这部分工作绝大多数还是基于传统的HMM框架来做，还没有跟CTC或者encoder-decoder相结合。

## CNN

Google[1]分析了CNN跟mel-scale filterbank之间的关系，convolution layer相当于学习一组FIR滤波器，学习到的这组filter对应的中心频率曲线跟mel-fb类似。

## CLDNN

Google[2]使用一层CNN来抽取特征，声学模型使用CLDNN，在2000h数据集上取得了跟log-mel filterbank特征相当的效果。 
![这里写图片描述](https://img-blog.csdn.net/20170420195455266?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 
第一层称为time-convolutional layer，用来学习滤波器参数。 
\- 每次输入M个采样点，帧移10ms 
\- 使用P个filter，对应于最后的P个频率输出 
\- max pooling，移除语音的short term phase信息

后面使用CLDNN的网络结构，这里面的convolutional layer称为fConv layer，相当于与频域信号做卷积，减少spectral variations。

## CNN+TDNN

[3]提出了CNN+TDNN的网络结构，相比于[2]来讲，抽取特征使用NIN的非线性变换来替换pooling，获得了更快的收敛速度。

## Reference

[1].SPEECH ACOUSTIC MODELING FROM RAW MULTICHANNEL WAVEFORMS 
[2].Learning the Speech Front-end With RawWaveform CLDNNs 
[3].Acoustic modelling from the signal domain using CNNs



