# 语音识别笔记





# 概述

虽然现在的语音识别中，DL已经应用的非常广泛了，但是语音识别终究还是有一些领域知识的，将之归类为DL或者ML，似乎都不妥当。特形成本系列文章，用以描述automatic speech recognition的领域知识和传统方法。

说起来还是要感谢DL，不然按照传统的行业划分，几乎不会有人同时研究CV和ASR。DL的出现，实际上大大降低了算法的领域迁移成本，领域知识的重要性相对下降了。

## 历史

早在电子计算机出现之前，人们就有了让机器识别语音的梦想。1920年生产的“Radio Rex”玩具狗可能是世界上最早的语音识别器，当有人喊“Rex”的时候，这只狗能够从底座上弹出来。

![这里写图片描述](https://img-blog.csdn.net/20180813093218910?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

但实际上它所用到的技术并不是真正的语音识别，而是通过一个弹簧，这个弹簧在接收到500赫兹的声音时会自动释放，而500赫兹恰好是人们喊出“Rex”中元音的第一个共振峰。

![这里写图片描述](https://img-blog.csdn.net/20180813092903163?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## 学校

SR领域最牛的高校主要是美国的CMU、Johns Hopkins University、英国的Cambridge University和日本的东京大学。

## 书籍

《Speech and Language Processing: An introduction to natural language  processing, computational linguistics, and speech recognition》，Daniel  Jurafsky & James H. Martin著。

> Daniel Jurafsky，1962年生，UCB本科（1983）+博士（1992）。斯坦福大学教授。 
>    个人主页： 
>    <https://web.stanford.edu/~jurafsky/>
>
> James H. Martin，哥伦比亚大学本科+UCB博士。University of Colorado Boulder教授。 
>    个人主页： 
>    <http://www.cs.colorado.edu/~martin/>

这本书比较老了（1999年），但毕竟是本1000页左右的书，传统方法该说的基本都说了。主要问题在于NLP和语义学的内容较多，相对来说ASR的内容就少了些。

《Spoken Language Processing-A Guide to Theory, Algorithm and System Development》，黄学东等著。

这本书基本上是ASR传统方法的大全了，无论理论还是工程实践都有相当大的篇幅，但也有些老了（2001年）。

《解析深度学习:语音识别实践》，俞栋、邓力著。

这本书算是中文写的比较好的教程了，而且DL的篇幅很大，内容非常新。(2016年)

## 教程

<http://tts.speech.cs.cmu.edu/courses/11492/schedule.html>

Speech Processing。CMU的这个教程主要包含ASR（Automatic Speech Recognition）、TTS（Text To Speech）和SDS（Spoken Dialog Systems）等三方面的内容。

> Alan W Black，苏格兰计算机科学家。Coventry University本科（1984）+University of Edinburgh硕博（1984,1993）。CMU教授。语音处理专家。 
>    个人主页： 
>    <http://www.cs.cmu.edu/~awb/> 
>    他的主页上有好多Speech、NLP方面的教程。他本人长得太像Java之父James Gosling了。

<http://web.stanford.edu/class/cs224s/index.html>

CS224S / LINGUIST285 - Spoken Language Processing。Stanford的教程相对比较新，DL涉及的比较多。

<http://www.inf.ed.ac.uk/teaching/courses/asr/index.html>

Automatic Speech Recognition。这个课程至少从2012年就开始了，每年都有更新。

<http://speech.ee.ntu.edu.tw/DSP2018Spring/>

国立台湾大学李琳山教授的课程。

> 李琳山，国立台湾大学本科（1974）+Stanford博士（1977）。国立台湾大学教授。

<http://www.cs.cmu.edu/afs/cs/user/bhiksha/WWW/courses/11-756.asr/spring2014/>

Theory and practice of speech recognition systems。CMU的Bhiksha Raj教授的课程，只有ASR的内容。

顺便说一句，Bhiksha Raj的主页上还有好多其他课程。

<https://cs.nyu.edu/~eugenew/asr13/>

这是MIT博士Eugene Weinstein在NYU当助教的时候（2013年）开的课程。

<http://berlin.csie.ntnu.edu.tw/Courses/Speech%20Processing/Speech%20Processing_Main_2016S.htm>

Speech Processing。国立台湾师范大学的陈柏琳教授的课程。陈教授教学多年，主页上还有好多其他课程。

<https://www.isip.piconepress.com/courses/msstate/ece_8463/lectures/current/index.html>

Mississippi State University：ECE 8463: fundamentals of speech recognition

<https://www.isip.piconepress.com/courses/msstate/ece_7000_speech/index.html>

ECE 8000: special topics in speech recognition

<https://www.isip.piconepress.com/courses/msstate/ece_8990_info/index.html>

ECE 8990: Information Theory。这门课偏重数学理论，包括Entropy、Markov Processes、Kolmogorov Complexity等内容，适合用于补数学基础。

<http://courses.cs.tamu.edu/rgutier/csce630_f14/>

CSCE 630: Speech Processing

<http://courses.cs.tamu.edu/rgutier/cpsc689_s07/>

CPSC 689-604: Special topics in Speech and Face Recognition

## blog

<http://www.cnblogs.com/welen/>

<https://blog.csdn.net/weiqiwu1986>

上面两个都是welen的blog，而且内容貌似还不重复。。。

<http://blog.csdn.net/xmdxcsj>

一个语音识别的blog

<https://blog.csdn.net/shichaog>

一个语音识别+Kaldi的blog

<https://blog.csdn.net/quhediegooo/>

一个语音识别的blog

<https://blog.csdn.net/dearwind153/article/category/6506891>

这哥们的blog很杂，这是语音相关的专栏

<http://www.cnblogs.com/JarvanWang/>

一个语音识别+Kaldi的blog

<https://www.zhihu.com/question/65516424>

语音识别kaldi该如何学习？

<http://vsooda.github.io/archive/>

一个语音识别+DL的blog

<https://zhuanlan.zhihu.com/codingmath>

一个语音识别的blog

## 项目

<https://en.wikipedia.org/wiki/List_of_speech_recognition_software>

List of speech recognition software

<https://mp.weixin.qq.com/s/LsVhMaHrh8JgfpDra6KSPw>

横向对比5大开源语音识别工具包

<https://github.com/lingochamp/kaldi-ctc>

英语流利说开源的kaldi-ctc

<https://zhuanlan.zhihu.com/p/23177950>

kaldi-ctc: CTC End-to-End ASR

## HTK

Hidden Markov Model Toolkit是Cambridge University开发的语音识别的工具包。它是GMM-HMM时代最为流行的语音识别工具，但近来流行度不如Kaldi。

官网：

<http://htk.eng.cam.ac.uk/>

HTK Book不仅是使用手册，也是一本介绍原理的书。

<http://speech.ee.ntu.edu.tw/homework/DSP_HW2-1/htkbook.pdf>

## CMU Sphinx

CMU Sphinx是李开复的博士课题项目，后来成为了CMU的长期项目。洪小文、黄学东也先后参与过。该项目比较早的将HMM应用于语音识别，这在当时算是一个重大创新。

> 李开复，1961年生，Columbia University本科（1983）+CMU博士（1988）。先后供职于Apple、SGI、Microsoft、Google。现为创新工场董事长。
>
> 洪小文，1963年生，台湾大学本科+CMU博士。先后供职于Apple、Microsoft，现为微软亚洲研究院院长。
>
> 黄学东，1962年生，湖南大学本科（1982）+清华大学硕士（1984）+University of Edinburgh博士（1989）。现为微软首席语音科学家。
>
> Raj Reddy，1937年生，印度裔美国计算机科学家。印度University of  Madras本科（1958）+澳大利亚University of New South Wales硕士（1960）+Stanford  University博士。CMU教授，首位亚裔图灵奖得主（1994）。 
>    他还是印度Rajiv Gandhi University of Knowledge Technologies创始人和International Institute of Information Technology, Hyderabad主席。 
>    他是李开复、洪小文的博士导师，黄学东的博士后导师。

官网：

<https://cmusphinx.github.io/>

注意：还有一个类似Elasticsearch的文本搜索引擎也叫Sphinx。它的官网是：

<http://sphinxsearch.com/>

## SPTK

The Speech Signal Processing Toolkit是日本的几个科学家开发的语音识别工具库。

官网：

<http://sp-tk.sourceforge.net/>

## Julius

Julius是另一个日本人开发的语音识别工具库。

官网：

<http://julius.osdn.jp/en_index.php>

## HTS

HMM/DNN-based Speech Synthesis System也是日本人开发的工具库，主要用于语音合成。

官网：

<http://hts.sp.nitech.ac.jp>

## Praat

Praat是一款跨平台的多功能语音学专业软件，由University of Amsterdam的Paul Boersma和David Weenink开发。主要用于对数字化的语音信号进行分析、标注、处理及合成等实验，同时生成各种语图和文字报表。

官网：

<http://www.fon.hum.uva.nl/praat/>

## eesen

论文：

《EESEN: End-to-End Speech Recognition using Deep RNN Models and WFST-based Decoding》

> 苗亚杰，南京邮电大学本科（2008）+清华硕士（2011）+CMU博士（2016）。 
>    个人主页： 
>    <http://www.cs.cmu.edu/~ymiao/>

官网：

<https://github.com/srvk/eesen>

eesen是基于Tensorflow开发的，苗博士之前还有个用Theano开发的叫PDNN的库。

## 公司

<http://www.aispeech.com/>

思必驰

<http://www.soundai.com/>

声智科技。偏重于语音信号处理。

<https://zhuanlan.zhihu.com/chenxl>

声智科技创始人陈孝良的专栏

## 数据集

<http://www.speech.cs.cmu.edu/databases/an4/>

The CMU Audio Databases。这个数据集非常老了（1991年），只有64M。

<http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz>

TensorFlow提供的Speech Commands Datasets

还有相关的工具：

<https://github.com/petewarden/extract_loudest_section>

抽取一段wav文件中声音最大的那部分

<https://www.kaggle.com/davids1992/speech-representation-and-data-exploration/notebook>

包含对Speech Commands Datasets的数据处理过程的blog

<https://catalog.ldc.upenn.edu/LDC93S1>

TIMIT数据集（收费）

<https://mp.weixin.qq.com/s/w9_D1_VVhk9md4RANaipDg>

Mozilla开源语音识别模型和世界第二大语音数据集

<http://www.voxforge.org/>

VoxForge是一个非常活跃的众包语音识别数据库和经过训练的模型库

<http://pan.baidu.com/s/1dEhUghz>

清华大学语音和语言技术研究中心（CSLT）公开的数据集。这个数据集除了包含thchs30之外，还包含了其他几个小语种的数据集。

<http://cn-mirror.openslr.org/18/>

单独的thchs30数据集

<http://cn-mirror.openslr.org/33/>

AISHELL数据库是THCHS-30之后，目前中文语音数据开源最大的数据库。

它是由北京希尔贝壳科技有限公司(<http://www.aishelltech.com>)录制的中文普通话数据。由400名来自不同方言区的发音人录制，男女比例均衡。按照设计好的文本，在相对安静环境中使用手机（Android和IOS系统）录制格式为16kHz、16bit单声道数据和高保真麦克风录制格式为44.1kHz、16bit单声道数据同时采集。

<http://www.aishelltech.com/aishell_2>

AISHELL-2的数据规模达到1000小时和更优秀的系统级recipe。数据目前以硬盘和网盘形式免费开放给高校科研教育机构。商用似乎还是要钱的。



# 基本框架

语音识别系统主要有四部分组成：信号处理和特征提取、声学模型、语言模型（Language Model, LM）和解码器(Decoder)。

![这里写图片描述](https://img-blog.csdn.net/20180820093632863?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**信号处理和特征提取**部分以音频信号为输入，通过消除噪音、信道失真等对语音进行增强，将语音信号从时域转化到频域，并为后面的声学模型提取合适的特征。

**声学模型**将声学和发音学的知识进行整合，以特征提取模块提取的特征为输入，生成声学模型得分。

**语言模型**估计通过重训练语料学习词之间的相互概率，来估计假设词序列的可能性，也即语言模型得分。如果了解领域或者任务相关的先验知识，语言模型得分通常可以估计得更准确。

**解码器**对给定的特征向量序列和若干假设词序列计算声学模型得分和语言模型得分，将总体输出分数最高的词序列作为识别结果。

![这里写图片描述](https://img-blog.csdn.net/20180820093549476?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图是一个实际的recognition engine的结构图。其原文地址：

<https://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/39654/1/MP-SS1-4.pdf>

王赟写的[《语音识别技术的前世今生》](https://www.baidu.com/s?wd=%E3%80%8A%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%8A%80%E6%9C%AF%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%E3%80%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)，写的非常好，下载地址：

<https://zhihu-live.zhimg.com/0af15bfda98f5885ffb509acd470b0fa>

> 王赟，清华本科（2010）+CMU在读博士。 
>    个人主页： 
>    <http://www.cs.cmu.edu/~yunwang/>

下面是网友写的注解版本：

<http://www.cnblogs.com/lyu0709/p/6929659.html>

《语音识别的前世今生：GMM+HMM & 深度学习》讲座笔记

<http://www.cnblogs.com/lyu0709/p/6929692.html>

[《语音识别的前世今生》](https://www.baidu.com/s?wd=%E3%80%8A%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%E3%80%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)Q&A

参考：

<https://blog.csdn.net/by21010/article/details/51506292>

语音识别系统结构——鸟瞰

<http://www.cnblogs.com/welen/p/7489504.html>

语音识别概述

<https://www.zhihu.com/question/20398418>

语音识别的技术原理是什么？

# Microphone

麦克风作为业界通俗的一种叫法，是英文Microphone的音译名称，国内的称呼乱一些，有时候也简单称作话筒，香港和台湾地区也会称作微音器、拾音器。麦克风的中文学术名称是传声器，这是一种将声音转换成电子信号的换能器，即把声信号转成电信号，这其实和光电转换的原理是完全一致的。  

消费级市场的麦克风基本都是标量麦克风，也就说只能采集单一的物理量信息——声压。声压是指声波通过媒质时，由振动所产生的压强改变量，也可以理解为声音的幅度或者强度。声压常用字母”p”表示，单位是帕斯卡（符号Pa）。声压的帕斯卡单位由于不方便记忆（比如20×10−6

Pa~20Pa），一般就以对数尺衡量有效声压相对于一个基准值的大小来表示，即声压级，其单位是分贝（符号dB）。

人类对于1KHz的声音的听阈为20×10−6

Pa，通常以此作为声压级的基准值。这样讲可能晦涩难懂，我们来简单的类比一下：人类的呼吸声压是

60×10−6

左右，声压级大约10dB，火箭发射的声压是4000Pa左右，声压级大约165dB，闪光弹的声压超过1万Pa，声压级大约175dB。

为了描述麦克风的性能，有几个性能指标是非常关键的，这包括了灵敏度、指向性、频率响应、阻抗、动态范围、信噪比、最大声压级（或AOP，声学过载点）、一致性等。

现在麦克风阵列主要使用的是**数字MEMS麦克风**，其最长尺寸仅有3.76MM。MEMS麦克风也是手机中大量使用的传感器件，一般手机至少有2个以上这类麦克风。MEMS麦克风实际上只是工艺上的改进，其原理依然属于电容式麦克风。

与MEMS麦克风直接PK的，就是**驻极体麦克风**。它的性能更优秀，但一致性不如MEMS麦克风，因此主要用在单麦上。而麦克风阵列一般都是MEMS麦克风。

被淘汰的技术：带式麦克风、碳精麦克风（老式电话）。

先进技术：压电麦克风、光纤麦克风、激光麦克风、矢量麦克风。

Microphone的主要厂商：

1.Knowles 美国

2.瑞声(AAC) 中国

3.歌尔(Goertek) 中国

4.BSE 韩国

5.ST 欧洲

参考：

<https://zhuanlan.zhihu.com/p/27610503>

盘点麦克风技术及市场，远场语音交互如何选型麦克风

# Microphone Array

麦克风阵列(Microphone Array)，从字面上，指的是麦克风的排列。也就是说由一定数目的声学传感器(一般是麦克风)组成，用来对声场的空间特性进行采样并处理的系统。

![这里写图片描述](https://img-blog.csdn.net/20180820093451290?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图是Amazon Echo所采用的6+1麦克风阵列。

根据声波传导理论，利用多个麦克风收集到的信号可以将某一方向传来的声音增强或抑制。利用这种方法，麦克风阵列可以将噪声环境中特定声音信号有效的增强。由于麦克风阵列技术具有很好的抑制噪声和语音增强的能力，又不需要麦克风时刻指向声源方向，因此在语音处理领域具有非常好的前景，可以用在非常广的应用领域。

## Microphone Array形状

麦克风阵列一般来说有线形、环形和球形之分，严谨的应该说成一字、十字、双L、平面、螺旋、球形等。

至于麦克风阵列的阵元数量，也就是麦克风数量，可以从2个到上千个不等。由于成本限制，消费级麦克风阵列的阵元数量一般不超过8个，所以市面上最常见的就是6麦和4麦的阵型。

智能音箱一般都是放置桌面，需要360度响应指令，所以环形阵列比较适合，而智能中控一般贴墙固定，仅照顾180度范围即可，这时候线形阵列就能满足。

双麦克风阵列：结构简单、成本低、容易实施、功耗低。

像空调、电视这类家电产品，它永远都是贴墙放，八个麦克风在实际应用上是多余的。而双麦克技术在任何产品上均可自然适配。

在机器人领域里，对声源定位的要求比较高，所以一般都会使用环形多麦克方案。这两年国内比较火的Rokid机器人就采用了8麦克的阵列。

参考：

<https://www.leiphone.com/news/201610/5Ye8zxxwtlLGiW1y.html>

技术解读：从亚马逊Echo到谷歌Home，双麦克风阵列更有优势？

## Microphone Array vs 人耳

偶尔会听到行业人士做的一个类比，人类有两只耳朵，所以两个麦克风就能达到同样性能。这实际上是一个误解，以现在技术来看，即便用100个麦克风，也未必能达到人耳的效果。人耳是极其复杂的一个结构，至今为止实际上科学也没搞清楚所有原理，更谈不上用简单的麦克风进行模拟了。现在的麦克风，实际上都是标量麦克风，所获取的仅仅是声压变化转成的电信号，而且还没有耳廓，更无法根据场景变化随动调整。

## Microphone Array与语音交互技术架构

前端主要解决了产品是否听得准的问题，这其中有五个核心指标：**远场语音唤醒率、复杂环境 误唤醒率、远场语音识别率、总体延迟时间和总体稳定性**。这五个核心指标决定了用户的第一体验。

1.**以Google为代表的纯云端技术架构。**麦克风阵列的阵元较多，产生的数据容量太大，而当前的网络上传带宽严重不足，所以只能权衡选择更少的麦克风。

2.**以科胜讯为代表的纯前端技术架构。**纯前端方案的优点就是容易集成到芯片上，缺点就是很难升级以及扩展，这恰好与人工智能不断迭代的趋势不太兼容，也是当前这种方案无法流行的主要原因。

这类方案能提供的功能有限，主要包括自适应回声抵消技术(AEC)、自动噪音抑制技术(ANS)和自动增益控制技术(AGC)。

3.**以Amazon为代表的前端+云端方案。**这种方案是把算法分别放置到前端和云端，根据具体场景可以调配优化，更容易优化性能并扩展功能。这种方案考虑了麦克风阵列与唤醒和识别技术一体化的问题，由于唤醒和识别严重依赖麦克风阵列的算法处理效果，实际上这三种技术是无法完全分割的，特别是麦克风阵列和唤醒技术更是浑然一体。

参考：

<https://zhuanlan.zhihu.com/p/29809882>

让机器听懂人类语言，主流麦克风阵列技术解读

<https://zhuanlan.zhihu.com/p/23420891>

麦克风阵列的语音信号处理技术

## Microphone Array技术难点

传统的阵列信号处理技术直接应用到麦克风阵列处理系统中往往效果不理想，其原因在于麦克风阵列处理有不同的处理特点：

### 阵列模型的建立

麦克风主要应用处理语音信号，拾音范围有限，且多用于近场模型，使得常规的阵列处理方法如雷达，声呐等平面波远场模型不再适用，在近场模型中，需要更加精准的球面波，需要考虑传播路径不同引起的幅度衰减不同。

### 宽带信号处理

通常的阵列信号处理多为窄带，即不同阵元在接受时延与相位差主要体现在载波频率，而语音信号未经过调制也没有载波，且高低频之比较大，不同阵元的相位延时与声源本身的特性关系很大—频率密切相关，使得传统的阵列信号处理方法不再完全适用。

### 非平稳信号处理

传统阵列处理中，多为平稳信号，而麦克风阵列的处理信号多是非平稳信号，或者短时平稳信号，因此麦克风阵列一般对信号做短时频域处理，每个频域均对应一个相位差，将宽带信号在频域上分成多个子带，每个子带做窄带处理，再合并成宽带谱。

### 混响

声音传播受空间影响较大，由于空间反射，衍射，麦克风收到的信号除了直达信号以外，还有多径信号叠加，使得信号被干扰，即为混响。在室内环境中，受房间边界或者障碍物衍射，反射导致声音延续，极大程度的影响语音的可懂度。

# 声源定位

## 近场模型和远场模型

![这里写图片描述](https://img-blog.csdn.net/20180820093419275?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

通常麦克风阵列的距离为1~3m，阵列处于近场模型，麦克风阵列接受的是球面波而不是平面波，声波在传播的过程中会发生衰减，而衰减因子与传播的距离成正比，因此声波从声源到达阵元时候的幅度也各不相同。而远场模型中，声源到阵元的距离差相对较小，可以忽略。通常，我们定义
$$
2L^2/\lambda
$$


为远近场临界值，L为阵列孔径，λ为声波波长，因此阵元接受信号不仅有相位延时还有幅度衰减。



参考：

<https://www.zhihu.com/question/48537863>

远场（far-field）语音识别的主流技术有哪些？

<https://mp.weixin.qq.com/s/1GjBWW5gn2WQb1GWExtB5A>

HOA声场重建原理



## 波束形成

声源定位的方法包括**波束形成，超分辨谱估计和TDOA**，分别将声源和阵列之间的关系转变为**空间波束，空间谱和到达时间差**，并通过相应的信息进行定位。

波束形成是通用的信号处理方法，这里是指将一定几何结构排列的麦克风阵列的各麦克风输出信号经过处理（例如加权、时延、求和等）形成空间指向性的方法。波束形成主要是抑制主瓣以外的声音干扰，这里也包括人声，比如几个人围绕Echo谈话的时候，Echo只会识别其中一个人的声音。

波束形成可分为常规的波束形成CBF（Conventional Beam Forming）、CBF+Adaptive Filter和自适应波束形成ABF（Adaptive Beam Forming）。

## 超分辨谱估计

如MUSIC，ESPRIT等，对其协方差矩阵（相关矩阵）进行特征分解，构造空间谱，关于方向的频谱，谱峰对应的方向即为声源方向。适合多个声源的情况，且声源的分辨率与阵列尺寸无关，突破了物理限制，因此成为超分辨谱方案。这类方法可以拓展到宽带处理，但是对误差十分敏感，如麦克风单体误差，通道误差，适合远场模型，矩阵运算量巨大。

## TDOA

TDOA（time difference of arrival）是先后估计声源到达不同麦克风的时延差，通过时延来计算距离差，再利用距离差和麦克风阵列的空间几何位置来确定声源的位置。分为TDOA估计和TDOA定位两步：

### TDOA估计

常用的有广义互相关GCC（Generalized Cross Correlation）和LMS自适应滤波。

### TDOA定位

TDOA估值进行声源定位，三颗麦克风阵列可以确定空间声源位置，增加麦克风会增高数据精度。定位的方法有MLE最大似然估计，最小方差，球形差值和线性相交等。

TDOA相对来讲应用广泛，定位精度高，且计算量最小，实时性好，可用于实时跟踪，在目前大部分的智能定位产品中均采用TDOA技术做为定位技术。

## 参考

<https://wenku.baidu.com/view/903f907f31b765ce05081431.html>

基于传声器阵列的声源定位

<https://zhuanlan.zhihu.com/p/35590325>

MIT提出像素级声源定位系统PixelPlayer：无监督地分离视频中的目标声源

<https://zhuanlan.zhihu.com/p/27921878>

揭秘武林绝学——“听声辨位”

# 其他前端问题

## 语音增强

语音增强是指当语音信号被[各种各样](https://www.baidu.com/s?wd=%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)的噪声(包括语音)干扰甚至淹没后，从含噪声的语音信号中提取出纯净语音的过程。

![img](https://img-blog.csdn.net/20180827100751320?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## 去混响（Dereverberation）

一般我们听音乐时，希望有混响的效果，这是听觉上的一种享受。合适的混响会使得声音圆润动听、富有感染力。混响（Reverberation）现象指的是声波在室内传播时，要被墙壁、天花板、地板等障碍物形成反射声，并和直达声形成叠加，这种现象称为混响。

但是，混响现象对于识别就没有什么好处了。由于混响则会使得不同步的语音相互叠加，带来了音素的交叠[掩蔽效应](https://www.baidu.com/s?wd=%E6%8E%A9%E8%94%BD%E6%95%88%E5%BA%94&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)（Phoneme Overlap Effect），从而严重影响语音识别效果。

影响语音识别的部分一般是晚期混响部分，所以去混响的主要工作重点是放在如何去除晚期混响上面，多年来，去混响技术抑制是业界研究的热点和难点。利用麦克风阵列去混响的主要方法有以下几种：

(1)基于盲语音增强的方法（Blind signal enhancement approach），即将混响信号作为普通的加性噪声信号，在这个上面应用语音增强算法。

(2)基于波束形成的方法（Beamforming based approach），通过将多麦克风对收集的信号进行加权相加，在目标信号的方向形成一个拾音波束，同时衰减来自其他方向的反射声。

(3)基于逆滤波的方法（An inverse filtering approach），通过麦克风阵列估计房间的房间冲击响应（Room Impulse Response, RIR），设计重构[滤波器](https://www.baidu.com/s?wd=%E6%BB%A4%E6%B3%A2%E5%99%A8&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)来补偿来消除混响。

## 声源信号提取

家里人说话太多，DingDong听谁的呢。这个时候就需要DingDong聪明的辨别出哪个声音才是指令。而麦克风阵列可以实现声源信号提取，声源信号的提取就是从多个声音信号中提取出目标信号，声源信号分离技术则是将需要将多个混合声音全部提取出来。

利用麦克风阵列做信号的提取和分离主要有以下几种方式：

(1)基于波束形成的方法，即通过向不同方向的声源分别形成拾音波束，并且抑制其他方向的声音，来进行语音提取或分离；

(2)基于传统的盲源信号分离（Blind Source Separation）的方法进行，主要包括主成分分析（Principal  Component Analysis，PCA）和基于独立成分分析（Independent Component Analysis，ICA）的方法。

## 回声抵消

严格来说，这里不应该叫回声，应该叫“自噪声”。回声是混响的延伸概念，这两者的区别就是回声的时延更长。一般来说，超过100毫秒时延的混响，人类能够明显区分出，似乎一个声音同时出现了两次，我们就叫做回声，比如天坛著名的回声壁。

实际上，这里所指的是语音交互设备自己发出的声音，比如Echo音箱，当播放歌曲的时候若叫Alexa，这时候麦克风阵列实际上采集了正在播放的音乐和用户所叫的Alexa声音，显然语音识别无法识别这两类声音。回声抵消就是要去掉其中的音乐信息而只保留用户的人声，之所以叫回声抵消，只是延续大家的习惯而已，其实是不恰当的。

## 参考

<https://zhuanlan.zhihu.com/p/27977550>

极限元：智能语音前端处理中的几个关键问题

<https://zhuanlan.zhihu.com/p/24139910>

远场语音交互中的麦克风阵列技术解读

<https://zhuanlan.zhihu.com/p/22512377>

自然的语音交互——麦克风阵列

# 语言模型

语言模型是针对某种语言建立的概率模型，目的是建立一个能够描述给定词序列在语言中的出现的概率的分布。

给定下边两句话：

定义机器人时代的大脑引擎，让生活更便捷、更有趣、更安全。

代时人机器定义引擎的大脑，生活让更便捷，有趣更，安更全。

语言模型会告诉你，第一句话的概率更高，更像一句”人话”。

语言模型技术广泛应用于语音识别、OCR、机器翻译、输入法等产品上。语言模型建模过程中，包括词典、语料、模型选择，对产品的性能有至关重要的影响。Ngram模型是最常用的建模技术，采用了马尔科夫假设，目前广泛地应用于工业界。

> 语言模型属于NLP的范畴，这里不再赘述。

参考：

<https://zhuanlan.zhihu.com/p/23504402>

语言模型技术

# 声学模型

声学模型主要有两个问题，分别是特征向量序列的可变长和音频信号的丰富变化性。

**可变长特征向量序列**问题在学术上通常有动态时间规划（Dynamic Time Warping, DTW）和隐马尔科夫模型（Hidden Markov Model, HMM）方法来解决。

**音频信号的丰富变化性**是由说话人的各种复杂特性或者说话风格与语速、环境噪声、信道干扰、方言差异等因素引起的。声学模型需要足够的鲁棒性来处理以上的情况。

在过去，主流的语音识别系统通常使用梅尔倒谱系数（Mel-Frequency Cepstral Coefficient,  MFCC）或者线性感知预测（Perceptual Linear Prediction,  PLP）作为特征，使用混合高斯模型-隐马尔科夫模型（GMM-HMM）作为声学模型。

在近些年，区分性模型，比如深度神经网络（Deep Neural Network,  DNN）在对声学特征建模上表现出更好的效果。基于深度神经网络的声学模型，比如上下文相关的深度神经网络-隐马尔科夫模型（CD-DNN-HMM）在语音识别领域已经大幅度超越了过去的GMM-HMM模型。

参考：

<https://zhuanlan.zhihu.com/p/23567981>

声学模型

# 解码器技术

解码器模块主要完成的工作包括：给定输入特征序列*x**T*1

的情况下，在由声学模型、声学上下文、发音词典和语言模型等四种知识源组成的搜索空间（Search Space）中，通过维特比（Viterbi）搜索，寻找最佳词串$[w_1^N]^{opt}=[w_1,\dots,w_N]_{opt}$，使得满足：

$[w_1^N]^{opt}=\mathop{\arg\max}_{w_1^N,N}p(w_1^N\mid x_1^T)$

在解码过程中，各种解码器的具体实现可以是不同的。按搜索空间的构成方式来分，有动态编译和静态编译两种方式。

**静态编译**，是把所有知识源统一编译在一个状态网络中，在解码过程中，根据节点间的转移权重获得概率信息。由AT&T提出的Weighted Finite State Transducer（WFST）方法是一种有效编译搜索空间并消除冗余信息的方法。

**动态编译**，预先将发音词典编译成状态网络构成搜索空间，其他知识源在解码过程中根据活跃路径上携带的历史信息动态集成。

参考：

<https://zhuanlan.zhihu.com/p/23648888>

语音识别之解码器技术简介

# 人类声音

成年男性：80-140 Hz

成年女性：130-220 Hz

儿童：180-320 Hz

从信号处理的角度，人类声音的处理方式和普通的雷达信号处理并无本质差异，主要的区别在于：**雷达信号经过了载波调制，而人类声音则没有这个步骤。**

参考：

<https://wenku.baidu.com/view/6123ba2f0066f5335a8121fe.html>

人声频率范围及各频段音色效果

# 建模单元

建模单元是指声音建模的最小单元。从细到粗，一般有**state、phoneme、character**三级。

描述一种语言的基本单位被称为音素phoneme，例如BRYAN这个词就可以看做是由B, R, AY, AX, N五个音素构成的。这种模式也叫做单音素monophone模式。

然而语音没有图像识别那么简单，因为我们再说话的时候很多发音都是连在一起的，很难区分，所以一般用左中右三个HMM  state来描述一个音素，也就是说BRYAN这个词中的R音素就变成了用B-R, R, R-AY三个HMM  state来表示。这种模式又被称作三音素triphone模式。

character显然是个最粗的划分，尽管英语是表音文字，然而一个字母有多个发音，仍然是个普遍现象。

在GMM-HMM时代，人们倾向于细粒度建模，因为模型越细，效果越好。但DL时代，人们更倾向于粗粒度建模，因为这样做，可以加快语音识别的解码速度，从而可以使用更深、更复杂的神经网络建模声学模型。







# DTW

Dynamic Time Warping是Vintsiuk于1968年提出的算法。

> Taras Klymovych Vintsiuk，1939～2012，乌克兰科学家，毕业于Kyiv Polytechnic Institute。模式识别专家，语音识别领域的奠基人之一。

![这里写图片描述](https://img-blog.csdn.net/20180903103210159?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**图1**

如上图所示，因为语音信号具有相当大的随机性，即使同一个人在不同时刻发同一个音，也不可能具有完全的时间长度。而且同一个单词内的不同音素的发音速度也不同，比如有的人会把“A”这个音拖得很长，或者把“i”发的很短。在这些复杂情况下，使用传统的欧几里得距离，无法有效地求得两个时间序列之间的距离（或者相似性）。

回到上面的图。如果我们将两个序列中相关联的点，用上图中的虚线连接的话，就会发现这两个序列实际上是很相似的。

那么如何用数学的方式描述上述DTW算法的思想呢？

假设现在有一个标准的参考模板R，是一个M维的向量，即*R*={*R*(1),*R*(2)，⋯，*R*(*M*)}，每个分量可以是一个数或者是一个更小的向量。现在有一个才测试的模板T，是一个N维向量，即*T*={*T*(1),*T*(2)，⋯，*T*(*N*)}

同样每个分量可以是一个数或者是一个更小的向量，注意M不一定等于N，但是每个分量的维数应该相同。

然后，将两个序列二维展开得到下图：

![这里写图片描述](https://img-blog.csdn.net/20180903103128715?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

这样，两个序列中点与点之间的关联关系，就可以用这个二维矩阵W来表述。比如，可以用W(i,j)表示第1个序列中的第i个点和第2个序列中的第j个点相对应。所有这样的W(i,j)最终构成了上图中的曲线。这条曲线也被称作归整路径（Warp Path）。

显然，这个归整路径不是随意选择的，它需要满足以下几个约束：

1）**边界条件**：$w_1=(1,1)$和$w_k=(m,n)$。任何一种语音的发音快慢都有可能变化，但是其各部分的先后次序不可能改变，因此所选的路径必定是从左下角出发，在右上角结束。

2）**连续性**：如果$w_{k-1}= (a’, b’)$，那么对于路径的下一个点$w_k=(a, b)$需要满足(*a*−*a*′)≤1和(*b*−*b*′)≤1。也就是不可能跨过某个点去匹配，只能和自己相邻的点对齐。这样可以保证R和T中的每个坐标都在W中出现。

3）**单调性**：如果$w_{k-1}= (a’, b’)$，那么对于路径的下一个点$w_k=(a, b)$需要满足0≤(*a*−*a*′)和0≤(*b*−*b*′)。这限制W上面的点必须是随着时间单调进行的。以保证图1中的虚线不会相交。

结合连续性和单调性约束，每一个格点的路径就只有三个方向了。例如如果路径已经通过了格点(*i*,*j*)

，那么下一个通过的格点只可能是下列三种情况之一：(*i*+1,*j*)，(*i*,*j*+1)或者(*i*+1,*j*+1)。

归整路径实际上就是满足上述约束的所有路径中，cumulative distances最小的那条路径，即：
$$
D(i,j)=Dist(i,j)+\min(D(i-1,j),D(i,j-1),D(i-1,j-1)), D(1,1) = 0
$$
这里的距离可以使用欧氏距离，也可以使用马氏距离。

DTW实例的具体计算过程可参见：

<http://www.cnblogs.com/tornadomeet/archive/2012/03/23/2413363.html>

从一个实例中学习DTW算法

从中可以看出，DTW实际上是一个动态规划问题。

更一般的，DTW也可用于计算两个离散的序列(不一定要与时间有关)的相似度。和《机器学习（二十二）》的EMD距离相比，DTW距离能够**保持序列的形状信息**。

除此之外，我们还可以增加别的约束：

**全局路径窗口(Warping Window)**：$\mid \phi_x(s)-\phi_y(s)\mid \leq r$。比较好的匹配路径往往在对角线附近，所以我们可以只考虑在对角线附近的一个区域寻找合适路径(r就是这个区域的宽度);

**斜率约束(Slope Constrain)**：$\dfrac{\phi_x(m)-\phi_x(n)}{\phi_y(m)-\phi_y(n)}\leq p$和$\dfrac{\phi_y(m)-\phi_y(n)}{\phi_x(m)-\phi_x(n)}\leq q$，这个可以看做是局部的Warping Window，用于避免路径太过平缓或陡峭，导致短的序列匹配到太长的序列或者太长的序列匹配到太短的序列。

![这里写图片描述](https://img-blog.csdn.net/20180903102921705?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图是两种常见的约束搜索空间的方法。

DTW的缺点：

1.运算量大；

2.识别性能过分依赖于端点检测；

3.太依赖于说话人的原来发音；

4.不能对样本作动态训练；

5.没有充分利用语音信号的时序动态特性；

DTW适合于特定人基元较小的场合，多用于孤立词识别；

参考：

<http://blog.csdn.net/zouxy09/article/details/9140207>

动态时间规整（DTW）

<https://blog.csdn.net/raym0ndkwan/article/details/45614813>

DTW动态时间规整

<http://www.cnblogs.com/luxiaoxun/archive/2013/05/09/3069036.html>

Dynamic Time Warping动态时间规整算法

<https://zhuanlan.zhihu.com/p/39450321>

时间序列的搜索

# Spectrogram

## Window function

Fourier  transform研究的是整个时间域和频率域的关系。但实际的信号处理过程，不可能对无限长的信号进行测量和运算，而是取其有限的时间片段进行分析。做法是从信号中截取一个时间片段，然后用截取的信号时间片段进行周期延拓处理，得到虚拟的无限长的信号，然后就可以对信号进行FT、相关分析等数学处理。

无限长的信号被截断以后，其频谱发生了畸变，原来集中在f(0)处的能量被分散到两个较宽的频带中去了（这种现象称之为频谱能量泄漏）。

为了减少频谱能量泄漏，可采用不同的截取函数对信号进行截断，这些截断函数称为Window function。

常用的Window function有：Hann window、Rectangular window、Triangular window、Hamming window、Gaussian window等。

不同的窗函数对信号频谱的影响是不一样的。例如，Rectangular window主瓣窄，旁瓣大，频率识别精度最高，幅值识别精度最低；Blackman window主瓣宽，旁瓣小，频率识别精度最低，但幅值识别精度最高。

对Window function更详细的叙述参见：

<https://en.wikipedia.org/wiki/Window_function>

## Hann window

Hann window虽然是以Julius Ferdinand von  Hann的名字命名，但却是Blackman和Tukey的作品。他们和同一实验室的Claude E. Shannon, Hendrik Wade  Bode，合称为Information Age的四大先锋。

> Julius Ferdinand von Hann，1839～1921，奥地利气象学家。现代气象学之父。
>
> Ralph Beebe Blackman，1904～1990，美国数学家。长期供职于AT&T Bell Laboratories。二战时，参与了防空火炮控制系统的平滑研究。
>
> John Wilder Tukey，1915～2000，美国数学家。Princeton University博士，长期供职于AT&T Bell Laboratories。英国皇家学会会员。Cooley–Tukey FFT算法发明者。

$$
w(n) = \sum_{k = 0}^{K} (-1)^k a_k\; \cos\left( \frac{2 \pi k n}{N-1} \right),\quad 0\le n \le N-1
$$

上式是Cosine-sum windows的计算公式，令K=1，则：
$$
w(n) = a_0 - \underbrace{(1-a_0)}_{a_1}\cdot \cos\left( \tfrac{2 \pi n}{N - 1} \right),\quad 0\le n \le N-1这类Window function有好几个特例：
$$
这类Window function有好几个特例：

Hann window：
$$
w(n) = 0.5\; \left[1 - \cos \left ( \frac{2 \pi n}{N-1} \right) \right] = \sin^2 \left ( \frac{\pi n}{N-1} \right)
$$
Hamming window：
$$
w(n) = 0.54 - 0.46\cdot \cos\left( \tfrac{2 \pi n}{N - 1}\right)
$$

> Richard Wesley Hamming，1915～1998，美国数学家。University of  Chicago本科（1937）+University of  Nebraska硕士（1939）+UIUC博士（1942）。参与曼哈顿计划，后长期供职于Bell  Lab。通信和计算机工程领域的宗师级人物，美国工程院院士，图灵奖得主（1968）。Hamming code 、Hamming  distance等都是他的贡献。

## STFT

$$
\mathbf{STFT}\{x(t)\}(\tau,\omega) \equiv X(\tau, \omega) = \int_{-\infty}^{\infty} x(t) w(t-\tau) e^{-j \omega t} \, dt
$$

上式是STFT（Short-time Fourier transform）的定义。和FT相比，STFT将FT中的被积函数*x*(*t*)，换成了

*x*(*t*)*w*(*t*−*τ*)。其中，w(t)是窗函数（Window function），因此STFT又叫做加窗傅立叶变换。

## Spectrogram

DTW是一种时域方法，作为信号处理自然少不了频域方法。这里我们先来了解一个叫声谱图的东西。

![这里写图片描述](https://img-blog.csdn.net/20180903102757638?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

这段语音被分为很多帧，每帧语音都对应于一个频谱（通过短时FFT计算），频谱表示频率与能量的关系。在实际使用中，频谱图有三种，即线性振幅谱、对数振幅谱、自功率谱（对数振幅谱中各谱线的振幅都作了对数计算，所以其纵坐标的单位是dB（分贝）。这个变换的目的是使那些振幅较低的成分相对高振幅成分得以拉高，以便观察掩盖在低幅噪声中的周期信号）。

![这里写图片描述](https://img-blog.csdn.net/20180903102731841?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

我们先将其中一帧语音的频谱通过坐标表示出来。

![这里写图片描述](https://img-blog.csdn.net/20180903102708157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

再将左边的频谱旋转90度。

![这里写图片描述](https://img-blog.csdn.net/2018090310265046?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

然后把这些幅度映射到一个灰度级表示的直方图。0表示白色，255表示黑色。幅度值越大，相应的区域越黑。

![这里写图片描述](https://img-blog.csdn.net/20180903102559532?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

这样我们会得到一个随着时间变化的频谱图，这个就是描述语音信号的spectrogram声谱图。

# Cepstrum Analysis

![这里写图片描述](https://img-blog.csdn.net/20180903102541469?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图是一个语音的频谱图。峰值就表示语音的主要频率成分，我们把这些峰值称为共振峰（formants），而共振峰就是携带了声音的辨识属性（就是个人身份证一样）。所以它特别重要。用它就可以识别不同的声音。

![这里写图片描述](https://img-blog.csdn.net/20180903102526514?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

既然它那么重要，那我们就是需要把它提取出来！我们要提取的不仅仅是共振峰的位置，还得提取它们转变的过程。所以我们提取的是频谱的包络（Spectral Envelope）。这包络就是一条连接这些共振峰点的平滑曲线。

原始的频谱由两部分组成：包络和频谱的细节。这里用到的是对数频谱，所以单位是dB。

![这里写图片描述](https://img-blog.csdn.net/20180903102436630?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

怎么把他们分离开呢？也就是，怎么在给定log*X*[*k*]的基础上，求得log*H*[*k*]和log*E*[*k*]以满足log*X*[*k*]=log*H*[*k*]+log*E*[*k*]

呢？

![这里写图片描述](https://img-blog.csdn.net/20180903102418356?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

为了达到这个目标，我们需要Play a Mathematical Trick。这个Trick是什么呢？就是对频谱做FFT。



这里，我们对Fourier transform做一个简单的回顾。

设h(t)是一个时域函数，而H(f)是一个频域函数，则Fourier transform为：
$$
H(f)=\int_{-\infty}^\infty h(t)e^{2\pi i ft}\mathrm{d}t
$$
inverse Fourier transformation为：
$$
h(t)=\int_{-\infty}^\infty H(f)e^{-2\pi i ft}\mathrm{d}f
$$
因此，对频谱做FT，也被叫做inverse FT，简称IFT。

> 从上式还可以看出，FT和IFT的公式非常类似，因此从编程角度，一个FT函数既可以做FT，也可以稍作修改后，做IFT运算。因此在不强调目的性的情况下，IFT也可以直接称为FT。比如，MFCC特征最后的IDFT变换，实际上是DCT变换。

传统的IFFT的结果是一个时域函数，然而这里是对log frequency domain做IFFT，因此，它的值域只能被称作pseudo-frequency domain。

从上图可以看出，**Spectral Envelope主要是低频成分，而Spectral details主要是高频成分。**

![这里写图片描述](https://img-blog.csdn.net/20180910094224692?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

显然，如果把Spectral Envelope和Spectral details叠加起来就是原来的频谱信号了。

换句话说，我们知道了log*X*[*k*]，就可以求出*x*[*k*]，经过低通滤波就可以得到*h*[*k*]。

这里的*x*[*k*]被称作倒谱Cepstrum（这个是一个新造出来的词，把spectrum的前面四个字母顺序倒过来就是倒谱的单词了）。

而我们所关心的*h*[*k*]就是倒谱的低频部分，它在语音识别中被广泛用于描述特征。

# Mel-Frequency Analysis

## Mel scale

Mel scale是Stevens、Volkmann和Newman于1937年发明的一种主观音阶标准。

> Stanley Smith Stevens，1906～1973，Harvard University心理学教授。
>
> John E. Volkmann，1905～1980，Radio Corporation of America研究员。
>
> Edwin B. Newman，1908~1989，Harvard University心理学教授。

声音作为一种波动，一般以Hz作为频率差异的客观标准，然而相同频率差的两组声音，在人耳听来，其频率差（也就是所谓的音阶）实际上是不同的。因此，Stevens等人采取实验的方法，确定了人耳的主观音阶标准。

该标准以Mel作为单位，规定1000Hz的声音所对应的音阶为1000Mel。

Mel scale从严格的定义上并没有一个简单的公式来表示。但一般采用如下公式进行转换：
$$
m = 2595 \log_{10}\left(1 + \frac{f}{700}\right)
$$
从中可以看出，**人耳对于高频声音的分辨率实际上是不如低频声音的**。

![这里写图片描述](https://img-blog.csdn.net/20180910094123111?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> Mel是melody的别称，有的blog上说Mel是个人，他发明了MFCC，这纯粹是胡说八道。

## MFCC

Mel-frequency Cepstral Coefficients是由Paul Mermelstein提出的一种音频特征。

> Paul G. Mermelstein，明尼苏达大学神经科学教授。

![这里写图片描述](https://img-blog.csdn.net/20180910094104984?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

由之前对Mel scale的介绍可知：人耳对于高频声音的分辨率实际上是不如低频声音的。

因此，我们可以使用一组Triangular window对声音进行滤波（如上图所示）。这里的Triangular window不是均匀分布的，而是低频部分更密集一些。

这些Triangular window被称作**Mel-Filters**。被Mel-Filters过滤之后的Spectrum，被称作**Mel-Spectrum**。

对Mel-Spectrum执行Cepstrum Analysis，就得到了Mel-Frequency Cepstral Coefficients，也就是MFCC。

![这里写图片描述](https://img-blog.csdn.net/20180910094044979?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图是MFCC的计算流程。

除了MFCC之外，delta MFCC和double-delta MFCC也是常用的特征。他们的计算过程如下所示：

![这里写图片描述](https://img-blog.csdn.net/20180910094612215?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FudGtpbGxlcmZhcm0=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

可见，delta MFCC和double-delta MFCC，实际上就是MFCC的一阶差分和二阶差分。

在实际中使用的语音特征，往往是各种特征的组合。比如，常用的**39维MFCC特征**，其组成如下：

12 MFCC feature

1 energy feature

12 delta MFCC features

12 double-delta MFCC features

1 delta energy feature

1 double-delta energy feature

## 计算能量谱

energy的计算比较简单，无论是如上图的时域能量统计，还是在DFT之后进行频域能量统计都是可以的。参见《数学狂想曲（一）》。

需要注意的是，频域能量包含了实部能量+虚部能量。

## Discrete Cosine Transform

离散傅立叶变换需要进行复数运算，尽管有FFT可以提高运算速度，但在图像编码、特别是在实时处理中非常不便。离散傅立叶变换在实际的图像通信系统中很少使用，但它具有理论的指导意义。

根据离散傅立叶变换的性质，实偶函数的傅立叶变换只含实的余弦项，因此构造了一种实数域的变换——离散余弦变换(DCT)。

通过研究发现，DCT除了具有一般的正交变换性质外，其变换阵的基向量很近似于Toeplitz矩阵的特征向量，后者体现了人类的语言、图像信号的相关特性。因此，在对语音、图像信号变换的确定的变换矩阵正交变换中，DCT变换被认为是一种准最佳变换。

相对应的还有IDCT。

DCT还有一个特点是，对于一般的语音信号，这一步的结果的前几个系数特别大，后面的系数比较小，可以忽略。比如Mel-Filters一般取40个三角形，所以DCT的结果也是40个点；实际中，一般仅保留前12~20个，这就进一步压缩了数据。

类似的，还有Discrete Sine Transform，它和DCT的区别在于：DST用于实奇对称数据，而DCT用于实偶对称数据。这里的对称指的是采样对称，而非物理数值上的对称。

除此之外，针对人耳的听觉特性，还有Constant-Q transform。它与STFT的公式基本相同，差别在于后者的filter的中心频点间隔均匀，而前者的间隔越往高频越稀疏：
$$
\delta f_k = 2^{1/n} \cdot \delta f_{k-1} = \left( 2^{1/n} \right)^k \cdot \delta f_\text{min}
$$
上式中的$f_k$

即为filter的中心频点。

## 参考

<http://blog.csdn.net/zouxy09/article/details/9156785>

梅尔频率倒谱系数（MFCC）

<https://my.oschina.net/jamesju/blog/193343>

语音特征参数MFCC提取过程详解

<https://liuyanfeier.github.io/2017/10/26/2017-10-27-Kaldi%E4%B9%8Bfbank%E5%92%8Cmfcc%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/>

kaldi之fbank和mfcc特征提取

<https://zhuanlan.zhihu.com/p/26680599>

语音信号预处理及特征参数提取

# FBank

Filter bank和MFCC的计算步骤基本一致，只是没有做IDFT而已。

FBank与MFCC对比：

1.计算量：MFCC是在FBank的基础上进行的，所以MFCC的计算量更大

2.特征区分度：FBank特征相关性较高（相邻滤波器组有重叠），MFCC具有更好的判别度，这也是在大多数语音识别论文中用的是MFCC，而不是FBank的原因

3.使用对角协方差矩阵的GMM由于忽略了不同特征维度的相关性，MFCC更适合用来做特征。

4.DNN/CNN可以更好的利用这些相关性，使用fbank特征可以更多地降低WER。

参考：

<http://blog.csdn.net/wxb1553725576/article/details/78048546>

Kaldi特征提取之-FBank

# Pitch Detection

<http://blog.csdn.net/zouxy09/article/details/9141875>

基音周期估计（Pitch Detection）

# Vector Quantization

<http://blog.csdn.net/zouxy09/article/details/9153255>

矢量量化（Vector Quantization）

# fMLLR

<https://blog.csdn.net/xmdxcsj/article/details/78512645>

声学特征变换fMLLR

# SGMM

<https://blog.csdn.net/quhediegooo/article/details/68946100>

子空间高斯混合模型-SGMM

# PLP

Perceptual Linear Prediction

《Perceptual Time Varying Linear Prediction Model for Speech Applications》

<https://www.isip.piconepress.com/courses/msstate/ece_8463/lectures/current/lecture_17/index.html>

SPECTRAL TRANSFORMATIONS

# VTLN

<https://blog.csdn.net/jiangyangbo/article/details/6535928>

VTLN(Vocal Tract Length Normalisation)

# HMM与语音识别

HMM的基本概念参见[《机器学习（二十二）》](https://www.baidu.com/s?wd=%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%89%E3%80%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)，这里谈一下HMM在语音识别领域的应用。

从概率的角度来说，语音识别的目标是寻找最可能的*P*(*W*∣*O*)。这里的W表示word，O表示observation。

直接找显然没这么容易，所以要用到Bayes公式：
$$
\frac{P(W)P(O\mid W)}{P(O)}
$$
这里只有*P*(*O*)

已知，剩下的两个参数都需要额外提供。其中HMM提供*P*(*O*∣*W*)，LM提供*P*(*W*)。

由于HMM的path上的概率是各个transition probability的乘积，而这些概率都小于1，因此他们的乘积必然是更小的数。这时可以考虑使用对数，不仅可将乘法变为加法，同时数值的范围也得到了改善。

# 语音识别的评价指标

语音识别的评价指标主要是Word Error Rate（WER）。

错误的情况包括三种：

1.Substitutions：错词。

2.Deletions：漏词。

3.Insertions：多词。
$$
WER=100\%\times \frac{Subs+Dels+Ins}{\text{word in correct sentence}}
$$
类似的还有CER/PER：Character/Phoneme Error Rate。

需要注意的是，评价WER时，需要在ASR output和Label之间进行对齐操作，而不是简单的从左往右匹配，否则将无法正确处理Deletions和Insertions的情形。

还有根据公式可知，WER是可以大于100%的。

参考：

<https://blog.csdn.net/quhediegooo/article/details/56834417>

语音识别评估标准-WER

# 声学模型进阶

## 语音质量

更高的采样率可以降低WER。一般来说，16KHz相比8KHz的WER要小10%左右。

## Voice Detection

长时间的silence会增加WER，因此我们需要判断当前是否在说话。

Voice Detection包括两个方面：

1.Beginning-Point Detection。也叫做Voice Activity Detection（VAD）。有些类似于唤醒检测，但并不局限于设备的开机时刻。

2.End-Point Detection。

参考：

<https://zhuanlan.zhihu.com/p/24432663>

Voice Active Detection(VAD)的过去时与现在时

<https://blog.csdn.net/wxb1553725576/article/details/78069089>

Kaldi特征提取之-VAD

<https://blog.csdn.net/shichaog/article/details/78257068>

VAD综述

## Feature normalization

有时候需要对Feature进行normalization。例如，对MFCC特征减去均值，可以有效提升在噪声环境下的识别率。

## Tri-phone Models

英语一般包含43个音素，因此Tri-phone共有433≈80*K*

种不同组合。

但是这些组合的概率是众寡悬殊的，有些组合很常见，而有些组合很罕见。因此我们需要合并相似的发音组合。这通常采用CART决策树来进行聚类。这样做还可以减少状态数量，提高计算效率。

## 发音词典

Pronunciation Dict用于将文本转换成对应的发音。比较常用的有CMU的发音词典，用于美国英语，包含了100K的单词。用法参见《LSTM Speech Recognition实战》。

然而，无论多大的词典都会有遇到Unknown Words的情况。一般可根据现有发音构建统计模型，来预测发音。这也是符合人们的认知规律的：人遇到一个陌生的新词，也会根据过往的经验，来预测词的发音。通常这样做，会有70%～85%的准确率。











