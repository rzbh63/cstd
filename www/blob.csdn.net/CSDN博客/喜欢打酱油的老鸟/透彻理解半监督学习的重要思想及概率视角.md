
# 透彻理解半监督学习的重要思想及概率视角 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月29日 08:45:41[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：178标签：[半监督学习																](https://so.csdn.net/so/search/s.do?q=半监督学习&t=blog)[先验参数																](https://so.csdn.net/so/search/s.do?q=先验参数&t=blog)[概率视角																](https://so.csdn.net/so/search/s.do?q=概率视角&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=先验参数&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=半监督学习&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)
[
																								](https://so.csdn.net/so/search/s.do?q=半监督学习&t=blog)


[https://www.toutiao.com/a6650994292544242179/](https://www.toutiao.com/a6650994292544242179/)
2019-01-27 12:55:07
> 半监督学习本质上，是从小标记集和大非标记集学习。
半监督学习（**Semi-supervised Learning**）：**训练集同时包含有标记样本数据和未标记样本数据**，**不需要人工干预**，让学习器不依赖外界交互、自动地利用未标记样本来提升学习性能，就是半监督学习。
# 半监督学习的重要性与必要性
在许多机器学习的实际应用中，很容易找到海量的无类标签的样例，但需要使用特殊设备或经过昂贵且用时非常长的实验过程进行人工标记才能得到有类标签的样本。
![透彻理解半监督学习的重要思想及概率视角](http://p1.pstatp.com/large/pgc-image/0904ea210b4f4c0f809029e438d66533)

> 现在有新闻，有一些地方兴起人工智能产业中的标注产业，有大量的数字标注工厂，号称“数字富士康”，这说明大量无标注的存在，特别是在人工智能新兴产业下，更是如此；另一方面，说明了科学利用无标签数据的必要性和重要性。
![透彻理解半监督学习的重要思想及概率视角](http://p1.pstatp.com/large/pgc-image/97ac6da1176d40b19124c174d941d81e)
因此，现实世界中**极少量的有类标签的样本和大量的无类标签的样例**。人们尝试将大量的无类标签的样例加入到有限的有类标签的样本中一起训练来进行学习，期望能对学习性能起到改进的作用，由此产生了半监督学习。半监督学习避免了数据和资源的浪费，同时解决了有监督学习的模型泛化能力不强和无监督冠心病的模型不精确等问题。
![透彻理解半监督学习的重要思想及概率视角](http://p3.pstatp.com/large/pgc-image/88070ab466654fe7ab5a0dab86f42816)
在不考虑无标签数据时，只有1和2两个有标签数据，此时的决策边界是图中的虚线，当我们将无标签数据考虑以后，两类样本所服从的分布发生改变，从而导致决策边界向右偏移，变成黑色实线。上述过程的直观理解就是随着我们能够拿到的样本集的增多，我们对于正负两类样本的信息掌握更加充分，从而使我们做出更好的决策。
# 半监督学习的重要思想
# 无标签是现实世界的本来的存在和普遍的存在
在现实世界中，对于事物的认识是一个逐渐迭代的过程，是从无到有，从陌生到熟悉，从熟悉到精确的过程。有监督学习中的标签也是人类在自然实践与社会实践中，逐步修正已有认识，并且运用归纳和演绎推理，得出的，并且仍然在完善的过程中。比如对于猫和狗的分类，对于猫又有很多不同的各类，如波斯猫、挪威森林猫、中国狸花猫,等等，很多，很多。
> 所以半监督学习就是这样一个动态过程的一个很好的模拟。

# 为什么半监督能行？模型假设符合相似性聚簇原理
半监督学习为什么可行？这个就要了解一下半监督学习中的理论前提，即模型假设，当模型假设正确时，无标签的样例能够帮助改进学习性能。而理论前提，就是我们之前在提到过的相似性原理，相似性聚簇，**在现实世界中普遍存在，在晚上，在中国大片土地上用灯光的聚簇就可以定位大城市、特大城市。**
平滑假设（Smoothness Assumption）
位于稠密数据区域的两个距离很近的样例的类标签相似。或者说，当两个样例被稠密数据区域中的边连接时，它们有相同的类标签概率要大；相反地，当两个样例被稀疏数据区域分开时，它们的类标签趋于不同．
聚类假设（Cluster Assumption）
当两个样例位于同一聚类簇时，它们有相同类标签的概率很大。这个假设的等价定义为低密度分离假设（Low Sensity Separation Assumption），即**分类决策边界应该穿过稀疏数据区域，而避免将稠密数据区域的样例分到决策边界两侧。**
流形假设（Manifold Assumption）
将高维数据嵌入到低维流形中，当两个样例位于低维流形中的一个小局部邻域内时，它们具有相似的类标签。
许多实验研究表明当不满足这些假设或模型假设不正确时，无类标签的样例不仅不能对学习性能起到改进作用，反而会恶化学习性能，导致半监督学习的性能下降。
# 实现半监督算法的概率视角
**半监督生成模型是以概率视角求解半监督算法的。**该方法假设所有数据（无论是否有标记）都是由同一潜在的**模型生成的，分布类型相同**。通过这个假设潜在模型的参数将未标记数据与学习目标联系起来，而未标记数据的标记可作为模型的**缺失参数**，通常基于EM算法进行极大似然估计求解。生成式方法的重点是对于生成式模型的假设，不同的模型假设将产生不同的方法。
![透彻理解半监督学习的重要思想及概率视角](http://p1.pstatp.com/large/pgc-image/e4ebcec20ec14c1bb782060cc9a9b953)

> 这个方法的关键也就是这个模型假设必须准确，即假设的生成式模型必须与真实数据分布吻合；否则利用未标记数据反倒会降低泛化性能。 这个不好控制，也是好多学习机器学习的人比较讨厌的，但这是错误的！模型的构建过程允许试错，也就是当结果不是很理想，可以反思先验是否准确，继而，修正先验，继续迭代模型。

# 监督生成模型
在监督学习中，二分类中生成模型的数据有C1和C2两类数据组成，寻找最可能的先验概率P（Ci）和类依赖概率P（x | Ci），假设每一类的数据都是服从高斯分布，我们可以通过分布得到参数均值μ和和方差Σ。P（x | Ci）是由μ_和Σ参数化的高斯分布。如下图所示，我们要求得一个决策边界。
![透彻理解半监督学习的重要思想及概率视角](http://p3.pstatp.com/large/pgc-image/a8f88cd15ca742adafd613c0534df5e6)
利用参数可以知道**P(C1)**和**P(C2)**、**μ1、μ2、Σ**。并利用这些参数计算某一个例子属于某一类别的概率 。
![透彻理解半监督学习的重要思想及概率视角](http://p1.pstatp.com/large/pgc-image/6fda1b5391924db4986d0536032a942e)

# 半监督生成模型
过程和监督生成模型类似，所不同的是，如下图所示，标记为绿色的点，是无标记的数据，让分布更加**丰满，由椭圆到圆**。
![透彻理解半监督学习的重要思想及概率视角](http://p9.pstatp.com/large/pgc-image/bc113b83bd154d6ab81df304a7d3339d)
这个时候如果仍在使用之前的数据分布明显是不合理的，需要重新估计数据分布的参数，这个时候可能分布式一个类似于圆形的形状。这里就需要用未标签数据来帮助估计新的
**（_1），（_2），μ1，μ2，Σ。**
> 由贝叶斯定理的思想，我们知道，未标记的数据_对于这些参数的重新估计趋向更加精确发挥着重要的作用。

# EM算法
具体可以采用如下的EM算法进行估计
EM算法是一种流行的迭代算法，用于在丢失数据的问题中进行最大似然估计。EM算法包括两个步骤，**1、期望步骤，即填写缺失的数据；2、最大化步骤 - 计算参数的新的最大后验估计。**
初始化：θ={ P(C_1 ),P(C_2 ), μ1, μ2, Σ }
步骤一：计算无标记数据的后验概率：P_θ(C_1 |x_u )
步骤二：更新模型：
![透彻理解半监督学习的重要思想及概率视角](http://p1.pstatp.com/large/pgc-image/de6f8387ce124f6594a33097243f1379)
![透彻理解半监督学习的重要思想及概率视角](http://p9.pstatp.com/large/pgc-image/fc65b8e3481d493d8774549a95cec103)
首先对参数进行初始化，之后利用参数计算无标签数据的后验概率；然后利用得到的后验概率更新模型参数，再返回step1，循环执行直至模型收敛。这个算法最终会达到收敛，但是**先验**（初始化）对于结果的影响也很大。
# 为什么先验对结果的影响大？
![透彻理解半监督学习的重要思想及概率视角](http://p3.pstatp.com/large/pgc-image/f8fd4ec995a64757873bae2d4dc3f1ff)
标记数据的最大可能性：
![透彻理解半监督学习的重要思想及概率视角](http://p1.pstatp.com/large/pgc-image/d97029faeb5a4bfba0331d5236a906ce)
已标记+未标记数据的最大可能性：
![透彻理解半监督学习的重要思想及概率视角](http://p1.pstatp.com/large/pgc-image/8fdb7aa0fff04a2aa44110ee033fdd32)
X_u可以来自C1、C2，迭代求解：
![透彻理解半监督学习的重要思想及概率视角](http://p3.pstatp.com/large/pgc-image/8dc4a6f8bace4f3f9e5a08218f75509c)
由这个最后的概率表达式，可见，**先验参数**是非常重要的。

