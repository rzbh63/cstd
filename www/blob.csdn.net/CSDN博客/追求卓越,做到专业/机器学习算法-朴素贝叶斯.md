
# 机器学习算法 - 朴素贝叶斯 - 追求卓越,做到专业 - CSDN博客


2019年03月13日 00:36:46[Waldenz](https://me.csdn.net/enter89)阅读数：18


贝叶斯定理是关于[随机](https://baike.baidu.com/item/%E9%9A%8F%E6%9C%BA)事件A和B的[条件概率](https://baike.baidu.com/item/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87/4475278)（或[边缘概率](https://baike.baidu.com/item/%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87/2572198)）的一则定理。
![](https://img-blog.csdnimg.cn/20190312234859988.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2VudGVyODk=,size_16,color_FFFFFF,t_70)
其中P(A|B)是在 B 发生的情况下 A 发生的可能性。
```python
P(A)是 A 的先验概率，之所以称为“先验”是因为它不考虑任何 B 方面的因素。
P(A|B)是已知 B 发生后 A 的条件概率，也由于得自 B 的取值而被称作 A 的后验概率。
P(B|A)是已知 A 发生后 B 的条件概率，也由于得自 A 的取值而被称作 B 的后验概率。
P(B)是 B 的先验概率，也作标淮化常量（normalizing constant）。
```
按这些术语，贝叶斯定理可表述为：
后验概率 = (相似度 * 先验概率)/标淮化常量
# 朴素贝叶斯
朴素就是假设各个特征之间相互独立。
换成分类任务的表达式即为：
![](https://img-blog.csdnimg.cn/2019031300280671.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2VudGVyODk=,size_16,color_FFFFFF,t_70)
零概率问题：
防止 P(A)为0，将这一项初始化为1，为了保证概率相等，分母对应加k，k即为k分类，术语叫做拉普拉斯平滑处理，laplace smoothing，分母加k使之满足全概率公式。
优点：
对小规模的数据表现好，适合多分类任务，适合增量训练式训练;
算法逻辑简单，容易实现;
分类过程中时空开销小（假设特征相互独立，只会涉及到二维存储);
缺点：
对输入数据的表达形式很敏感；(离散的，连续的)
实际过程中，属性之间往往不是相互独立的，属性之间相关性越大，分类误差也就越大。
常见应用：
垃圾邮件检测、文章分类、情感分类、人脸识别等
## 朴素贝叶斯的三种模型
**多项式模型**：
特征是离散的。适用于文本分类，单词的次数，如文本情感判断，垃圾邮件分类等；
**伯努利模型**：
与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).
**高斯模型**：
当特征是连续变量时，用多项式模型会导致很多P(xi|yk)=0（不做平滑的情况下）,即使做平滑，得到的条件概率也难以描述真实情况；所以处理连续的特征变量，应该采用高斯模型；它假设每一维特征都服从高斯分布(正态分布)。
如训练数据为男女的身高、体重、脚掌长度等，对已知的身高、体重、脚掌预测性别；在身高、体重、脚掌上都服从高斯分布；
三种模型的代码实现参考：[https://blog.csdn.net/u012162613/article/details/48323777](https://blog.csdn.net/u012162613/article/details/48323777)

## 多项式模型和伯努利模型在文本分类中的应用
在多项式模型中：
> 在多项式模型中， 设某文档d=(t1,t2,…,tk)，tk是该文档中出现过的单词，允许重复，则

> 先验概率P(c)= 类c下单词总数/整个训练样本的单词总数

> 类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)

> V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），|V|则表示训练样本包含多少种单词。 P(tk|c)可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。
在伯努利模型中：
> P(c)= 类c下文件总数/整个训练样本的文件总数

> P(tk|c)=(类c下包含单词tk的文件数+1)/(类c下单词总数+2)




