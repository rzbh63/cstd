# 人工机器：机器学习的哲学原理、基础及完备性的来由

置顶 2018年03月11日 14:37:44 [wishchin](https://me.csdn.net/wishchin) 阅读数：707



​        观测->假设->归纳->演绎->过拟合，这是ML的一般套路和基础指导准则。

**导言**

​        对于人工智能，有诸多定义，也有诸多质疑。各家的定义不用多追究。从各个领域提出对机器学习的理解，同时也表示出对定义的狭隘理解。从[《计算机科学的离散结构》](https://www.baidu.com/s?wd=%E3%80%8A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E7%9A%84%E7%A6%BB%E6%95%A3%E7%BB%93%E6%9E%84%E3%80%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)、到《pattern recognition》、到[《人工智能：一种现代方法》](https://www.baidu.com/s?wd=%E3%80%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%9A%E4%B8%80%E7%A7%8D%E7%8E%B0%E4%BB%A3%E6%96%B9%E6%B3%95%E3%80%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)，以及[古老的1963年](http://blog.sina.com.cn/s/blog_4e7b72a30100sgho.html)图灵先生的[《论数字计算在决断难题中的应用》](https://www.baidu.com/s?wd=%E3%80%8A%E8%AE%BA%E6%95%B0%E5%AD%97%E8%AE%A1%E7%AE%97%E5%9C%A8%E5%86%B3%E6%96%AD%E9%9A%BE%E9%A2%98%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E3%80%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)图灵机模型，都给出了人工智能的诸多定义。

​        百科上这样阐述：

​        学习是人类具有的一种重要智能行为，但究竟什么是学习，长期以来却众说纷纭。社会学家、逻辑学家和心理学家都各有其不同的看法。比如，Langley（1996) 定义的机器学习是“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在[经验](https://baike.baidu.com/item/%E7%BB%8F%E9%AA%8C)学习中改善具体算法的性能”。（Machine learning is a science of the artificial. The field's main objects of study are artifacts, specifically algorithms that improve their performance with experience.'）

​        Tom Mitchell的机器学习(1997)对[信息论](https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E8%AE%BA)中的一些概念有详细的解释,其中定义机器学习时提到，“机器学习是对能通过[经验](https://baike.baidu.com/item/%E7%BB%8F%E9%AA%8C)自动改进的计算机算法的研究”。（Machine Learning is the study of computer algorithms that improve automatically through experience.）

​         Alpaydin（2004）同时提出自己对机器学习的定义，“机器学习是用数据或以往的[经验](https://baike.baidu.com/item/%E7%BB%8F%E9%AA%8C)，以此优化计算机程序的性能标准。”（Machine learning is programming computers to optimize a performance criterion using example data or past experience.）。
​         机器学习和专家系统不可缺少的基础概念有粒度、规则、拟合、假设与事实、以及完备性。

​         

**其他条目**

​        **观测系统**：一个系统使用自身概念对另一个系统进行描述的过程，称为观测。主语为观测系统，宾语为环境。对环境进行描述要求必须对环境进行结构定性描述和定量描述，其中定“性”为维度、也称为基，"量"为尺度。

​       **自然观测系统**：一个广泛存在的观测系统为人类的认知系统，而一个标志性的环境为自然环境，其中人类观测系统对自然环境的观测描述随人类本身不断进化。近代物理学为观测自然环境而使用自身概念拆解出7个基本基，七个基本物理量(长度m，时间s，质量kg，热力学温度K(开)，电流单位A（安），[光强度](https://baike.baidu.com/item/%E5%85%89%E5%BC%BA%E5%BA%A6)cd(坎德拉)，[物质的量](https://baike.baidu.com/item/%E7%89%A9%E8%B4%A8%E7%9A%84%E9%87%8F)mol(摩尔)）。

​       在已知常用物理学中，这七个物理量是正交的。最近一百年，物理哲学的相对论证明了时间描述基t和空间描述基l是不正交的，这个牵涉到机器复杂的物理学和数学证明以及大量的实验，不便多言。

​       时间只是空间的一个多胞胎，或者说时间即是空间与质量，时间在粒子之上的尺度上才有意义，且对于基于时间的系统而言，所有一切都是不可逆的，不存在跃迁和循环。

​       **接口**：观测系统用以解释环境的系统内通用规则称为接口。接口是环境概念到观测系统内部概念的转换基底。

​      **本质和表象**：表象为所见即所得，是可（视觉）观测系统映射到（视觉）系统，形成的内在描述，其基础方法是穷举和遍历。本质为环境结构，系统描述为概念逻辑推演，是意识系统对可观测系统的概念映射，要求合理性完备性，其基础方法是归纳和演绎以及泛化。

​       **人工智能**：广义的人工智能包含范围较广，同时定义也较为明确，泛指来自于人类知识并由人类构建的非自然实体智能。实体性：AI是有实体的，程序实体也是实体，物质实体也是实体。人工/非自然属性：实体具有非自然属性，不能是来自于自然界的培育和成长，等价于人工构造性，即表示创造性。途径：可学习而不可克隆。

​       所以，程序生命是人工智能，而人工合成生命也是人工智能，而克隆生命不是人工智能。人可以模仿自然生命方法重构生命，是为人工智能。

​       因此从范畴级别说，机械主义和符号主义（逻辑主义）具有等价的地位，甚至作为两种途径的一种哲学方法的链接主义，也可以作为人工智能的一种方法，只不过是黑箱的。

​       人工智能其中重要的一环为识别和决策，即模式识别。模式识别：[模式识别的两种方法](http://blog.csdn.net/wishchin/article/details/22654357)—知识和数据。其中基于粒状知识的模式识别系统为专家系统，基于数据的方法为机器学习。

​       **感知**：意识对环境的反馈叫做感知，以用于生存环境的改进。感知到的信息被意识系统描述被称为知识。理论上，感知环境的所有信息需要遍历。但是自然界自有其规律，为自然规律，这就是模式的事实空间。

​       而信息转化为知识的过程人默认使用了归纳演绎。归纳方法抽取环境样本的一些信息，归结为属性，默认使用的规律，组成了假设空间...

​        **归纳法**：从表象函数域到可见定义的规律总结过程，未归纳法，在数学领域，叫函数拟合。归纳和演绎以及命题和谓词逻辑构成了智能的完备基。

​      **基本方法：**《计算机科学的离散结构》《泛函分析》《拓扑空间引论》《最优化理论》。**经典综述**《AI：A Modern Approach 》。系统论断《专家系统》《系统论》《现代自动控制理论》。

​    

**第一节 基础概念**

​     

​       **AI基本原理**：有多少人工就有多少智能。当然这是在人工智能可以主动学习之前发生的事情，即所谓的强人工智能还未出现的时候，而且这个阶段预测还要持续很久。

​       **机器学习**：使用人类构建的非自然程序体，重现人类期待的知识和智能行为，称为机器学习。

​       **专家系统**：计算机和人工智能定义的专家系统明确了定义了 字母表、语法和结构，参考定义。广义的专家系统是指，由可验证性专家知识构建的系统。

​       **专家系统与机器学习**：狭义的专家系统基于明确粒度和推理规则，每一个规则都是可解释的知识，最难克服的困难是推理矛盾，即解释的完备性，本质问题还是粒度不够，粒度分解和系统扩充需要可能需要大规模的重构，甚至不能复用。机器学习系统基于数学函数，用概率解释粒度问题，因此给出了一些列解释矛盾和系统扩充的方法，基础理论为PAC可学习理论。机器学习给了一个有效的模型扩充方法，即集成学习EnsembleLearning。

​       **哥德尔（第一）完备性定理**：1929年由[**库尔特·哥德尔**](https://baike.baidu.com/item/%E5%BA%93%E5%B0%94%E7%89%B9%C2%B7%E5%93%A5%E5%BE%B7%E5%B0%94)首先证明，使用100页的论文证明基于逻辑推导的证明论是完备的。等价描述为：形在一阶[谓词演算](https://baike.baidu.com/item/%E8%B0%93%E8%AF%8D%E6%BC%94%E7%AE%97)中所有逻辑上有效的[公式](https://baike.baidu.com/item/%E5%85%AC%E5%BC%8F)都是可以证明的。

​       **哥德尔（第一）不完全性定理**：1931年，希尔伯特计划大厦覆灭，哥德尔证明。任何一个[形式系统](https://baike.baidu.com/item/%E5%BD%A2%E5%BC%8F%E7%B3%BB%E7%BB%9F)，只要包括了简单的[初等数论](https://baike.baidu.com/item/%E5%88%9D%E7%AD%89%E6%95%B0%E8%AE%BA)描述，而且是[自洽](https://baike.baidu.com/item/%E8%87%AA%E6%B4%BD)的，它必定包含某些系统内所允许的方法既不能证明真也不能证伪的命题。因此证明了模型论的模型不可完备问题。等价描述为：一个不知其有限性的语义系统，不能自证其完备性。

 

**第二节 ML方法**

​     

​       **概率**：ML方法里面存在这一种被称为机器学习基础的东西——统计机器学习理论。统计机器学习给出了**VC维**、**验证标准**、以及**PAC学习**理论，在概率系统分析层面几乎是完备的。

​       不过观测系统总有一个不可忽略的事实，基于命题和谓词逻辑，即概念和知识（主语）以及谓词（谓语）的粒度是系统准确和稳固性的基础。概率本质上是在更高的粒度上描述细粒度的东西，并称之为“率”，表示描述的不可确定性。

​       而科学的本质要求实验的精确重现和以及科学可信性基础哲学所要求的逻辑证明完备性。

​       概率以一种偷懒的方式，甚至不使用归纳方法，不去深入粒度，企图使用表象简单地拟合描述系统的规则，是一个暂时的方案。如果一个表象以概率的形式出现在观测者面前，且观测者不能给出解释，那么概率描述将只是一个暂时方案，在证明可行性之后，需要对系统进行更细粒度的分析，重新归纳规则。

​       **粒度**：科学是工程性的，要求一定粒度的精确重现，对概率选项是次级选择。哲学要求完备性，对概率是排斥的，因此概率只能描述概率问题，不代表真正严谨的科学，而人工智能需要一个更精确的系统，就像我希望我的爱人是爱我的，并且十分确信，且可以验证。也因此概率机器学习不能带来真正的人工智能。

​       举个栗子，概率不能从整数的域上进行了大量的遍历得出整数和四则运算是一个环，但事实整数和四则远算是一个环，而这个环的描述可以用专家系统精确重构，即使环的判定简单的规则基于一个巨大的数学正则系统。

​       **深入**：概率是在更高的粒度上描述细粒度的系统，PAC的意义在于给出规则和规则集合可行性，以及可行性的可信度即概率可行性。概率的根本价值在于价值评估，而非可信系统。

​       **遍历**：完备可信的对环境描述为遍历环境所有概念，一般情况下是计算不可行的。真实可信的观测为遍历所有真实规则，这要求所有规则是正确的，即从观察样本能规约到规则，对真实规则是完全拟合的。

​      **统计机器学习**：统计机器学习的作用是系统价值评估，而非构建可靠的专业的系统，或者可以用于构建不太严谨的应用系统。而现实中的大多数应用系统都是一个不严谨的应用系统，如人脸识别或者目标检测，并不需要对系统的完备理论进行阐述，所以在大多数模式识别系统中，基于机器学习的方法是可用的。

​      

**第三节 数学方法**

​     

​       **概率**：对环境的遍历是学习所有知识的完备方法，而此种情况一般不会发生，因为元素和路径组合以指数量级爆炸增长，穷举一般是现实计算上不可能的。而环境中现象的产生被认为是遵循环境基本物理定律，因此可以根据观测数据去探索其中的物理规律以及定理路径，得到“假设规则”或者数学上的“函数”，在数学上称之为“拟合”，在哲学上称之 为“归纳”。

​       **假设空间**：所有可能归纳出的规则和规则的概率准确性，在观察者意识中形成了固定限制，被称为“假设空间”，即假设规则或者函数集合，在ML中称为算法模型。而环境中的被观测实体分布遵循的真实规则所能遍历的空间，为真实规则集合，被称为“规则空间”，真实模型或者环境模型。

​       经典力学系统和**林乃分类法**：地球物理系统遵循一般宇宙运行规则，至于规则是否存在的哲学辩论超出了此文范畴，不再思辨。而早期建立的经典力学被认为遍历了物理力学系统的基本规则。现代生物分类法源于林奈的系统，他根据物种共有的生理特征分类。在林奈之后，根据达尔文关于共同[祖先](https://baike.baidu.com/item/%E7%A5%96%E5%85%88)的原则，此系统被逐渐改进。近年来，[分子系统学](https://baike.baidu.com/item/%E5%88%86%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%AD%A6)应用了生物信息学方法分析基因组DNA，正在大幅改动很多原有的分类。

​       林乃分类法基于一个强假设——共同祖先原则。至于这个假设是否成立，不得而知，只是这个假设规则是强归纳的，也被称为是生物分类学的基础。

​       **所见即所得**：观察到的即是假设事实，基于此准则，贝叶斯方法产生。

​       **贝叶斯方法**：将关于未知参数的[先验信息](https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E4%BF%A1%E6%81%AF/6944433)与样本信息综合，再根据[贝叶斯公式](https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F)，得出后验信息，然后根据后验信息去推断未知参数的方法。在估计随机分布参数时，认为待估计参数是随机变量，存在概率分布，且此分布是对事件和规则的主观判断，即主观概率或者归纳概率，而一般普遍陷入为主的主观概率模型为高斯模型。因高斯模型是在拟合随机变量时的误差模型，为基本f分布二项分布随机变量误差分布的最大似然。

​       **混合高斯模型**：混合高斯模型为参数和为1的线性高斯模型，最主要的作用还是用于EM算法，是多个贝叶斯模型的线性叠加。参考：[浅谈CLustering：高斯混合模型](http://blog.csdn.net/bitcarmanlee/article/details/52209726)。高斯混合模型是一个先验知识最少的模型，是个概率模型，一般不建议使用。

 

**第三节 抽丝剥茧**

​        **概率**：概率本质是粗粒度对细粒度不得已的描述方式，概率的背后，隐藏着深刻的本质，等待去发掘证明直至完备，就如高斯模型本质上是二项分布的极限分布，而二项分布是最基础的0-1分布的最大似然，证明过程即解释了bool型到连型的映射过程。需要抽丝剥茧，深入探讨每层概率的来由。

​       **专家**：经过长期的学习和时间，可以从无数松散的表象出“规律”，构建知识，且达到了一定的认可，被成为“专家”，意思未专业的指导者和特定的权威性。

​       **假设规则**：不同层次的专家从表象中总结的规则，仍然是基于公理推理，被称为假设规则。一组假设规则被称为“假设空间”，如果规则集是可以自行完备的，即整个系统是自证明无矛盾的，被称为“完备假设空间”。每一条假设规则的提出，都是对“概率”的抽丝剥茧，提出的规则的过程被称为“拟合”。

​       **机器学习假设**：以最简单的多项式拟合规则为例。从可观测到的样本中尽可能的规约知识，并使之能更广泛的适应环境，为机器学习。

​      ![img](https://img-blog.csdn.net/20180326093224945)

​      但除非环境主动告知，规则仅仅是一种拟合。可能采样的第三种分布恰好是真实分布，那么第二种假设就是错的。

 

**第四节 归纳与遍历**

​       **粒度与遍历：**自然界不是以一个小包一个小包来到我们面前的。这句话是不是真命题不得而知，但我们对环境的感知确实是基于粒度的，即理解必须做到特定粒度遍历。

​       完全遍历的所有可信粒度概念，为知识规则系统。而不能完全可信遍历的概念，若强加规则，则归结为概率系统，这就需要使用机器学习。

 

**第五节 机器学习**

​        写了一堆....firefox崩溃了，真是悲剧！![大哭](http://static-blog.csdn.net/xheditor/xheditor_emot/default/wail.gif)

​        **样本集、训练集、验证集、测试集**：

​        可观测集合、适用于调整模型的集合、不断更新的测试集。

​        **超参数与参数**：

​        专家知识-规则、 模型形式、模型参数

​        复合函数、初等函数

​        写的几千字泛函空间的文字被覆盖了，上个图![大哭](http://static-blog.csdn.net/xheditor/xheditor_emot/default/wail.gif)

​       ![img](https://img-blog.csdn.net/20180322154923911?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lzaGNoaW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

 

​        **线性非线性**

​        线性与非线性、SVM方法、近邻方法、SVM神经网络、核函数、最优化与二次优化....

 

​        **训练方法**

 

​        那个黄皮的教科书里面比较经典，参考：ML：[ML中的最优化方法](https://blog.csdn.net/wishchin/article/details/50392669)进阶

​       线性规划、单纯形法、匈牙利算法、一维搜索、坐标轮换法、梯度下降法、牛顿迭代法、拟牛顿法、LBFGS算法、百度的shooting算法。

​        针对DCNN出现了各种详细的优化方法，有许多许多的小tricks。

​       **纤维与纤维丛**：

​       纤维的分布，纤维的链接与宽度

​       从泛函理论到拓扑流行学，马天的那本拓扑流行学对于这些非数学专业的外行还是算比较容易入门的。

​       **欠拟合与过拟合**：假设规则一定是假设。一个学习算法会有一个前提假设，这里被称作“归纳偏执 (bias)”（bias 这个英文词在机器学习和统计里还有其他许多的意思）。例如线性回归，目的是要找一个函数尽可能好地拟合给定的数据点，它的归纳偏执就是“满足要求的函数必须是线性函数”。一个没有归纳偏执的学习算法从某种意义上来说毫无用处，就像一个完全没有归纳能力的人一样，在第一次看到鱼的时候有人告诉他那是鱼，下次看到另一条鱼了，他并不知道那也是鱼，因为两条鱼总有一些地方不一样的，或者就算是同一条鱼，在河里不同的地方看到，或者只是看到的时间不一样，也会被他认为是不同的，因为他无法归纳，无法提取主要矛盾、忽略次要因素，只好要求所有的条件都完全一样──然而哲学家已经告诉过我们了：世界上不会有任何样东西是完全一样的，所以这个人即使是有无比强悍的记忆力，也绝学不到任何一点知识。这个问题在机器学习中称作“过拟合 (Overfitting)”。

​       上面的例子依然是基于粒度描述的解释。定义共性之初，首先定义粒度。以林奈分类法为例，界门纲目科属种是每一个大类的粒度。而每一种的分类准则，只能是**生殖隔离**。

​       

**第六节 PAC学习理论**

​       艹！又被Firefox自动销毁了![大哭](http://static-blog.csdn.net/xheditor/xheditor_emot/default/wail.gif) Firefox开的页面太多了，就算是休眠，也要保存一下页面，血的教训！

​      还是添加一个连接：[PAC学习理论：ML那些事](http://blog.csdn.net/wishchin/article/details/53634396)......

 

**第七节 CNN与过拟合**

​         永远过拟合的CNN模型..........................

​        NN从诞生之初就被称为一个黑盒模型，围绕NN的结构和效果有一大波理论诞生。从单个perception到MLP，扩展到其他结构，RNN啦、LSTM啦、CNN啦。MLP容易陷入局部最优解，因为MLP一般是一个参数量很大的模型。ANN[网络堆叠进化-从感知机到DBRN](http://blog.csdn.net/wishchin/article/details/45067177)。MLP容易过拟合，且难以训练。

​        在图像处理领域，广泛使用过的三层ANN、SVM网络、随机森林模型、和CNN模型，都是一个归纳模型，永远离不开归纳演绎泛化的通用理论和框架限制：[人工机器-作为归纳系统的深度学习](http://blog.csdn.net/wishchin/article/details/71195098)。

​        2012年AlexNet大爆发，CNN称为图像处理领域的流行方法，也不能避免参数众多带来的过拟合问题。在形态更丧失一般性，准确度更高，而泛化能力更差的网络中，会出现这种问题：[修改一个像素让神经网络处理出错](http://blog.csdn.net/Uwr44UOuQcNsUQb60zk2/article/details/78372154)（参见：[经得住考验的「假图片」：用 TensorFlow 为神经网络生成对抗样本](https://www.jiqizhixin.com/articles/2017-08-11-9)）。

 

**第八节 函数自动优化求解**

​         GAN的使用，使用生成式模型辅助训练。

​         基础介绍：[GAN的发展对通用人工智能的意义](https://www.zhihu.com/question/57668112/answer/155367561) ...    

​         从通用人工智能高层次来看，这个模型率先使用神经网络来指导神经网络，颇有一种奇妙的美感：仿佛是在辩日的两小儿一样，一开始两者都是懵懂的幼儿，但通过观察周围，相互讨论，逐渐进化出了对外界的认知。 这不正是吾等所期望的终极智能么 -- 机器的知识来源不再局限于人类，而是可以彼此之间相互交流相互学习。也难怪Yann Lecun赞叹GAN是机器学习近十年来最有意思的想法[https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f](https://link.zhihu.com/?target=https%3A//medium.com/%40devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f) 。
​         GAN暂时不能对网络结构进行拓扑重构，可以对参数调优方面进行自动化完成，但不一定达到专家手动效果。GANs的效果理论上不超过一般意义上的主动强化学习。Gan存在的意义，证明了通过**对抗博弈**思路，不需要大量的数据，**仅仅根据规则指导，便可以达到较优的解**。即在判别式模型和数据生成模型所代表的假设空间， 寻求假设空间里的次优解，完全可以由这个假设空间本身的判别式模型和数据生成模型自行完成。

 

**第八节 参数调优与结构重构**

​         参数优化需要避免的问题是局部最优解，而过拟合的解决方法只能是结构重构，在DCNN方法上，重构意味着巨大的困难..Layer概念的诞生对复杂度降解产生巨大帮助，同时也有很大的限制。

​       

**总结：**

​      两个没有知识不明所以的小孩下棋，即是给了下棋的所有规则，让这两个小孩反复对弈，最后这两个小孩的水平会出现距离真实世界的极大的偏置，却是最适合规则的水平，GAN模型就是在一个棋局离塞入两个小孩。

​       最后，ML最大的作用还是人类专家。

 