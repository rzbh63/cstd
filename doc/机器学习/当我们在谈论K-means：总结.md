# 当我们在谈论K-means：总结

[![余文毅](https://pic3.zhimg.com/a3e2422ec_xs.jpg)](https://www.zhihu.com/people/wyyualan)

[余文毅](https://www.zhihu.com/people/wyyualan)

不自由职业者

5 人赞同了该文章

本系列意在长期连载分享，内容上可能也会有所增删改减； 

**因此如果转载，请务必保留源地址，非常感谢！**

**知乎专栏：当我们在谈论数据挖掘**

**博客园：当我们在谈论数据挖掘（暂时公式显示有问题）**

## 概述

通过前面阅读K-means相关论文，大致能梳理出K-means算法发展过程中的一些轨迹。由于本人所阅读的仅仅是一部分，因此还会有更多的方面，欢迎大家补充（补充时请给出具体例子）。

- \1. K-means算法的提出

- \2. 对K-means算法的性质进行分析的文章相继发出

- \3. 对K-means算法思想进行扩展：

- - 有作者提出“Maximum Entropy”算法，并表示K-means为其一种特殊形式
  - 后又有作者提出“Mean Shift”算法，并表示“Maximum Entropy”也是其特殊形式

- \4. 针对K-means缺陷，对K-means算法进行修改（一般仅适用于某场景）：

- - 提出online的K-means
  - 提出针对非凸数据集的K-means
  - 提出应用在FPGA中的K-means
  - 提出自动对特征进行加权的K-means
  - Intelligent K-means算法使用异常检测的思想聚类

- \5. 对K-means算法进行优化：

- - KD树加速的K-means
  - 利用SVD分解加速K-means
  - K-means++的初始化聚类中心算法

- \6. 将K-means与新提出的思想融合：

- - 结合Ensembling与K-means

## K-means存在的问题

K-means由于简单有效被大量的用于数据预处理、数据分析等。在K-means被实际应用的过程中，大家也逐渐发现它本身存在很多的问题。如：

- 计算量大
- 聚类数量K需要提前设定，并影响聚类效果
- 聚类中心需要人为初始化，并影响聚类效果
- 异常点的存在，会影响聚类效果
- 只能收敛到局部最优

其中每个问题都有作者分析，并尝试提出解决办法：

- 计算量大

- - KD树加速K-means

- 聚类数量K需要提前设定，并影响聚类效果

- - 各种估计K的方法

- 聚类中心需要人为初始化，并影响聚类效果

- - K-means++方法
  - 其他初始化聚类中心方法

- 异常点的存在，会影响聚类效果

- - 数据预处理

- 只能收敛到局部最优

- - 未知

以下我们对其中两点（“类别数量估计”，“初始化聚类中心”）进行更多的介绍

## 类别数量估计

估计类别数量，现在还没有很通用的方法。以下介绍常见的估计类别数量的一些方式：

1. 数据的先验知识，或者数据进行简单分析能得到
2. 基于变化的算法：即定义一个函数，随着K的改变，认为在正确的K时会产生极值。如Gap Statistic（Estimating the number of clusters in a data set via the gap statistic, Tibshirani, Walther, and Hastie 2001），Jump Statistic （finding the number of clusters in a data set, Sugar and James 2003）
3. 基于结构的算法：即比较类内距离、类间距离以确定K。这个也是最常用的办法，如使用平均轮廓系数，越趋近1聚类效果越好；如计算类内距离/类间距离，值越小越好；等。
4. 基于一致性矩阵的算法：即认为在正确的K时，不同次聚类的结果会更加相似，以此确定K。
5. 基于层次聚类：即基于合并或分裂的思想，在一定情况下停止获得K。
6. 基于采样的算法：即对样本采样，分别做聚类；根据这些结果的相似性确定K。如，将样本分为训练与测试样本；对训练样本训练分类器，用于预测测试样本类别，并与聚类的类别比较。

## 初始化聚类中心

接下来介绍几个看到的初始化聚类中心的方法。需要强调的是，在任何场景下都合适的方法是不存在的。理想情况应该是针对数据的特点，挑选或设计出适合的方法。

1. K-means++已经被证明是一种简单、好用的方法
2. 先计算整体样本中心，然后根据样本点到中心的距离，由近至远均匀采样作为初试聚类中心
3. 初步将数据分成K个区域，将每个区域中心作为初始聚类中心
4. 计算出每个点的”密度“，认为”密度“较大的是聚类中心。先把”密度“最大的挑出作为第一个聚类中心，从剩下的点中找出密度最大，且离所有已有聚类中心大于一定距离的点作为下一个聚类中心，直到选择了K个
5. 计算整体均值，作为第一个聚类中心。从剩下的点中顺序寻找，当遇到离所有已有聚类中心大于一定距离的点，则作为下一个聚类中心，直到选择了K个

## 其他聚类算法总结

聚类算法非常多，这里仅列举在前文中介绍的几个常用聚类算法。更多的聚类算法在以后实践过程中，会继续补充

- 基于层次聚类的算法

- - 算法：Hierarchical Clustering
  - 优点：适用于任意形状和属性的数据集；灵活控制不同层次的聚类粒度；
  - 缺点：时间复杂度高；

- 基于平方误差的聚类算法

- - 算法：Fuzzy C-Means
  - 优点：能求出样本属于每一类的概率；
  - 缺点：结果依赖于对初始聚类中心的选择；容易陷入局部最优解；对K值的选择没有准则可依循；对异常数据较为敏感；

- 基于模型的聚类算法

- - 算法：Mixture of Gaussians
  - 优点：对样本精确建模；参数求解简单；
  - 缺点：要求样本服从某种分布；

- 基于密度的聚类算法

- - 算法：DBSCAN；Density Peaks
  - 优点：不需要设定类别数量；适用于任意形状的数据集；
  - 缺点：时间复杂度高；密度不均匀时效果较差

## **本系列其他文章：**

- [当我们在谈论K-means：数据概述](https://zhuanlan.zhihu.com/p/24911284?refer=data-miner)
- [当我们在谈论K-means：论文概述（1）](https://zhuanlan.zhihu.com/p/25024565?refer=data-miner)
- [当我们在谈论K-means：论文概述（2）](https://zhuanlan.zhihu.com/p/25032775?refer=data-miner)
- [当我们在谈论K-means：其他聚类算法](https://zhuanlan.zhihu.com/p/25032935?refer=data-miner)
- 当我们在谈论K-means：总结