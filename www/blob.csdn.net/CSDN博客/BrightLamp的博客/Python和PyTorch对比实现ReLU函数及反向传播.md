
# Python和PyTorch对比实现ReLU函数及反向传播 - BrightLamp的博客 - CSDN博客


2018年11月21日 20:36:38[BrightLampCsdn](https://me.csdn.net/oBrightLamp)阅读数：180所属专栏：



## 摘要
本文使用纯 Python 和 PyTorch 对比实现 ReLU 函数及其反向传播.
## 相关
*原理和详细解释, 请参考文章 :*
ReLU函数详解及反向传播中的梯度求导
*系列文章索引 :*
[https://blog.csdn.net/oBrightLamp/article/details/85067981](https://blog.csdn.net/oBrightLamp/article/details/85067981)
## 正文
```python
import
```
```python
torch
```
```python
import
```
```python
numpy
```
```python
as
```
```python
np
```
```python
class
```
```python
Relu
```
```python
:
```
```python
"""
    http://cs231n.github.io/
    """
```
```python
def
```
```python
__init__
```
```python
(
```
```python
self
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
x
```
```python
=
```
```python
None
```
```python
def
```
```python
__call__
```
```python
(
```
```python
self
```
```python
,
```
```python
x
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
x
```
```python
=
```
```python
x
```
```python
# Must use copy in numpy to avoid pass by reference.
```
```python
out
```
```python
=
```
```python
self
```
```python
.
```
```python
x
```
```python
.
```
```python
copy
```
```python
(
```
```python
)
```
```python
out
```
```python
[
```
```python
out
```
```python
<
```
```python
0
```
```python
]
```
```python
=
```
```python
0
```
```python
return
```
```python
out
```
```python
def
```
```python
backward
```
```python
(
```
```python
self
```
```python
,
```
```python
d_loss
```
```python
)
```
```python
:
```
```python
relu_mask
```
```python
=
```
```python
(
```
```python
self
```
```python
.
```
```python
x
```
```python
>=
```
```python
0
```
```python
)
```
```python
dx
```
```python
=
```
```python
d_loss
```
```python
*
```
```python
relu_mask
```
```python
return
```
```python
dx

np
```
```python
.
```
```python
set_printoptions
```
```python
(
```
```python
precision
```
```python
=
```
```python
6
```
```python
,
```
```python
suppress
```
```python
=
```
```python
True
```
```python
,
```
```python
linewidth
```
```python
=
```
```python
120
```
```python
)
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
seed
```
```python
(
```
```python
123
```
```python
)
```
```python
x_numpy
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
random
```
```python
(
```
```python
(
```
```python
3
```
```python
,
```
```python
7
```
```python
)
```
```python
)
```
```python
x_tensor
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
x_numpy
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
relu_numpy
```
```python
=
```
```python
Relu
```
```python
(
```
```python
)
```
```python
relu_tensor
```
```python
=
```
```python
torch
```
```python
.
```
```python
nn
```
```python
.
```
```python
ReLU
```
```python
(
```
```python
)
```
```python
out_numpy
```
```python
=
```
```python
relu_numpy
```
```python
(
```
```python
x_numpy
```
```python
)
```
```python
out_tensor
```
```python
=
```
```python
relu_tensor
```
```python
(
```
```python
x_tensor
```
```python
)
```
```python
d_loss_numpy
```
```python
=
```
```python
np
```
```python
.
```
```python
random
```
```python
.
```
```python
random
```
```python
(
```
```python
(
```
```python
3
```
```python
,
```
```python
7
```
```python
)
```
```python
)
```
```python
d_loss_tensor
```
```python
=
```
```python
torch
```
```python
.
```
```python
tensor
```
```python
(
```
```python
d_loss_numpy
```
```python
,
```
```python
requires_grad
```
```python
=
```
```python
True
```
```python
)
```
```python
dx_numpy
```
```python
=
```
```python
relu_numpy
```
```python
.
```
```python
backward
```
```python
(
```
```python
d_loss_numpy
```
```python
)
```
```python
out_tensor
```
```python
.
```
```python
backward
```
```python
(
```
```python
d_loss_tensor
```
```python
)
```
```python
dx_tensor
```
```python
=
```
```python
x_tensor
```
```python
.
```
```python
grad
```
```python
print
```
```python
(
```
```python
"代码输出 :"
```
```python
)
```
```python
print
```
```python
(
```
```python
"numpy out : \n"
```
```python
,
```
```python
out_numpy
```
```python
)
```
```python
print
```
```python
(
```
```python
"tensor out : \n"
```
```python
,
```
```python
out_tensor
```
```python
.
```
```python
data
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
)
```
```python
print
```
```python
(
```
```python
"dx_numpy : \n"
```
```python
,
```
```python
dx_numpy
```
```python
)
```
```python
print
```
```python
(
```
```python
"dx_tensor : \n"
```
```python
,
```
```python
dx_tensor
```
```python
.
```
```python
data
```
```python
.
```
```python
numpy
```
```python
(
```
```python
)
```
```python
)
```
```python
"""
代码输出 :
numpy out : 
 [[ 0.696469  0.286139  0.226851  0.551315  0.719469  0.423106  0.980764]
 [ 0.68483   0.480932  0.392118  0.343178  0.72905   0.438572  0.059678]
 [ 0.398044  0.737995  0.182492  0.175452  0.531551  0.531828  0.634401]]
tensor out : 
 [[ 0.696469  0.286139  0.226851  0.551315  0.719469  0.423106  0.980764]
 [ 0.68483   0.480932  0.392118  0.343178  0.72905   0.438572  0.059678]
 [ 0.398044  0.737995  0.182492  0.175452  0.531551  0.531828  0.634401]]
dx_numpy : 
 [[ 0.849432  0.724455  0.611024  0.722443  0.322959  0.361789  0.228263]
 [ 0.293714  0.630976  0.092105  0.433701  0.430863  0.493685  0.42583 ]
 [ 0.312261  0.426351  0.893389  0.94416   0.501837  0.623953  0.115618]]
dx_tensor : 
 [[ 0.849432  0.724455  0.611024  0.722443  0.322959  0.361789  0.228263]
 [ 0.293714  0.630976  0.092105  0.433701  0.430863  0.493685  0.42583 ]
 [ 0.312261  0.426351  0.893389  0.94416   0.501837  0.623953  0.115618]]
"""
```

