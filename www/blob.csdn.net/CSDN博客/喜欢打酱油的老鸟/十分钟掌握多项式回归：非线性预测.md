
# 十分钟掌握多项式回归：非线性预测 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月04日 08:21:35[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：83


[https://www.toutiao.com/a6642221691839709704/](https://www.toutiao.com/a6642221691839709704/)
2019-01-03 18:52:31
之前我们曾经学习了简单线性回归模型的推导、sklearn实战，并尝试从零搭建了一个简单线性回归的模型工具。
**有任何问题都可以在下方留言，我都会耐心解答。**
但是我们遇到的数据并不总是线性的，这时如果我们还拿线性模型去拟合，我们模型的效果就会大打折扣。不过不用担心，我们仍然可以使用线性回归的方法来拟合非线性的数据，只不过我们要先对输入数据做一些处理。
# 一、快速理解多项式回归原理
我们先来回顾一下简单线性回归的假设：
![十分钟掌握多项式回归：非线性预测](http://p1.pstatp.com/large/pgc-image/b6b62a96f2404e6bba7136612e0b26df)

假如我们通过散点图发现变量y与x之间的关系大致符合二次分布，那么上述的假设就不太合适了，我们可以假设：
![十分钟掌握多项式回归：非线性预测](http://p1.pstatp.com/large/pgc-image/aac3efc299f74d98b939131d41742f21)

我们的残差依然是:
![十分钟掌握多项式回归：非线性预测](http://p1.pstatp.com/large/pgc-image/2b4692a8a9064c3a86416876bf772b22)

与简单线性回归相同，我们的目标是最小化残差平方和：
![十分钟掌握多项式回归：非线性预测](http://p3.pstatp.com/large/pgc-image/eaa883c465ed42539ab087ec6e7ec342)

然后我们分别对α、β1和β2求偏导，使其为0，我们可以得到三个等式，求解即可。
这部分推理与简单线性回归的推理部分极为相似，感兴趣的可以直接阅读我的《三步教你从零掌握简单线性回归》一文。
# 二、scikit-learn实战
那么接下来，我们就直接来看scikit-learn实战部分了。先放代码和输出，然后我们再详解一下：
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
X_train = [[6], [8], [10], [14], [18]]
y_train = [[7], [9], [13], [17.5], [18]]
X_test = [[6], [8], [11], [16]]
y_test = [[8], [12], [15], [18]]
\# 简单线性回归
model = LinearRegression()
model.fit(X_train, y_train)
xx = np.linspace(0, 26, 100)
yy = model.predict(xx.reshape(xx.shape[0], 1))
plt.scatter(x=X_train, y=y_train, color='k')
plt.plot(xx, yy, '-g')
\# 多项式回归
quadratic_featurizer = PolynomialFeatures(degree=2)
X_train_quadratic = quadratic_featurizer.fit_transform(X_train)
X_test_quadratic = quadratic_featurizer.fit_transform(X_test)
model2 = LinearRegression()
model2.fit(X_train_quadratic, y_train)
xx2 = quadratic_featurizer.transform(xx[:, np.newaxis])
yy2 = model2.predict(xx2)
plt.plot(xx, yy2, '-r')
print('X_train:
', X_train)
print('X_train_quadratic:
', X_train_quadratic)
print('X_test:
', X_test)
print('X_test_quadratic:
', X_test_quadratic)
print('简单线性回归R2：', model.score(X_test, y_test))
print('二次回归R2：', model2.score(X_test_quadratic, y_test));输出为：
X_train:
 [[6], [8], [10], [14], [18]]
X_train_quadratic:
 [[ 1. 6. 36.]
 [ 1. 8. 64.]
 [ 1. 10. 100.]
 [ 1. 14. 196.]
 [ 1. 18. 324.]]
X_test:
 [[6], [8], [11], [16]]
X_test_quadratic:
 [[ 1. 6. 36.]
 [ 1. 8. 64.]
 [ 1. 11. 121.]
 [ 1. 16. 256.]]
简单线性回归R2： 0.809726797707665
二次回归R2： 0.8675443656345073![十分钟掌握多项式回归：非线性预测](http://p9.pstatp.com/large/pgc-image/f77dc75017f347649d837427328253c5)

# 三、步骤详解
我们来看看在每一步我们都做了什么。
第一步，我们导入了必要的库。
第二步，我们创建了训练集和测试集。
第三步，我们拟合了简单线性回归，并且绘制了预测的直线。
第四步，我们使用sklearn.preprocessing.PolynomialFeatures方法，将我们的原始特征集生成了n*3的数据集，其中第一列对应常数项α，相当于x的零次方，因此这一列都是1；第二列对应一次项，因此这一列与我们的原始数据是一致的；第三列对应二次项，因此这一列是我们原始数据的平方。
第四步，我们拿前边用PolynomialFeatures处理的数据集做一个多元线性回归，然后用训练好的模型预测一条曲线，并将其绘制出来。
第五步，输出数据方便理解；输出模型分数用于对比效果。
看到这里你可能已经明白了，多项式回归虽然拟合了多项式曲线，但其本质仍然是线性回归，只不过我们将输入的特征做了些调整，增加了它们的多次项数据作为新特征。其实除了多项式回归，我们还可以使用这种方法拟合更多的曲线，我们只需要对原始特征作出不同的处理即可。
你学会了吗？

