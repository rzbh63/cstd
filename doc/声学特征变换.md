# 声学特征变换



# fMLLR





------

## 含义

![这里写图片描述](https://img-blog.csdn.net/20171112162836203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
当测试数据YY和模型ΛxΛx不匹配的时候，可以通过变换的方式进行匹配[1]： 
\- model-space 也就是ΛxΛx转化为ΛyΛy 
\- feature-space 也就是YY转化为XX

其中model-space的变换又可以分为两种： 
\- unconstrained: 均值和方差无关 
\- constrained: 均值和方差变换是相同的形式

对于constrained model-space transformations，虽然出发点是对模型的均值和方差做转换，但是公式推导[2]最后的形式可以看成对输入的特征做线性变化，所以Constrained Maximum Likelihood Linear Regression (CMLLR)也称为feature-space MLLR (fMLLR)。 
fMLLR主要用于SAT（speaker adaptive training）训练，基本思想是训练得到的转化矩阵，使得adaptation数据在当前模型获得最大似然值。

## kaldi使用

steps/train_sat.sh

```powershell
#1.获得trans
#  假设特征40维，每个speaker对应一个40*41维的矩阵
gmm-est-fmllr
#使用trans
transform-feats --utt2spk=ark:$sdata/JOB/utt2spk ark,s,cs:$dir/trans.JOB ark:- ark:- |
#查看trans矩阵
copy-matrix ark:trans.1 ark,t:trans.1.txt1234567
```

## 参考

[1].A Maximum-Likelihood Approach to Stochastic Matching for Robust Speech Recognition 
[2].[Maximum likelihood linear transformations for HMM-based speech recognition](https://pdfs.semanticscholar.org/2109/f8f91301abec8497286160cd6b0f2e65ed05.pdf)







# STC/MLLT

## 背景

Global Semi-tied Covariance (STC)/Maximum Likelihood Linear Transform (MLLT) estimation 
gmm建模方差使用对角矩阵的前提是假设特征之间相互独立，使用full或者block-diagonal矩阵可以对相关性的特征建模，但是参数增多。为了解决使用这个问题，有两种方法： 
\- feature-space 使用DCT或者LDA去相关 
\- model-space 不同的模型可以使用不同的转换，更灵活

semi-tied covariance matrices是model-space里面的一种形式，也是为了解决使用full covariance的参数量大的问题。相比于full covariance，这种方法的每个高斯分量有两个方差矩阵： 
\1. diagonal covariance∑(m)diag∑diag(m) 
\2. semi-tied class-dependent nondiagonal matrix H(r)H(r)，可以在多个高斯分量之间共享（比如所有monophone对用状态的高斯分量）。

最后的方差矩阵:∑(m)=H(r)∑(m)diagH(r)T∑(m)=H(r)∑diag(m)H(r)T,使用最大似然估计结合EM算法求解对应的参数。 
如果做变换A(r)=H(r)−1A(r)=H(r)−1，把在分母上的semi-tied covariance转换到分子上面，也就是相当于特征o(τ)o(τ)和均值μ(m)μ(m)同时乘以A(r)A(r)，也就是对特征和gmm的均值同时做MLLT。

## kaldi实现

steps/train_lda_mllt.sh 
假设特征40维，得到的mllt转换矩阵是40*40，转换矩阵同时作用于gmm的均值和特征

## 参考

1.Semi-tied covariance matrices for hidden Markov models





# LDA

## 含义

[Linear Discriminant Analysis 线性判别式分析](https://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90)是一种降维算法，特征经过映射以后，在新的空间有最大的类间距离和最小的类内距离；LDA降维的维度跟类别的个数有关 
相关公式推导可以参考[这篇博客](http://www.cnblogs.com/jerrylead/archive/2011/04/21/2024384.html)

## kaldi实现

### 特征降维

特征做完splice以后进行降维

```powershell
steps/train_lda_mllt.sh
acc-lda #使用pdf-id作为类别，获得统计量
est-lda #获得lda转换矩阵123
```

### 数据归一化

![这里写图片描述](https://img-blog.csdn.net/20171112163106592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveG1keGNzag==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 
为了加快网络训练，一般需要对输入特征进行处理[1]： 
\- 零均值 
\- 去相关 
\- 方差规整

在方差处理的时候，如果已知某些维度比较重要，可以增大它们的方差，有益于网络训练。 
这里做LDA并不降维，求得到的转换矩阵是一些较大的特征值对应的特征向量（特征值的解释参考[知乎](https://www.zhihu.com/question/20507061)）组成的，可以挑选出重要的特征维度。

```powershell
steps/libs/nnet3/train/chain_objf/acoustic_model.py: compute_preconditioning_matrix
```