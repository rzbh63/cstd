
# 分类模型的评估方法-精确率(Precision) - saltriver的专栏 - CSDN博客


2017年06月29日 21:53:46[saltriver](https://me.csdn.net/saltriver)阅读数：1411


上一篇文章中，提到**正确率(Accuracy)**是机器学习分类任务中过得一个模型评估方法，并指出**正确率(Accuracy)**虽然简单直观，但在很多时候并不是一个真正正确的评估指标。
那还有什么评估方法呢?我们还是举地震、癌症、信用卡交易欺诈的例子，在这些情况下，**我们显然关心的是有没有地震，有没有癌症，有没有欺诈交易**。如果有地震、有癌症、有欺诈，竟然预测错了，这显然是非常严重的后果。
那么，要怎样评估计算呢?这里就要引出”**阴阳**“的概念，我们做体检时，经常会出现有阳性、阴性的检查结果。什么意思呢?检查出“**阳性**”一般代表就有异常结果，例如可能得了某病，或者怀孕等。“**阴性**”就代表正常结果，我们当然关心异常结果，也就是”阳性”的结果。
![这里写图片描述](https://img-blog.csdn.net/20170629214254863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FsdHJpdmVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
当然，检查出“阳性”，并不代表真有某种疾病，这称为“**假阳**”；同理，检查出“阴性”，也并不代表没有某种疾病，这称为”**假阴**“。
这里把”**阳性/阴性**“从疾病检查中引申出来，把我们关心的异常结果，例如地震、欺诈交易等都称为”阳性“，同理，正常的结果没有地震、没有欺诈交易都称为”**阴性**”。
那么，对于分类模型对测试集中每条数据样本的预测，排列组合下，就只有4种可能性：
`1.原本是阳性，预测成阳性：真阳
2.原本是阳性，预测成阴性：假阴
3.原本是阴性，预测成阳性：假阳
4.原本是阴性，预测成阴性：真阴`怎么样，是不是感觉都有点像武功秘籍《九阳真经》、《九阴真经》了。
前面说过，我们关注的是异常结果，即”阳性“，如果检查出的”阳性”都是真阳，没有”假阳”当然是好的。所以，我们用下式来评估模型的好坏：

$$
Precision=\frac{真阳}{真阳+假阳}
$$
这就是**精确率(Precision)**评估方法。
举个例子，地震局的地壳活动数据，100万个测试数据样本中，只有10个是有地震的，另外的999990个数据是没有地震的，如果我们的分类模型预测对了这999990个没有地震的数据中的999900个；另外10个有地震的预测对了6个。
那么真阳=6，真阴=999900，假阳=90.假阴=4。

$$
Precision=\frac{6}{6+90}=0.0625
$$
这个得分就很低了。如果按照”正确率/准确率(Accuracy)“来计算，就是(999900+6)/1000000=99.9906%。
因为我们更关心有地震的情况，所以，采用精确率(precision)的计算方法显然更合适。
**精确率(Precision)**还有一个名称，叫**查准率**。但是注意，**查准率这个名称主要用在信息检索领域**。例如一个论文数据库，搜索”机器学习”相关论文，搜出来的文档数量为10000条，其中真正与”机器学习“相关的文档数量为9000条，那么查准率就是90%。
实际应用中，大家经常把**正确率、准确率、精确率、查准率**等混在一起，不同人说的都不是同一个东西。因此，建议使用英文原文来表达更清晰，即**Accuracy**和**Precision**。

