# 活体检测-利用多帧人脸来预测更精确的深度

 

京东金融和中科院联合发表的“Exploiting temporal and depth information for multi-frame face anti-spoofing”[1]

它的主要创新和贡献是:利用了多帧的时空信息来更精准地预测深度图，再而进行活体检测

Related Work

先把提一下之前的state-of-the-art，就是MSU发表在CVPR2018上的工作 [2]。

文中的主要思路是：

\1. 通过活体与非活体的单帧人脸图，来预测其深度图（因为先验知识知道 真人图 与 纸张或屏幕攻击载体 的深度很不同）

\2. 通过活体与非活体的多帧人脸图，来预测其rPPG信号的频域分布（同理先验知道 真人人脸的rPPG信号 与 无生命的纸张或屏幕 很不同）

上述共享一个 backbone，后接两个分支。分支一直接回归深度图；分支二用来预测rPPG频域分布：即是通过non-rigid registration层来将pose都归一到正脸同姿态，后接RNN来获得temporal信息。这里就列下共享的主干网络，因为京东这文章也是用相同的网络：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/31zxgfme3h.jpeg?imageView2/2/w/1620)

图1.主干网络 

依据与Motivation

作者认为，MSU上述方法有一定drawback:

\1. 因为使用了non-rigid注册层去除脸部表情和姿态的影响，这样忽略了重要的线索：非活体脸部不同表情与姿态的不自然变化（unnatural changes）

\2. 只用了单帧图像来预测深度，忽略了多帧间的空间微变化可以帮助重构环境3D信息。

基于上面两点，作者发现可以把该问题看出multi-view的SLAM问题，无论是摄像头在动，还是人脸在动，它们帧间的动态信息都可以用来重构3D空间，即**用多帧信息理论上会比单帧更好地重构深度图**。

作者画出下面草图来描述活体与非活体间帧间的微变化，可见在 左边(a)活体场景，明显侧脸时鼻子与耳朵的角度比正脸时大；而对于右侧(b)屏幕攻击，则反之。

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/5izdx6byr1.jpeg?imageView2/2/w/1620)

图2.活体与非活体的多帧视差

算法框架

总框架主要分两部分（单帧部分和多帧部分），如下图所示：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/vt23i1hjta.jpeg?imageView2/2/w/1620)

图3.网络总框架图

单帧部分和MSU文章的主干网络基本一致（唯一的不同就是MSU用了 RGBHSV 6通道作为输入，本文用的是RGB三通道），就是每帧单独预测深度图：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/9a9idles7f.jpeg?imageView2/2/w/1620)

图4. 单帧网络部分

多帧部分主要由 optical flow guided features (OFF) Block 和 ConvGRU Unit 组成，因为OFF Blcok主要构建在相邻两帧间，而 ConvGRU 是构建在整个多帧的clips间，故前者用来获取short-term信息，后者则获得Long-term信息。

下图是OFF Block架构图，我们来看看都做了什么：

\1. *Fl*(t)为当前帧特征经过1x1卷积后降维的特征

\2. *F*l*S*(t)为当前帧特征经过Sobel算子后的空间XY方向梯度

\3. *F*l*T*(t)为当前帧特征与相邻后一帧特征的差异（空间梯度）

\4. *F*l*S*(t+△t)为相邻后一帧特征的Sobel算子后的空间XY方向梯度

\5. OF*F*l-1(t)为上个OFF Blcok输出的特征 （即多个OFF Block是 stacked）

最后把上述5个特征都concatenate在一起，3x3卷积再降维~~

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/k0mk562ps5.jpeg?imageView2/2/w/1620)

图5. OFF Block的架构

至于作者为什么要这样设计呢？OFF全称是optical flow guided features，则作者希望使用相邻帧间feature-level的光流，这样比起传统光流，表征能力更强且计算消耗更小。传统光流公式如下：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/0umdquid5a.png?imageView2/2/w/1620)

通过泰勒分解和变形后，得到：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/c3cx61ej27.png?imageView2/2/w/1620)

这里的 (*v*x,*v*y) 就是光流，而通过上面公式可得正交关系：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/nx6an3e2q1.png?imageView2/2/w/1620)

is orthogonal to 

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/24jb27l7s2.png?imageView2/2/w/1620)

故

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/vyi656ka2r.png?imageView2/2/w/1620)

是被光流引导的。而通常传统光流需要通过 Low-level 和 high-level 特征去匹配得到，故我们将上式的输入图像 I 换成特征图来输入，则使用多级特征图的X方向梯度，Y方向梯度和时间梯度，便可类似地表示光流。所以OFF block里的5个元素，就是按照这个来的~~

PS：文中光流假设有点问题，文中说光流前后(x,y) 与(x+△t,y+△t)的亮度应不变。不过在人脸的应用中，肯定是会换的，即人脸相同位置的pixel，随着心脏驱动的血液流动，它的亮度值也会周期性地变化，这就是做rPPG的原理~~Whatever, 我们这里先不谈这个，作者开心就好~~

最后我们来说说 Loss function，主要由三部分组成:

\1. 二值分类误差（活体or非活体） 

\2. 每帧深度图的 L1 loss

\3. 作者自己提出的每帧深度图的 contrasive depth regression loss:

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/dp9t3cqn7r.png?imageView2/2/w/1620)

目的是更好学到每个 pixel 的拓扑关系，更强约束其与周边neighbor的对比度。对应的Kernel如下图所示：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/ibii84v7he.jpeg?imageView2/2/w/1620)

图6.对比深度损失的Kernel

实验结果

在Oulu-NPU上的结果：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/6x3fjn8fjh.jpeg?imageView2/2/w/1620)

图7. Oulu-NPU结果 

FAS-BAS 指的是 MSU文章[2] 的方法，可见京东的方法用单纯的Depth，还是要比MSU的 rPPG+Depth 方法性能要好~~

接着我们来看看网络里各个模块及Loss的作用：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/g8bc8mqgv5.jpeg?imageView2/2/w/1620)

可见 OFF-Block 和 Contrastive Depth loss 的作用还是蛮大的~~

最后来定性可视化下出来的深度图的可判别性如何：

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/jynf1c3wjx.jpeg?imageView2/2/w/1620)

图8. 深度图可视化

使用多帧来重构的深度图，对于Replay屏幕攻击有明显的改善。对于Print打印攻击，好像还更糟糕了一点。

总结与展望未来

文章给出了很好的思路和结论来使用多帧，这也是继MSU使用多帧来预测rPPG频域后的一大进步，这样未来face anti-spoofing将更多focus在多帧上；而不是单帧深度，单帧color texture~~

未来展望的话，可以看看其他图像预测深度图的文章如字节跳动DeepLens[3]等等，来激发灵感用于活体的任务~~另外正如前面综述所说，探索脸部微变化如rPPG等，和结合人脸检测，[人脸识别](https://cloud.tencent.com/product/facerecognition)，人脸微表情等任务来找关联性都是值得探索~~

Reference:

[1] Zezheng Wang et al. Exploiting temporal and depth information for multi-frame face anti-spoofing, 2018

[2] Yaojie Liu, Amin Jourabloo, Xiaoming Liu, Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision ，CVPR2018

[3]LIJUN WANG et al. DeepLens: Shallow Depth Of Field From A Single Image, 2018