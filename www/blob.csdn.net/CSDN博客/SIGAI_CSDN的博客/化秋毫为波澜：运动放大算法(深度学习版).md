
# 化秋毫为波澜：运动放大算法(深度学习版) - SIGAI_CSDN的博客 - CSDN博客
# [SIGAI_CSDN的博客](https://blog.csdn.net/sigai_csdn)


[博客首页](https://blog.csdn.net/SIGAI_CSDN)
[关于我们](https://me.csdn.net/SIGAI_CSDN)

2018年10月17日 16:32:04[SIGAI_csdn](https://me.csdn.net/SIGAI_CSDN)阅读数：281


> 什么是运动放大(Motion Magnification)？

> 将视频中对应位置的运动进行放大，简单理解的话，就是找到时间段内的运动矢量，进行放大，然后权值叠加回去。

> 为什么需要运动放大？

> 因为很多自然界或者生物的 subtle behaviour 不易被肉眼察觉（如飞机翼的震动，受风影响摇晃的建筑，生物皮肤变化等等），这些微变化只有通过运动放大，才能更好地被机器或者人类来做后续的视频视觉任务。

> 运动放大的难点？

> 如何在运动放大的同时，尽量保持 apperance 不变？如何不引入大量噪声？ 如何保证放大后的动作平滑？没有现存的数据集来训练？

## 传统方法的发展历程：
MIT在2012年首次提出了 Eulerian Video Magnification[1] ，第一次实时且相对鲁棒地应用到一些场景，如远程心率脉搏提取，记得多年前看到宣传的video是非常地震撼~~因为之前做运动放大，都不是用Eulerian方法，而是用Lagrangian视角去做(即运动估计，tracking啥的，非常耗时)
问题描述如下：
![\hat{I}(x,t)=f(x+(1+\alpha)\delta(t))](https://www.zhihu.com/equation?tex=%5Chat%7BI%7D(x%2Ct)%3Df(x%2B(1%2B%5Calpha)%5Cdelta(t)))
原始信号![I(x,t)](https://www.zhihu.com/equation?tex=I(x%2Ct))表示图像在位置![x](https://www.zhihu.com/equation?tex=x)和时刻![t](https://www.zhihu.com/equation?tex=t)t的亮度值，而![\delta(t)](https://www.zhihu.com/equation?tex=%5Cdelta(t))表示运动偏差。目标就是通过调整运动放大系数![\alpha](https://www.zhihu.com/equation?tex=%5Calpha)来生成放大后的信号![\tilde{I}(x,t)](https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D(x%2Ct))
文中通过实验发现，temporal filter可以模拟 spatial translation，故问题就简化为 提空间特征+设计时间维度上的滤波器。
算法的流程如下：
1.对视频每一帧都进行拉普拉斯金字塔处理，得到Multi-scale的边缘及形状描述
2. 对每个scale的特征voxel进行pixel-wise 时间上的带通滤波，增强感兴趣频率上的信号，过滤掉不感兴趣频率的噪声
3. 对filtered完的信号进行运动放大，叠加回滤波前的特征voxel；最后将金字塔重构融合。
![](https://img-blog.csdn.net/20181017161952547?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)Eulerian 运动放大框架[1]
该方法的cons：
1. 滤波器只能抑制某些频率的噪声，但乘以运动放大系数后，在带通频段的噪声也会放大
2. 若物体本身非静止，而在运动，该放大算法生成的图很模糊
故根据以上不足，后面又有两个经典的变形：
2013年MIT又提出了 Phase-based 运动放大[2]，使用了complex-valued steerable pyramids 来得到相位图再进行滤波，抗噪能力更强；
CVPR2017，Zhang et al.提出了 Acceleration Magnification[3]，设计了二阶加速度滤波器，使生成的视频在 large motion 下效果更好。

## Learning-based 运动放大[4] ECCV2018 oral
难得的好文章，不像很多文章直接设计个网络，设计个loss完事~~作者对传统方法的每个stage认知很深，然后把深度学习只是当成一个工具，来复现改造传统方法的每个stage。
Main Contribution:
1. 第一个使用深度学习端到端地进行运动放大，不需要手工设计特征，从数据里自主学到滤波器。
2. 在无现有数据集训练的情况下，利用COCO和VOC生成数据来训练。
3. 与传统方法的temporal filter相兼容，且在运动系数适中的情况下，效果比单独使用全局Learning的效果还好。
再看回同一条公式：![\tilde{I}(x,t)=f(x+(1+\alpha)\delta(x,T))](https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D(x%2Ct)%3Df(x%2B(1%2B%5Calpha)%5Cdelta(x%2CT)))
同一个目标：通过网络来学习前后![\tilde{I}(x,t)=f(x+(1+\alpha)\delta(x,T))](https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D(x%2Ct)%3Df(x%2B(1%2B%5Calpha)%5Cdelta(x%2CT)))帧(![X_{a}, X_{b}](https://www.zhihu.com/equation?tex=X_%7Ba%7D%2C%20X_%7Bb%7D)的空间特征，然后通过两帧特征差异来得到![\delta(x,t)](https://www.zhihu.com/equation?tex=%5Cdelta(x%2Ct))；接着通过对感兴趣部分的运动特征进行放大![\alpha](https://www.zhihu.com/equation?tex=%5Calpha)叠加回前帧![X_{a}](https://www.zhihu.com/equation?tex=X_%7Ba%7D)；最后生成运动放大的后帧![\tilde{Y}](https://www.zhihu.com/equation?tex=%5Ctilde%7BY%7D)(即![\tilde{I}(x,t)](https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D(x%2Ct))的集合)。整体架构如下图所示：
![](https://img-blog.csdn.net/2018101716254241?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)整体框架图[4]
算法流程如下：
Stage1： 设计Encoder 进行形状特征和纹理特征的提取，类比文章[1]中的拉普拉斯金字塔提取的多尺度边缘形状特征。给定前后帧(![X_{a}, X_{b}](https://www.zhihu.com/equation?tex=X_%7Ba%7D%2C%20X_%7Bb%7D))，通过网络![G_{e}](https://www.zhihu.com/equation?tex=G_%7Be%7D)可以得到它们各自的形状特征![M_{a}](https://www.zhihu.com/equation?tex=M_%7Ba%7D),![M_{b}](https://www.zhihu.com/equation?tex=M_%7Bb%7D)和纹理特征![V_{a}](https://www.zhihu.com/equation?tex=V_%7Ba%7D),![V_{b}](https://www.zhihu.com/equation?tex=V_%7Bb%7D)。这里的纹理特征不进行运动放大，主要用于后续约束Intensity放大导致的噪声。
![](https://img-blog.csdn.net/20181017162250853?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)Encoder[4]
Stage2： 把前后帧的形状特征送进 Manipulator，来模拟文章[1]中带通滤波器及运动放大，权值叠加的过程。这里的修改器![G_{m}](https://www.zhihu.com/equation?tex=G_%7Bm%7D)G_{m} 有线性和非线性两种表达：
![G_{m}(M_{a},M_{b},\alpha)=M_{a}+\alpha(M_{b}-M_{a})](https://www.zhihu.com/equation?tex=G_%7Bm%7D(M_%7Ba%7D%2CM_%7Bb%7D%2C%5Calpha)%3DM_%7Ba%7D%2B%5Calpha(M_%7Bb%7D-M_%7Ba%7D))
![G_{m}(M_{a},M_{b},\alpha)=M_{a}+h(\alpha\cdot g(M_{b}-M_{a}))](https://www.zhihu.com/equation?tex=G_%7Bm%7D(M_%7Ba%7D%2CM_%7Bb%7D%2C%5Calpha)%3DM_%7Ba%7D%2Bh(%5Calpha%5Ccdot%20g(M_%7Bb%7D-M_%7Ba%7D)))
而下图网络给出的修改器网络是基于非线性表达，就是加了![g(\cdot)](https://www.zhihu.com/equation?tex=g(%5Ccdot))和![h(\cdot)](https://www.zhihu.com/equation?tex=h(%5Ccdot))运算，文中给出的实验结果图是非线性的效果更好，具体也没解释。个人觉得非线性能更好地模拟带通滤波器的功能，能放大部分更感兴趣的，过滤掉一部分不太感兴趣的~~
![](https://img-blog.csdn.net/20181017162653902?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)Manipulator[4]
Stage3： Decoder，来模拟文章[1]中金字塔重构融合过程。这里解码器需要用到放大后的形状特征![M_{a}](https://www.zhihu.com/equation?tex=M_%7Ba%7D)及前帧的纹理特征![V_{a}](https://www.zhihu.com/equation?tex=V_%7Ba%7D)
![](https://img-blog.csdn.net/20181017162822792?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)Decoder[4]
讲完了网络结构，来说说 Loss function，共由4部分loss组成，都用L1距离来作为度量。为了让Encoder能区分地学出形状特征和纹理特征，随机地加了color intensity扰动在某些帧上，这里 perturbed frames 及 其运动放大的 groundtruth 分别为![X_{b}^{'}](https://www.zhihu.com/equation?tex=X_%7Bb%7D%5E%7B%27%7D)和![Y^{'}](https://www.zhihu.com/equation?tex=Y%5E%7B%27%7D)
![L1(Y,\tilde{Y})](https://www.zhihu.com/equation?tex=L1(Y%2C%5Ctilde%7BY%7D))约束生成的运动放大图![\tilde{Y}](https://www.zhihu.com/equation?tex=%5Ctilde%7BY%7D)与 groundtruth![Y](https://www.zhihu.com/equation?tex=Y)尽量一致
![L1(V_{a},V_{b})](https://www.zhihu.com/equation?tex=L1(V_%7Ba%7D%2CV_%7Bb%7D))约束前后帧的纹理表达尽量一致
![L1(V_{b}^{'},V_{Y}^{'})](https://www.zhihu.com/equation?tex=L1(V_%7Bb%7D%5E%7B%27%7D%2CV_%7BY%7D%5E%7B%27%7D))约束运动放大前后的扰动帧的纹理表达尽量一致
![L1(M_{b},M_{b}^{'})](https://www.zhihu.com/equation?tex=L1(M_%7Bb%7D%2CM_%7Bb%7D%5E%7B%27%7D))约束扰动前后的形状表达尽量一致
故总Loss为：
![Loss = L1(Y,\tilde{Y}) + \lambda(L1(V_{a},V_{b})+L1(V_{b}^{'},V_{Y}^{'})+L1(M_{b},M_{b}^{'}))](https://www.zhihu.com/equation?tex=Loss%20%3D%20L1(Y%2C%5Ctilde%7BY%7D)%20%2B%20%5Clambda(L1(V_%7Ba%7D%2CV_%7Bb%7D)%2BL1(V_%7Bb%7D%5E%7B%27%7D%2CV_%7BY%7D%5E%7B%27%7D)%2BL1(M_%7Bb%7D%2CM_%7Bb%7D%5E%7B%27%7D)))
两种不同模式 与 基于时间维度滤波的修改器：
Static mode：这是最原始的公式定义的形式，总是拿第一帧作为anchor，则送进网络的两帧为 (![X_{0}](https://www.zhihu.com/equation?tex=X_%7B0%7D),![X_{t}](https://www.zhihu.com/equation?tex=X_%7Bt%7D))
Dynamic mode: 总是把当前帧和上一帧作为网络的输入 (![X_{t-1}, X_{t}](https://www.zhihu.com/equation?tex=X_%7Bt-1%7D%2C%20X_%7Bt%7D)）
Temporal filtering based manipulator：
在修改器中，不需要再进行![M_{a}](https://www.zhihu.com/equation?tex=M_%7Ba%7D)和![M_{b}](https://www.zhihu.com/equation?tex=M_%7Bb%7D)的特征差异运算；直接类似文章[1]中的 pixel-wise temporal filter 操作，进行时间维度的带通滤波 (这环节不需要训练，是在静态或者动态模式训练好的网络基础上，直接接上filters前向传播即可) 。公式如下：
![G_{m},temporal(M(t),\alpha)=M(t)+\alpha\tau(M(t))](https://www.zhihu.com/equation?tex=G_%7Bm%7D%2Ctemporal(M(t)%2C%5Calpha)%3DM(t)%2B%5Calpha%5Ctau(M(t)))
具体关于生成的数据集如何carefully designed，感兴趣的同志自己找回原文看了，主要围绕几个因素去设计：多变的前景和背景；低对比度纹理；全局运动；静态场景；不同放大系数的运动；亚像素运动 等等。
实验结果：
如下图，可见放大8倍情况下，基于学习的算法，比Phase-based[2]的结果要少很多artifacts，也不那么blurred
![](https://img-blog.csdn.net/20181017162902907?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)Decoder[4]
再如下图，在不加 Temporal filter 的静态模式下，结果也不错，基本没有人工的噪声和blur；而加了 Temporal filter 后的结果更加平滑稳定。
![](https://img-blog.csdn.net/20181017162941876?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)加上Temporal Filter的效果[4]
通过可视化encoder中的kernel可以发现，学到的形状特征更多是基于 Gabor-like, Laplacian-like, Corner detector-like滤波器；而纹理特征对应的更多像blur kernel。证明设计的loss约束引导的learning是很有效很成功的~~
![](https://img-blog.csdn.net/20181017163039957?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)Encoder形状核[4]
![](https://img-blog.csdn.net/20181017163107757?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)Encoder纹理核[4]
总结展望：
文中作者也说了，该网络一大惊喜就是，学出来的形状特征是呈线性的，又因为线性 temporal filter 与运动偏移是兼容的，故形状特征也能兼容 temporal filter。但是在 放大倍数很大的情况下 (如50倍)，使用 temporal filter 的效果并不好，图像质量退化严重。
故未来方向之一是，如何使temporal filter更好地兼容网络。从我个人来看，是否可以直接在manipulator 里设计个temporal pooling的小网络来自适应地实现 temporal filter 的功能？当然啦，这样输入的话就很多帧，而不是 two frames 了。。。
Reference:
[1] Hao-Yu Wu, Eulerian Video Magnification for Revealing Subtle Changes in the World, 2012
[2]Neal Wadhwa, Phase-Based Video Motion Processing, 2013
[3]Yichao Zhang, Video Acceleration Magnification，CVPR2017
[4]Tae-Hyun Oh, Learning-based Video Motion Magnification，ECCV2018


