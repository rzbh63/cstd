
# 线性回归损失函数为什么要用平方形式 - saltriver的专栏 - CSDN博客


2017年02月26日 23:10:37[saltriver](https://me.csdn.net/saltriver)阅读数：12321标签：[线性回归																](https://so.csdn.net/so/search/s.do?q=线性回归&t=blog)[损失函数																](https://so.csdn.net/so/search/s.do?q=损失函数&t=blog)[最小二乘																](https://so.csdn.net/so/search/s.do?q=最小二乘&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=损失函数&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=线性回归&t=blog)个人分类：[机器学习																](https://blog.csdn.net/saltriver/article/category/6399667)
[
																								](https://so.csdn.net/so/search/s.do?q=线性回归&t=blog)


我们在前面的《[线性回归](http://blog.csdn.net/saltriver/article/details/52423934)》中了解到，对于训练数据样本$({x_i},{y_i})$，我们有如下的拟合直线：

$$
{\widehat y_i} = {\theta _0} + {\theta _1} \bullet {x_i}
$$
我们构建了一个损失函数：

$$
C = \sum\limits_{i = 1}^n {{{({y_i} - {{\widehat y}_i})}^2}}
$$
表示每个训练数据点$({x_i},{y_i})$到拟合直线${\widehat y_i} = {\theta _0} + {\theta _1} \bullet {x_i}$的竖直距离的平方和，通过最小化这个损失函数来求得拟合直线的最佳参数$\theta$，实际上就是求损失函数C在取得最小值情况下$\theta$的值。那么损失函数为什么要用平方差形式呢，而不是绝对值形式，一次方，三次方，或四次方形式？
简单的说，是因为使用平方形式的时候，使用的是“最小二乘法”的思想，这里的“二乘”指的是用平方来度量观测点与估计点的距离（远近），“最小”指的是参数值要保证各个观测点与估计点的距离的平方和达到最小。
最小二乘法以估计值与观测值的平方和作为损失函数，在误差服从正态分布的前提下，与极大似然估计的思想在本质上是相同。对于极大似然估计，可以参考下前期文章《[极大似然估计](http://blog.csdn.net/saltriver/article/details/53364037)》。
我们设观测输出与预估数据之间的误差为：

$$
{\varepsilon _i} = {y_i} - {\widehat y_i}
$$
我们通常认为$\varepsilon$服从正态分布，即：

$$
f({\varepsilon _i};u,{\sigma ^2}) = \frac{1}{{\sigma \sqrt {2\pi } }} \bullet \exp [ - \frac{{{{({\varepsilon _i} - u)}^2}}}{{2{\sigma ^2}}}]
$$
我们求的参数$\varepsilon$的极大似然估计$(u,{\sigma ^2})$，即是说，在某个$(u,{\sigma ^2})$下，使得服从正态分布的$\varepsilon$取得现有样本${\varepsilon _i}$的概率最大。那么根据极大似然估计函数的定义，令：

$$
L(u,{\sigma ^2}) = \prod\limits_{i = 1}^n {\frac{1}{{\sqrt {2\pi } \sigma }} \bullet } \exp ( - \frac{{{{({\varepsilon _i} - u)}^2}}}{{2{\sigma ^2}}})
$$
取对数似然函数：

$$
\log L(u,{\sigma ^2}) =  - \frac{n}{2}\log {\sigma ^2} - \frac{n}{2}\log 2\pi  - \frac{{\sum\limits_{i = 1}^n {{{({\varepsilon _i} - u)}^2}} }}{{2{\sigma ^2}}}
$$
分别求$(u,{\sigma ^2})$的偏导数，然后置0，最后求得参数$(u,{\sigma ^2})$的极大似然估计为：

$$
u = \frac{1}{n}\sum\limits_{i = 1}^n {{\varepsilon _i}}
$$

$$
{\sigma ^2} = \frac{1}{n}\sum\limits_{i = 1}^n {{{({\varepsilon _i} - u)}^2}}
$$
我们在线性回归中要求得最佳拟合直线${\widehat y_i} = {\theta _0} + {\theta _1} \bullet {x_i}$，实质上是求预估值${\widehat y_i}$与观测值${y_i}$之间的误差${\varepsilon _i}$最小（最好是没有误差）的情况下$\theta$的值。而前面提到过，$\varepsilon$是服从参数$(u,{\sigma ^2})$的正态分布，那最好是均值$u$和方差$\sigma$趋近于0或越小越好。即:
$u = \frac{1}{n}\sum\limits_{i = 1}^n {{\varepsilon _i}}  = \frac{1}{n}\sum\limits_{i = 1}^n {({y_i} - {{\widehat y}_i})}$趋近于0或越小越好
${\sigma ^2} = \frac{1}{n}\sum\limits_{i = 1}^n {{{({\varepsilon _i} - u)}^2}}  = \frac{1}{n}\sum\limits_{i = 1}^n {{{({y_i} - {{\widehat y}_i} - u)}^2}}  \approx \frac{1}{n}\sum\limits_{i = 1}^n {{{({y_i} - {{\widehat y}_i})}^2}}$趋近于0或越小越好。
而这与最前面构建的平方形式损失函数本质上是等价的。

