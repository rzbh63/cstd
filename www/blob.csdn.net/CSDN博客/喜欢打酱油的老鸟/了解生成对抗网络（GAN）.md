
# 了解生成对抗网络（GAN） - 喜欢打酱油的老鸟 - CSDN博客


2019年01月16日 08:09:17[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：17



2019-01-08 23:29:25
介绍
什么是Generative Adversarial Networks（生成对抗性网络）？本文中，我们将看到对抗性训练是一种很有启发性的想法，其简单而实用，这代表了机器学习的真正概念进展，尤其是生成模型。
在进入细节之前，让我们快速概述一下GANs的用途。生成对抗性网络属于一组生成模型集。这意味着他们能够生成新的内容。为了说明这种“生成模型”的概念，我们可以看一些用GANs获得结果的著名的例子。
![了解生成对抗网络（GAN）](http://p9.pstatp.com/large/pgc-image/634604de44ad4d17931ccc0bcf3e46ef)
这些是由GANs在对两个数据集进行训练后生成的样本：MNIST和TFD。
当然，这种生成新内容的能力使GANs看起来有点“神奇”。在以下部分中，我们将深入了解这些模型背后的想法，数学和建模。我们不仅会讨论GANs所依赖的基本概念，我们还会一步一步地构建这些概念，并从一开始就对这些概念进行推理。
> 注意：尽管我们试图使本文尽可能独立，但仍需要具备机器学习的基本先验知识。
目录
1、我们将讨论从给定的分布生成随机变量的过程。
2、我们将通过一个例子展示GANs试图解决的问题可以表示为随机变量生成问题。
3、我们将讨论基于匹配的生成网络。
4、我们将呈现带有损失函数的通用架构，并且与之前的所有部分建立连接。
生成随机变量
我们先讨论生成随机变量的方法：逆变换方法，它允许从简单的均匀随机变量生成复杂的随机变量。我们将在后面看到与生成模型存在的深层联系。
随机变量可以是伪随机生成的
从理论上讲，生成真正随机的数字是不可能的。但是，可以定义生成数字序列的算法，其特征非常接近理论随机数序列的属性。特别是，计算机能够使用伪随机数生成器生成一个数字序列，该数字序列近似地遵循0和1之间的均匀随机分布。均匀的情况是非常简单的，可以以不同的方式建立更复杂的随机变量。
随机变量表示的是运算或过程的结果
有不同的技术可以生成更复杂的随机变量。我们可以找到逆变换方法，拒绝采样法，Metropolis-Hasting算法等。所有这些方法都依赖于不同的数学技巧，这些技巧主要包括表示我们希望作为运算或过程的结果生成的随机变量。
拒绝采样是针对复杂问题的一种随机采样方法，拒绝采样是指不从复杂分布中采样，而是从已知的简单分布中采样，并根据一定条件接受或拒绝采样值的过程的结果。重复这个过程直到采样值被接受，我们可以证明在接受的条件正确的情况下，有效采样的值将遵循正确的分布。
在Metropolis-Hasting算法中，使用的是马尔可夫链蒙特卡罗（MCMC）方法，用于从难以直接采样的概率分布中获得随机样本序列。该序列可用于近似分布或计算积分。我们可以使用马尔科夫链（MC），使得MC的平稳分布对应于我们需要从中抽样的随机变量的分布，一旦找到这个MC，考虑到我们已经达到了一个稳定的状态，我们就可以在这个MC上模拟出足够长的轨迹，然后我们以这种方式获得的最后一个值可以被认为是从有用的分布中得出的。
逆变换 方法
逆变换方法的概念仅仅是为了表示我们的复杂，而不是数学意义中的随机变量最为函数应用于一个我们知道如何生成的均匀随机变量的结果。
在下面的一维例子中。设X是我们想要从中采样的复杂随机变量，U是[0,1]上的均匀随机变量，我们知道如何从中采样。我们赋予随机变量由其累积分布函数（CDF）完全定义。随机变量的CDF是从随机变量的定义域到区间[0,1]的函数，并且在一个维度中定义，例如：
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/65164944d26c420b8eed4bda5be7c52b)
在特定情况下U是我们的均匀随机变量：
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/6ed93db3b3d4470380f4dd17cb08a497)
为简单起见，我们假设函数CDF_X是可逆的：
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/68d5bbf7297d42cbb34b373a3c182f5a)
（这个方法可以通过使用泛化的逆函数来简单地扩展到非可逆的例子但这并不是我们想要关注的重点）。然后：
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/3a8cc9f8a06f4ba6a3801a8c6d737922)
我们可以得到：
![了解生成对抗网络（GAN）](http://p1.pstatp.com/large/pgc-image/a053349795d7404da06524590c8317d5)
我们可以看到，Y和X具有相同的CDF，然后定义相同的随机变量。因此，通过如上定义Y（作为均匀随机变量的函数），我们设法定义了具有目标分布的随机变量。
综上所述，逆变换方法是通过使均匀随机变量经过精心设计的“变换函数”（逆CDF）来生成遵循给定分布的随机变量的方式。事实上，这种“逆变换方法”的概念可以扩展到“变换方法”的概念，“变换方法”更简单地说，在生成随机变量作为一些更简单的随机变量的函数时（不一定是均匀的，然后变换函数是不再是逆CDF）。从概念上讲，“变换函数”的目的是使初始概率分布变形/重塑：变换函数从初始概率分布相对于目标概率分布过高的位置，并将其置于初始概率分布过低的位置。
![了解生成对抗网络（GAN）](http://p9.pstatp.com/large/pgc-image/d50bbc81f4c84d2689e70ae38a79bde8)
逆变换方法的图示。蓝色：均匀分布在[0,1]上。橙色：标准高斯分布。灰色：从均匀到高斯分布的映射（逆
**生成模型**
我们试图生成非常复杂的随机变量......
假设我们生成大小为n*n像素的狗的黑白方形图像。我们可以将每个数据重新塑造为N = n×n维向量（通过在彼此之上堆叠），使得狗的图像可以由向量表示。然而，这并不意味着所有的向量都代表了曾经被塑造成方形的狗！因此，我们可以说，在N维空间的向量空间中，可以有效地生成一种看起来像条狗的图形，这是根据一种特定的概率分布在整个N维向量空间上的。本着同样的精神，在这个N维向量空间上存在猫，鸟等图像的概率分布。
然后，生成狗的新图像的问题等同于在N维向量空间上按照“狗概率分布”生成一个的新向量的问题。事实上，我们面临的问题是根据特定的概率分布生成一个随机变量。
在这一点上，我们可以提到两件重要的事情。首先，我们提到的“狗概率分布”是在非常大的空间内非常复杂的分布。其次，即使我们可以假设存在这样的基础分布，但我们显然不知道如何明确地表达这种分布。之前的两点都使得从该分布生成随机变量的过程非常困难。但让我们尝试着解决这两个问题。
使用神经网络的变换方法作为函数！
当我们尝试生成狗的新图像时，我们的第一个问题是N维向量空间上的“狗概率分布”是一个非常复杂的问题，我们不知道如何直接生成复杂的随机变量。然而，正如我们非常清楚如何生成N个不相关的均匀随机变量，我们可以利用变换方法。为此，我们需要将N维随机变量表示为将非常复杂的函数应用于简单的N维随机变量的结果！
这里，我们可以强调的事实是，发现变换函数并不像我们在描述逆变换方法时所做的那样简单。变换函数无法明确表达，因此，我们必须从数据中学习它。
在大多数情况下，非常复杂的函数自然意味着神经网络建模。然后，通过神经网络对变换函数进行建模，该神经网络将一个简单的N维均匀随机变量作为输入，并将其作为输出返回另一个N维随机变量，该随机变量在训练后应遵循正确的“dog概率分布” 。网络架构设计完成后，我们仍然需要对其进行训练。后面，我们将讨论训练这些生成网络的两种方法，包括GANs背后的对抗训练的概念。
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/aef57f9907804c1d8fb0585af5eea121)
使用神经网络的生成模型概念的图。显然，我们真正谈论的维度远高于此处所表示的维度。
**生成匹配网络**
**训练生成模型**
到目前为止，我们已经证明了我们生成狗的新图像的问题可以被重新描述为在N维向量空间中生成遵循“狗概率分布”的随机向量的问题，并且我们建议使用变换方法，用神经网络来模拟变换函数。
现在，我们仍然需要训练（优化）网络以表达正确的变换函数。我们选择两种不同的训练方法：直接训练方法和间接训练方法。直接训练方法是比较真实概率分布和生成的概率分布，并通过网络反向传播差异（误差）。这是规则生成匹配网络（GMNs）的想法。对于间接训练方法，我们不直接比较真实和生成的概率分布。相反，我们训练生成网络，让这两个分布通过选择一个下游任务，这样生成网络相对于下游任务的优化过程将强制生成的分布接近真实的分布。
**比较基于样本的两个概率分布**
如上所述，GMN的概念是通过直接比较生成分布和真实分布来训练生成网络。如果我们有一种比较基于样本的概率分布的方法，我们可以使用它来训练网络。实际上，我们有一个真实数据样本，我们可以在训练过程的每次迭代中生成一个生成数据的样本。
虽然理论上可以使用任何基于样本的距离(或相似性度量)来有效地比较两个分布，但我们可以特别提到最大均值差异（MMD）方法。MMD定义了可以基于这些分布的样本计算（估计）的两个概率分布之间的距离。
**分布匹配误差的反向传播**
如果我们定义了一种基于样本比较两种分布的方法，我们就可以定义GMN中生成网络的训练过程。给定具有均匀概率分布的随机变量作为输入，我们希望所生成的输出的概率分布是“狗概率分布”。然后，GMN的想法是通过重复以下步骤来优化网络：
产生一些统一的输入
使这些输入通过网络并收集生成的输出
比较真实的“狗概率分布”和基于可用样本生成的一个
使用反向传播来进行梯度下降的一个步骤，以降低真实分布和生成分布之间的距离（例如MMD）
如上所述，当遵循这些步骤时，我们在网络上应用梯度下降，其具有损失函数，该函数是当前迭代时真实分布与生成分布之间的距离。
![了解生成对抗网络（GAN）](http://p9.pstatp.com/large/pgc-image/ebf250f92bcf429c8eb6177ee2b67d48)
生成匹配网络采用简单的随机输入，生成新数据，直接比较生成数据的分布与真实数据的分布，并反向传播匹配误
**生成对抗网络**
**“间接”训练方法**
上面提出的“直接”方法在训练生成网络时直接比较生成分布与真实分布。规则GAN的好处在于用间接的比较替换这种直接比较，后者采用这两种分布的下游任务的形式，然后对该任务进行生成网络的训练，使得它迫使所生成的分布越来越接近真实分布。
GANs的下游任务是区分真样本和生成样本的任务。因此，在GAN架构中，我们有一个鉴别器，它从真实的和生成的数据中提取样本并尽可能地对它们进行分类，还有一个生成器，它被训练成尽可能地欺骗鉴别器。
**理想的情况：完美的生成器和鉴别器**
为了更好地理解为什么训练生成器来欺骗鉴别器会得到与直接训练生成器匹配目标分布相同的结果，让我们采用一个简单的一维示例。我们暂时忘记掉如何表示生成器和鉴别器，并将它们视为抽象概念。此外，两者都被认为是“完美的”，因为它们不受任何类型（参数化）模型的约束。
假设我们有一个真正的分布，例如一维高斯分布，并且我们想要一个从这个概率分布中采样的生成器。我们所谓的“直接”训练方法将包括迭代地调整生成器（梯度下降迭代）以校正真实分布和生成分布之间的测量差异/误差。最后，假设优化过程完美，我们应该最终得到与真实分布完全匹配的生成分布。
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/2a9379d5257c4c66a0d2aeb0a4f995ea)
直接匹配方法的概念的例证。蓝色的分布是真实的，而生成的分布用橙色表示。通过逐次迭代，我们比较两个分布
对于“间接”方法，我们还必须考虑一个鉴别器。我们现在假设这个鉴别器是一种oracle，它确切知道什么是真实和生成的分布，并且能够根据这些信息预测任何给定点的类（“真”或“生成”）。如果这两个分布很明显，那么鉴别器将能够轻松地进行分类，并且可以对我们提供给它的大多数点进行分类。如果我们想欺骗鉴别器，我们必须使生成的分布接近真实的分布。当两个分布在所有点上相等时，鉴别器将很难预测类：
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/969a437804c648b48120c25f4668e0ee)
对抗性方法的直觉。
蓝色分布是真实的，橙色是生成的分布。在灰色中，右边有相应的y轴，如果它选择每个点中密度较高的类（假设“真实”和“生成”数据的比例相等），我们就会显示鉴别器的概率为真。两个分布越接近，鉴别器就越容易出错。在训练时，目标是将“绿色区域”移向“红色区域”。
在这一点上，似乎有理由怀疑这种间接方法是否真的是一个好方法。实际上，它似乎更复杂。对于第一点，直接比较基于样本的两个概率分布的难度抵消了间接方法的明显较高的复杂性。对于第二点，很明显，鉴别器是未知的。
**对抗性神经网络**
现在让我们描述采用GANs架构中的生成器和鉴别器的具体形式。生成器是一个模拟转换函数的神经网络。它将一个简单的随机变量作为输入，并且必须在训练后返回一个遵循目标分布的随机变量。由于鉴别器结构复杂且未知，我们决定用另一种神经网络对其进行建模。这个神经网络模型具有辨别功能。它将一个点作为输入，并将该点的概率作为输出返回为“真”。
现在我们强加一个参数化模型来表达生成器和鉴别器，实际上并没有对上面给出的理论产生影响：我们只是在一些参数化空间而不是理想的空间中工作，因此，在理想情况下我们应达到的最佳点可以被看作是“圆”，是由参数化模型的精确性来判定的。
一旦确定，这两个网络就可以(同时)以相反的目标联合训练：
生成器的目标是欺骗鉴别器，因此训练生成神经网络使最终分类误差最大化（真实数据和生成数据之间）
鉴别器的目标是检测假的生成数据，因此训练判别神经网络使最终分类误差最小化
因此，在训练过程的每次迭代中，生成网络的权重都会更新，以便增加分类误差，同时更新判别网络的权重以减少分类误差。
![了解生成对抗网络（GAN）](http://p1.pstatp.com/large/pgc-image/b22732dc147d46a6b680f8bb70c78d97)
生成性对抗网络表示
生成器将简单随机变量作为输入并生成新数据。鉴别器采用“真实”和“生成”数据并尝试区分它们，构建分类器。生成器的目标是欺骗鉴别器（通过将尽可能多的生成数据与真实数据混合来增加分类错误），鉴别器的目标是区分真实数据和生成数据。
这些相反的目标和两个网络的对抗性训练的隐含概念解释了“对抗性网络”的名称：两个网络都试图相互击败对方，这样做的目的是让对方都变得越来越好。他们之间的竞争使这两个网络在各自的目标方面“进步”。从博弈论的角度来看，我们可以将此设置视为极小极大双玩家游戏，其中均衡状态对应于发生器从精确目标分布产生数据并且鉴别器预测“真实”或“生成”的情况，它接收的任何一点的概率为1/2。
关于GAN的数学方面
神经网络建模本质上需要定义两件事：架构和损失函数。我们已经描述了GANs的架构。它包含两个网络：
生成网络G，随机输入z定义为Pz，并返回一个输出Xg= G（z），该输出应该遵循（训练后）目标概率分布
判别网络D，输入X可以是“ 真实”的（Xt，用Pt表示）或“生成的”Xg，（用Pg表示是由Pz经过G输出的）并将X的概率D（x）返回为“真实”数据
现在让我们仔细看看GAN的“理论”损失函数。如果我们以相同的比例向鉴别器发送“真实”和“生成”的数据，则鉴别器的预期绝对误差可以表示为
![了解生成对抗网络（GAN）](http://p1.pstatp.com/large/pgc-image/45004b695bdf4075b01bc1c2017462c5)
生成器的目标是欺骗鉴别器，其目标是能够区分真实数据和生成数据。因此，在训练生成器时，我们希望误差最大化，同时我们想要使鉴别器的误差最小化。
![了解生成对抗网络（GAN）](http://p1.pstatp.com/large/pgc-image/31622e4197f845139273249418b1817e)
对于任何给定的生成器G，最佳可能的鉴别器是最小化的鉴别器
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/689243fc81ec4f3ebcf07593a784d3be)
为了最小化(关于D)这个积分，我们可以最小化每个x值在积分内的函数，然后定义给定生成器的最佳鉴别器
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/b287e1dc74f0443ba12b450fe064c261)
然后我们搜索G最大化
![了解生成对抗网络（GAN）](http://p1.pstatp.com/large/pgc-image/06affc61516b47139f40d333c9bedae8)
同样，为了最大化G，我们可以最大化X的每个值的积分内的函数。
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/94d87f8d109d4c3c856b56243d2d3493)
当然，由于Pg是应该与1整合的，我们必然拥有最佳的G
![了解生成对抗网络（GAN）](http://p1.pstatp.com/large/pgc-image/78eeddda31964191b0e9df2ada98d1cf)
因此，我们已经证明，在具有无限容量生成器和鉴别器的理想情况下，对抗性设置的最佳点使得生成器产生与真实数据点相同的数据点，并且鉴别器不能比真实的更好。最后，还要注意G最大化
![了解生成对抗网络（GAN）](http://p3.pstatp.com/large/pgc-image/69602723bb8e4ebaa294d425e817dc5d)
在这种形式下，我们更好地看到G想要最大化鉴别器出错的预期概率。
**本文的主要内容是：**
计算机基本上可以生成简单的伪随机变量（例如，它们可以生成非常接近均匀分布的变量）
存在不同的方法来生成更复杂的随机变量，包括“变换方法”的概念，其包括将随机变量表示为一些更简单的随机变量的函数。
在机器学习中，生成模型试图从给定（复杂）概率分布生成数据
深度学习生成模型被建模为神经网络（非常复杂的函数），它将一个简单的随机变量作为输入，并返回一个遵循目标分布的随机变量（“变换方法”）
这些生成网络可以“直接”训练（通过比较生成数据与真实分布的分布）：这就是生成匹配网络的想法
这些生成网络也可以“间接”训练（通过试图欺骗同时训练的另一个网络来判断“生成的”数据和“真实”数据）：这就是生成对抗网络的想法

