
# 吴恩达机器学习笔记 —— 5 多变量线性回归 - 喜欢打酱油的老鸟 - CSDN博客


2018年08月05日 08:52:24[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：79


[http://www.cnblogs.com/xing901022/p/9321045.html](http://www.cnblogs.com/xing901022/p/9321045.html)
本篇主要讲的是多变量的线性回归，从表达式的构建到矩阵的表示方法，再到损失函数和梯度下降求解方法，再到特征的缩放标准化，梯度下降的自动收敛和学习率调整，特征的常用构造方法、多维融合、高次项、平方根，最后基于正规方程的求解。
更多内容参考机器学习&深度学习
在平时遇到的一些问题，更多的是多特征的
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_1.png)
多变量的表示方法
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_2.png)
多元线性回归中的损失函数和梯度求解
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_3.png)
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_4.png)
有时候特征各个维度是不同规模的，比如房间的平米数和房间数，两个数量级相差很大。如果不丛任何处理，可能导致梯度优化时的震荡。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_5.png)
一般如果特征时在可接受的范围内，是不需要做特征缩放的。如果很大或者很小，就需要考虑进行特征的缩放了。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_6.png)
标准化，即
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_7.png)
自动收敛测试：如果梯度在优化后变化很小，比如10^-3，那么就认为梯度优化已经收敛。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_8.png)
如果发现误差在不断的增加或者不断的抖动，那么应该减小学习率，这一版都是由于学习率过大导致的震荡。但是如果学习率设置的很小，收敛的速度又会很慢。一般都是采用不同的学习率来测试，比如0.001, 0.01, 0.1, 1 ....
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_9.png)
有的时候我们选择的特征，并不是直接使用数据，而是通过数据拟合出新的特征。比如我们有房子的长宽，但是使用特征的时候，可以构造出一个面积特征，会更有效果。
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_10.png)
通过x构造新的特征替换高维特征
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_11.png)
如果不希望房子的价格出现下降，可以构造平方根的特征：
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_12.png)
基于正规方程解
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_13.png)
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_14.png)
基于梯度下降和正规方程的区别
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_15.png)
如果特征之间共线，会导致矩阵不可逆
![](https://images.cnblogs.com/cnblogs_com/xing901022/1187174/o_16.png)

