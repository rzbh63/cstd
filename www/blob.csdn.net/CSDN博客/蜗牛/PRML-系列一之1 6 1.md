
# PRML-系列一之1.6.1 - 蜗牛 - CSDN博客


2015年05月16日 14:36:28[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：762


## 相关熵和交互信息
到目前为止，我们已经介绍了一些信息论的概念，包括熵的关键概念。我们现在开始这些想法到模式识别。考虑未知分布p（x），假设我们用近似分布q（x）为期建模。为了发送x的值到接收器，如果我们用q（x）来构造编码方案，那么指定x值所需的平均信息附加量（假设我们选择了一个高效的编码方案）由下式给出
![这里写图片描述](https://img-blog.csdn.net/20150516132202149)[ ](https://img-blog.csdn.net/20150516132202149)
这被称为分布p（x）和q（x）之间的相对熵或Kullback-Leibler散度或KL散度（Kullback和Leibler，1951）。注意它不是一个对称量，即![这里写图片描述](https://img-blog.csdn.net/20150516132254035)[。 ](https://img-blog.csdn.net/20150516132254035)
我们现在展示Kullbace-Leibler散度满足![这里写图片描述](https://img-blog.csdn.net/20150516133122365)[，当且仅当p（x）=q（x）时等号成立。要做到这一点，我们首先介绍凸函数的概念。对于函数f（x），如果每个弦位于或者在函数的上面（如图1.31），那么该函数就是凸函数。在区间x=a至x= b之间的任何x值可被写成λa+（1 - λ）b的形式，其中](https://img-blog.csdn.net/20150516133122365)![这里写图片描述](https://img-blog.csdn.net/20150516133134873)[。弦上的对应点由给定λf（a）+（1 - λ）f（b）。函数的对应值是f (λa + (1 − λ)b)。凸面意味着: ](https://img-blog.csdn.net/20150516133134873)
![这里写图片描述](https://img-blog.csdn.net/20150516133527504)[ ](https://img-blog.csdn.net/20150516133527504)
这相当于函数的二阶导数都为正值。凸函数的例子是xlnx（x>0）和![这里写图片描述](https://img-blog.csdn.net/20150516134107975)[。如果只在λ=0和λ=1时等号成立，那么该函数叫做严格凸函数。如果函数具有相反的特性，即每个弦位于或低于函数值，它被称为凹，严格凹的定义类似。如果函数f（x）是凸的，则-f（x）是凹的。 ](https://img-blog.csdn.net/20150516134107975)
![这里写图片描述](https://img-blog.csdn.net/20150516133454459)[ ](https://img-blog.csdn.net/20150516133454459)
用归纳法证明方法，我们可以得到凸函数f（x）满足:
![这里写图片描述](https://img-blog.csdn.net/20150516134429309)[ ](https://img-blog.csdn.net/20150516134429309)
其中对于任何点{xi}![这里写图片描述](https://img-blog.csdn.net/20150516134812077)[。（1.115）结果被称为詹森不平等。如果我们将λI解释为离散变量x的概率分布，则（1.115）可以写成 ](https://img-blog.csdn.net/20150516134812077)
![这里写图片描述](https://img-blog.csdn.net/20150516134900858)[ ](https://img-blog.csdn.net/20150516134900858)
其中E[]表示期望。对于连续变量，詹森不等式的形式为：
![这里写图片描述](https://img-blog.csdn.net/20150516135017096)[ ](https://img-blog.csdn.net/20150516135017096)
我们可以将詹森不等式（1.117）的形式用到Kullback-Leibler散度（1.113）得到
![这里写图片描述](https://img-blog.csdn.net/20150516135253626)[ ](https://img-blog.csdn.net/20150516135253626)
在这里，我们使用事实即-ln x是一个凸函数，连同归一化条件![这里写图片描述](https://img-blog.csdn.net/20150516135519654)[。事实上，-ln x是一个严格凸函数，所以对于所有x当且仅当q（x）= p（x）时等号成立。因此，我们可以将Kullback-Leibler散度理解为两个分布p（x）和q（x）之间差异的量度。 ](https://img-blog.csdn.net/20150516135519654)
我们看到数据压缩和密度估计（即为一个未知概率分布建模的问题）之间有紧密的关系，因为当我们知道真实分布后就可以实现最有效的压缩。如果我们使用不同于真实的分布，那么我们肯定有一个效率较低的编码，并且必须传输的平均附加信息（至少）等于两个分布之间的Kullback-Leibler散度。
假设数据是从一个未知分布p（x）（我们所希望的模型）生成的。我们可以用一些参数分布q（x|θ）来尝试近似这种分布，参数分布由一组可调整的参数θ控制，例如多元高斯。确定θ的一种方法是最小化p（x）和q（x|θ）之间的Kullback-Leibler散度。我们无法直接这样做，因为我们不知道p（x）。然而，假设我们已经观察到一组有限的训练点xn，对于n =1，， ，N，从p（x）得到。那么p（x）的期望可通过这些点上的有限和（1.35）来近似得到，使得
![这里写图片描述](https://img-blog.csdn.net/20150516141652417)[ ](https://img-blog.csdn.net/20150516141652417)
（1.119）的右手侧第二项与θ无关，且第一项是负对数似然函数。因此，我们看到最小化Kullback-Leiber散度相当于最大化似然函数。
现在考虑两组变量x和y之间的联合分布p（x，y）。如果变量的集合是独立的，那么他们的联合分布将因式分解成其边缘分布p（x，y）= p（x）p（y）。如果变量不是独立的，通过考虑联合分布和边缘乘积之间的Kullback-Leibler散度，我们可以得到它们是否与独立比较接近
![这里写图片描述](https://img-blog.csdn.net/20150516142222385)[ ](https://img-blog.csdn.net/20150516142222385)
这就是所谓的变量x和y之间的交互信息。根据Kullback-Leibler散度的特性，我们可以看到![](https://img-blog.csdn.net/20150516142535529)[当且仅当x和y是独立的等号成立。使用求和与乘积规则概率，我们看到交互信息和条件熵是相关的 ](https://img-blog.csdn.net/20150516142535529)
![这里写图片描述](https://img-blog.csdn.net/20150516142506419)[ ](https://img-blog.csdn.net/20150516142506419)
因此，我们可以认为通过被告知y值交互信息减少了x的不确定性（反之亦然）。从贝叶斯的角度来看，在观察到新数据y后，我们可以将ρ（x）看做先验分布，和p（x |y）作为后验分布。因此，由于新观察y，交互信息表示x不确定性的下降。
[            ](https://img-blog.csdn.net/20150516142506419)
[
](https://img-blog.csdn.net/20150516142535529)
