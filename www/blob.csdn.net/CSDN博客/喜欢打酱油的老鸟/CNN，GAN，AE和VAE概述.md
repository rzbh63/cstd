
# CNN，GAN，AE和VAE概述 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月13日 10:38:16[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：327


[https://www.toutiao.com/a6643380572892692999/](https://www.toutiao.com/a6643380572892692999/)
2019-01-06 21:49:34
# CNN
表示卷积神经网络。这是一种特殊类型的神经网络，是为具有空间结构的数据而设计的。例如，具有自然空间顺序的图像非常适合于CNN。卷积神经网络由许多“filters”组成，它们对数据进行卷积或滑动，并在每个滑动位置产生激活。这些激活产生一个“feature map”，它表示该区域的数据激活了filter(是一个神经元)的数量。例如，假设我们有一个经过训练的filter来识别人脸，这些可能是它输出的feature map:
![CNN，GAN，AE和VAE概述](http://p1.pstatp.com/large/pgc-image/b9b8a318792245cfb80b96a30627b00d)
卷积神经网络的特殊之处在于它们在空间上是不变的，这意味着无论图像的显著部分出现在哪里，它都将被网络检测到。这是因为filter权重在图像的不同部分不会改变 - 由于filter在图像上滑动，因此图像的每个部分的权重都相同。
CNN的这种空间不变性不仅适用于二维图像，也适用于三维视频甚至一维时间序列。CNN也被认为是一种伪循环神经网络，因为filter可以在时间步中滑动，而不是数据部分，允许它基于过去的数据点做出决策。
# GAN
GAN代表Generative Adversarial Networks。这是一种生成模型，因为它们学会复制您提供的数据的数据分布，因此可以生成看起来相似的新颖图像。
GAN被称为“对抗性”的原因是因为它涉及两个相互竞争的网络。实际上，GAN通常被比作警察（discriminator）和伪造者（generator）的类比。伪造者起初不知道真正的钱是什么样的，所以它会产生一些看似完全虚假的钱。对于伪造者来说幸运的是，警察也不知道真正的钱是什么样的。
![CNN，GAN，AE和VAE概述](http://p3.pstatp.com/large/pgc-image/dc2006915ca24cb6aaa76dd43b3336e5)
但是，当我们有了真实的现金时，警察部门开始教警察真正的钱是什么样的，以及伪造者的假钱看起来是什么样的，并让他区分真假钱。
然后，造假者会做更多的练习，并学会让现金变得更真实一点，最后欺骗警察。
这个循环重复了一段时间，直到（理想情况下）警察无法分辨假现金和真实现金之间的区别，因为假现金看起来与真实现金完全一样。一旦完成，我们就可以使用generator永久地制造假现金。
让我们将其扩展到图像。generator是一个神经网络，它接收随机变量Z的向量，并产生一个图像，。
discriminator也是一个神经网络，它接收图像，并产生单个输出p（决定图像是真实的概率）。当p = 1时，discriminator相信图像是真实的，当p = 0时，discriminator认为图像是假的。
discriminator被输入生成器图像，我们将输入并且被教导图像是假的。更具体地说，discriminator最大化（1-）。然后，discriminator被输入真实图像，并且被教导图像是真实的，它最大化（）。generator则相反，并试图使discriminator最大化它认为假图像是真实的概率，因此generator试图最大化（）。一旦我们这样训练了一段时间，我们就会开始看到一些非常逼真的照片。
# AE
也称为自动编码器，它们非常简单，它们所做的只是输入，并尽可能地重现输入。如果我输入数字“2”的照片，则自动编码器应输出完全相同的照片。
![CNN，GAN，AE和VAE概述](http://p3.pstatp.com/large/pgc-image/1fe772b80001494b9df204a59b4acc5f)
这看似简单，毫无意义，但有一些有趣的属性。我们通常不仅有输入和输出层，因为网络只能将像素从输入复制到输出，这是完全没用的。我们通常在输入和输出层之间有一个（或多个）隐藏层，它们充当bottleneck层。
bottleneck可以有多种不同的方式，但我只关注最简单的方法：拥有更少的隐藏神经元。
如果隐藏层中的神经元小于输入图像中的像素数，则网络必须“压缩”它看到的数据。
这种压缩意味着只有图像最显著的特征才能保留 - 其他一切都是不必要的。在隐藏的神经元中，特征可以编码关于数据的大部分信息。
这使得自动编码器（理论上）很有用，因为如果我们的监督训练数据很少，我们就可以给自动编码器提供一堆未标记的数据，它将学习有用的特征。然后我们可以将这些特征放入一个更强大的神经网络中，并在小型监督数据集上进行训练。尽管监督数据集很小，但它（理论上）仍然可以很好地学习，因为它是由自动编码器引导的。
不幸的是，自动编码器并不是他们所宣传的那样，这种训练(称为预训练)很少用于自动编码器。但是，我们可以使 Helmholtz boltzmann machine进行初始权重分配。
# VAE
VAE代表变分自动编码器。根据它的名字，你可以说VAE与自动编码器非常相似，从技术上讲，它有一个主要的变化。
自编码器只需要复制输入，而变分自编码器则需要复制输出，同时将其隐藏的神经元保持在特定的分布。这意味着，网络的输出将不得不适应基于分布的隐藏神经元的输出，因此我们可以生成新的图像，只需从分布中取样，并将其输入网络的隐藏层。
假设目标分布是正态分布，均值为0，方差为1。当我们将图像输入到VAE中时，隐藏节点不会输出直接由输出使用的值，而是将输出均值和方差。每一个隐藏的节点都有自己的高斯分布。我们将隐藏的节点值表示为ℎ和ℎ。然后，我们从实际正态分布中采样值，我们将其称为，其大小与隐藏层相同。然后，我们将使用multiple多个元素，并使用add添加元素。这使得网络可以改变正态分布的方差。这就是它编码信息的方式。在对元素进行加法和乘法运算之后，我们就得到了一个潜在向量。我们将这个潜在向量输入输出层，输出层尝试生成输入的副本。自动编码器的损失是最小化reconstruction loss（自动编码器的输出与其输入的相似程度）和latent loss（隐藏节点与正常分布的接近程度）。latent loss越小，可编码的信息越少，因此reconstruction loss增加。结果，VAE被锁定在latent loss和reconstruction loss之间的权衡中。如果latent loss很小，我们的新生成的图像将与训练时的图像非常相似，但它们看起来都很糟糕。如果reconstruction loss很小，那么在训练时刻重建的图像将看起来非常好，但是我们的新生成的图像与重构图像完全不同。显然我们两个都想要，所以找到一个平衡很重要。
这些解释并没有很好地构建，只是为了给出这些架构的一般概念。

