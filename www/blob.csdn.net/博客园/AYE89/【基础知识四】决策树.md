
# 【基础知识四】决策树 - AYE89 - 博客园|[AYE89](https://www.cnblogs.com/eniac1946/)
|coding & learning|
|posts - 153, comments - 3, trackbacks - 0, articles - 2

|
|导航
|[博客园](https://www.cnblogs.com/)|[首页](https://www.cnblogs.com/eniac1946/)|[新随笔](https://i.cnblogs.com/EditPosts.aspx?opt=1)|[联系](https://msg.cnblogs.com/send/AYE89)![订阅](//www.cnblogs.com/images/xml.gif)|[订阅](https://www.cnblogs.com/eniac1946/rss)|[管理](https://i.cnblogs.com/)
|公告


|[【基础知识四】决策树](https://www.cnblogs.com/eniac1946/p/8253146.html)
|Posted on|2018-01-09 19:34|[AYE89](https://www.cnblogs.com/eniac1946/)|阅读(|...|) 评论(|...|)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=8253146)|[收藏](#)
|决策树可以使用不熟悉的数据集合，并从中提取出一系列的|规则（如if...else）|，根据数据集创建规则的过程，就是机器学习的过程。专家系统中经常使用决策树。
|一、基本思想：分而治之的策略
|决策树的生成是一个递归过程；
![](https://images2017.cnblogs.com/blog/1181483/201801/1181483-20180109195001051-405375639.png)
|对于属性值是离散值的情况，结点生成问题转化为：从当前|属性集合|中选择哪种|属性|（作为分支结点）
|何为“最优划分属性”？     即|属性选择|的依据
|所选属性下，（分支结点）所包含的样本尽可能属于同一类别，通俗地说就是属性的“区分度”强，属性的“|纯度|”高；
|二、划分选择
|信息增益：基于“信息熵”；
|典型算法：
|ID3：信息增益
|C4.5|：增益率
|CART|：“基尼指数”
|三、防止“过拟合”
|剪枝：包括“预剪枝”、“后剪枝”
|四、连续与缺失值处理







|[刷新评论](javascript:void(0);)|[刷新页面](#)|[返回顶部](#top)






|
|Powered by:
|博客园
|Copyright © AYE89
|
