
# 正则化概述 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月20日 18:07:55[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：19标签：[正则化																](https://so.csdn.net/so/search/s.do?q=正则化&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)


[https://www.toutiao.com/i6643765591406543374/](https://www.toutiao.com/i6643765591406543374/)
2019-01-07 22:43:39
![正则化概述](http://p1.pstatp.com/large/pgc-image/4697aedc53a34527bcf68fabda178ce7)
在本文中，我们将讨论正则化的必要性及其不同类型。
在监督机器学习模型中，我们将在训练数据上训练模型。模型将从这些数据中学习。在某些情况下，模型会从训练数据中的数据模式和噪声中学习，在这种情况下，我们的模型将有一个高方差，我们的模型将被过度拟合。这种过拟合模型泛化程度较低。这意味着他们会在训练数据方面表现良好，但在新数据方面表现不佳。我们的机器学习算法的目标是学习数据模式并忽略数据集中的噪声。
在训练数据上避免模型过度拟合的方法有交叉验证、特征约简、正则化等。
随着模型复杂性的增加，正则化基本上增加了惩罚。正则化参数（λ）惩罚除截距之外的所有参数，以便模型泛化数据并且不会过度拟合。
**让我们了解惩罚性损失函数如何帮助避免过度拟合**
我们将以线性回归的形式解释正则化。线性回归的成本函数是
![正则化概述](http://p1.pstatp.com/large/pgc-image/d9f6cdd4ead44fec98aa943aabd2727a)
我们的目标是最小化成本函数J（θ0，θ1）。
![正则化概述](http://p3.pstatp.com/large/pgc-image/20ec1fc95eac4246a1591393671b8d9c)
假设如下
![正则化概述](http://p3.pstatp.com/large/pgc-image/bd9dab967ef24dc08affa4b2f91a875a)
我们想要消除θ2，θ3，θ4等参数的影响，而不是实际摆脱这些特征或改变我们假设的形式，我们可以改为修改我们的成本函数。这里让我们修改θ3和θ4的影响。
![正则化概述](http://p3.pstatp.com/large/pgc-image/a1bb4db5b3034595a489ecf0d3bed762)
现在，为了使成本函数接近零，我们必须将*θ3*和*θ4*的值减小到接近零。这又会在我们的假设函数中降低*θ*3*X*3³和*θ*4*X*4⁴*的值*。
![正则化概述](http://p3.pstatp.com/large/pgc-image/867cbb07f2654956a6806e86d3f80dc6)
这意味着θ3，θ4等较小的参数值,假设将更简单,模型将不再那么容易过度拟合。
考虑具有大量特征（x1，x2，x3 ...... .x100）的情况。然后我们将有大量参数（*θ1，θ2......θ100）。*在这种情况下，我们可以通过使用正则化来做同样的事情。为此，我们将为我们的成本函数添加一个新的正则化项。
![正则化概述](http://p3.pstatp.com/large/pgc-image/11f86db6e5ea466f9c351bd26ca108bb)
这里**λ**是**正则化参数。**在这里，我们必须最小化J（θ）*。*随着正则化参数值的增大，系数值减小，方差减小，而不会丢失数据中的重要特征。
**如果我们选择更高的λ值应该如何处理？**
对于更高的**λ**值**，**算法甚至不拟合训练数据并导致**欠**拟合。梯度下降也不会收敛。像*θ1，θ2...θn这样的参数*变得接近零。然后
![正则化概述](http://p3.pstatp.com/large/pgc-image/2296969ad0b142519c3d0648d6af7c5c)
**如何选择λ的正确值呢？**
它将介于0和较大值之间。我们需要找到λ的最佳值，以便泛化误差很小。我们可以使用像k-fold交叉验证这样的方法。
正则化的类型
正则化主要有两种类型。
L1正则化（Lasso正则化）
L2正则化（Ridge正则化）
L1正则化（Lasso回归）
L1正则化加L1罚值等于系数绝对值。当我们的输入特征的权重接近于零时，就会产生稀疏的L1范数。在稀疏解中，大多数输入特征的权值为零，只有极少数特征的权值为非零。
![正则化概述](http://p1.pstatp.com/large/pgc-image/3005ff2a27d24be9bf89fccc075c5497)
**特征：**
L1惩罚权重的绝对值之和。
L1有一个稀疏解
L1生成的模型简单且可解释，但不能学习复杂的模式
L1对异常值具有鲁棒性
**L2正则化（Ridge正则化）**
L2正则化类似于L1正则化。但它将**系数的平方量**作为惩罚项加到损失函数上。L2*不会*产生稀疏模型，并且所有系数都会被同一因子缩小（没有像L1回归那样被消除）
![正则化概述](http://p3.pstatp.com/large/pgc-image/778e9340a3ba4749974abe591ca14864)
**特征：**
L2正则化惩罚平方权重之和。
L2有一个非稀疏解
L2正则化能够学习复杂的数据模式
L2没有特征选择
L2对异常值不太好
这些技术之间的关键区别在于，L1回归将不太重要的特征系数缩小为零，从而完全删除某些特征，而L2回归将其减少到接近零。因此，如果我们有大量的特征，L1回归可以很好地用于特征选择。

