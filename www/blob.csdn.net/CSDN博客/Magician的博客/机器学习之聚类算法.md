
# 机器学习之聚类算法 - Magician的博客 - CSDN博客


2018年09月15日 17:00:54[春雨里de太阳](https://me.csdn.net/qq_16633405)阅读数：112所属专栏：[自学机器学习之路](https://blog.csdn.net/column/details/26597.html)



[1、知道几个关于”差”的概念](#1知道几个关于差的概念)
[2、理解相似度度量的各种方法和相互关系](#2理解相似度度量的各种方法和相互关系)[2.1、欧氏距离](#21欧氏距离)
[2.2、jaccard相似系数](#22jaccard相似系数)
[2.3、余弦相似度](#23余弦相似度)
[2.4、Pearson相似系数](#24pearson相似系数)
[2.5、相对熵](#25相对熵)
[3、掌握掌握K-Means算法](#3掌握掌握k-means算法)[3.1、知道聚类的思想](#31知道聚类的思想)
[3.2、K-Means算法原理](#32k-means算法原理)
[3.3、知道K-Mediods聚类](#33知道k-mediods聚类)
[3.4、如何选取K-means的初始值](#34如何选取k-means的初始值)
[3.5、K-means的目标函数](#35k-means的目标函数)
[4、知道聚类的衡量效果的指标](#4知道聚类的衡量效果的指标)
[5、了解层次聚类的思路和方法](#5了解层次聚类的思路和方法)
[6、理解密度聚类](#6理解密度聚类)
[7、掌握谱聚类的算法](#7掌握谱聚类的算法)[7.1、了解谱（方阵的全体特征值称为方阵的谱）以及谱半径（最大的特征值称为谱半径）](#71了解谱方阵的全体特征值称为方阵的谱以及谱半径最大的特征值称为谱半径)
[7.2、几个概念](#72几个概念)
[7.2、了解谱分析的整体的过程](#72了解谱分析的整体的过程)
[7.3、其他谱聚类算法](#73其他谱聚类算法)
[8、知道拉布拉斯矩阵](#8知道拉布拉斯矩阵)
[9、知道标签传递算法](#9知道标签传递算法)


# 1、知道几个关于”差”的概念
均值、方差、标准差、协方差：
![这里写图片描述](https://img-blog.csdn.net/20180915155653657?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180915155653657?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
协方差就是这样一种用来度量两个随机变量关系的统计量，我们可以仿照方差的定义：
![这里写图片描述](https://img-blog.csdn.net/20180915155814591?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 2、理解相似度度量的各种方法和相互关系
![这里写图片描述](https://img-blog.csdn.net/20180915160010235?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 2.1、欧氏距离
**欧氏距离**：p=2（又称为第二范式）。曼哈顿距离：p=1（又称为第一范式,p为多少也就为第几范式）。
## 2.2、jaccard相似系数
用于比较有限样本集之间的相似性与差异性。Jaccard系数值越大，样本相似度越高。
## 2.3、余弦相似度
![这里写图片描述](https://img-blog.csdn.net/20180915160303774?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 2.4、Pearson相似系数
两个变量之间的协方差和标准差的商。
![这里写图片描述](https://img-blog.csdn.net/20180915160509624?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
当两个变量的线性关系增强时，相关系数趋于1或-1；
当一个变量增大，另一个变量也增大时，表明它们之间是正相关的，相关系数大于0；
如果一个变量增大，另一个变量却减小，表明它们之间是负相关的，相关系数小于0；
如果相关系数等于0，表明它们之间不存在线性相关关系。
## 2.5、相对熵
又称为KL散度（Kullback–Leibler divergence，简称KLD）[1]，信息散度（information divergence），**信息增益（information gain）**。
**余弦相似度与Pearson相似系数之间的关系**
![这里写图片描述](https://img-blog.csdn.net/20180915160612826?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 3、掌握掌握K-Means算法
## 3.1、知道聚类的思想
先做一个初始的划分之后通过迭代来改变样本和簇之间的隶属关系。
![这里写图片描述](https://img-blog.csdn.net/20180915161025655?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 3.2、K-Means算法原理
1、选取初始的K个类别中心数据（可以自定义也可以随机选取K个样本）
2、对于每个样本将其标记为距离类别中心最近的类别（对样本进行聚类处理）。
3、将每个类别中心更新为隶属于该类别的所有样本的均值。
4、重复2、3直到类别中心的变化小于某阈值（迭代次数/簇中心变化率/最小平方误差MSE）
![这里写图片描述](https://img-blog.csdn.net/20180915161122993?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 3.3、知道K-Mediods聚类
类别中心的更新依据从数据的均值改为中位数。
![这里写图片描述](https://img-blog.csdn.net/20180915161213562?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 3.4、如何选取K-means的初始值
首先，知道K-means是初值敏感的（K的输入很关键），也就是每个类别中初始值的选择是很关键的。那么如何有效的选取K-Means的初始值？
得到第一个初始值后求其他点到该初始值的距离时将距离初始值较远的点作为簇点的概率大些，这样就可以得到较好的初始簇点。
![这里写图片描述](https://img-blog.csdn.net/20180915161649670?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 3.5、K-means的目标函数
目标函数：求每个簇的样本点到每个簇中心的距离的加和；求导后就相当于将周围点的平均值付给μ作为新的簇中心。
![这里写图片描述](https://img-blog.csdn.net/20180915161821584?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 4、知道聚类的衡量效果的指标
均一性和完整性就类似于之前半监督中的准确率和召回率
V-measure：均一性和完整性的加权平均
![这里写图片描述](https://img-blog.csdn.net/20180915162021626?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180915162021626?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
了解这个轮廓系数（衡量分类好坏的指标）：
先了解上面的簇内不相似系数ai（越小越好）：簇内的样本点i到簇内其他样本点平均距离。
![这里写图片描述](https://img-blog.csdn.net/20180915164509804?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180915164509804?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
簇间不相似系数bi（越大越好）：样本i到某簇的所有样本的平均距离。
轮廓系数含义见下图
![这里写图片描述](https://img-blog.csdn.net/2018091516453159?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 5、了解层次聚类的思路和方法
知道这两个算法AGNES和DIANA
![这里写图片描述](https://img-blog.csdn.net/20180915164707173?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180915164707173?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![这里写图片描述](https://img-blog.csdn.net/2018091516471469?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/2018091516471469?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![这里写图片描述](https://img-blog.csdn.net/20180915164740276?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 6、理解密度聚类
![这里写图片描述](https://img-blog.csdn.net/20180915164852975?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180915164852975?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
了解DBSCAN算法：
算法原理：
1、如果点p的邻域包含多于m个对象，则创建一个p作为核心对象的新簇。
2、寻找并合并核心对象周围直接密度可达的对象
3、没有新点可以更新簇时，算法结束。
![这里写图片描述](https://img-blog.csdn.net/20180915164936324?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 7、掌握谱聚类的算法
## 7.1、了解谱（方阵的全体特征值称为方阵的谱）以及谱半径（最大的特征值称为谱半径）
![这里写图片描述](https://img-blog.csdn.net/20180915165107977?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 7.2、几个概念
di代表行列式的第i行的累加值。
邻接矩阵W（又称为相似矩阵）又为对称矩阵（Wij与Wji的值相等即i、j的距离相等），主对角线的值取0
![这里写图片描述](https://img-blog.csdn.net/20180915165407777?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
## 7.2、了解谱分析的整体的过程
任意两个点之间的相似度可以用径向量机函数来求（也就是两点之间的距离）
![这里写图片描述](https://img-blog.csdn.net/20180915165143189?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180915165143189?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
过程：
1、由度矩阵D和邻接矩阵W得到对应的L（拉普拉斯矩阵）的特征值行列式。
2、特征值值行列式中的每个λ代表的一个列向量
3、分成K个类别也就是取前K个特征值，如下图所示。
4、特征值行列式的行向量的第i行也就代表第i个样本的特征表示
5、有了m个样本的特征表示就可以利用K-Means来进行聚类处理。
## 7.3、其他谱聚类算法
不同的谱聚类算法只是对应的拉普拉斯矩阵改变了，其他的步骤都没变。
![这里写图片描述](https://img-blog.csdn.net/20180915165612392?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 8、知道拉布拉斯矩阵
![这里写图片描述](https://img-blog.csdn.net/20180915165705428?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)[ ](https://img-blog.csdn.net/20180915165705428?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![这里写图片描述](https://img-blog.csdn.net/20180915165713338?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
# 9、知道标签传递算法
![这里写图片描述](https://img-blog.csdn.net/20180915165839253?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

[
](https://img-blog.csdn.net/20180915165705428?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2NjMzNDA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
