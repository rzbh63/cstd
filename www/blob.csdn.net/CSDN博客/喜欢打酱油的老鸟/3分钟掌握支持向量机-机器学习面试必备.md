
# 3分钟掌握支持向量机-机器学习面试必备 - 喜欢打酱油的老鸟 - CSDN博客


2018年09月16日 09:56:59[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：67


## 简介
## 支持向量机(Support Vector Machine，SVM)是Corinna Cortes和Vapnik等于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。
**总体概述**
在机器学习中，支持向量机（SVM，还支持矢量网络）是与相关的学习算法有关的监督学习模型，可以分析数据，识别模式，用于分类和回归分析。
支持向量机方法是建立在统计学习理论的VC维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折中，以求获得最好的推广能力 。
更正式地说，一个支持向量机的构造一个超平面，或在高或无限维空间，其可以用于分类，回归，或其它任务中设定的超平面的。直观地，一个良好的分离通过具有到任何类（所谓官能余量）的最接近的训练数据点的最大距离的超平面的一般实现中，由于较大的裕度下分类器的泛化误差。
而原来的问题可能在一个有限维空间中所述，经常发生以鉴别集是不是在该空间线性可分。出于这个原因，有人建议，在原始有限维空间映射到一个高得多的立体空间，推测使分离在空间比较容易。保持计算负荷合理，使用支持向量机计划的映射被设计成确保在点积可在原空间中的变量而言容易地计算，通过定义它们中选择的核函数k（x，y）的计算以适应的问题。
在高维空间中的超平面被定义为一组点的点积与该空间中的向量是恒定的。限定的超平面的载体可被选择为线性组合与参数\alpha_i中发生的数据的基础上的特征向量的图像。这种选择一个超平面，该点中的x的特征空间映射到超平面是由关系定义：\字型\sum_i\alpha_ik（x_i中，x）=\mathrm{常数}。注意，如果k（x，y）变小为y的增长进一步远离的x，在求和的每一项测量测试点x的接近程度的相应数据基点x_i的程度。以这种方式，内核上面的总和可以被用于测量各个测试点的对数据点始发于一个或另一个集合中的要被鉴别的相对接近程度。注意一个事实，即设定点的x映射到任何超平面可以相当卷积的结果，使集未在原始空间凸出于各之间复杂得多歧视。
我们通常希望分类的过程是一个机器学习的过程。这些数据点是n维实空间中的点。我们希望能够把这些点通过一个n-1维的超平面分开。通常这个被称为线性分类器。有很多分类器都符合这个要求。但是我们还希望找到分类最佳的平面，即使得属于两个不同类的数据点间隔最大的那个面，该面亦称为最大间隔超平面。如果我们能够找到这个面，那么这个分类器就称为最大间隔分类器。
**支持原因**
支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。建立方向合适的分隔超平面使两个与之平行的超平面间的距离最大化。其假定为，平行超平面间的距离或差距越大，分类器的总误差越小。
一个极好的指南是C.J.C Burges的《模式识别支持向量机指南》。
**支持向量概述**
所谓支持向量是指那些在间隔区边缘的训练样本点。 这里的“机（machine，机器）”实际上是一个算法。在机器学习领域，常把一些算法看做是一个机器。
支持向量机(Support vector machines，SVM)与神经网络类似，都是学习型的机制，但与神经网络不同的是SVM使用的是数学方法和优化技术。

## 相关技术支持
支持向量机是由Vapnik领导的AT&T Bell实验室研究小组在1995年提出的一种新的非常有潜力的分类技术，SVM是一种基于统计学习理论的模式识别方法，主要应用于模式识别领域。由于当时这些研究尚不十分完善，在解决模式识别问题中往往趋于保守，且数学上比较艰涩，这些研究一直没有得到充分的重视。
直到90年代，统计学习理论 (Statistical Learning Theory，SLT)的实现和由于神经网络等较新兴的机器学习方法的研究遇到一些重要的困难，比如如何确定网络结构的问题、过学习与欠学习问题、局部极小点问题等，使得SVM迅速发展和完善，在解决小样本、非线性及高维模式识别问题中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。从此迅速的发展起来，已经在许多领域(生物信息学，文本和手写识别等)都取得了成功的应用。
在地球物理反演当中解决非线性反演也有显著成效，例如（支持向量机在预测地下水涌水量问题等）。已知该算法被应用的主要有：石油测井中利用测井资料预测地层孔隙度及粘粒含量、天气预报工作等。
支持向量机中的一大亮点是在传统的最优化问题中提出了对偶理论，主要有最大最小对偶及拉格朗日对偶。
SVM的关键在于核函数。低维空间向量集通常难于划分，解决的方法是将它们映射到高维空间。但这个办法带来的困难就是计算复杂度的增加，而核函数正好巧妙地解决了这个问题。也就是说，只要选用适当的核函数，就可以得到高维空间的分类函数。在SVM理论中，采用不同的核函数将导致不同的SVM算法。
在确定了核函数之后，由于确定核函数的已知数据也存在一定的误差，考虑到推广性问题，因此引入了松弛系数以及惩罚系数两个参变量来加以校正。在确定了核函数基础上，再经过大量对比实验等将这两个系数取定，该项研究就基本完成，适合相关学科或业务内应用，且有一定能力的推广性。当然误差是绝对的，不同学科、不同专业的要求不一。
支持向量机的理解需要数据挖掘或机器学习的相关背景知识，在没有背景知识的情况下，可以先将支持向量机看作简单分类工具，再进一步引入核函数进行理解。
[https://mp.weixin.qq.com/s/5MvX8YW0TGdry4bs1bmpsA](https://mp.weixin.qq.com/s/5MvX8YW0TGdry4bs1bmpsA)

