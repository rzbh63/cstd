
# 文本表示简介 - SIGAI_CSDN的博客 - CSDN博客
# [SIGAI_CSDN的博客](https://blog.csdn.net/sigai_csdn)


[博客首页](https://blog.csdn.net/SIGAI_CSDN)
[关于我们](https://me.csdn.net/SIGAI_CSDN)

2018年08月20日 16:02:58[SIGAI_csdn](https://me.csdn.net/SIGAI_CSDN)阅读数：1367


> SIGAI特邀作者：徐国海

> 研究方向：自然语言处理和知识图谱
本文及其它机器学习、深度学习算法的全面系统讲解可以阅读《机器学习与应用》，清华大学出版社，雷明著，由SIGAI公众号作者倾力打造，自2019年1月出版以来已重印3次。
[书的购买链接](https://link.zhihu.com/?target=https%3A//item.jd.com/12504554.html)
[书的勘误，优化，源代码资源](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_78.html)
一、引言
文本分类是自然语言处理中研究最为广泛的任务之一，通过构建模型实现对文本内容进行自动分类，有很多应用场景，比如新闻文章主题分类，产品评论情感分类，检索中用户查询的意图分类等等。文本分类的大致流程：文本预处理，抽取文本特征，构造分类器。其中研究最多的就是文本特征抽取，更广义上说是文本表示。
关于文本表示，研究者从不同的角度出发，提出大量的文本表示模型。本文重点梳理现有模型，大致分为三类，即基于向量空间模型、基于主题模型和基于神经网络的方法，针对每类给出一些具有代表性的模型，阐述其基本思想，对于具体的细节，读者可以阅读给出的参考文献。
二、 基于向量空间模型的方法
向量空间模型是将文本表示成实数值分量所构成的向量，一般而言，每个分量对应一个词项，相当于将文本表示成空间中的一个点。向量不仅可以用来训练分类器，而且计算向量之间的相似度可以度量文本之间的相似度。
最常用的是TF-IDF计算方式，即向量的维度对应词表的大小，对应维度使用TF-IDF计算。向量空间模型的优点是简单明了，向量维度意义明确，效果不错，但也存在明显的缺点，其一，维度随着词表增大而增大，且向量高度稀疏；其二，无法处理“一义多词”和“一词多义”问题。
在向量空间模型中，文档集合相当于表示成高维稀疏矩阵，如图1中所示，文档集合矩阵的维度是N*V，其中N是文档数目，V是词表的大小。为了更好的提升文本的语义表示能力，有人提出通过矩阵分解的方法，对高维稀疏矩阵进行分解，最为著名的便是潜在语义分析（Latent semantic analysis, LSA），具体而言，LSA会构建一个文档与词项的共现矩阵，矩阵的元素一般通过TFIDF计算得到，最终通过奇异值分解的方法对原始矩阵降维，可以得到文档向量和词项向量。如图1所示，分解后，每个文档可以用k维向量表示（k << V），相当于潜在语义分析实现对文档的低维语义表示。但是，以上过程通过矩阵分解得到，空间中维度的物理含义不明确，无法解释。
![](https://img-blog.csdn.net/20180820140805180?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图1 LSA
三、基于主题模型的方法
第2节中提到LSA算法通过线性代数中奇异值分解实现文档映射到低维语义空间里的向量，但是空间中每一个维度是没有明确物理意义的，主题模型尝试从概率生成模型的角度实现文本的表示，每一个维度是一个“主题（topic）”，这个主题通常是一组词的聚类，因此可以通过主题大概猜测每个维度所代表的语义，具有一定的解释性。
最早的主题模型pLSA （probabilistic LSA），假设文档具有主题分布，文档中的词从主题对应的词分布中抽取。如图2所示，以d表示文档，w表示词，z表示主题（隐变量），则文档和词的联合概率p(d, w)的生成过程可表示为：
![](https://img-blog.csdn.net/20180820140841454?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中p(z|d)和p(w|z)作为参数可以用EM算法进行学习。然而，pLSA没有假设主题的的先验分布，导致参数随训练文档的数目呈线性增长，参数空间很大。
![](https://img-blog.csdn.net/20180820140906913?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图2 pLSA
于是，有人提出更加完善的主题的模型LDA（Latent Dirichlet allocation）,可以认为pLSA体现频率学派的思想，而LDA是贝叶斯学派的思想，LDA在已有的模型上中的2个多项式分布引入了狄利克雷先验分布，从而解决pLSA中存在的问题。如图3所示，每个文档的主题分布为多项式分布Mult(![\theta](https://www.zhihu.com/equation?tex=%5Ctheta)\theta )，其中![\theta](https://www.zhihu.com/equation?tex=%5Ctheta)\theta从狄利克雷先验分布Dir(![\alpha](https://www.zhihu.com/equation?tex=%5Calpha)\alpha )抽取，同理，对于主题的词分布为多项式分布Mult(![\phi](https://www.zhihu.com/equation?tex=%5Cphi)\phi )，参数![\phi](https://www.zhihu.com/equation?tex=%5Cphi)\phi也是从狄利克雷先验Dir(![\beta](https://www.zhihu.com/equation?tex=%5Cbeta)\beta )抽取得到。
![](https://img-blog.csdn.net/2018082014092280?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图3 LDA
基于主题模型的方法，引入“主题”概念，具有一定的物理意义，从而得到文档的主题分布表示。当然，主题模型很存在一些问题，比如训练参数多导致训练时间长，对短文本的建模效果不好，主题数目的设置需要人工设定可能导致不合理。后来，也有很多人提出改进的方法，在这就不一一赘述了。
四、基于神经网络的方法
现今，基于神经网络的方法受到广泛关注，各种各样的模型被相继提出，本节总结其中最具有代表性的模型，将其分为三类：第一类，基于词向量合成的模型，该类方法仅是在词向量基础上简单合成；
第二类，基于RNN/CNN的模型，该类方法利用更复杂的深度学习模型对文本进行建模；
第三类，基于注意力机制的模型，在已有神经网络模型基础上，引入注意力机制，提升文本建模效果。
4.1 基于词向量合成的模型
2003年Bengio等人开始使用神经网络来做语言模型的工作，尝试得到词的低维、稠密的向量表示，2013年Mikolov等人提出简化的模型，即著名的Word2Vec，包含两个模型CBOW和Skip-gram，前者通过窗口语境预测目标词出现的概率，后者使用目标词预测窗口中的每个语境词出现的概率。语义上相似或相关的词，得到的表示向量也相近，这样的特性使得Word2Vec获得巨大成功。
后来，Mikolov等人又延续Word2Vec的思想，提出Doc2Vec，核心思想是将文档向量当作“语境”，用来预测文档中的词。Doc2Vec算法可以得到词向量和文档向量。如图4所示，两个算法的思想基本一致。
![](https://img-blog.csdn.net/20180820141016832?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图4 Word2Vec和Doc2Vec比较
其实，也可以通过最简单的合成方式实现从词向量到句子向量的表示，fastText就是这样简单有效的模型，如图5所示，输入层是词向量，然后通过把句子里的词向量平均就得到句子的表示，最后送到分类器中。不过，输入端会另外补充一些n-gram信息来捕捉局部序列信息。fastText是线性分类模型，实验表明在诸多“简单”文本分类任务中表现出色，且具备训练速度非常快的优点，所以可以成为很好的Baseline。
![](https://img-blog.csdn.net/20180820141057414?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图5 fastText模型
4.2 基于RNN/CNN的模型
自然语言中，词构成句子，句子构成文档，有很多工作尝试合理表示词向量同时，也有很多模型被提出来建模句子和文档，其中最常见的网络结构便是LSTM和CNN。
2014年Kim提出基于卷积神经网络的文本分类，如图6所示，输入是句子对应的词向量矩阵，经过一层卷积层和一层Max Pooling层，得到句子的表示，送入到全连接层，最后softmax输出。卷积神经网络擅长提取重要的局部特征，在文本分类中，可以理解为不同大小的卷积核在提取不同n-gram特征。一般认为，卷积神经网络无法考虑长距离的依赖信息，且没有考虑词序信息，在有限的窗口下提取句子特征，会损失一些语义信息。
![](https://img-blog.csdn.net/2018082014111523?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图6 CNN网络用于文本分类
针对CNN的不足之处，LSTM和GRU等循环神经网络因为擅长捕捉长距离信息，所以也被大家尝试用来文本表示。如图7所示，图中利用双向LSTM来建模输入句子，输入是句子的词向量，输入至BiLSTM中进行序列建模。最后句子表示，可以尝试两种方法，其一，选择最后的hidden state用来表示句子；其二，所有hidden state的平均用于表示句子。
![](https://img-blog.csdn.net/2018082014113071?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图7 BiLSTM用于文本表示
刚才分析到，CNN擅长提取局部特征，而LSTM擅长捕捉长距离信息，不难想到，有人尝试结合两种网络的优点，提出RCNN用于文本建模。如图8所示，整个网络结构主要有两部分组成，循环结构和Max Pooling。循环结构，可以理解为，在已有词向量为输入的基础上，通过双向RNN网络学习每一个词的左、右上下文信息，接着将三部分(left context, word embedding, right context)表示向量拼接起来，作为句子中每一个词的表示，然后使用变换得到中间语义表示；Max Pooling层，采用element-wise的max pooling方式，可以从变长的输入中得到固定的句子表示。
![](https://img-blog.csdn.net/20180820141143924?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图8 RCNN用于文本表示
4.3 基于注意力机制的模型
注意力被认为是一种有效选择信息的方式，可以过滤掉大量与任务无关的信息，最先在机器翻译任务中被提出，解决seq2seq中encoder过程把源序列映射成固定大小的向量存在“损失”信息的情况。紧接着，Attention被推广到各种NLP任务中，文本表示任务当然不例外。这里，主要介绍两种Attention的形式，Hierarchical Attention 和 Self-Attention。
Hierarchical Attention网络结构，如图9所示，该模型基于两个基本假设，其一，文档是分层结构的，词构成句子，句子构成文档；其二，文档中不同词或句子提供的信息量不一样的，该模型适合用来表示包含多个句子的文档的表示问题。模型主要由word encoder和sentence encoder，以及相应的attention组成，word encoder部分用于得到句子的表示，该层的输入为句子的词向量，经过双向GRU后得到中间表示，word attention部分对中间表示按attention值进行加权得到此句最终的句子表示；sentence encoder部分用于得到文档的表示，使用word encoder部分得到文档中所有句子的表示后，将此作为sentence encoder部分的输入，类比word encoder的计算，可以最终得到文档的表示。简言之，利用Hierarchical Attention结构，一层词输入得到句子表示，一层句子输入得到文档表示。即使文档长度较长，但是在注意力的作用下，依然可以较好的捕捉到有效的特征信息，忽略无意义的输入。
![](https://img-blog.csdn.net/20180820141238131?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图9 Hierarchical Attention
Self-Attention网络结构，如图10所示，大多数神经网络模型将文本表示成一维的向量，但是此模型通过二维矩阵来表示句子，包括两部分，一部分是双向的LSTM，另一部分是自注意力机制，自注意力机制实现对双向LSTM中所有的隐藏状态以不同权重的方式线形组合，每一次组合获得句子的一部分表示，多次组合便得到矩阵表示（图中矩阵M）。
![](https://img-blog.csdn.net/20180820141253974?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图10 Self-Attention
五、 总结
本文简述了具有代表性的文本表示模型，将现有模型分为三类进行介绍，包括基于向量空间模型、基于主题模型和基于神经网络的方法。不过，本文中提及的神经网络方法大部分都是有监督方法，通常都是结合具体的应用根据有监督的信息进行训练，其实也有大量的方法是通过无监督的方法获得普适性的文本表示，感兴趣的读者可以自行去翻阅相关文献。

参考文献
1. 信息检索导论，第14、18章
2. 短文本理解, 计算机研究与发展2016
3. LDA数学八卦
4. Distributed Representations of Words and Phrases and their Compositionality, NIPS2013
5. Distributed Representations of Sentences and Documents, ICML2014
6. Bag of Tricks for Efficient Text Classification, EACL2017
7. Convolutional Neural Networks for Sentence Classification, EMNLP2014
8. Recurrent Convolutional Neural Networks for Text Classification, AAAI2015
9. Hierarchical Attention Networks for Document Classification, NAACL2016
10. A Structured Self-attentive Sentence Embedding, ICLR2017
推荐阅读
[1][机器学习-波澜壮阔40年](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483705%26idx%3D1%26sn%3Dc6e7c4a2e14a2469308b41eb60f155ac%26chksm%3Dfdb69caecac115b8712653600e526e99a3f6976fdaa2f6b6a09388fa6f9677ccb57b40c40ae3%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0413.
[2][学好机器学习需要哪些数学知识？](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483713%26idx%3D1%26sn%3D1e7c81381d16806ac73e15691fe17aec%26chksm%3Dfdb69cd6cac115c05f1f90b0407e3f8ae9be8719e454f908074ac0d079885b5c134e2d60fd64%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0417.
[3][人脸识别算法演化史](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483726%26idx%3D1%26sn%3D9fef4cc1766ea4258749f8d40cc71a6e%26chksm%3Dfdb69cd9cac115cf4eba16081780c3b64c75e1e55a40bf2782783d5c28f00c6f143426e6f0aa%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0420.
[4][基于深度学习的目标检测算法综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483731%26idx%3D1%26sn%3D237c52bc9ddfe65779b73ef8b5507f3c%26chksm%3Dfdb69cc4cac115d2ca505e0deb975960a792a0106a5314ffe3052f8e02a75c9fef458fd3aca2%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0424.
[5][卷积神经网络为什么能够称霸计算机视觉领域？](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483816%26idx%3D1%26sn%3Dfc52765b012771d4736c9be4109f910e%26chksm%3Dfdb69c3fcac115290020c3dd0d677d987086a031c1bde3429339bb3b5bbc0aa154e76325c225%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0426.
[6][用一张图理解SVM的脉络](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483937%26idx%3D1%26sn%3D84a5acf12e96727b13fd7d456c414c12%26chksm%3Dfdb69fb6cac116a02dc68d948958ee731a4ae2b6c3d81196822b665224d9dab21d0f2fccb329%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0428.
[7][人脸检测算法综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483950%26idx%3D1%26sn%3Da3a5b7907b2552c233f654a529931776%26chksm%3Dfdb69fb9cac116af5dd237cf987e56d12b0d2e54c5c565aab752f3e366c0c45bfefa76f5ed16%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0503.
[8][理解神经网络的激活函数](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483977%26idx%3D1%26sn%3D401b211bf72bc70f733d6ac90f7352cc%26chksm%3Dfdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI2018.5.5.
[9][深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484037%26idx%3D1%26sn%3D13ad0d521b6a3578ff031e14950b41f4%26chksm%3Dfdb69f12cac11604a42ccb37913c56001a11c65a8d1125c4a9aeba1aed570a751cb400d276b6%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0508.
[10][理解梯度下降法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484111%26idx%3D1%26sn%3D4ed4480e849298a0aff828611e18f1a8%26chksm%3Dfdb69f58cac1164e844726bd429862eb7b38d22509eb4d1826eb851036460cb7ca5a8de7b9bb%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0511.
[11][循环神经网络综述—语音识别与自然语言处理的利器](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484310%26idx%3D1%26sn%3D0fc55a2784a894100a1ae64d7dbfa23d%26chksm%3Dfdb69e01cac1171758cb021fc8779952e55de41032a66ee5417bd3e826bf703247e243654bd0%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0515
[12][理解凸优化](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484439%26idx%3D1%26sn%3D4fa8c71ae9cb777d6e97ebd0dd8672e7%26chksm%3Dfdb69980cac110960e08c63061e0719a8dc7945606eeef460404dc2eb21b4f5bdb434fb56f92%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】 SIGAI0518
[13][【实验】理解SVM的核函数和参数](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484495%26idx%3D1%26sn%3D4f3a6ce21cdd1a048e402ed05c9ead91%26chksm%3Dfdb699d8cac110ce53f4fc5e417e107f839059cb76d3cbf640c6f56620f90f8fb4e7f6ee02f9%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0522
[14][【SIGAI综述】行人检测算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484523%26idx%3D1%26sn%3Dddaa70c4b92f6005d9bbd6b3a3fe4571%26chksm%3Dfdb699fccac110ea14e6adeb873a00d6ee86dd4145ddf8e90c9b878b908ac7b7655cfa51dab6%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0525
[15][机器学习在自动驾驶中的应用—以百度阿波罗平台为例(上)](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484540%26idx%3D1%26sn%3D733332042c31e1e18ad800f7f527893b%26chksm%3Dfdb699ebcac110fd6607c1c99bc7ebed1594a8d00bc454b63d7f518191bd72274f7e61ca5187%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0529
[16][理解牛顿法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484651%26idx%3D1%26sn%3Da0e4ca5edb868fe3eae9101b71dd7103%26chksm%3Dfdb6997ccac1106a61f51fe9f8fd532045cc5d13f6c75c2cbbf1a7c94c58bcdf5f2a6661facd%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0531
[17][【群话题精华】5月集锦—机器学习和深度学习中一些值得思考的问题](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484658%26idx%3D1%26sn%3Df5c9f92c272c75883bf8e6f532559f11%26chksm%3Dfdb69965cac11073f49048caef5d7b9129614090a363d9ef7f3d1b9bc59948d2217d2bca7b7b%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI 0601
[18][大话Adaboost算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484692%26idx%3D1%26sn%3D9b389aa65208c778dddf17c601afbee1%26chksm%3Dfdb69883cac1119593934734e94c3b71aa68de67bda8a946c1f9f9e1209c3b6f0bf18fed99b8%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0602
[19][FlowNet到FlowNet2.0：基于卷积神经网络的光流预测算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484711%26idx%3D1%26sn%3Dbb7644e101b5924f54d6800b952dc3aa%26chksm%3Dfdb698b0cac111a6605f5b9b6f0478bf21a8527cfad2342dbaaf624b4e9dcc43c0d85ae06deb%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0604
[20][理解主成分分析(PCA)](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484754%26idx%3D1%26sn%3Db2c0d6798f44e13956bb42373e51d18c%26chksm%3Dfdb698c5cac111d3e3dca24c50aafbfb61e5b05c5df5b603067bb7edec8db049370b73046b24%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0606
[21][人体骨骼关键点检测综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484784%26idx%3D1%26sn%3Dceafb54203f4e930ae457ad392b9f89c%26chksm%3Dfdb698e7cac111f13d8229d7dcc00b4a7305d66de3da1bd41e7ecc1d29bfa7be520d205c53e9%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0608
[22][理解决策树](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484827%26idx%3D1%26sn%3D043d7d0159baaddfbf92ed78ee5b1124%26chksm%3Dfdb6980ccac1111a9faeae7f517fee46a1dfab19612f76ccfe5417487b3f090ab8fc702d18b8%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0611
[23][用一句话总结常用的机器学习算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484859%26idx%3D1%26sn%3D2c4db22fb538953a62a90983e3e1f99d%26chksm%3Dfdb6982ccac1113a82e92be325bb07a947d54090274654375f3b50e11e1abd809fb7358bde16%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0611
[24][目标检测算法之YOLO](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484909%26idx%3D1%26sn%3Dc02ee17e5175230ed39ad63e73249f5c%26chksm%3Dfdb6987acac1116c0108ec28424baf4ea16ca11d2b13f20d4a825d7b2b82fb8765720ebd1063%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0615
[25][理解过拟合](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484954%26idx%3D1%26sn%3Dc28b7f07c22466e91b1ef90e9dbe3ad1%26chksm%3Dfdb69b8dcac1129bc6e78fca1d550e2b18238ad1c240c73b280d4e529f9f93c4626b3ac45ea2%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0618
[26][理解计算：从√2到AlphaGo ——第1季 从√2谈起](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484981%26idx%3D1%26sn%3Dd3003468b9853851923844812993e060%26chksm%3Dfdb69ba2cac112b4dac620d52100ebd033eb679f29340726a67297c4d6980b16c7cc91122028%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0620
[27][场景文本检测——CTPN算法介绍](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485005%26idx%3D1%26sn%3D0d4fb43b8db2a8046c64a9cfcbf3f478%26chksm%3Dfdb69bdacac112cce05c8b735b4f8b1ccf2348bea55a30af2055fc328958bb8f1ffd0f819bd2%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0622
[28][卷积神经网络的压缩和加速](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485042%26idx%3D1%26sn%3Dcdcf8d4b07acf64c7a6f5f7c1a731a12%26chksm%3Dfdb69be5cac112f377766984afb87313c1e1c58d94c80005f0f6f6af61ee5a4bd1bf6c6157b6%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0625
[29][k近邻算法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485074%26idx%3D1%26sn%3D0ebf1bf8f49e9c46075fe3803d04c95d%26chksm%3Dfdb69b05cac112132d280c70af3923ca4c3cccfa5fcd8628b79d4b246b3b2decbc80a180abb3%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0627
[30][自然场景文本检测识别技术综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485142%26idx%3D1%26sn%3Dc0e01da30eb5e750be453eabe4be2bf4%26chksm%3Dfdb69b41cac11257ae22c7dac395e9651dab628fc35dd6d3c02d9566a8c7f5f2b56353d58a64%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0627
[31][理解计算：从√2到AlphaGo ——第2季 神经计算的历史背景](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485155%26idx%3D1%26sn%3D990cc7400751c36e9fef0a261e6add2a%26chksm%3Dfdb69b74cac112628bdae14c6435120f6fece20dae9bf7b1ffc8b8b25e5496a24160feca0a72%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0704
[32][机器学习算法地图](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485306%26idx%3D1%26sn%3Dfc8cc8de313bdb61dcd39c1dedb240a4%26chksm%3Dfdb69aedcac113fb4b18c74248a313536ded50bade0e66b26f332ab247b148519da71ff2a3c0%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0706
[33][反向传播算法推导-全连接神经网络](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485446%26idx%3D1%26sn%3D57d7d866443810c20c4ea2c6ee8018cc%26chksm%3Dfdb69591cac11c8773638b396abe43c0161e4d339f0fa845e54326be3e8c4933a3b6a2713dae%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0709
[34][生成式对抗网络模型综述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485551%26idx%3D1%26sn%3D213f48c4e55bee688cf0731097bb832c%26chksm%3Dfdb695f8cac11ceef3ef246c54d811dd64d8cc45fc75488c374c7aa95f72c1abfb55555ef0b7%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0709.
[35][怎样成为一名优秀的算法工程师](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485570%26idx%3D1%26sn%3D5e71437a6d3ddf6d05144fc99e7633cc%26chksm%3Dfdb69515cac11c030cf713ec85293b7ad3bbe0f20d43fc2729cc976ff628aabf636834ccd904%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0711.
[36][理解计算：从根号2到AlphaGo——第三季 神经网络的数学模型](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485592%26idx%3D1%26sn%3D1c5236972402ea8cb168161bc41e8e7e%26chksm%3Dfdb6950fcac11c19ad047e7cb9ced96447a85b41e21b10789a86ae4a211e4fb2ca1f911a7fc5%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0716
[37][【技术短文】人脸检测算法之S3FD](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485609%26idx%3D1%26sn%3Dd9068aecfbf150b40103210de538fea9%26chksm%3Dfdb6953ecac11c28361435306a7a09632ea79000abf1bf626e50afb3cda48eb3e47b96c6e7cd%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0716
[38][基于深度负相关学习的人群计数方法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485617%26idx%3D1%26sn%3D7955bfefc8e4b28221ae5247812f8235%26chksm%3Dfdb69526cac11c30e1051edc4378d7dfdedf46925236dbe33e7912b4bea882e515f074eee4c9%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0718
[39][流形学习概述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485668%26idx%3D1%26sn%3Df70547fc671d164e28fddcea6473524d%26chksm%3Dfdb69573cac11c65ee9fc98ac8fad093282a3d244748e7c88541c133ac55a32cb07472dc80e0%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0723
[40][关于感受野的总结](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485700%26idx%3D1%26sn%3Dc0425495fe0ae9cb2120dbcb246f49b1%26chksm%3Dfdb69493cac11d8542f7a8e662a7ecdeece4fd2270c71504023e8b58128575d1e4fdadf60cf5%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0723
[41][随机森林概述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485718%26idx%3D1%26sn%3Dc05c217af81173ae2c1301cbda5f7131%26chksm%3Dfdb69481cac11d975d86ff2e280371963d04b5709dfa0a9e874d637b7cf3844cad12be584094%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0725
[42][基于内容的图像检索技术综述——传统经典方法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485741%26idx%3D1%26sn%3Df8217e523d54842daaa4be38fa1792eb%26chksm%3Dfdb694bacac11dacfd8d7e4882166e2774c4685c043379ce0d2044e99c3b3c8b0140480bbf8e%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0727
[43][神经网络的激活函数总结](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485762%26idx%3D1%26sn%3De1e9fc75b92999177d3c61c655b0e06e%26chksm%3Dfdb694d5cac11dc37dac1a7ce32150836d66f0012f35a7e04e3dceaf626b8453dc39ee80172b%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0730
[44][机器学习和深度学习中值得弄清楚的一些问题](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485811%26idx%3D1%26sn%3Da87082f0e47b01bb8c22443ba5b1139c%26chksm%3Dfdb694e4cac11df20ea1deb8b55cf297ad44e48a4c7ca45cfce387284143403fcd342ac98ec0%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0802
[45][基于深度神经网络的自动问答系统概述](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485842%26idx%3D1%26sn%3Dd7485054d6e93225b6ac0c77f8706cf7%26chksm%3Dfdb69405cac11d1355b84f692c2cbe49a3852a10e074b6941c95618598caea6ed64103c4ee4c%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取码】SIGAI0806
[46][机器学习与深度学习核心知识点总结 写在校园招聘即将开始时](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247486105%26idx%3D1%26sn%3Dd0b33e7e23b0e2fc442bd6b3e2a9d952%26chksm%3Dfdb6970ecac11e18085ea36f3b654028b2d4ba33a0cdc89c4ea25ac81570969f95f84c6939ac%26token%3D1065243837%26lang%3Dzh_CN%23rd)【获取 码】SIGAI0808
[47][理解Spatial Transformer Networks](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247486133&idx=1&sn=31c64e83511ad89929609dbbb0286890&chksm=fdb69722cac11e34da58fc2c907e277b1c3153a483ce44e9aaf2c3ed468386d315a9b606be40&token=283993194&lang=zh_CN#rd)【获取码】SIGAI0810
[48][AI时代大点兵-国内外知名AI公司2018年最新盘点](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247486140&idx=1&sn=6157951b026199422becca8863f18a17&chksm=fdb6972bcac11e3d7427847df818bd6cc7893a261daa0babe0b11bd01a403dc4f36e2b45650e&token=283993194&lang=zh_CN#rd)【获取码】SIGAI0813
[49][理解计算：从√2到AlphaGo ——第2季 神经计算的历史背景](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247486202&idx=1&sn=bf66be1e71272be42508e557ed93acbf&chksm=fdb6976dcac11e7b9d0f878233e8d9907825e57851e7a095f1be3a23abc9327370a77f4e2c03&token=283993194&lang=zh_CN#rd)【获取码】SIGAI0815
[50][基于内容的图像检索技术综述--CNN方法](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247486268&idx=1&sn=dff53feb3d78ea7698f70bede08b3b19&chksm=fdb696abcac11fbdcde5f4acc72d34c14119a21234b9e6cd0c1886b85c330e7f77d6e3da9122&token=283993194&lang=zh_CN#rd)【获取码】SIGAI0817
原创声明：本文为 SIGAI 原创文章，仅供个人学习使用，未经允许，不能用于商业目的


