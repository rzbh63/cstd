
# 机器学习入门｜快速掌握逻辑回归模型 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月20日 18:08:24[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：33标签：[机器学习																](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)[逻辑回归																](https://so.csdn.net/so/search/s.do?q=逻辑回归&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)


[http://blog.itpub.net/29829936/viewspace-2558236/](http://blog.itpub.net/29829936/viewspace-2558236/)
2019-01-14 17:30:19
**主要内容**：
> 一、逻辑回归的原理

> 二、极大似然估计

> 三、逻辑回归的极大似然估计

> 四、Python中的逻辑回归
**预告：**
> 本文将会带领大家一步步理解逻辑回归的原理，并且会用几行代码快速实现一个逻辑回归模型训练和预测的例子。

> 之后，我计划专门用一篇文章来演示
> 如何评估逻辑回归模型的表现以及如何调优
> ，这部分内容会更加
> 偏重于实战
> ，感兴趣的同学
> 欢迎关注
> 后续的更新！
目前来看，逻辑回归这一经典算法的应用极为广泛。如果要按照应用广度来做个排名的话，它很可能就是一哥了。从垃圾邮件分类到广告点击预测，从商品推荐到信用欺诈检测，到处都是它的身影。
它能得到如此广泛的应用，除了其分类效果良好之外，最大的原因就是在输出分类的结果时，它还能给出样本属于预测分类的概率，这就给了模型极好的解释性。很多分类模型类似黑盒，我们只能知道调整了某些参数之后分类效果可能明显提升了或者下降了，但是并不清楚某一个样本为什么被分到了某一类，而逻辑回归则不存在这一问题。今天我们就从理论到应用来扒掉逻辑回归的外衣，嘿嘿。
# 一、逻辑回归的原理
首先，我们知道线性回归的输出值域为负无穷到正无穷，因此它不太适合直接应用于分类，一个很好的办法就是通过一个函数，将大于0的值映射为1，小于0的值映射为0，这样我们就可以达到分类的目的。
我们先将线性回归模型写作：
![机器学习入门｜快速掌握逻辑回归模型](http://p3.pstatp.com/large/pgc-image/395129c64ab14f8a8429553bc8fb9e91)
现在我们要对线性回归的值做一个变换，即：
![机器学习入门｜快速掌握逻辑回归模型](http://p3.pstatp.com/large/pgc-image/a370c9664b524229ba99a86faab49c2a)
一种方法是令g(z)为单位阶跃函数，即：
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/15130e77fd1b49cfb0b80862d2b656e7)

但是我们发现，它并不连续，也就不满足单调可导的性质。因此我们找到了更好的替代品：对数几率函数，即：
![机器学习入门｜快速掌握逻辑回归模型](http://p9.pstatp.com/large/pgc-image/b41c167a2ace44fe9fc65d38d6c9df08)

对数几率函数的分布形态为：
![机器学习入门｜快速掌握逻辑回归模型](http://p3.pstatp.com/large/pgc-image/0435f10ba50544a9bfedbbe4f9997dca)
可以看到，对数几率函数是一种“Sigmoid”函数，它将z值转化为一个接近0或1的y值，且当z值远离0时，y值迅速靠近0或1。
如果我们将y视为样本x作为正例的可能性，则1-y就是其作为反例的可能性，两者的比值被称作“几率”（odds，又名发生比、优势比等），反映了x作为正例的**相对可能性**。对数几率则是对几率取对数：
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/85cf2ede0b3944b4bae663669bb31da6)

我们可以将（2）式中的y视为类后验概率估计p(y=1|x)，于是（2）可以重写为：
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/00810de2a1b24f99a1d1b1b3b67c0643)

从（1）中我们知道：
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/e7d1988687294ba6b352ee6a7f4c42bb)

于是：
![机器学习入门｜快速掌握逻辑回归模型](http://p3.pstatp.com/large/pgc-image/b5410d18d2bb4db0b56a9b730f24c4ab)

也就是说，线性函数（0）中z的值越接近正无穷，几率就越大，样本属于正例的概率值就越接近1；反之，z的值越接近负无穷，样本属于正例的概率就越接近0。
# 二、极大似然估计
我们用极大似然估计法（Maximum Likelihood Estimate）来估计逻辑回归模型的参数，这里我们有必要先简单解释一下极大似然估计。
极大似然估计的原理是找出与样本分布最接近的概率分布模型，似然是指在已知结果的前提下，随机变量分布的最大可能。我们用经典的抛硬币例子来理解极大似然估计。
假如我们抛了10次硬币，其中6次为正，4次为反，那么发生这一事件的概率为：
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/518646d48c34487bb86639740e95c10d)

这里等号右边的部分就是似然函数，极大似然估计就是找到这样一个P正，使得似然函数的值最大。我们用下图来观察随着P正取值的变化，似然函数大小的变化趋势：
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/7c24d86f9be54319ad3e4c0f03223689)
可以看到，当P正为0.6时，似然函数达到最大，因此我们估计的P正就是0.6。这就是极大似然估计的过程。
一般来说，对于参数θ和训练数据集X，似然函数的一般形式为：
![机器学习入门｜快速掌握逻辑回归模型](http://p3.pstatp.com/large/pgc-image/45007067862345a3a9befc9921629ba9)

不过在实际的计算过程中，我们常对似然函数取对数，得到对数似然函数：
![机器学习入门｜快速掌握逻辑回归模型](http://p9.pstatp.com/large/pgc-image/a48ef4e1099c4e0f802d4b3783f275f0)

我们的数据集往往由大小为N的特征集X与目标集（分类变量）Y组成，因此（10）更常被写作如下形式：
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/c85747ac31bf40bea2b040a96237744d)

而极大似然估计的一般形式为：
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/787a9fa092de4d81804e6295bef1c5fd)

# 三、逻辑回归的极大似然估计
我们根据（10）来写出逻辑回归的对数似然函数：
![机器学习入门｜快速掌握逻辑回归模型](http://p3.pstatp.com/large/pgc-image/9bc7fb522fee457483f3f696038f01e8)

其中，
![机器学习入门｜快速掌握逻辑回归模型](http://p1.pstatp.com/large/pgc-image/3049c143421e4f5ba469aced88ca3ce9)

可以看到，不管yi是0还是1，式（13）都成立（代入一下就知道了）。然后我们把（4）（5）代入（13），然后再整体代入（12），得到：
![机器学习入门｜快速掌握逻辑回归模型](http://p9.pstatp.com/large/pgc-image/7f5b4d6db3d54002aa942ed4491f1caa)

在最后一步，分别考虑yi=0和yi=1的情况即可得到。到了这一步以后，我们就可以通过梯度下降法、牛顿法等数值优化算法来对（14）式求解得到参数(w,b)了。关于数值优化算法这里就不再赘述，接下来我们看一下如何在Python中完成逻辑回归模型的训练和预测。
# 四、Python中的逻辑回归
这里我们仍然选择经典的iris数据集（鸢尾花）来演示如何在Python中应用逻辑回归模型。需要注意的是，虽然我们上边演示的推导过程为二元逻辑回归，但是其推导是可以推广到多元逻辑回归过程的。这里的鸢尾花数据集中，目标变量有三种分类水平，就是一个多元逻辑回归的例子，而强大的sklearn对于多元逻辑回归提供了原生的支持。
```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
logr = LogisticRegression(solver='newton-cg', multi_class='multinomial')
logr.fit(X_train, y_train)
y_pred = logr.predict(X_test)
acc = sum(y_pred == y_test)/len(y_pred)
print('准确率：{0:.2f}%'.format(acc * 100))
```
输出为：
```python
准确率：97.37%
```

