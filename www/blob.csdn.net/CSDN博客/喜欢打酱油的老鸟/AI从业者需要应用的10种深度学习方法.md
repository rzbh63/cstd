
# AI从业者需要应用的10种深度学习方法 - 喜欢打酱油的老鸟 - CSDN博客


2018年09月05日 14:07:32[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：217


**AI****从业者需要应用的10种深度学习方法**
[https://www.cnblogs.com/DicksonJYL/p/9591732.html](https://www.cnblogs.com/DicksonJYL/p/9591732.html)
**摘要：**想要了解人工智能，不知道这十种深度学习方法怎么能行？
在过去十年中，人们对**机器学习的**兴趣激增。几乎每天，我们都可以在各种各样的计算机科学课程、行业会议、华尔街日报等等看到有关机器学习的讨论。在所有关于机器学习的讨论中，许多人把机器学习能做的事情和他们希望机器学习做的事情混为一谈。从根本上讲，机器学习是使用算法从原始数据中提取信息，并在某种类型的模型中表示这些信息。我们使用这个模型来推断还没有建模的其他数据。
神经网络是机器学习的一种模型，它们至少有50年历史了。神经网络的基本单元是节点（node），基本上是受哺乳动物大脑中的生物神经元启发。神经元之间的连接也以生物的大脑为模型，这些连接随着时间的推移而发展的方式是为“训练”。
在20世纪80年代中期和90年代初期，许多重要的模型架构进步都是在神经网络中进行的。然而，为了获得良好性能所需的时间和数据越来越多，这极大的降低了研究人员的兴趣。在21世纪初期，计算能力呈指数级增长，研究人员看到了计算机技术的“寒武纪爆发”。作为该领域的一个重要竞争者——深度学习，因为计算能力的爆炸式增长，赢得了许多重要的机器学习竞赛。截至目前，这种趋势仍然没有减退;今天，我们看到机器学习的每个角落都提到了深度学习。
为了让自己赶上潮流，我参加了**Udacity****的“深度学习”的课程**，这个课程很好的介绍了深度学习的动机以及如何从**TensorFlow中学习对**大规模数据集学习的智能系统设计介绍。在课堂上，我开发了用于图像识别的卷积神经网络、用于自然语言处理的嵌入式神经网络，以及使用循环神经网络／长短期记忆网络的字符级文本生成。你们可以在Jupiter Notebook适用代码，所有代码都能在这个GitHub存储库中找到。
最近，我又开始阅读有关该深度学习的学术论文。根据我的研究，以下是一些对该领域的发展产生巨大影响的出版物：
• 纽约大学**基于梯度的学习应用于文档识别**（1998），它将卷积神经网络引入机器学习世界。
• 多伦多大学的**Deep Boltzmann Machines**（2009），它为Boltzmann机器提供了一种新的学习算法，包含许多隐藏变量层。
• 斯坦福和谷歌**使用大规模无监督学习构建高级功能**（2012），解决了仅使用未标记数据构建高级，类特定功能检测器的问题。
• Berkeley的**DeCAF-一种用于通用视觉识别的深度卷积激活功能**（2013），它发布了DeCAF，这是**一种深度卷积激活功能**的开源实现，以及所有相关的网络参数，使视觉研究人员能够进行深度实验跨越一系列视觉概念学习范例的表示。
• DeepMind**使用Deep Reinforcement Learning**（2016）**播放Atari**，它提供了第一个深度学习模型，可以使用强化学习直接从高维感觉输入成功学习控制策略。
通过研究和学习论文，我学到了很多关于深度学习的丰富知识。在这里，我想分享AI工程师可以应用于机器学习问题的**10种强大的深度学习方法**。但首先，让我们来定义深度学习是什么。深度学习对于许多人来说是一个挑战，因为它的形式在过去十年中逐渐发生了改变。为了向各位更好的说明深层学习的地位，下图说明了人工智能，机器学习和深度学习之间关系的概念。
![](https://img-blog.csdn.net/20180905140358767?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
人工智能领域很广泛，并且已经存在了很长时间。深度学习是机器学习领域的一个子集，而机器学习只是人工智能的一个子领域。将深度学习网络与之前的前馈多层网络进行区分：
• 深度学习比以前的网络更多的神经元；
• 深度学习中有更复杂的连接层的方式；
• “寒武纪爆炸”的提供的计算能力；
• 深度学习可以自动进行特征提取。
当我说到“更多神经元”时，是指近年来神经元的数量不断增加，深度学习就可以表示更为复杂的模型。层也从多层网络中每一层的完全连接，进化成卷积神经网络中神经元片段的局部连接，以及与递归神经网络中的同一神经元的循环连接（与前一层的连接除外）。
深度学习可以被定义为具有大量参数和层数的神经网络：
• 无人监督的预训练网络；
• 卷积神经网络；
• 循环神经网络；
• 递归神经网络。
在这篇文章中，我主要对后三种网络进行讲解。
**卷积神经网络（CNN）**基本上式已经跨越使用共享权重的空间延伸的标准神经网络。CNN旨在通过在内部的卷积来识别图像，该卷积看到图像上识别对象的边缘。
递归神经网络基本上是一个使用时间延伸扩展空间的标准神经网络，它提取进入下一时间步的边沿，而不是在同一时间进入下一层。
RNN进行序列识别，例如语音或文本信号，因其内部具有循环，意味着在RNN网络中存在短时记忆。递归神经网络更类似于分层网络，其中输入序列实际上与时间无关，但输入必须以树状方式分层处理。
下面的10种方法可以应用于所有这些架构。

## 1-
## 反向传播
Back-prop反向传播只是一种简单计算函数的偏导数的方法，它具有函数组合的形式（如神经网络中）。当你使用基于梯度的方法解决最优化问题（梯度下降只是其中之一）时，你希望在每次迭代时计算函数渐变，这个时候它便可以发挥作用。
![](https://img-blog.csdn.net/20180905140415337?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
对于神经网络，其目标函数具有组合的形式。你如何计算梯度？有两种常见的方法可以做到：（i）**分析微分法**。如果你知道函数的形式，你只需使用链式规则（基本微积分）计算导数。（ii）**有限差分的近似微分**。该方法在计算上是昂贵的，因为评估函数的数量是*O（N）*，其中*N*是参数的数量。与解析微分相比，这种方法的计算成本是昂贵的。在调试时，通常使用有限差分验证反向传播的执行效果。

## 2-
## 随机梯度下降
想象梯度下降的直观方式是想象一条源自山顶的河流的路径。梯度下降的目标正是河流努力实现的目标，即从山顶流到最低点。
现在，如果山的地形形状使得河流在到达其最终目的地之前不必完全停在任何地方，这是我们想要的理想情况。在机器学习中，这相当于说，我们已经从初始点（山顶）开始找到解决方案的全局最小值（或最优值）。然而，可能由于地形性质，导致河流路径出现若干的坑洼，会迫使河流困住和停滞。在机器学习方面，这种坑洼被称为局部最优解，这是我们不想要的情况。当然有很多方法可以解决局部最优解问题，这里我不打算进一步讨论。
![](https://img-blog.csdn.net/2018090514045222?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

因此，梯度下降倾向于陷入局部最小值，这取决于地形的性质（或ML术语中的函数）。但是，当你有一种特殊的山地形状（形状像一个碗，用ML术语称为凸函数）时，算法始终能够找到最优值。你可以想象将这条河流可视化。在机器学习中，这些特殊的地形（也称为凸函数）总是需要优化的。另外，你从山顶开始（即函数的初始值）的位置不同，最终你到达山底的路径也完全不同。同样，根据河流的流淌速度（即梯度下降算法的学习速率或步长），你可能会以不同的方式到达目的地。你是否会陷入或避免一个坑（局部最小），都会被这两个标准影响。

## 3-
## 学习速率衰减
![](https://img-blog.csdn.net/20180905140514100?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
调整随机梯度下降优化程序的学习速率可以提高性能并缩短训练时间。有时这也被称为学习率退火或自适应学习率。在训练期间最简单且最常用的学习率调整是随时间降低学习率的技术。在训练初期使用较大的学习速率值，可以对学习速率进行大幅调整；在训练后期，降低学习速率，使模型以一个较小的速率进行权重的更新。这种技术在早期可以快速学习获得一些较好的权重，并在后期对权重进行微调。
两种流行且易于使用的学习率衰减如下：
• 在每个环节逐渐降低学习率。
• 在特定时期使用大幅下降来降低学习速率。

## 4-Dropout
具有大量参数的深度神经网络是非常强大的机器学习系统。然而，过度拟合是这种网络中的严重问题。大型网络使用起来也很慢，因此在测试时将许多不同的大型神经网络的预测结合起来很难处理过度拟合。Dropout就是一种解决此问题的技术。
![](https://img-blog.csdn.net/20180905140529941?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
关键思想是在训练期间从神经网络中随机删除单元及其连接，这可以防止单元间的过度适应。在训练期间，从指数数量的不同“稀疏”网络中抽取样本。在测试时，通过简单地使用具有较小权重的单解开网络（untwinednetwork），很容易近似平均所有这些稀疏网络以达到预测的效果。这显着减少了过度拟合，并且比其他正则化方法表现的更好。
Dropout已被证明可以改善神经网络在计算机视觉，语音识别，文档分类和计算生物学等领域的监督学习任务的性能，并在许多基准数据集上获得最先进的结果。

## 5-
## 最大池化
最大池化是基于样本的离散化过程。目的是对输入表示（图像、隐藏层输出矩阵等）进行下采样，通过降低其维数并允许对包含在子区域中的特征进行合并。
通过提供表征的抽象形式，这种方法在某种程度上有助于解决过拟合。同样，它也通过减少学习参数的数量和提供基本的内部表征的转换不变性来减少计算量。最大池化是通过将最大过滤器应用于通常不重叠的初始表征子区域来完成的。

## 6-
## 批量归一化
当然，包括深度网络在内的神经网络需要仔细调整权重初始化和学习参数。批量归一化有助于让中国过程更简单一点。
**权重问题：**
· 无论那种权重的初始化，随机还是凭经验选择，它们都和学习权重差别很大。考虑一个小批量数据集，在最初的时期，在特征激活时都会有许多异常值。
· 深度神经网络本身脆弱的，即初始层的微小扰动都会导致后面层很大的变化。
在反向传播期间，这些现象会导致梯度偏移，这意味着在学习权重以产生所需输出之前，梯度必须补偿异常值。这也将导致需要额外的时间来收敛。
![](https://img-blog.csdn.net/20180905140552480?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180905140603491?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
批量归一化将这些梯度从离散规则化为正常值，并在小批量的范围内朝向共同目标（通过归一化它们）流动。
**学习率问题：**通常，学习率保持较小，使得只有一小部分的梯度用来校正权重，原因是异常激活的梯度不应该影响已经学习好的权重。通过批量归一化，这些异常值被激活的可能性就会减少，因此可以使用更高的学习率来加速学习过程。

## 7-
## 长短期记忆：
LSTM网络具有以下三个方面，使其与递归神经网络中的常规神经元区分开来：
1、它可以控制何时让输入进入神经元。
2、它可以控制何时记住上一个时间步骤中计算的内容。
3、它可以控制何时将输出传递给下一个时间戳。
LSTM的优点在于它根据当前输入本身决定所有这些，如下图所示：
![](https://img-blog.csdn.net/20180905140616724?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
当前时间戳处的输入信号x（t）决定所有上述3个点。输入门决定点1.遗忘门在点2上做出决定，输出门在点3上做出决定。输入门能单独能够做出这三个决定。这受到了我们的大脑是如何工作的启发，并且可以处理突然的上下文切换。

## 8-Skip-gram
## ：
词嵌入模型的目标是为了每个词汇学习一个高维密集表征，其中嵌入向量之间的相似性显示了相应单词之间的语义或句法相似性。Skip-gram是学习词嵌入算法的模型。
skip-gram模型（以及许多其他词嵌入模型）背后的主要思想如下：如果两个词汇有相似的上下文，则它们是相似的。
![](https://img-blog.csdn.net/2018090514063140?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
换句话说，假设你有一句话，比如“猫是哺乳动物”。如果你使用术语“狗”而不是“猫”，句子仍然是一个有意义的句子。因此在该示例中，“狗”和“猫”可以共享相同的背景（即“是哺乳动物”）。
基于上述假设，你可以考虑一个上下文窗口（一个包含k个连续术语的窗口）。然后你应该跳过其中一个单词，并尝试学习除了跳过的一个术语之外的所有术语并预测跳过的术语的神经网络。因此如果两个单词在大型语料库中重复地共享相似的上下文，那些这些术语的嵌入向量将具有相似的向量。

## 9-
## 连续的词袋模型（Continuous Bag of Words）：
在自然语言处理问题中，我们希望学习将文档中的每个单词表示为数字向量，使得出现在相似上下文中的单词具有彼此接近的向量。在连续词袋模型中，目标是能够使用围绕特定单词的上下文并预测特定单词。
![](https://img-blog.csdn.net/20180905140645855?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
我们通过在一个大型语料库中抽取大量句子来做到这一点，每次看到一个单词时，我们都会使用其上下文单词。然后我们将上下文单词输入到一个神经网络中，并预测该上下文中心的单词。
当我们有数千个这样的上下文单词和中心单词时，我们就有一个神经网络数据集的实例。我们训练神经网络，在经过编码的隐藏层的输出表示特定单词的嵌入。碰巧的是，当我们在大量句子上训练时，类似上下文中的单词会得到类似的向量。

## 10-
## 迁移学习：
考虑下图像是如何通过卷积神经网络的。假设你有一个图像，你应用卷积，你得到像素组合作为输出。如果碰到了边缘，则再次应用卷积，所以现在输出是边或线的组合。然后再次应用卷积，此时的输出将是线的组合，依此类推。你可以将其视为每个层寻找特定模式。神经网络的最后一层往往变得非常专业。如果你正在使用ImageNet，那么你的网络最后一层将寻找儿童或狗或飞机或其他什么。再后退几层你可能会看到网络正在寻找眼睛或耳朵或嘴或轮子。
![](https://img-blog.csdn.net/20180905140659423?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
深度CNN中的每个层逐渐建立了更高和更高级别的特征表征。最后几层往往专注于你输入模型的任何数据。另一方面，早期的图层更通用，是在更大类的图片中找到很多简单的模式。
迁移学习是指你在一个数据集上训练CNN，切断最后一层，在其他不同的数据集上重新训练模型的最后一层。直观地说，你正在重新训练模型以识别不同的更高级别的功能。因此，模型训练的时间会大大减少，因此当你没有足够的数据或者训练所需的太多资源时，迁移学习是一种有用的工具。
本文仅显示这些方法的一般概述。我建议阅读以下文章以获得更详细的解释：
· Andrew Beam的**“深度学习101”**；
· 安德烈库林科夫的**“神经网络与深度学习简史”**；
· Adit Deshpande的**“理解卷积神经网络的初学者指南”**；
· 克里斯奥拉的**“理解LSTM****网络”**；
· Algobean的**“人工神经网络”**；
· Andrej Karpathy的**“**回归**神经网络的不合理有效性”****；**

深度学习是非常注重技术实践的。本文中的每个新想法都没有太多具体的解释。对于大多数新想法都附带了实验结果来证明它们能够运作。深度学习就像玩乐高，掌握乐高与任何其他艺术一样具有挑战性，但相比之下入门乐高是容易的。
以上为译文，由阿里云云栖社区组织翻译。
译文链接
文章原标题《the-10-deep-learning-methods-ai-practitioners-need-to-apply》，
作者：James Le译者：虎说八道**，审校：。**
文章为简译，更为详细的内容，请查看原文。


