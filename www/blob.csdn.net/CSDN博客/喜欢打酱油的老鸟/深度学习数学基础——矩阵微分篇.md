
# 深度学习数学基础——矩阵微分篇 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月05日 10:37:00[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：92


[https://www.toutiao.com/a6641771475994952206/](https://www.toutiao.com/a6641771475994952206/)
2019-01-02 13:45:27
> 深度学习是一个令人兴奋的领域，具有巨大的现实世界影响力。 本文是Terence Parr和Jeremy Howard撰写的基于'深度学习的矩阵运算'的笔记集合。
![深度学习数学基础——矩阵微分篇](http://p1.pstatp.com/large/pgc-image/d6623a3990424f25931105617841a2a2)
![深度学习数学基础——矩阵微分篇](http://p1.pstatp.com/large/pgc-image/eaf86b8e97cc49a29622f1f4f581b680)
![深度学习数学基础——矩阵微分篇](http://p3.pstatp.com/large/pgc-image/4f3101eecdd44e7f9af082bc8772f757)

> 感兴趣同学可关注本头条号 私信回复
> 深度学习矩阵微分
> 获取下载链接。

> 以下是该论文第一小节的中文翻译，后续的中文翻译我会陆续上传，请大家持续关注哦！

深度学习是基于线性代数的。它使用许多层的神经网络来解决复杂的问题。模型输入，多层神经元权重，激活函数等都可以定义为向量。操作/转换很自然地需要使用神经网络进行训练，同时应用于所有输入。矢量/矩阵表示和可用于它们的线性代数运算，非常适合神经网络的流水线的数据流模型。当输入、权重和函数被视为向量，**值的流动可被视为矩阵上的运算时，数学形式变得非常简单**。
**深度学习也是基于差异化的！**在训练阶段计算变化率对于优化损失函数至关重要。从任意一组网络模型权重w开始，目标是得到一个“最优”权重集合，以使给定的损失函数最小。几乎所有的神经网络都使用反向传播方法来找到这样一组权重。这个过程涉及权重值的变化如何影响输出。基于此，我们决定按比例增加或减少权重值。测量输出如何随着权重的变化而变化，与计算输出w.r.t权重w的（部分）导数相同。对于所有的训练样例，对于所有层中的所有权重，重复该过程。
*矩阵运算利用数学的两个基本分支 -***线性代数和微积分**。绝大多数人将线性代数和微积分分开学。这两个学科在各自的领域本身就都很重要。没有多少本科课程着重于矩阵运算。人们通常依靠直觉来弥补理解上的空白，同时还要考虑反向传播等概念。大多数机器学习算法中的反向传播步骤都是基于**计算向量和矩阵中的导数来更新值。**大多数机器学习框架本身做了大量的工作，我们永远不会看到实际导数计算的过程。然而，理解框架本身的内部工作是必要的，如果你打算成为一个合格的从业者或想要从头开发ML库，这是非常重要的。
虽然该论文面向DL从业者和编程人员，但它本质上是数学的。注意通过符号来巩固你的理解是非常重要的。特别注意诸如矢量的形状（长或高），标量或矢量，矩阵的尺寸等。**矢量用粗体字表示。没有经验的人可能不会注意到粗体f和斜体****f****字体之间的差异，但这在等式中有很大的差异。**向量的形状和方向也是一样的。为了理解术语，我尝试了很多方法才对这些有些理解，顺便说一下我一开始理解的很多术语很多都有问题。
庆幸的是，函数的概念（以及计算它们的衍生物的方法）是从简单到复杂的方式定义的。首先，我们从*f(x)*表示的简单参数函数开始。函数和参数*x*是标量（用斜体表示），我们可以使用传统的导数规则来计算*f(x)*的导数。其次，我们会看到的函数往往有很多变量与之相关联,以*f(x,y,z)*的形式。为了计算这些函数的导数，我们使用针对特定参数计算的偏导数。处理这些函数要用到多变量微积分知识。
将输入变量*x, y, z统一用*粗体**x**的向量描述，我们可以将输入参数向量的标量函数表示为*f*(**x**)。该领域的运算是向量运算，其中*f*(**x**)的偏导数被表示为向量本身并且适合于各种向量操作。最后，深度学习最有用的是同时表示多个这样的函数。我们使用**f(x)**来表示一组*f*(**x**)形式的函数。微积分领域最常见的是矩阵微积分（matrix calculus）。
回顾一下，*f(x)*是标量函数（使用简单的导数规则），*f*(**x**)是向量变量**x**（使用向量运算规则）的标量函数，**f(x)**是许多向量标量值函数，每个函数依赖于输入**x**的向量（使用矩阵微积分规则）。本文演示了如何计算简单函数的导数，以及多元函数中的偏导数（∂/∂x），矢量演算中的梯度∇f函数和和矩阵演算中的雅可比矩阵J。差不多可以说，∇*f(***x***)*函数是矢量形式*f*的偏导数的集合。**f(x)**的雅可比矩阵每行基本上是单独的∇*f(***x***)*。
在计算偏导数的过程中，本文做了一些假设。要记住计算输出函数的偏导数（y = w.x +b）和损失函数是很重要的。第一个假设是向量**x**的基数等于**f**中的标量函数的个数。这提供了一个方形雅可比矩阵。如果你想知道为什么他们需要相等，考虑这样一个情况，每个神经元*xi*的输入都与权重*wi*相关（这里的标量函数类似于*xi*wi*），所以我们拥有和*x*一样多的*w*。
另一个重要的假设是关于元素对角线性质。基本上，该属性表明**f(x)**中的第i个标量函数是（仅）矢量x中第项的函数。想象常见的神经元模式你发现，这样做更有意义。输入*xi*的贡献与单个参数*wi*成比例。假设元素对角线性质使雅可比行列式（由第一个假设制成的方形）变成对角矩阵，所有非对角线项都为零。
论文的接下来的几节将解释计算更复杂函数的导数的过程。函数可以从简单到复杂有几种方式。首先，考虑通过在两个向量（当然具有相同大小）上应用基于元素的二元运算符得到的函数。这些函数的形式为**f(x,y)**=**x + y**或max**(x, y)**。请注意，在这种情况下，**(x, y)**是向量。接下来，有一些标量扩展函数是通过将矢量乘法/加入标量。该操作涉及将标量“扩展”到与矢量相同的维度，然后执行元素的乘法和加法操作。例如，**y**=**x**+ b 被扩展到向量**b**，并且被元素地添加到**x**。
第三，考虑将向量中的值转化为单个值的函数。最常见的例子是计算神经网络的损失，通常是形式*y*=*sum*(**f(x)**)。这里*y*是通过将向量**f(x)**的元素相加得到的标量值。
本文计算了这三种情况下的衍生方法。有些函数可能很复杂，需要使用导数的链式规则。本文描述了简单标量函数的链式规则，并逐渐将其扩展到所有向量的链式规则。

