
# 【机器学习-学习笔记】单-多变量线性回归、多项式回归、逻辑回归、过拟合、正则化 - 喜欢打酱油的老鸟 - CSDN博客


2018年08月09日 08:10:36[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：35标签：[机器学习																](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)[线性回归																](https://so.csdn.net/so/search/s.do?q=线性回归&t=blog)[过拟合																](https://so.csdn.net/so/search/s.do?q=过拟合&t=blog)[正则化																](https://so.csdn.net/so/search/s.do?q=正则化&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=过拟合&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=线性回归&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)
[
																								](https://so.csdn.net/so/search/s.do?q=线性回归&t=blog)
[
				](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)
[
			](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)

# [https://blog.csdn.net/m511655654/article/details/81507857](https://blog.csdn.net/m511655654/article/details/81507857)
|问题|描述|表达式描述|表达式|
|---|---|---|---|
|单变量线性回归|只含有一个特征/输入变量|一元一次表达式![](https://img-blog.csdn.net/20180808144228368?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)|
|多变量线性回归|含有多个特征/输入变量|多元一次函数![](https://img-blog.csdn.net/20180808144842395?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)|
|多项式回归|含有多个特征/输入变量|多元多次函数![](https://img-blog.csdn.net/20180808145409949?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)|
|逻辑回归|含有多个离散输出，解决分类问题|sigmoid函数![](https://img-blog.csdn.net/20180808150256658?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)|
|过拟合|有效适应训练集，但泛化性能差|1、剔除一些不必要特征，如PCA
|2、正则化，保留所有特征，但减少参数的大小
|
|问题|梯度下降|
|---|---|
|单变量线性回归![](https://img-blog.csdn.net/20180808144228368?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808144436605?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808144609138?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808144617275?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|
|多变量线性回归![](https://img-blog.csdn.net/20180808144500151?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180808144503364?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808144922275?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808145022491?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|特征缩放：
![](https://img-blog.csdn.net/20180808145153230?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|学习率：
![](https://img-blog.csdn.net/20180808145226848?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|
|逻辑回归![](https://img-blog.csdn.net/20180808150248839?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808145954129?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|等价于
![](https://img-blog.csdn.net/20180808150510102?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808150514844?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|求偏导得：
![](https://img-blog.csdn.net/2018080815055616?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|
|正则化|修改代价函数：
![](https://img-blog.csdn.net/20180808153548138?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|令λ值很大，为了使cost function很小，所有θ（不包括θ0）都会一定程度较少。
|
|正则化线性回归![](https://img-blog.csdn.net/20180808154114132?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808154136457?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|
|正则化逻辑回归![](https://img-blog.csdn.net/20180808154242165?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180808154302561?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L201MTE2NTU2NTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
|

