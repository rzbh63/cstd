# sigmoid与条件概率的关系及推导过程

sigmoid是机器学习中常用的激活函数，其优势作者这里不一一赘述，本文主要从另一个角度，判别模型（discriminative model)条件概率的角度对sigmoid函数进行推导。

机器学习判别模型主要想通过神经网络对 $P(Y|X)$ 进行建模。

假设 $x\epsilon {R}^{n}$且$y\epsilon{y_{1},y_{2}}$且$P(x|y_{1}) \sim N(\mu_{1} ,\sum)$且 $P(x|y_{1}) \sim N(\mu_{1} ,\sum)$
根据上述假设可得： 
$$
P(x|y_{1}) ＝ \frac{1}{{(2\pi)}^{n/2} \sum^{1/2}} {e}^{-{(x-\mu_{1})^{T}{\sum}^{-1}(x-\mu_{1})}}\\
P(x|y_{2}) ＝ \frac{1}{{(2\pi)}^{n/2} \sum^{1/2}} {e}^{-{(x-\mu_{2})^{T}{\sum}^{-1}(x-\mu_{1})}}
$$



根据贝叶斯公式： 
$$
P(y_{1}|x) =\frac {P(y_{1},x)}{p(x)} ＝\frac {P(x|y_{1})P(y_{1})} {P(x)} = \frac {P(x|y_{1})P(y_{1})}{P(x|y_{1})P(y_{1})+P(x|y_{2})P(y_{2})} = \frac{1}{1+\frac {P(x|y_{1})P(y_{1})} {P(x|y_{2})P(y_{2})}}
$$


观察 $\frac {P(x|y_{1})} {P(x|y_{2})}$ 带入假设$P(x|y_{1}),P(x|y_{2})$概率密度函数，可得
$$
\begin{align}
\frac {P(x|y_{1})} {P(x|y_{2})}  
&=\frac{\frac{1}{{(2\pi)}^{n/2} \sum^{1/2}} {e}^{-{(x-\mu_{1})^{T}{\sum}^{-1}(x-\mu_{1})}}}{\frac{1}{{(2\pi)}^{n/2} \sum^{1/2}} {e}^{-{(x-\mu_{2})^{T}{\sum}^{-1}(x-\mu_{1})}}}\\
&= {e}^{{(x-\mu_{2})}^T{\sum}^{-1}{(x-\mu_{2})}-{(x-\mu_{1})}^T{\sum}^{-1}{(x-\mu_{1})}}={e}^{({(x-\mu_{2})}^T{(x-\mu_{2})}-{(x-\mu_{1})}^T{(x-\mu_{1})}){\sum}^{-1}}\\
&= {e}^{{x}^{T}x-{x}^{T}{\mu_{2}}-{\mu_{2}}^{T}x+{\mu_{2}}^T\mu_2-{x}^{T}x+{x}^{T}{\mu_{1}}+{\mu_{1}}^{T}x-{\mu_{1}}^T\mu_{1}}\\
&= e ^{-x^{T}\mu_{2}-{\mu_{2}}^{T}{x}+{\mu_{2}}^{T}{\mu_{2}} +{x}^{T}{\mu_{1}}+{\mu_{1}}^{T}x-{\mu_{1}}^T\mu_{1} }\\
&=e^{-2x^{T}(\mu_{1}-\mu_{2})-(\mu_{1}{\mu_{1}}^{T}-\mu_{2}{\mu_{2}}^{T})}
\end{align}
$$


进一步可以化简为,将$\frac {P(y_{1})}{P(y_{2})} = e^{ln^{\frac {P(y_{1})}{P(y_{2})}}}$: 
则$\frac {P(x|y_{1})P(y_{1})} {P(x|y_{2})P(y_{2})}$ 可以化简为：
$$
e^{-2x^{T}(\mu_{1}-\mu_{2})-(\mu_{1}{\mu_{1}}^{T}-\mu_{2}{\mu_{2}}^{T})+ln^{\frac {P(y_{1})}{P(y_{2})}}} 
$$


进一步可以简写为$e^{-(x^{T}w+b)}$

所以条件概率可以写为：
$$
P(y_{1}|x) ＝  \frac{1}{1+\frac {P(x|y_{1})P(y_{1})} {P(x|y_{2})P(y_{2})}} = \frac{1}{1+ e^{-(x^{T}w+b)}}
$$
这个就是sigmoid函数的形式。

**注**：（1）同理也可以通过假设多个条件概率密度函数，也可以求得softmax形式。（2）为了便于推导方便这里假设协方差矩阵相同，不同的话也没有问题，不影响最终线性表达式的形式



