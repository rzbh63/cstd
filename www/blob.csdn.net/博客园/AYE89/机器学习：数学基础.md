
# 机器学习：数学基础 - AYE89 - 博客园|[AYE89](https://www.cnblogs.com/eniac1946/)
|coding & learning|
|posts - 153, comments - 3, trackbacks - 0, articles - 2

|
|导航
|[博客园](https://www.cnblogs.com/)|[首页](https://www.cnblogs.com/eniac1946/)|[新随笔](https://i.cnblogs.com/EditPosts.aspx?opt=1)|[联系](https://msg.cnblogs.com/send/AYE89)![订阅](//www.cnblogs.com/images/xml.gif)|[订阅](https://www.cnblogs.com/eniac1946/rss)|[管理](https://i.cnblogs.com/)
|公告


|[机器学习：数学基础](https://www.cnblogs.com/eniac1946/p/7842867.html)
|Posted on|2017-11-16 09:57|[AYE89](https://www.cnblogs.com/eniac1946/)|阅读(|...|) 评论(|...|)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=7842867)|[收藏](#)
|线性代数
|矩阵和向量的乘法
|A|(m*n)|·|x|(n*1) =|b|(m*1)
|线性变换：|这个公式实际上和机器学习中一些线性模型相似（线性回归）——|x|存在于一个n维的实数向量空间，|b|存在于一个m维的实数向量空间，那么|A|的作用相当于一个映射，     　　　　   而且它们之间是线性计算的关系。
|两种理解：
|1：|A|（每一行） *|x|（列 /每个元素） -->|b|（对应一个元素）
|含义：
|A中的每一行相当于x中每个元素的权重，它们的就算结果|汇总|成一个数，相当于对x中的元素做|加权求和。|——每一次相当于，内积运算，表示了|相关程度
|线性变换相当于对x完成了m次内积运算（x与A的行向量的相关程度），b是A与x相关程度的汇总。m次汇总
|从这种角度看A的含义：A表示|向量的某一抽象“特征”|，x与“特征”越接近（相关），线性变换结果越大。
|（即b就是x与A的行向量的相关程度的汇总，A代表了理想向量具备的“特征”，x像A则会结果比较大，不像结果比较小）
|2：|A|中每一个列向量和|x|中的每一个元素依次相乘，相乘后的向量再加在一起得到结果|b
|x|的每一个元素成为|A|列向量的权重
|直观理解：|A|的每一个列向量表示了结果|b|所在空间的一种向量表达，那么|x|的作用就是平衡这些向量表达并将这些表达糅合成一个新的向量
|（把A的每个列向量看成一个坐标轴，那么b的每一维就是x在A的各个坐标轴上投影的值）
|概率论
|概率分布：
|离散随机变量，二值的有伯努利分布；
|连续随机变量，经典的有高斯分布；
|信息论
|KL散度
|z是隐变量，x是观测变量，如果要找到一个相对简单的概率分布|q|，使它尽可能地近似待分析的后验概率|p|（z|x），这里的目标函数就是KL散度。
|KL散度可以很好的度量两个概率分布之间的距离。两个分布越接近，那么KL散度越小；如果越远，KL散度就会越大。
|机器学习的目标函数
|SquareLoss = 1/2 (y-t)|2
|CrossEntropyLoss
![](https://images2017.cnblogs.com/blog/1181483/201712/1181483-20171227164827566-972385421.png)
|两个服从伯努利分布的随机变量，它们的交叉熵|H|(P,Q)为：
|CrossEntropyLoss = - P(0)|log|Q(0) - (1 - P(0))|log|(1 - Q(0))
|选择：
|如果输出的结果是一个回归问题的一个连续型随机变量，使用|平方损失|函数更合适；
|如果输出是分类问题的一个离散One-Hot向量，那么|交叉熵|损失函数更合适；
|分析：
|在模型优化过程中，交叉熵损失的梯度只和正确分类的预测结果有关，而平方损失函数考虑的内容更多，还与错误的分类有关。
|分类问题上平方损失函数，除了让正确分类尽可能大，还会让错误分类都变得更平均，但实际中后面这个调整是不必要的，所以平方损失实际上完成了额外的工作；
|而这个调整在回归问题上非常重要，因此回归问题上用交叉熵损失显然就不合适了。







|[刷新评论](javascript:void(0);)|[刷新页面](#)|[返回顶部](#top)






|
|Powered by:
|博客园
|Copyright © AYE89
|
