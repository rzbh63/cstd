# 语音识别的痛点在哪，从交互到精准识别如何做？

2016年08月28日 22:33:13 [我把葡萄酿成酒](https://me.csdn.net/ffmpeg4976) 阅读数：14170



语音识别是目前应用最成熟的人机交互方式，从最初大家体验过的有屏手持设备这种近场的语音识别，如Siri以及各种语音助手，到现在，语音识别的应用已经完成了向智能硬件以及机器人上延伸，不过，新的人机交互对硬件、算法要求更加苛刻，各企业正面临着巨大的挑战。

那么实现人机交互需要解决哪些问题？这其中的关键技术有哪些？人机交互的未来趋势如何？本期硬创公开课邀请了科大讯飞的研发主管赵艳军为大家答疑解惑。

> 分享嘉宾：赵艳军，AIUI项目负责人，科大讯飞云平台研发主管，负责讯飞开放平语音识别、语音合成、声纹、唤醒等多项产品研发，同时负责人机交互解决方案AIUI的开发，致力于把核心技术更好的产品化，使人与机器的交流像人与人之间一样简单，自然。

人机交互的痛点 
语音作为目前人机交互的主要方式，大家如果使用过，应该都能举出一些例子。比如说话要靠近，发音要标准，环境要安静，不能持续对话，不能打断等。 
![这里写图片描述](https://img-blog.csdn.net/20160828214019957)

不只是语音，包括图像、障碍物检测等技术，都会遇到这样的问题，比如人脸的识别，对光线、角度、距离都有一定的要求。 归结为一点就是，当前人机交互在复杂环境的自适应性方面，还有很多问题需要解决。这还只是**感知**层面，另外还包括**认知**层面，AI目前还不如我们想象的这么聪明，目前不能完全自主学习，仍然需要人的介入，比如知识库的导入，机器行为的纠正等，都需要人的参与。

当前的人机交互产品，在真正面对用户时，在面对复杂环境时，鲁棒性还不够好。今天的分享，我们一起探讨如何解决这些问题，不管是通过算法，还是工程，抑或产品，都是我们可以选择的途径。

大家首先要有个共识，人机交互目前所面临的问题，不是一朝一夕，一蹴而就能解决的，需要在各个方向在核心技术上不断进步。

### 科大讯飞AIUI是怎么做的？

AIUI作为人机智能交互界面，旨在实现人机之间交互无障碍，使人与机器之间的交流，像人与人一样，可以通过语音、图像、手势等自然交互的方式，进行持续，双向，自然地沟通。它由一套由云和客户端相结合服务框架构成，包括音视频前端信号处理、云+端相配合交互引擎、内容和知识平台以及接口、用户个性化系统等。平台具备开放性，第三方可进行灵活配置、业务扩展、内容对接等。

以前的语音交互产品，包括讯飞在内，大家提供的都是单点的能力，比如语音合成、语音唤醒、语音识别、语义理解，另外还有人脸识别、声纹识别等。大家拿到这么多产品和能力，需要花很大的工作量，去开发人机交互功能。 
这种方式问题比较明显：

一方面是产品集成的工作量太大，造成很多中小开发者无力去承担这部分工作量；

另外就是因为交互流程太长，细节不好处理，造成各家产品的交互体验参差不齐。 
所以AIUI交互方案首先要解决的就是这个问题。AIUI是把**麦克风阵列、前端声学处理、语音唤醒、端点检测、语音识别、语义理解、语音合成等技术在整个交互链上进行深度融合**的系统。 
而且AIUI还支持了全双工、多轮交互等新特性，并在单点技术上进行突破和演进，包括声源定位与追踪、持续在线，有效人声智能检测、基于用户意图的动态语音端点检测、支持上下文交互的语义理解、基于对话场景自适应的语音识别等。 
科普：语音交互的过程 
![这里写图片描述](https://img-blog.csdn.net/20160828214545496) 
首先，人机之间的语音交互（这里主要指智能硬件、机器人等），区别于传统的有屏手持设备，在传统的语音交互中，因为是近场，语音信号质量相对较高，而且有触摸屏辅助，所以交互链路可以相对简单。通过点击屏幕触发，再通过点击屏幕或者能量VAD检测，来结束语音信号采集，即可完成一次交互，整个过程通过语音识别、语义理解、语音合成即可完成。

而对于人机之间的交互，由于涉及到远场，环境比较复杂，而且无屏交互，如果要像人与人之间的交流一样自然、持续、双向、可打断，整个交互过程需要解决的问题更多，AIUI为完成类人的语音交互，是一个需要软硬件一体、云+端相互配合的过程。 
![这里写图片描述](https://img-blog.csdn.net/20160828215232483) 
我们来看下交互的整个流程，从大的方面来说，还是以语音识别、语义理解、语音合成为主线，只不过每个过程需要解决更多的问题。

首先来看下**语音唤醒**，唤醒是人机交互的主要触发方式，就像你要和一个人说话，就得首先喊一下这个人的名字，才能开始进行交流。而在交流的过程中，如果有其他人喊你呢，你也是需要响应的，需要支持持续唤醒。

机器被唤醒后，就需要知道**说话人的方位**，这样机器才可以做出更友好的响应，比如转身，移动等。只有明确说话人的方位后，才可以定向的拾音，做降噪处理，同时对说话人进行语音增强。这个声源定位和语音增强主要是用麦克风阵列的相关技术，下面会有详细解释。

在语音识别这个重要模块中，首先要解决的就是**远场识别**，通过上面提到的**麦克风阵列和声源定位**，可以较好的实现远距离拾音，解决**噪声、混响、回声**带来的影响。作为全双工交互，持续的音频流拾取，就要解决人声检测和断句问题，机器需要能够过滤无效语音，做出快速的反馈和响应。

人声和端点的检测不能只依赖于能量检测的技术方案，需要解决更为复杂的环境，具体怎么解决呢，下面再展开。

对于识别来说，首先要保障的是远场环境下的识别率，除了前面提到的麦克风阵列解决了前端声学的问题，还要有**专门针对远场环境下，基于大量数据训练的声学模型**，这样才能保证识别率满足交互需求。

除了云端的语音识别，端上的识别也是需要做的，需要云+端方式进行结合，这样才能满足复杂网络下的使用场景。不过端上主要是去做一些命令式交互，响应一些本地操作。比如关机、打电话、动作指令等。

本地不做成无限制的响应呢，因为对于很多的用户意图来说，是需要基于网络去获取内容的，所以本地只做辅助使用，是为了解决一些复杂网络环境所做的手段。这个地方的难点是需要做好云和端的PK策略，需要基于置信度、响应时间等信息来综合决策。作为持续的语音交互，不可避免要吸收很多无效的语音，拒识成为必须，否则会造成对话的混乱和无意义的响应。

对于支持多轮的交互理解，**语义引擎不再是无状态的**，系统更加复杂，需要有对话管理、历史信息存储等模块。语义理解不只包含用户说话意图，还要包括内容的获取，这样才能在接下来的**端点检测、语音识别**等交互过程中，共享信息，做到场景自适应，以提高准确率。

完成语音识别和语义的理解后，机器需要通过**语音合成**，把信息传递给用户。合成这一块没有太多需要展开的，讯飞提供了几十种不同的发音人，而且支持不同情感进行朗读，至于**该以何种情境、情感进行播报，这是在语义理解中需要给出的信息**。相比传统的交互，现在的流程会看起来复杂很多。

## 功能：远场识别、全双工、多轮交互

远场识别 
![这里写图片描述](https://img-blog.csdn.net/20160828215937420)

远场识别，需要前后端结合去完成，一方面在前端使用麦克风阵列硬件，通过声源定位及自适应波束形成做语音增强。在前端完成远场拾音，并解决噪声、混响、回声等带来的影响。

不过这还不够，因近场、远场的语音信号，在**声学**上有一定的规律差异，所以在后端的语音识别上，还需要结合基于大数据训练、针对远场环境的声学模型，这样才能较好解决识别率的问题。

全双工 
![这里写图片描述](https://img-blog.csdn.net/20160828220057205) 
全双工交互，是一个全链路的贯穿过程，不只是持续的拾音和网络传输，更需要包括持续的语音唤醒、智能有效人声检测、动态语音端点检测、无效语音拒识等各个模块相互配合，才能完成。

支持**连续的语音唤醒**是必须。在传统的语音唤醒方案中，是一次唤醒后，进行语音识别和交互，交互完成再进入待唤醒状态。但是在实际人与人的交流中，人是可以与多人对话的，而且支持**被其他人插入和打断**。AIUI中语音唤醒采用BN（Bottle Neck）技术方案，支持低功耗的待机。

全双工交互作为一个持续的交互过程，语音识别和语义理解，需要能够做出快速的响应。这就需要**人声检测和智能断句**。传统的断句是基于能量的检测来判定，但是有两个主要缺点，**一是无法过滤噪音和无效的语音，另外就是对说话人的要求较高，中间不能有停顿。如果后端点设置的太短，容易造成截断；后端点太长，又会造成响应不及时**。

AIUI的做法是，采用**基于模型的有效人声智能检测**和**基于用户意图的动态语音端点检测**。基于模型的检测可以有效解决噪音和无效语音。这块主要是通过采集不同环境的噪音，基于深度神经网络的训练出对应声学模型，进行过滤，把有效的语音传送到云端进行交互。

动态端点检测算法实现从连续输入的数据流中检测出包含完整用户意图的语音送入**语义理解模块**，可以很好的解决用户的停顿，因为在人机的交流过程中，在一句包含完整意图语音中，停顿是很常见的现象，这在我们对用户的行为分析中得到验证。

另外在持续的语音交互过程中，必然会有无效的语音和无关说话内容被吸收进来，所以拒识是必须。在AIUI系统中，我们针对全双工交互中的这个问题，专门构建了一套**基于深度神经网络的拒识系统**，从声学信号、语义等多个方面对接收的语音进行拒识判断。

## 多轮交互

![这里写图片描述](https://img-blog.csdn.net/20160828221617777)

同样的，对于多轮交互中的语义理解和对话管理两个模块，我们也采用深度学习+海量数据的方式，使用用户的实际数据，训练鲁棒的语义理解和对话管理模型。

结合基于LSTM（长短时记忆）的循环神经网络，使得模型具有长时记忆的能力，结合对话上下文进行准确的语义理解，相信配合科大讯飞深度神经网络+大数据+“涟漪效应”的研究思路，我们的多轮交互会越来越准确、好用。

### 关键技术：麦克风阵列

![这里写图片描述](https://img-blog.csdn.net/20160828221759887) 
大家通过上图可以看到，现实环境中噪音、混响、人声干扰、回声等因素，带来的影响因素还是比较大的，我们一般是通过麦克风阵列来解决。 
![这里写图片描述](https://img-blog.csdn.net/20160828221856435)

麦克风阵列是利用一定数目，一定空间构型的声学传感器（一般是麦克风）组成，用来对声场的空间特性进行采样并处理的系统。麦克风阵列能做很多事情，对于环境噪声，它可以采用**自适应波束形成做语音增强**，从**含噪语音信号中提取纯净语音**；对于说话人说话位置的不确性，它可以通过**声源定位技术**来计算目标说话人的角度，来**跟踪说话人**以及后续的语音定向拾取；对于室内声音反射，导致语音音素交叠，识别率较低的问题，它可以通过**去混响技术**，减小混响，提高识别率。

线性、环形、球形麦克风在原理上并无太大区别，只是由于空间构型不同，导致它们可分辨的空间范围也不同。比如，在声源定位上，线性阵列只有一维信息，只能分辨180度；环形阵列是平面阵列，有两维信息，能分辨360度；球性阵列是立体三维空间阵列，有三维信息，能区分360度方位角和180度俯仰角。

其次麦克风的个数越多，对说话人的定位精度越高，但是定位精度的差别体现在交互距离的远近上，如果交互距离不是很远，5麦和8麦的定位效果差异不是很大。此外，麦克风个数越多，波束能区分的空间越精细，在嘈杂环境下的拾音质量越高，但是在一般室内的安静环境下，5麦和8麦的识别率相差不是很大。麦克风个数越多，成本也越高，具体的产品，要综合考虑应用场景和实际的产品定位，选择合适的麦克风个数和阵型。

### 方案：破解环境对语音识别的影响

![这里写图片描述](https://img-blog.csdn.net/20160828222306061)

复杂的环境，一方面是外在环境的复杂，另一方面是方言和口音。外在环境复杂包括噪声、混响、回声等，而且噪音又分为不同的会议室、户外、商场等不同环境，为了解决这些问题，除了**单通道语音增强技术**，现在基本是采用上面提到的**麦克风阵列硬件和相关算法**实现。

在方言、口音方面，大家都知道，在我们国家，几十种方言，每个人都有自己的独特口音，一般的解决方法的是**基于各种方言数据，通过深度神经网络，训练各种方言模型**，以提高识别率，这是业内的通用做法。

为解决两方面的问题，讯飞正通过以下的技术方案，去适应各种复杂环境的要求。包括以下方面：

1） 提供条形、环形、球形的四麦、五麦、八麦等多种不同的麦克风阵列构型，以适应不同的产品需求，比如叮咚音箱采用的就是环形8麦的方案。

麦克风阵列技术虽然已经可以达到相当的技术水平，但是总体上还是存在一些问题的，比如当麦克风和信号源距离太远时(比如10m、20m距离)，录制信号的信噪比会很低，算法处理难度很大;对于便携设备来说，受设备尺寸以及功耗的限制，麦克风的个数不能太多，阵列尺寸也不能太大。而**分布式麦克风阵列技术**则是解决当前问题的一个可能途径。

所谓分布式阵列就是将子阵元或子阵列布局到更大的范围内，相互之间通过有线或者无线的方式进行数据的交换和共享，并在此基础上进行广义上的声源定位、波束形成等技术实现信号处理。

相对于目前集中式的麦克风阵列，分布式阵列的优势也是非常明显的。首先分布式麦克风阵列(尤其无线传输)的尺寸的限制就不存在了;另外，阵列的节点可以覆盖很大的面积。总会有一个阵列的节点距离声源很近，录音信噪比大幅度提升，算法处理难度也会降低，总体的信号处理的效果也会有非常显著的提升，目前科大讯飞已经开始了相关技术研究的布局工作。

2）不同环境的语音识别声学模型，如上面提到的远场拾音，专门针对远距离拾音的环境进行训练；

3）在方言方面，讯飞支持普通话、粤语等20多种方言，是目前涵盖方言范围最广的。积累了一定量的多方言资源库，并基于特殊的深度神经网路结构和上线数据的半监督训练，实现了多方言数据信息的共享和方言的自动迭代更新。

另外方言最大的难点，在于**方言的自适应**，如何能够根据用户的语音，自动匹配模型，讯飞已经验证完成。

4）在口音的适配方面，讯飞已经有一套完整的基于用户的训练系统，可以针对每个用户，建立闭环的优化流程，为每个用户建立自己的个性化声学模型，目前正在讯飞输入法试点，对于一些注册用户进行灰度体现；

5）基于特定人群的模型训练，目前已经为面向儿童的玩具方案，专门训练针对儿童的声学匹配模型；

6）为每个应用、每个用户，提供个性化的语言模型；

### AIUI的服务

![这里写图片描述](https://img-blog.csdn.net/20160828222917179)

AIUI对外完全开放，不管是个人开发者还是公司，在我们的平台上都可使用。因AIUI需要与硬件相结合，所以现在是以**评估板**的形式开放。如果评估效果满足产品需求，我们提供**模块或者软核的方式**支持产品量产。

开放平台除了提供整体的方案和几十个业务场景，还提供产品的个性化定制能力，主要包括**唤醒词定制**、**发音人定制**、**交互语义理解定制**、**语音识别资源定制**、**流程参数配置**等，这些都是在Web平台上开放的功能，开发者可以根据产品需求，在平台上进行个性化的配置和编辑。

如语义开放平台，提供私有语义编写、自定义问答导入，这一块相信是大家最为关心的，每个机器人如何回答用户的提问，主要就通过这方面来体现。

还有很重要一点，AIUI允许第三方系统接入，AIUI作为可扩展的伸缩服务，经过语音云处理的识别和语义结果，只要在平台上配置，第三方业务系统即可通过Http服务接入，以满足更为复杂的个性化需求。

当然讯飞开放平台还提供深度的定制服务，包括唤醒词训练、发音人训练、语义及内容制作等。通过平台开放和深度定制两种方式，可以满足各个产品之间的差异化。

### 未来：人机交互会融合？

融合必然是未来人机交互的趋势。

以AIUI为例，在定义之初，就没有把语音作为唯一的交互方式，而是把它设想为结合了人脸、人体追踪、手势、红外等多种方式为一体的人机交互解决方案。

我们也在不断的尝试**把语音和其他方式相结合**，比如我们现在已经上线的人脸、声纹融合身份验证，即是最直接的例子，通过这种方式将能够有效解决用户的身份认证问题。

在AIUI中，也有很多的场景需要借助于不同方式来相互配合。举个例子，在AIUI中，为解决远场的持续交互，使用了麦克风阵列，采用定向拾音的方式来解决这个问题，但是由于**定向拾音的角度有限**，造成持续交互过程中，说话人的移动成为问题，这个时候，就需要有移动的声源定位和追踪，简单依靠声音的辨识和追踪，很难有效解决这个问题，这个时候如果能**结合人体的追踪**，**比如图像和红外手段**，将可以进一步降低出错概率。其他的场景还有人的年龄、性别等属性识别，如果采用图像、声音相结合方式，将可以极高提升精准度，提升机器的感知能力，AI也将更加智能。