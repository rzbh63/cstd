
# 漫步最优化三十七——共轭梯度法 - 蜗牛 - CSDN博客


2017年11月03日 21:24:38[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：371



$\textbf{我卸不下对你的喜欢，}$
$\textbf{因为爱会慢慢增加重量。}$
$\textbf{我醉心于你的发香，}$
$\textbf{因为它让回想有了画面感。}$
$\textbf{在虚拟的土壤与}$
$\textbf{真实的肉体上，}$
$\textbf{文字与真心蔓延滋长了}$
$\textbf{我们的感情。}$
$\textbf{脑海储存着幸福，}$
$\textbf{不断放送着你可爱的模样。}$
$\qquad\textbf{——畅宝宝的傻逼哥哥}$
Hestenes与Stiefel提出了一种生成共轭方向的有效方法，就是共轭梯度法。该方法中，每次迭代生成方向，当迭代第$k+1$次时，用前一个方向$\mathbf{d}_k$生成新的点$\mathbf{x}_{k+1}$，然后$\beta\mathbf{d}_k$加上$-\mathbf{g}_{k+1}$(新点处的负梯度)生成新的方向$\mathbf{d}_{k+1}$。
共轭方向法基于下面的定理，除了定义生成共轭方向的方法外，其余都与上篇文章的定理1一样。
$\textbf{定理1：}$(a)如果$\mathbf{H}$是正定矩阵，那么对任意初始点$\mathbf{x}_0$与初始方向

$$
\mathbf{d}_0=-\mathbf{g}_0=-(\mathbf{b}+\mathbf{Hx}_0)
$$
由递推关系

$$
\begin{equation}
\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k\mathbf{d}_k\tag1
\end{equation}
$$
生成的序列收敛到唯一解$\mathbf{x}^*$，其中

$$
\begin{align}
\alpha_k&=-\frac{\mathbf{g}_k^T\mathbf{d}_k}{\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k}\tag2\\
\mathbf{g}&=\mathbf{b}+\mathbf{Hx}_k\tag3\\
\mathbf{d}_{k+1}&=-\mathbf{g}_{k+1}+\beta_k\mathbf{d}_k\tag4\\
\beta_k&=\frac{\mathbf{g}_{k+1}^T\mathbf{Hd}_k}{\mathbf{d}}_k^T\mathbf{Hd}_k\tag5
\end{align}
$$
(b)梯度$\mathbf{g}_k$与$\{\mathbf{g}_0,\mathbf{g}_1,\ldots,\mathbf{g}_{k-1}\}$正交，即

$$
\mathbf{g}_k^T\mathbf{g}_i=0\quad for\ 0\leq i<k
$$
$\textbf{证明：}$收敛性的证明与上篇文章的定理1一样，所以还需要证明的就是方向$\mathbf{d}_0,\mathbf{d}_1,\ldots,\mathbf{d}_{n-1}$组成一个共轭集合，即

$$
\mathbf{d}_k^T\mathbf{Hd}_i=0\quad\text{for}\quad 0\leq i<k\ \text{and}\ 1\leq k\leq n
$$
接下来我们用归纳法进行证明。假设

$$
\begin{equation}
\mathbf{d}_k^T\mathbf{Hd}_i=0\quad\text{for}\quad 0\leq i<k\tag6
\end{equation}
$$
我们需要说明

$$
\mathbf{d}_{k+1}^T\mathbf{Hd}_i=0\quad\text{for}\quad 0\leq i<k+1
$$
令$S(\mathbf{v}_0,\mathbf{v}_1,\ldots,\mathbf{v}_k)$是向量$\mathbf{v}_0,\mathbf{v}_1,\ldots,\mathbf{v}_k$生成的子空间，因为

$$
\begin{equation}
\mathbf{g}_{k+1}=\mathbf{g}_k+\alpha_k\mathbf{Hd}_k\tag7
\end{equation}
$$
故当$k=0$时，我们有

$$
\mathbf{g}_1=\mathbf{g}_0+\alpha_0\mathbf{Hd}_0=\mathbf{g}_0-\alpha_0\mathbf{Hg}_0
$$
因为$\mathbf{d}_0=-\mathbf{g}_0$。另外，由等式4可得

$$
\mathbf{d}_1=-\mathbf{g}_1+\beta_0\mathbf{d}_0=-(1+\beta_0)\mathbf{g}_0+\alpha_0\mathbf{Hg}_0
$$
即$\mathbf{g}_1,\mathbf{d}_1$是$\mathbf{g}_0,\mathbf{Hg}_0$的线性组合，所以

$$
S(\mathbf{g}_0,\mathbf{g}_1)=S(\mathbf{d}_0,\mathbf{d}_1)=S(\mathbf{g}_0,\mathbf{Hg}_0)
$$
同样地，对于$k=2$我们有

$$
\begin{align*}
\mathbf{g}_2=
&\mathbf{g}_0-[\alpha_0+\alpha_1(1+\beta_0)]\mathbf{Hg}_0+\alpha_0\alpha_1\mathbf{H}^2\mathbf{g}_0\\
\mathbf{d}_2=
&-[1+(1+\beta_0)\beta_1]\mathbf{g}_0+[\alpha_0+\alpha_1(1+\beta_0)+\alpha_0\beta_1]\mathbf{Hg}_0\\
&-\alpha_0\alpha_1\mathbf{H}^2\mathbf{g}_0
\end{align*}
$$
因此

$$
\begin{align*}
S(\mathbf{g}_0,\mathbf{g}_1,\mathbf{g}_2)
&=S(\mathbf{g}_0,\mathbf{Hg}_0,\mathbf{H}^2\mathbf{g}_0)\\
S(\mathbf{d}_0,\mathbf{d}_1,\mathbf{d}_2)
&=S(\mathbf{g}_0,\mathbf{Hg}_0,\mathbf{H}^2\mathbf{g}_0)
\end{align*}
$$
继续用归纳法，我们可以得到

$$
\begin{align}
S(\mathbf{g}_0,\mathbf{g}_1,\ldots,\mathbf{g}_k)
&=S(\mathbf{g}_0,\mathbf{Hg}_0,\ldots,\mathbf{H}^k\mathbf{g}_0)\tag8\\
S(\mathbf{d}_0,\mathbf{d}_1,\ldots,\mathbf{d}_k)
&=S(\mathbf{g}_0,\mathbf{Hg}_0,\ldots,\mathbf{H}^k\mathbf{g}_0)\tag9
\end{align}
$$
现在根据等式4可得

$$
\begin{equation}
\mathbf{d}_{k+1}^T\mathbf{Hd}_i=-\mathbf{g}_{k+1}^T\mathbf{Hd}_i+\beta_k\mathbf{d}_k^T\mathbf{Hd}_i\tag{10}
\end{equation}
$$
当$i=k$时，等式5得出

$$
\begin{equation}
\mathbf{d}_{k+1}^T\mathbf{Hd}_k=-\mathbf{g}_{k+1}^T\mathbf{Hd}_k+\beta_k\mathbf{d}_k^T\mathbf{Hd}_k=0\tag{11}
\end{equation}
$$
当$i<k$时，等式9表明

$$
\mathbf{Hd}_i\in S(\mathbf{d}_0,\mathbf{d}_1,\ldots,\mathbf{d}_k)
$$
所以$\mathbf{Hd}_i$可以用线性组合

$$
\begin{equation}
\mathbf{Hd}_i=\sum_{i=1}^ka_i\mathbf{d}_i\tag{12}
\end{equation}
$$
来表示，其中$\alpha_i,i=0,1,\ldots,k$是常数。接下来根据等式10与等式12

$$
\begin{align}
\mathbf{d}_{k+1}^T\mathbf{Hd}_i
&=-\sum_{i=0}^ka_i\mathbf{g}_{k+1}^T\mathbf{d}_i+\beta_k\mathbf{d}_k^T\mathbf{Hd}_i\notag\tag{13}\\
&=0\quad\text{for}\ i<k\tag{14}
\end{align}
$$
根据上篇文章定理2第一部分的正交性，上式的第一项等于零，而根据假设等式6可知，上式第二项等于零。结合等式11与13我们有

$$
\begin{equation}
\mathbf{d}_{k+1}^T\mathbf{Hd}_i=0\quad\text{for}\ 0\leq i<k+1\tag{15}
\end{equation}
$$
对于$k=0$，等式14得出

$$
\mathbf{d}_1^T\mathbf{Hd}_i=0\quad\text{for}\ 0\leq i<1
$$
且根据等式6与14，我们得出

$$
\begin{align*}
\mathbf{d}_2^T\mathbf{Hd}_i&=0\quad\text{for}\ 0\leq i<2\\
\mathbf{d}_3^T\mathbf{Hd}_i&=0\quad\text{for}\ 0\leq i<3\\
\vdots&\qquad\qquad\qquad\vdots\\
\mathbf{d}_k^T\mathbf{Hd}_i&=0\quad\text{for}\ 0\leq i<k
\end{align*}
$$
(b)根据等式8与9可知，$\mathbf{g}_0,\mathbf{g}_1,\ldots,\mathbf{g}_k$生成的子空间与$\mathbf{d}_0,\mathbf{d}_1,\ldots,\mathbf{d}_k$是一样的，因此他们是线性无关的，由此可得

$$
\mathbf{g}_i=\sum_{j=0}^ia_j\mathbf{d}_j
$$
其中$a_j$是常数，$j=0,1,\ldots,i$。根据上篇文章的定理2可知

$$
\mathbf{g}_k^T\mathbf{g}_i=\sum_{j=0}^ia_j\mathbf{g}_k^Td_j=0\quad\text{for}\ 0\leq i<k
$$
上面定理中$\alpha_k,\beta_k$的表达式可以进一步化简，根据等式4可得

$$
-\mathbf{g}_k^T\mathbf{d}_k=\mathbf{g}_k^T\mathbf{g}_k-\beta_{k-1}\mathbf{g}_k^T\mathbf{d}_{k-1}
$$
其中根据上篇文章的定理2可知

$$
\mathbf{g}_k^T\mathbf{d}_{k-1}=0
$$
故

$$
-\mathbf{g}_k^T\mathbf{d}_k=\mathbf{g}_k^T\mathbf{g}_k
$$
所以等式2的$\alpha$表达式可以改成

$$
\begin{equation}
\alpha_k=\frac{\mathbf{g}_k^T\mathbf{g}_k}{\mathbf{d}_k^T\mathbf{H}\mathbf{d}_k}\tag{16}
\end{equation}
$$
另一方面，因为

$$
\mathbf{Hd}_k=\frac{1}{\alpha_k}(\mathbf{g}_{k+1}-\mathbf{g}_k)
$$
所以

$$
\begin{equation}
\mathbf{g}_{k+1}^T\mathbf{Hd}_k=\frac{1}{\alpha_k}(\mathbf{g}_{k+1}^T\mathbf{g}_{k+1}-\mathbf{g}_{k+1}^T\mathbf{g}_k)\tag{17}
\end{equation}
$$
接下来根据等式8与9可得

$$
\mathbf{g}_k\in S(\mathbf{d}_0,\mathbf{d}_1,\ldots,\mathbf{d}_k)
$$
或者

$$
\mathbf{g}_k=\sum_{i=0}^ka_i\mathbf{d}_i
$$
又因为

$$
\begin{equation}
\mathbf{g}_{k+1}^T\mathbf{g}_k=\sum_{i=0}^ka_i\mathbf{g}_{k+1}^T\mathbf{d}_i=0\tag{18}
\end{equation}
$$
所以等式5,15,16与17得到

$$
\beta_k=\frac{\mathbf{g}_{k+1}^T\mathbf{g}_{k+1}}{\mathbf{g}_{k}^T\mathbf{g}_{k}}
$$
上面的原则与定理得到了下面的算法：

$$
\begin{align*}
&\textbf{算法1：共轭梯度算法}\\
&\textbf{步骤1}\\
&\text{输入}\mathbf{x}_0\text{并初始化容忍误差}\varepsilon\\
&\textbf{步骤2}\\
&\text{计算}\mathbf{g}_0\text{并令}\mathbf{d}_0=-\mathbf{g}_0,k=0\\
&\textbf{步骤3}\\
&\text{输入}\mathbf{H}_k,\text{即}\mathbf{x}_k\text{处的海森矩阵}\\
&\text{计算}\\
&\alpha_k=\frac{\mathbf{g}_k^T\mathbf{g}_k}{\mathbf{d}_k^T\mathbf{H}_k\mathbf{d}_k}\\
&\text{令}\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k\mathbf{d}_k\text{并计算}f_{k+1}=f(\mathbf{x}_{k+1})\\
&\textbf{步骤4}\\
&\text{如果}\lVert\alpha_k\mathbf{d}_k\rVert<\varepsilon,\text{输出}\mathbf{x}^*=\mathbf{x}_{k+1},f(\mathbf{x}^*)=f_{k+1}\text{算法结束}\\
&\textbf{步骤5}\\
&\text{计算}\mathbf{g}_{k+1}\\
&\text{计算}\\
&\beta_k=\frac{\mathbf{g}_{k+1}^T\mathbf{g}_{k+1}}{\mathbf{g}_k^T\mathbf{g}_k}\\
&\text{生成新的方向}\\
&\mathbf{d}_{k+1}=-\mathbf{g}_{k+1}+\beta_k\mathbf{d}_k\\
&\text{令}k=k+1\text{然后回到步骤3}
\end{align*}
$$
对于二维凸二次问题，上述算法得到的解的轨迹如图1所示，注意$\mathbf{x}_1=\mathbf{x}_0-\alpha_0\mathbf{g}_0$，其中$\alpha_0$是最小化$f(\mathbf{x}_0-\alpha\mathbf{g}_0)$的$\alpha$值，与最速下降法一样。
共轭梯度算法的主要优点为：
梯度是有限的，并且与前面的方向向量线性无关，当然除了问题的解本身外。
计算相对简单，相比最速下降法稍微复杂一点点。
不需要线搜索。
对于凸二次问题，该算法n次迭代就能收敛。
第一次选的方向就是最速下降的方向，所以第一次得带就能很好的减少f(\mathbf{x})。
因为方向是基于梯度信息的，当应用到非二次问题时，该算法有较好的收敛性。
不需要考虑海森矩阵的逆。
该算法的缺点为：
需要存储，计算海森矩阵。
对于非二次问题，存在极个别情况会无法达到收敛。

![这里写图片描述](https://img-blog.csdn.net/20171103210937961?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)[ ](https://img-blog.csdn.net/20171103210937961?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图1

[
						](https://img-blog.csdn.net/20171103210937961?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
[
	](https://img-blog.csdn.net/20171103210937961?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
