
# 反向传播算法推导-全连接神经网络 - SIGAI_CSDN的博客 - CSDN博客
# [SIGAI_CSDN的博客](https://blog.csdn.net/sigai_csdn)


[博客首页](https://blog.csdn.net/SIGAI_CSDN)
[关于我们](https://me.csdn.net/SIGAI_CSDN)

2018年07月07日 10:33:51[SIGAI_csdn](https://me.csdn.net/SIGAI_CSDN)阅读数：3004


本文及其它机器学习、深度学习算法的全面系统讲解可以阅读《机器学习与应用》，清华大学出版社，雷明著，由SIGAI公众号作者倾力打造，自2019年1月出版以来已重印3次。
[书的购买链接](https://link.zhihu.com/?target=https%3A//item.jd.com/12504554.html)
[书的勘误，优化，源代码资源](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_78.html)
反向传播算法是人工神经网络训练时采用的一种通用方法，在现代深度学习中得到了大规模的应用。全连接神经网络（多层感知器模型，MLP），卷积神经网络（CNN），循环神经网络（RNN）中都有它的实现版本。算法从多元复合函数求导的链式法则导出，递推的计算神经网络每一层参数的梯度值。算法名称中的“误差”是指损失函数对神经网络每一层临时输出值的梯度。反向传播算法从神经网络的输出层开始，利用递推公式根据后一层的误差计算本层的误差，通过误差计算本层参数的梯度值，然后将差项传播到前一层。
反向传播算法是一个通用的思路。全连接神经网络给出的是全连接层的反向传播实现；卷积神经网络引入了卷积层和池化层，对这两种层的反向传播做了自己的处理；循环神经网络因为在各个时刻共享了权重矩阵和偏置向量，因此需要进行特殊处理，为此出现了BPTT算法，误差项沿着时间轴反向传播。
本文推导全连接神经网络的反向传播算法。首先推导一个简单神经网络的反向传播算法，接下来把它推广到更一般的情况，得到通用的反向传播算法。很多同学对市面上已有的推导过程讲解的困惑在于不清楚矩阵和向量的求导公式和来历，在这里我们避免此问题，直接根据几个浅显易懂的基本结论进行推导。在[SIGAI](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485306%26idx%3D2%26sn%3Dcde2177542c9162a4d5731110b1c59d8%26chksm%3Dfdb69aedcac113fbc5e09e9dcc81269996a5fb87853fe595eeb065a98ae63394486c86290758%26scene%3D21%23wechat_redirect)后续的公众号文章中，我们将会推导卷积神经网络和循环神经网络的反向传播算法，感兴趣的读者可以关注我们的公众号。
## 算法的历史
反向传播算法最早出现于1986年，用于解决多层神经网络的训练问题，由Rumelhart和Hinton等人提出，这篇论文当时发表在Nature上：
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by back-propagating errors. Nature, 323(99): 533-536, 1986.
要知道，计算机的论文想发到Nature和Science上是非常困难的。笔者列举了一下发到这些期刊上的一些典型文章：
Roweis, Sam T and Saul, Lawrence K. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500). 2000: 2323-2326.
这是流形学习的开山之作。
Tenenbaum, Joshua B and De Silva, Vin and Langford, John C. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500). 2000: 2319-2323.
G.E.Hinton, et al. Reducing the Dimensionality of Data with Neural Networks, Science 313, 504,2006.
这是深度学习的开山之作。Hinton同志几十年如一日孜孜不倦的探索着神经网络，在最低谷的时候也毅然坚持，在这里我们先向他致敬！
下面是这篇论文的原貌（截图自原文）：
![](https://img-blog.csdn.net/20180821113933988?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
单纯从数学的角度看，反向传播算法并没有什么创新，函数求导的链式法则是微积分的发明人莱布尼兹时代出现的成果。但用于多层神经网络却让这种机器学习算法真正具有了实用价值。
## 链式法则
在正式介绍神经网络的原理之前，先回顾一下多元函数求导的链式法则。对于如下的多元复合函数：
![](https://img-blog.csdn.net/20180821114652258?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
在这里，x和y是自变量。其中u，v，w是x的函数，u，v是y的函数，而f又是u，v，w的函数。根据链式法则，函数f对x和y的偏导数分别为：
![](https://img-blog.csdn.net/2018082111472384?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
总结起来，函数自变量x的偏导数等于函数对它上一层的复合节点的偏导数（在这里是u，v，w）与这些节点对x的偏导数乘积之和。因此，我们要计算某一个自变量的偏导数，最直接的路径是找到它上一层的复合节点，根据这些节点的偏导数来计算。
## 神经网络原理简介
在推导算法之前，我们首先简单介绍一下人工神经网络的原理。大脑的神经元通过突触与其他神经元相连接，接收其他神经元送来的信号，经过汇总处理之后产生输出。在人工神经网络中，神经元的作用和这类似。下图是一个神经元的示意图，左侧为输入数据，右侧为输出数据：
![](https://img-blog.csdn.net/20180821114800736?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这个神经元接受的输入信号为向量(x1,x2,x3,x4,x5)，向量(w1,w2,w3,w4,w5)为输入向量的组合权重，b为偏置项，是标量。神经元对输入向量进行加权求和，并加上偏置项，最后经过激活函数变换产生输出：
![](https://img-blog.csdn.net/20180821114509183?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
为表述简洁，我们把公式写成向量和矩阵形式。对每个神经元，它接受的来自前一层神经元的输入为向量x，本节点的权重向量为w，偏置项为b，该神经元的输出值为：
![](https://img-blog.csdn.net/20180821114838741?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
先计算输入向量与权重向量的内积，加上偏置项，再送入一个函数进行变换，得到输出。这个函数称为激活函数，典型的是sigmoid函数。为什么需要激活函数以及什么样的函数可以充当激活函数，在[SIGAI](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485306%26idx%3D2%26sn%3Dcde2177542c9162a4d5731110b1c59d8%26chksm%3Dfdb69aedcac113fbc5e09e9dcc81269996a5fb87853fe595eeb065a98ae63394486c86290758%26scene%3D21%23wechat_redirect)之前的公众号文章“[理解神经网络的激活函数](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483977%26idx%3D1%26sn%3D401b211bf72bc70f733d6ac90f7352cc%26chksm%3Dfdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3%26scene%3D21%23wechat_redirect)”中已经进行了介绍。
神经网络一般有多个层。第一层为输入层，对应输入向量，神经元的数量等于特征向量的维数，这个层不对数据进行处理，只是将输入向量送入下一层中进行计算。中间为隐含层，可能有多个。最后是输出层，神经元的数量等于要分类的类别数，输出层的输出值被用来做分类预测。
下面我们来看一个简单神经网络的例子，如下图所示：
![](https://img-blog.csdn.net/20180821114920494?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这个网络有3层。第一层是输入层，对应的输入向量为x，有3个神经元，写成分量形式为（x1,x2,x3）它不对数据做任何处理，直接原样送入下一层。中间层有4个神经元，接受的输入数据为向量x，输出向量为y，写成分量形式为(y1,y2,y3,y4,y5)。第三个层为输出层，接受的输入数据为向量y，输出向量为z，写成分量形式为(z1,z2)。第一层到第二层的权重矩阵为W(1)，第二层到第三层的权重矩阵为W(2)。权重矩阵的每一行为一个权重向量，是上一层所有神经元到本层某一个神经元的连接权重，这里的上标表示层数。
如果激活函数选用sigmoid函数，则第二层神经元的输出值为：
![](https://img-blog.csdn.net/20180821115030650?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180821115056364?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
第三层神经元的输出值为：
![](https://img-blog.csdn.net/20180821115132321?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果把yi代入上面二式中，可以将输出向量z表示成输出向量x的函数。通过调整权重矩阵和偏置项可以实现不同的函数映射，因此神经网络就是一个复合函数。
需要解决的一个核心问题是一旦神经网络的结构（即神经元层数，每层神经元数量）确定之后，怎样得到权重矩阵和偏置项。这些参数是通过训练得到的，这是本文推导的核心任务。
一个简单的例子
首先以前面的3层神经网络为例，推导损失函数对神经网络所有参数梯度的计算方法。假设训练样本集中有m个样本(xi,zi)。其中x为输入向量，z为标签向量。现在要确定神经网络的映射函数：
![](https://img-blog.csdn.net/20180821115206918?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
什么样的函数能很好的解释这批训练样本？答案是神经网络的预测输出要尽可能的接近样本的标签值，即在训练集上最小化预测误差。如果使用均方误差，则优化的目标为：
![](https://img-blog.csdn.net/20180821115236502?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中h(x)和![z_{i}](https://www.zhihu.com/equation?tex=z_%7Bi%7D)都是向量，求和项内部是向量的2范数平方，即各个分量的平方和。上面的误差也称为欧氏距离损失函数，除此之外还可以使用其他损失函数，如交叉熵、对比损失等。
优化目标函数的自变量是各层的权重矩阵和梯度向量，一般情况下无法保证目标函数是凸函数，因此这不是一个凸优化问题，有陷入局部极小值和鞍点的风险（对于这些概念和问题，[SIGAI](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485306%26idx%3D2%26sn%3Dcde2177542c9162a4d5731110b1c59d8%26chksm%3Dfdb69aedcac113fbc5e09e9dcc81269996a5fb87853fe595eeb065a98ae63394486c86290758%26scene%3D21%23wechat_redirect)之前的公众号文章“[理解梯度下降法](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484111%26idx%3D1%26sn%3D4ed4480e849298a0aff828611e18f1a8%26chksm%3Dfdb69f58cac1164e844726bd429862eb7b38d22509eb4d1826eb851036460cb7ca5a8de7b9bb%26scene%3D21%23wechat_redirect)”，“[理解凸优化](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484439%26idx%3D1%26sn%3D4fa8c71ae9cb777d6e97ebd0dd8672e7%26chksm%3Dfdb69980cac110960e08c63061e0719a8dc7945606eeef460404dc2eb21b4f5bdb434fb56f92%26scene%3D21%23wechat_redirect)”中已经做了详细介绍），这是神经网络之前一直被诟病的一个问题。可以使用梯度下降法进行求解，使用梯度下降法需要计算出损失函数对所有权重矩阵、偏置向量的梯度值，接下来的关键是这些梯度值的计算。在这里我们先将问题简化，只考虑对单个样本的损失函数：
![](https://img-blog.csdn.net/20180821115306294?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
后面如果不加说明，都使用这种单样本的损失函数。如果计算出了对单个样本损失函数的梯度值，对这些梯度值计算均值即可得到整个目标函数的梯度值。
![w^{(1)}](https://www.zhihu.com/equation?tex=w%5E%7B%281%29%7D)和![w^{(2)}](https://www.zhihu.com/equation?tex=w%5E%7B%282%29%7D)要被代入到网络的后一层中，是复合函数的内层变量，我们先考虑外层的![w^{(2)}](https://www.zhihu.com/equation?tex=w%5E%7B%282%29%7D)和![b^{(2)}](https://www.zhihu.com/equation?tex=b%5E%7B%282%29%7D)。权重矩阵![w^{(2)}](https://www.zhihu.com/equation?tex=w%5E%7B%282%29%7D)是一个2x4的矩阵，它的两个行分别为向量![w_{1}^{(1)}和w_{2}^{(1)}](https://www.zhihu.com/equation?tex=w_%7B1%7D%5E%7B%281%29%7D%E5%92%8Cw_%7B2%7D%5E%7B%281%29%7D)，![b^{(2)}](https://www.zhihu.com/equation?tex=b%5E%7B%282%29%7D)是一个2维的列向量，它的两个元素为![b_{1}^{(2)}和b_{2}^{(2)}](https://www.zhihu.com/equation?tex=b_%7B1%7D%5E%7B%282%29%7D%E5%92%8Cb_%7B2%7D%5E%7B%282%29%7D)。网络的输入是向量x，第一层映射之后的输出是向量y。
首先计算损失函数对权重矩阵每个元素的偏导数，将欧氏距离损失函数展开，有：
![](https://img-blog.csdn.net/20180821115359767?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果i = 1，即对权重矩阵第一行的元素求导，上式分子中的后半部分对![w_{ij}](https://www.zhihu.com/equation?tex=w_%7Bij%7D)来说是常数。根据链式法则有：
![](https://img-blog.csdn.net/20180821115522763?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果i = 2，即对矩阵第二行的元素求导，类似的有：
![](https://img-blog.csdn.net/20180821115556477?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
可以统一写成：
![](https://img-blog.csdn.net/20180821115624684?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
可以发现，第一个下标i决定了权重矩阵的第i行和偏置向量的第i个分量，第二个下标j决定了向量y的第j个分量。这可以看成是一个列向量与一个行向量相乘的结果，写成矩阵形式为：
![](https://img-blog.csdn.net/20180821115655877?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
上式中乘法![\odot](https://www.zhihu.com/equation?tex=%5Codot)为向量对应元素相乘，第二个乘法是矩阵乘法。![f(w^{(2)}y+b^{(2)})-z](https://www.zhihu.com/equation?tex=f%28w%5E%7B%282%29%7Dy%2Bb%5E%7B%282%29%7D%29-z)是一个2维列向量，![f(w^{(2)}y+b^{(2)})](https://www.zhihu.com/equation?tex=f%28w%5E%7B%282%29%7Dy%2Bb%5E%7B%282%29%7D%29)也是一个2维列向量，两个向量执行![\odot](https://www.zhihu.com/equation?tex=%5Codot)运算的结果还是一个 2 维列向量。y 是一个 4 元素的列向量，其转置为 4 维行向量，前面这个二维列向量与![y^{T}](https://www.zhihu.com/equation?tex=y%5E%7BT%7D)的乘积为 2x4 的矩阵，这正好与矩阵![w^{(2)}](https://www.zhihu.com/equation?tex=w%5E%7B%282%29%7D)的尺寸相等。在上面的公式中，权重的偏导数在求和项中由 3 部分组成，分别是网络输出值与真实标签值的误差![f(w^{(2)}y+b^{(2)})-z](https://www.zhihu.com/equation?tex=f%28w%5E%7B%282%29%7Dy%2Bb%5E%7B%282%29%7D%29-z)，激活函数的导数![f(w^{(2)}y+b^{(2)})](https://www.zhihu.com/equation?tex=f%28w%5E%7B%282%29%7Dy%2Bb%5E%7B%282%29%7D%29)，本层的输入值 y。神经网络的输出值、激活函数的导数值、本层的输入值都可以在正向传播时得到，因此可以高效的计算出来。对所有训练样本的偏导数计算均值，可以得到总的偏导数。
对偏置项的偏导数为：
![](https://img-blog.csdn.net/20180821115729467?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果i = 1，上式分子中的后半部分对![b_{1}](https://www.zhihu.com/equation?tex=b_%7B1%7D)来说是常数，有：
![](https://img-blog.csdn.net/20180821115806243?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果i = 2，类似的有：
![](https://img-blog.csdn.net/20180821115840253?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这可以统一写成
![](https://img-blog.csdn.net/20180821115908118?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
写成矩阵形式为：
![](https://img-blog.csdn.net/20180821115940757?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
偏置项的导数由两部分组成，分别是神经网络预测值与真实值之间的误差，激活函数的导数值，与权重矩阵的偏导数相比唯一的区别是少了![y^{}T](https://www.zhihu.com/equation?tex=y%5E%7B%7DT)。
接下来计算对![w^{(1)}和b^{(1)}](https://www.zhihu.com/equation?tex=w%5E%7B%281%29%7D%E5%92%8Cb%5E%7B%281%29%7D)的偏导数，由于是复合函数的内层，情况更为复杂。![w^{(1)}](https://www.zhihu.com/equation?tex=w%5E%7B%281%29%7D)是一个 4x3 的矩阵，它的 4 个行向量为![w_{1}^{1},w_{2}^{1},w_{3}^{1},w_{4}^{1}](https://www.zhihu.com/equation?tex=w_%7B1%7D%5E%7B1%7D%2Cw_%7B2%7D%5E%7B1%7D%2Cw_%7B3%7D%5E%7B1%7D%2Cw_%7B4%7D%5E%7B1%7D)。偏置项![b^{(1)}](https://www.zhihu.com/equation?tex=b%5E%7B%281%29%7D)是 4 维向量，4个分量分别是![b_{1}^{1},b_{2}^{1},b_{3}^{1},b_{4}^{1}](https://www.zhihu.com/equation?tex=b_%7B1%7D%5E%7B1%7D%2Cb_%7B2%7D%5E%7B1%7D%2Cb_%7B3%7D%5E%7B1%7D%2Cb_%7B4%7D%5E%7B1%7D)。首先计算损失函数对![w^{(1)}](https://www.zhihu.com/equation?tex=w%5E%7B%281%29%7D)的元素的偏导数:
![](https://img-blog.csdn.net/20180821120018157?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
而：
![](https://img-blog.csdn.net/20180821120049963?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
上式分子中的两部分都有y，因此都与![w^{(1)}](https://www.zhihu.com/equation?tex=w%5E%7B%281%29%7D)有关。为了表述简洁，我们令：
![](https://img-blog.csdn.net/20180821120149521?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
根据链式法则有：
![](https://img-blog.csdn.net/20180821120232411?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中![f(u_{1}^{2})-z_{1}和f^{‘}(u_{1}^{2}),f(u_{2}^{2})-z_{2}和f^{‘}(u_{2}^{2})](https://www.zhihu.com/equation?tex=f%28u_%7B1%7D%5E%7B2%7D%29-z_%7B1%7D%E5%92%8Cf%5E%7B%E2%80%98%7D%28u_%7B1%7D%5E%7B2%7D%29%2Cf%28u_%7B2%7D%5E%7B2%7D%29-z_%7B2%7D%E5%92%8Cf%5E%7B%E2%80%98%7D%28u_%7B2%7D%5E%7B2%7D%29)都是标量，![w_{1}^{2}y和w_{2}^{2}y](https://www.zhihu.com/equation?tex=w_%7B1%7D%5E%7B2%7Dy%E5%92%8Cw_%7B2%7D%5E%7B2%7Dy)是两个向量的内积，y 的每一个分量都是![w_{ij}^{1}](https://www.zhihu.com/equation?tex=w_%7Bij%7D%5E%7B1%7D)的函数。接下来计算![\frac{\alpha w_{1}^{(2)}y}{\alpha w_{ij}^{(1)}}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Calpha+w_%7B1%7D%5E%7B%282%29%7Dy%7D%7B%5Calpha+w_%7Bij%7D%5E%7B%281%29%7D%7D)和![\frac{\alpha w_{2}^{(2)}y}{\alpha w_{ij}^{(1)}}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Calpha+w_%7B2%7D%5E%7B%282%29%7Dy%7D%7B%5Calpha+w_%7Bij%7D%5E%7B%281%29%7D%7D)
![](https://img-blog.csdn.net/20180821120317836?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这里的![\frac{\alpha y}{\alpha w_{ij}^{(1)}}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Calpha+y%7D%7B%5Calpha+w_%7Bij%7D%5E%7B%281%29%7D%7D)是一个向量，表示y的每个分量分别对![w_{i j}^{(1)}](https://www.zhihu.com/equation?tex=w_%7Bi+j%7D%5E%7B%281%29%7D)求导。当i=1时有:
![](https://img-blog.csdn.net/2018082112050264?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
后面3个分量相对于求导变量![w_{ij}^{(1)}](https://www.zhihu.com/equation?tex=w_%7Bij%7D%5E%7B%281%29%7D)都是常数。类似的当i = 2时有：
![](https://img-blog.csdn.net/20180821120536919?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
i = 3和i = 4时的结果以此类推。综合起来有：
![](https://img-blog.csdn.net/20180821120620720?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
同理有：
![](https://img-blog.csdn.net/20180821120652401?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果令：
![](https://img-blog.csdn.net/20180821120722265?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
合并得到：
![](https://img-blog.csdn.net/20180821141831758?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
写成矩阵形式为：
![](https://img-blog.csdn.net/20180821141911107?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
最后计算偏置项的偏导数：
![](https://img-blog.csdn.net/20180821142003426?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
类似的我们得到：
![](https://img-blog.csdn.net/20180821142031907?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
合并后得到：
![](https://img-blog.csdn.net/20180821142100412?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
写成矩阵形式为：
![](https://img-blog.csdn.net/2018082114213687?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
至此，我得到了这个简单网络对所有参数的偏导数，接下来我们将这种做法推广到更一般的情况。从上面的结果可以看出一个规律，输出层的权重矩阵和偏置向量梯度计算公式中共用了![(f(u^{(2)}-z)\odot f^{'}(u^{(2)})](https://www.zhihu.com/equation?tex=%28f%28u%5E%7B%282%29%7D-z%29%5Codot+f%5E%7B%27%7D%28u%5E%7B%282%29%7D%29)。对于隐含层也有类似的结果。
## 完整的算法
现在考虑一般的情况。假设有m个训练样本(![x_{i},y_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D%2Cy_%7Bi%7D))，其中![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D)为输入向量，![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D)为标签向量。训练的目标是最小化样本标签值与神经网络预测值之间的误差，如果使用均方误差，则优化的目标为：
![](https://img-blog.csdn.net/20180821142205362?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中W为神经网络所有参数的集合，包括各层的权重和偏置。这个最优化问题是一个不带约束条件的问题，可以用梯度下降法求解。
上面的误差函数定义在整个训练样本集上，梯度下降法每一次迭代利用了所有训练样本，称为批量梯度下降法。如果样本数量很大，每次迭代都用所有样本进计算成本太高。为了解决这个问题，可以采用单样本梯度下降法，我们将上面的损失函数写成对单个样本的损失函数之和：
![](https://img-blog.csdn.net/20180821142239803?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
定义对单个样本(![x_{i},y_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D%2Cy_%7Bi%7D))的损失函数为：
![](https://img-blog.csdn.net/20180821142308610?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果采用单个样本进行迭代，梯度下降法第t + 1次迭代时参数的更新公式为：
![](https://img-blog.csdn.net/20180821142336186?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果要用所有样本进行迭代，根据单个样本的损失函数梯度计算总损失梯度即可，即所有样本梯度的均值。
用梯度下降法求解需要初始化优化变量的值。一般初始化为一个随机数，如用正态分布![N(0,\sigma^{2})](https://www.zhihu.com/equation?tex=N%280%2C%5Csigma%5E%7B2%7D%29)产生这些随机数，其中![\sigma](https://www.zhihu.com/equation?tex=%5Csigma)是一个很小的正数。
到目前为止还有一个关键问题没有解决：目标函数是一个多层的复合函数，因为神经网络中每一层都有权重矩阵和偏置向量，且每一层的输出将会作为下一层的输入。因此，直接计算损失函数对所有权重和偏置的梯度很复杂，需要使用复合函数的求导公式进行递推计算。
## 几个重要的结论
在进行推导之前，我们首先来看下面几种复合函数的求导。又如下线性映射函数：
![](https://img-blog.csdn.net/20180821142404556?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中x是n维向量，W是mxn的矩阵，y是m维向量。
问题 1:假设有函数 f(y)，如果把 x 看成常数，y 看成 W 的函数，如何根据函数对 y 的梯度值![▽_{y}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7By%7Df)计算函数对W的梯度值![▽_{w}f?](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bw%7Df%3F)根据链式法则，由于![w_{ij}](https://www.zhihu.com/equation?tex=w_%7Bij%7D)只和![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D)有关，和其他的![y_{k},k\ne i](https://www.zhihu.com/equation?tex=y_%7Bk%7D%2Ck%5Cne+i)无关，因此有:
![](https://img-blog.csdn.net/20180821142439446?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
对于W的所有元素有：
![](https://img-blog.csdn.net/20180821142537125?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
写成矩阵形式为：
![](https://img-blog.csdn.net/20180821142607170?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
问题2:如果将W看成常数，y将看成x的函数，如何根据![▽_{y}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7By%7Df)计算![▽_{x}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bx%7Df)?由于任意的![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D)和所有的![y_{j}](https://www.zhihu.com/equation?tex=y_%7Bj%7D)都有关系，根据链式法则有:
![](https://img-blog.csdn.net/20180821142641850?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
写成矩阵形式为：
![](https://img-blog.csdn.net/20180821142725170?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这是一个对称的结果，在计算函数映射时用矩阵W乘以向量x得到y，在求梯度时用矩阵W的转置乘以y的梯度得到x的梯度。
问题3：如果有向量到向量的映射：
![](https://img-blog.csdn.net/20180821142804789?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
写成分量形式为：
![](https://img-blog.csdn.net/2018082114284168?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
在这里每个![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D)只和对应的![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D)有关，和其他所有![x_{j},j\ne i](https://www.zhihu.com/equation?tex=x_%7Bj%7D%2Cj%5Cne+i)无关，且每个分量采用了相同的映射函数 g。对于函数*f (*y)，如何根据![▽_{y}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7By%7Df)计算![▽_{x}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bx%7Df)?根据链式法则，由于每个![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D)只和对应的![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D)有关，有:
![](https://img-blog.csdn.net/20180821142913161?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
写成矩阵形式为：
![](https://img-blog.csdn.net/20180821142945219?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
即两个向量对应元素相乘，这种乘法在上一节已经介绍。
问题4：接下来我们考虑更复杂的情况，如果有下面的复合函数：
![](https://img-blog.csdn.net/20180821143050161?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中g是向量对应元素一对一映射，即：
![](https://img-blog.csdn.net/20180821143116791?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果有函数*f(*y)，如何根据![▽_{y}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7By%7Df)计算![▽_{x}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bx%7Df)?在这里有两层复合，首先是从 x 到 u，然后是从 u 到 y。根据问题 2 和问题 3 的结论，有
![](https://img-blog.csdn.net/20180821143151591?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
问题5：x是n维向量，y是m维向量，有映射y=g(x)，即：
![](https://img-blog.csdn.net/20180821143225688?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这里的映射方式和上面介绍的不同。对于向量 y 的每个分量![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D)，映射函数![g_{i}](https://www.zhihu.com/equation?tex=g_%7Bi%7D)不同，而且![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D)和向量 x 的每个分量![x_{i}](https://www.zhihu.com/equation?tex=x_%7Bi%7D)有关。对于函数*f (*y)，如何根据![▽_{y}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7By%7Df)计算![▽_{x}f](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bx%7Df)?根据链式法则，由于任何的![y_{i}](https://www.zhihu.com/equation?tex=y_%7Bi%7D)和任何的![x_{j}](https://www.zhihu.com/equation?tex=x_%7Bj%7D)都有关系，因此有:
![](https://img-blog.csdn.net/20180821143309218?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
对于所有元素有：
![](https://img-blog.csdn.net/20180821143348727?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
写成矩阵形式有：
![](https://img-blog.csdn.net/20180821143414770?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中![\frac{\alpha y}{\alpha x}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Calpha+y%7D%7B%5Calpha+x%7D)为雅可比矩阵。对于如下向量到向量的映射函数:
![](https://img-blog.csdn.net/20180821143447436?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中向量![x\in R^{n},向量y\in R^{n}](https://www.zhihu.com/equation?tex=x%5Cin+R%5E%7Bn%7D%2C%E5%90%91%E9%87%8Fy%5Cin+R%5E%7Bn%7D)，这个映射写成分量形式为:
![](https://img-blog.csdn.net/20180821143513692?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
即输出向量的每个分量是输入向量的函数。雅克比矩阵定义为输出向量的每个分量对输入向量的每个分量的偏导数构成的矩阵：
![](https://img-blog.csdn.net/2018082114353815?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这是一个m行n列的矩阵，每一行为一个多元函数的梯度。对于如下向量映射函数：
![](https://img-blog.csdn.net/20180821143612618?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
它的雅克比矩阵为：
![](https://img-blog.csdn.net/2018082114364962?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
前面介绍的几个问题都是这个映射的特例。
完整的算法
根据上面的结论可以方便的推导出神经网络的求导公式。假设神经网络有![n_{l}](https://www.zhihu.com/equation?tex=n_%7Bl%7D)层，第l层神经元个数为![s_{l}](https://www.zhihu.com/equation?tex=s_%7Bl%7D)。第 l 层从第 l-1 层接收的输入向量为![x^{(l-1)}](https://www.zhihu.com/equation?tex=x%5E%7B%28l-1%29%7D)，本层的权重矩阵为![w^{(l)}](https://www.zhihu.com/equation?tex=w%5E%7B%28l%29%7D)，偏置向量为![b^{(l)}](https://www.zhihu.com/equation?tex=b%5E%7B%28l%29%7D)，输出向量为![x^{(l)}](https://www.zhihu.com/equation?tex=x%5E%7B%28l%29%7D)。该层的输出可以写成如下矩阵形式:
![](https://img-blog.csdn.net/20180821143725273?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中![w^{(l)}](https://www.zhihu.com/equation?tex=w%5E%7B%28l%29%7D)是![s_{l}\times s_{l-1}](https://www.zhihu.com/equation?tex=s_%7Bl%7D%5Ctimes+s_%7Bl-1%7D)的矩阵，![u^{(l)}](https://www.zhihu.com/equation?tex=u%5E%7B%28l%29%7D)和![b^{(l)}](https://www.zhihu.com/equation?tex=b%5E%7B%28l%29%7D)是![s_{l}](https://www.zhihu.com/equation?tex=s_%7Bl%7D)维的向量。神经网络一个层实现的变换如下图所示:
![](https://img-blog.csdn.net/20180821143807440?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果将神经网络按照各个层展开，最后得到一个深层的复合函数，将其代入欧氏距离损失函数，依然是一个关于各个层的权重矩阵和偏置向量的复合函数：
![](https://img-blog.csdn.net/20180821143849417?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
要计算某一层的权重矩阵和偏置向量的梯度，只能依赖于它紧贴着的外面那一层变量的梯度值，通过一次复合函数求导得到。
根据定义，![w^{(l)}](https://www.zhihu.com/equation?tex=w%5E%7B%28l%29%7D)和![b^{(l)}](https://www.zhihu.com/equation?tex=b%5E%7B%28l%29%7D)是目标函数的自变量，![u^{(l)}](https://www.zhihu.com/equation?tex=u%5E%7B%28l%29%7D)和![x^{(l)}](https://www.zhihu.com/equation?tex=x%5E%7B%28l%29%7D)可以看成是它们的函数。根据前面的结论，损失函数对权重矩阵的梯度为:
![](https://img-blog.csdn.net/20180821143926772?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
对偏置向量的梯度为：
![](https://img-blog.csdn.net/20180821143947597?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
现在的问题是，梯度![▽_{u^{(l)}}L](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bu%5E%7B%28l%29%7D%7DL)怎么计算?我们分两种情况讨论，如果第*l*层是输出层，在这里只考虑对单个样本的损失函数，根据上一节推导的结论，这个梯度为：
![](https://img-blog.csdn.net/20180821144021626?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这就是输出层的神经元输出值与期望值之间的误差。这样我们得到输出层权重的梯度为：
![](https://img-blog.csdn.net/20180821144051977?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
等号右边第一个乘法是向量对应元素乘；第二个乘法是矩阵乘，在这里是列向量与行向量的乘积，结果是一个矩阵，尺寸刚好和权重矩阵相同。损失函数对偏置项的梯度为：
![](https://img-blog.csdn.net/20180821144120371?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
下面考虑第二种情况。如果第层是隐含层，则有：
![](https://img-blog.csdn.net/20180821144150833?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
假设梯度![▽_{u^{(l+1)}}L](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bu%5E%7B%28l%2B1%29%7D%7DL)已经求出，根据前面的结论，有:
![](https://img-blog.csdn.net/20180821144221928?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这是一个递推的关系，通过![▽_{u^{(l+1)}}L](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bu%5E%7B%28l%2B1%29%7D%7DL)可以计算出![▽_{u^{(l)}}L](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bu%5E%7B%28l%29%7D%7DL)，递推的终点是输出层，而输出层的梯度值我们之前已经算出。由于根据![▽_{u^{(l)}}L](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bu%5E%7B%28l%29%7D%7DL)可以计算出![▽_{w^{(l)}}L](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bw%5E%7B%28l%29%7D%7DL)和![▽_{b^{(l)}}L](https://www.zhihu.com/equation?tex=%E2%96%BD_%7Bb%5E%7B%28l%29%7D%7DL)，因此可以计算出任意层权重与偏置的梯度值。
![](https://img-blog.csdn.net/20180821144255572?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
向量![\delta^{(l)}](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D)的尺寸和本层神经元的个数相同。这是一个递推的定义，![\delta^{(l)}](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D)依赖于![\delta^{(l+1)}](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%2B1%29%7D)，递推的终点是输出层，它的误差项可以直接求出。
根据误差项可以方便的计算出对权重和偏置的偏导数。首先计算输出层的误差项，根据他得到权重和偏置项的梯度，这是起点；根据上面的递推公式，逐层向前，利用后一层的误差项计算出本层的误差项，从而得到本层权重和偏置项的梯度。
单个样本的反向传播算法在每次迭代时的流程为：
1.正向传播，利用当前权重和偏置值，计算每一层对输入样本的输出值
2.反向传播，对输出层的每一个节点计算其误差：
![](https://img-blog.csdn.net/20180821144330727?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
3.对于![l=n_{l}-1](https://www.zhihu.com/equation?tex=l%3Dn_%7Bl%7D-1),...,2的各层，计算第*l*层每个节点的误差
![](https://img-blog.csdn.net/20180821144355978?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
4.根据误差计算损失函数对权重的梯度值：
![](https://img-blog.csdn.net/20180821144452358?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
对偏置的梯度为：
![](https://img-blog.csdn.net/20180821144520708?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
5.用梯度下降法更新权重和偏置：
![](https://img-blog.csdn.net/20180821144604433?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
实现时需要在正向传播时记住每一层的输入向量![x^{(l-1)}](https://www.zhihu.com/equation?tex=x%5E%7B%28l-1%29%7D)，本层的激活函数导数值![f^{'}(u^{(l)})](https://www.zhihu.com/equation?tex=f%5E%7B%27%7D%28u%5E%7B%28l%29%7D%29)。
神经网络的训练算法可以总结为:
复合函数求导 + 梯度下降法
训练算法有两个版本：批量模式和单样本模式。批量模式每次梯度下降法迭代时对所有样本计算损失函数值，计算出对这些样本的总误差，然后用梯度下降法更新参数；单样本模式是每次对一个样本进行前向传播，计算对该样本的误差，然后更新参数，它可以天然的支持增量学习，即动态的加入新的训练样本进行训练。
在数学中，向量一般是列向量，但在编程语言中，向量一般按行存储，即是行向量，因此实现时计算公式略有不同，需要进行转置。正向传播时的计算公式为：
![](https://img-blog.csdn.net/20180821144638679?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
感兴趣的读者可以阅读各个开源库，对比它们的计算公式。反向传播时的计算公式为：
![](https://img-blog.csdn.net/20180821144708838?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
对权重矩阵的计算公式为：
![](https://img-blog.csdn.net/20180821144737858?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这些向量都是行向量。
推荐文章
[往期文章汇总](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485142%26idx%3D3%26sn%3D9a8329cbf84282c1aa2991834b9d5ef6%26chksm%3Dfdb69b41cac11257ca539757aaa934b78fc2f9d5ebf1d3b3e878d9fd124757cc754a938b7609%26scene%3D21%23wechat_redirect)
[1][机器学习-波澜壮阔40年](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483705%26idx%3D1%26sn%3Dc6e7c4a2e14a2469308b41eb60f155ac%26chksm%3Dfdb69caecac115b8712653600e526e99a3f6976fdaa2f6b6a09388fa6f9677ccb57b40c40ae3%26scene%3D21%23wechat_redirect)SIGAI 2018.4.13.
[2][学好机器学习需要哪些数学知识？](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483713%26idx%3D1%26sn%3D1e7c81381d16806ac73e15691fe17aec%26chksm%3Dfdb69cd6cac115c05f1f90b0407e3f8ae9be8719e454f908074ac0d079885b5c134e2d60fd64%26scene%3D21%23wechat_redirect)SIGAI 2018.4.17.
[3][人脸识别算法演化史](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483726%26idx%3D1%26sn%3D9fef4cc1766ea4258749f8d40cc71a6e%26chksm%3Dfdb69cd9cac115cf4eba16081780c3b64c75e1e55a40bf2782783d5c28f00c6f143426e6f0aa%26scene%3D21%23wechat_redirect)SIGAI 2018.4.20.
[4][基于深度学习的目标检测算法综述](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483731%26idx%3D1%26sn%3D237c52bc9ddfe65779b73ef8b5507f3c%26chksm%3Dfdb69cc4cac115d2ca505e0deb975960a792a0106a5314ffe3052f8e02a75c9fef458fd3aca2%26scene%3D21%23wechat_redirect)SIGAI 2018.4.24.
[5][卷积神经网络为什么能够称霸计算机视觉领](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483816%26idx%3D1%26sn%3Dfc52765b012771d4736c9be4109f910e%26chksm%3Dfdb69c3fcac115290020c3dd0d677d987086a031c1bde3429339bb3b5bbc0aa154e76325c225%26scene%3D21%23wechat_redirect)[域？](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483816%26idx%3D1%26sn%3Dfc52765b012771d4736c9be4109f910e%26chksm%3Dfdb69c3fcac115290020c3dd0d677d987086a031c1bde3429339bb3b5bbc0aa154e76325c225%26scene%3D21%23wechat_redirect)SIGAI 2018.4.26.
[6][用一张图理解SVM的脉络 ](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483937%26idx%3D1%26sn%3D84a5acf12e96727b13fd7d456c414c12%26chksm%3Dfdb69fb6cac116a02dc68d948958ee731a4ae2b6c3d81196822b665224d9dab21d0f2fccb329%26scene%3D21%23wechat_redirect)SIGAI 2018.4.28.
[7][人脸检测算法综述](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483950%26idx%3D1%26sn%3Da3a5b7907b2552c233f654a529931776%26chksm%3Dfdb69fb9cac116af5dd237cf987e56d12b0d2e54c5c565aab752f3e366c0c45bfefa76f5ed16%26scene%3D21%23wechat_redirect)SIGAI 2018.5.3.
[8][理解神经网络的激活函数](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247483977%26idx%3D1%26sn%3D401b211bf72bc70f733d6ac90f7352cc%26chksm%3Dfdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3%26scene%3D21%23wechat_redirect)SIGAI 2018.5.5.
[9][深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484037%26idx%3D1%26sn%3D13ad0d521b6a3578ff031e14950b41f4%26chksm%3Dfdb69f12cac11604a42ccb37913c56001a11c65a8d1125c4a9aeba1aed570a751cb400d276b6%26scene%3D21%23wechat_redirect)SIGAI 2018.5.8.
[10][理解梯度下降法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484111%26idx%3D1%26sn%3D4ed4480e849298a0aff828611e18f1a8%26chksm%3Dfdb69f58cac1164e844726bd429862eb7b38d22509eb4d1826eb851036460cb7ca5a8de7b9bb%26scene%3D21%23wechat_redirect)SIGAI 2018.5.11.
[11][循环神经网络综述—语音识别与自然语言处理的利器](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484310%26idx%3D1%26sn%3D0fc55a2784a894100a1ae64d7dbfa23d%26chksm%3Dfdb69e01cac1171758cb021fc8779952e55de41032a66ee5417bd3e826bf703247e243654bd0%26scene%3D21%23wechat_redirect)SIGAI 2018.5.15
[12][理解凸优化](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484439%26idx%3D1%26sn%3D4fa8c71ae9cb777d6e97ebd0dd8672e7%26chksm%3Dfdb69980cac110960e08c63061e0719a8dc7945606eeef460404dc2eb21b4f5bdb434fb56f92%26scene%3D21%23wechat_redirect)SIGAI 2018.5.18
[13][【实验】理解SVM的核函数和参数](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484495%26idx%3D1%26sn%3D4f3a6ce21cdd1a048e402ed05c9ead91%26chksm%3Dfdb699d8cac110ce53f4fc5e417e107f839059cb76d3cbf640c6f56620f90f8fb4e7f6ee02f9%26scene%3D21%23wechat_redirect)SIGAI 2018.5.22
[14][ 【SIGAI综述】行人检测算法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484523%26idx%3D1%26sn%3Dddaa70c4b92f6005d9bbd6b3a3fe4571%26chksm%3Dfdb699fccac110ea14e6adeb873a00d6ee86dd4145ddf8e90c9b878b908ac7b7655cfa51dab6%26scene%3D21%23wechat_redirect)SIGAI 2018.5.25
[15][机器学习在自动驾驶中的应用—以百度阿波罗平台为例](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484540%26idx%3D1%26sn%3D733332042c31e1e18ad800f7f527893b%26chksm%3Dfdb699ebcac110fd6607c1c99bc7ebed1594a8d00bc454b63d7f518191bd72274f7e61ca5187%26scene%3D21%23wechat_redirect)(上) SIGAI 2018.5.29
[16][理解牛顿法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484651%26idx%3D1%26sn%3Da0e4ca5edb868fe3eae9101b71dd7103%26chksm%3Dfdb6997ccac1106a61f51fe9f8fd532045cc5d13f6c75c2cbbf1a7c94c58bcdf5f2a6661facd%26scene%3D21%23wechat_redirect)SIGAI 2018.5.31
[17][【群话题精华】5月集锦—机器学习和深度学习中一些值得思考的问题 ](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484658%26idx%3D1%26sn%3Df5c9f92c272c75883bf8e6f532559f11%26chksm%3Dfdb69965cac11073f49048caef5d7b9129614090a363d9ef7f3d1b9bc59948d2217d2bca7b7b%26scene%3D21%23wechat_redirect)SIGAI 2018.6.1
[18][大话Adaboost算法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484692%26idx%3D1%26sn%3D9b389aa65208c778dddf17c601afbee1%26chksm%3Dfdb69883cac1119593934734e94c3b71aa68de67bda8a946c1f9f9e1209c3b6f0bf18fed99b8%26scene%3D21%23wechat_redirect)SIGAI 2018.6.2
[19][FlowNet到FlowNet2.0：基于卷积神经网络的光流预测算法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484711%26idx%3D1%26sn%3Dbb7644e101b5924f54d6800b952dc3aa%26chksm%3Dfdb698b0cac111a6605f5b9b6f0478bf21a8527cfad2342dbaaf624b4e9dcc43c0d85ae06deb%26scene%3D21%23wechat_redirect)SIGAI 2018.6.4
[20][理解主成分分析(PCA)](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484754%26idx%3D1%26sn%3Db2c0d6798f44e13956bb42373e51d18c%26chksm%3Dfdb698c5cac111d3e3dca24c50aafbfb61e5b05c5df5b603067bb7edec8db049370b73046b24%26scene%3D21%23wechat_redirect)SIGAI 2018.6.6
[21][人体骨骼关键点检测综述](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484784%26idx%3D1%26sn%3Dceafb54203f4e930ae457ad392b9f89c%26chksm%3Dfdb698e7cac111f13d8229d7dcc00b4a7305d66de3da1bd41e7ecc1d29bfa7be520d205c53e9%26scene%3D21%23wechat_redirect)SIGAI 2018.6.8
[22][理解决策树](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484827%26idx%3D1%26sn%3D043d7d0159baaddfbf92ed78ee5b1124%26chksm%3Dfdb6980ccac1111a9faeae7f517fee46a1dfab19612f76ccfe5417487b3f090ab8fc702d18b8%26scene%3D21%23wechat_redirect)SIGAI 2018.6.11
[23][用一句话总结常用的机器学习算法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484859%26idx%3D1%26sn%3D2c4db22fb538953a62a90983e3e1f99d%26chksm%3Dfdb6982ccac1113a82e92be325bb07a947d54090274654375f3b50e11e1abd809fb7358bde16%26scene%3D21%23wechat_redirect)SIGAI 2018.6.13
[24][目标检测算法之YOLO](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484909%26idx%3D1%26sn%3Dc02ee17e5175230ed39ad63e73249f5c%26chksm%3Dfdb6987acac1116c0108ec28424baf4ea16ca11d2b13f20d4a825d7b2b82fb8765720ebd1063%26scene%3D21%23wechat_redirect)SIGAI 2018.6.15
[25][理解过拟合](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484954%26idx%3D1%26sn%3Dc28b7f07c22466e91b1ef90e9dbe3ad1%26chksm%3Dfdb69b8dcac1129bc6e78fca1d550e2b18238ad1c240c73b280d4e529f9f93c4626b3ac45ea2%26scene%3D21%23wechat_redirect)SIGAI 2018.6.18
[26][理解计算：从√2到AlphaGo ——第1季 从√2谈起](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247484981%26idx%3D1%26sn%3Dd3003468b9853851923844812993e060%26chksm%3Dfdb69ba2cac112b4dac620d52100ebd033eb679f29340726a67297c4d6980b16c7cc91122028%26scene%3D21%23wechat_redirect)SIGAI 2018.6.20
[27][场景文本检测——CTPN算法介绍](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485005%26idx%3D1%26sn%3D0d4fb43b8db2a8046c64a9cfcbf3f478%26chksm%3Dfdb69bdacac112cce05c8b735b4f8b1ccf2348bea55a30af2055fc328958bb8f1ffd0f819bd2%26scene%3D21%23wechat_redirect)SIGAI 2018.6.22
[28][卷积神经网络的压缩和加速](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485042%26idx%3D1%26sn%3Dcdcf8d4b07acf64c7a6f5f7c1a731a12%26chksm%3Dfdb69be5cac112f377766984afb87313c1e1c58d94c80005f0f6f6af61ee5a4bd1bf6c6157b6%26scene%3D21%23wechat_redirect)SIGAI 2018.6.25
[29][k近邻算法](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485074%26idx%3D1%26sn%3D0ebf1bf8f49e9c46075fe3803d04c95d%26chksm%3Dfdb69b05cac112132d280c70af3923ca4c3cccfa5fcd8628b79d4b246b3b2decbc80a180abb3%26scene%3D21%23wechat_redirect)SIGAI 2018.6.27
[30][自然场景文本检测识别技术综述](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485142%26idx%3D1%26sn%3Dc0e01da30eb5e750be453eabe4be2bf4%26chksm%3Dfdb69b41cac11257ae22c7dac395e9651dab628fc35dd6d3c02d9566a8c7f5f2b56353d58a64%26scene%3D21%23wechat_redirect)SIGAI 2018.6.27
[31][理解计算：从√2到AlphaGo ——第2季 神经计算的历史背景](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485155%26idx%3D1%26sn%3D990cc7400751c36e9fef0a261e6add2a%26chksm%3Dfdb69b74cac112628bdae14c6435120f6fece20dae9bf7b1ffc8b8b25e5496a24160feca0a72%26scene%3D21%23wechat_redirect)SIGAI 2018.7.4
[32][机器学习算法地图](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU4MjQ3MDkwNA%3D%3D%26mid%3D2247485306%26idx%3D1%26sn%3Dfc8cc8de313bdb61dcd39c1dedb240a4%26chksm%3Dfdb69aedcac113fb4b18c74248a313536ded50bade0e66b26f332ab247b148519da71ff2a3c0%26scene%3D21%23wechat_redirect)SIGAI2018.7.6


