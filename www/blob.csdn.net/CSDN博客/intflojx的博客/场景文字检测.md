
# 场景文字检测 - intflojx的博客 - CSDN博客


置顶2018年07月30日 15:36:36[paulpanwang](https://me.csdn.net/intflojx)阅读数：957


场景文字检测可以作为OCR的前端，为可以应用于商标识别等应用。
[https://blog.csdn.net/intflojx/article/details/81278393](https://blog.csdn.net/intflojx/article/details/81278393)
**1 赛题简介**
在互联网世界中，图片是传递信息的重要媒介。特别是电子商务，社交，搜索等领域，每天都有数以亿兆级别的图像在传播。图片文字识别（OCR）在商业领域有重要的应用价值，是数据信息化和线上线下打通的基础，也是学术界的研究热点。然而，研究领域尚没有基于网络图片的、以中文为主的OCR数据集。本竞赛将公开基于网络图片的中英混合数据集，该数据集数据量充分，涵盖几十种字体，几个到几百像素字号，多种版式，较多干扰背景。期待学术界可以在本数据集上作深入的研究，工业界可以藉此发展基于OCR的图片管控，搜索，信息录入等AI领域的工作。
**2 数据集**
我们提供20000张图像作为本次比赛的数据集。其中50%用来作为训练集，50%用来作为测试集。该数据集全部来源于网络图像，主要由合成图像，产品描述，网络广告构成。典型的图片如图1所示：
![图片 1.png](https://work.alibaba-inc.com/aliwork_tfs/g01_alibaba-inc_com/tfscom/TB1fz1tXY9YBuNjy0FgXXcxcXXa.tfsprivate.png)
图1：典型图片
这些图像是网络上最常见的图像类型。每一张图像或者包含复杂排版，或者包含密集的小文本或多语言文本，或者包含水印，这对文本检测和识别均提出了挑战。
对于每一张图像，都会有一个相应的文本文件（.txt）（UTF-8编码与名称：[图像文件名] .txt）。文本文件是一个逗号分隔的文件，其中每行对应于图像中的一个文本串，并具有以下格式：
X1，Y1，X2，Y2，X3，Y3，X4，Y4，“文本”
其中X1，Y1，Y2，X2，X3，X4，Y3，Y4分别代表文本的外接四边形四个顶点坐标。而“文本”是四边形包含的实际文本内容。
图2是标注的图片，红色的框代表标注的文本框。
图3是标注图片对应的文本文件。标注时我们对所有语言，所有看不清的文字串均标注了外接框（比如图2中的小字），但对于除了中文，英文以外的其它语言以及看不清的字符并未标注文本内容，而是以“\#\#\#”代替。
![2.png](https://work.alibaba-inc.com/aliwork_tfs/g01_alibaba-inc_com/tfscom/TB1cnOqXVuWBuNjSspnXXX1NVXa.tfsprivate.png)
图2：image.jpg
![3.png](https://work.alibaba-inc.com/aliwork_tfs/g01_alibaba-inc_com/tfscom/TB1qF1sX7yWBuNjy0FpXXassXXa.tfsprivate.png)
图3：image.txt
**3 任务描述**
网络图像的文本检测：
检测并定位图像中的文字行位置，允许使用其它数据集或者生成数据， 允许Fine-tuning 模型或者其他模型。入围团队提交报告中须对额外使用的数据集，或非本数据集训练出的模型做出说明。
**训练集：**
对于每个图像，只需要用[图像文件名] .txt里的坐标信息。即： X1，Y1，X2，Y2，X3，Y3，X4，Y4。
**测试集：**
输入：整图
输出：对于每一个检测到的文本框，按行将其顶点坐标输出到对应的[图像文件名] .txt中。
**提交：**
将所有图像对应的[图像文件名] .txt放到一个zip压缩包中，然后提交。
**4 评估标准**
文本定位评测遵循ICDAR2013 Born-Digital Image的主体思路。本次竞赛数据集以中文为主，标注较细致，所以按照论文中“one to many”和“many to one”[1]的思路更为准确。其中一些阈值进行了调整：
**第一，**为“单个目标框”筛选合格“多个合格框”的阈值tmany。“多”中的任意框与目标框交叉面积除以自身面积大于 tmany=0.7时，视为合格候选。
**第二，**计算“多个合格框”覆盖“单目标框”的面积阈值tone。“多”中的所有框覆盖了目标框总面积大于tone=0.7时，视单目标框可被召回或者属于正确检测范畴，视“多个合格框”为可被召回或者属于正确检测范畴。
**第三，**确定召回率和精度。计算“多”检测框对“单”标注框时，如果满足了tone，那么单标注框召回率为1，多个检测框（个数为k）的检测准确度为penal(K)。计算“多”标注框对“单”检测框时，如果满足tone，那么单检测框精度为1，多个标注框（个数为k）中每一个召回率为penal(K)。其中penal(K)为惩罚“分散”或者“合并”错误的函数，公式为：
penal(K) = 1/（1+ln(K)）               （1）
**第四，**处理“可忽略行”。对于行标注内容为“\#\#\#”的文本行。“可忽略行”不计算召回率。当某个检测框被“可忽略行”覆盖的面积除以自身面积大于tignore=0.5时，视该检测框为“可忽略检测行”。可忽略的标注行和检测行不计入最终结果。
**5. 解题**
刚好看到国内的旷视今年在CVPR2017的一篇文章：EAST: An Efficient and Accurate Scene Text Detector。而且有开放的代码，学习和测试了下。
题目说的是比较高效，它的高效主要体现在对一些过程的消除，其架构就是下图中对应的E部分，跟上面的比起来的确少了比较多的过程。这与去年经典的CTPN架构类似。不过CTPN只支持水平方向，而EAST在论文中指出是可以支持多方向文本的定位的。
![](https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/1c4b1de61db4393a413958316773c656f8676694/1-Figure2-1.png)
论文采用的架构如下：
![](https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/1c4b1de61db4393a413958316773c656f8676694/2-Figure3-1.png)
这个架构的细节应该包括几个部分：
(1) The algorithm follows the general design ofDenseBox [9], in which an image is fed into the FCN andmultiple channels of pixel-level text score map and geometryare generated. 从论文中这句话可以看出，参考了DenseBox的架构，采用FCN网络，同时在多个通道中进行特征层的输出与几何的生成。
(2) 文中采用了两种几何对象，rotated box (RBOX) and quadrangle (QUAD)，通过这两种，可以实现对多方向场景文本的检测。
(3) 采用了Locality-Aware NMS来对生成的几何进行过滤，这也是代码中lanms(C++)代码的因素。
实验部分：
由于该源码已经公布，进行了测试，效果如下：
(1) ICDAR相关的数据集测试，有相当部分的效果还是可以的。
![](https://img-blog.csdn.net/20180730152607446?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180730152659568?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180730152717466?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180730153021586?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
(2 )改进：
首先：用EAST跑比赛训练集，在此基础上进行以下改进尝试：
**样本：**
通过统计平衡样本比例
采用focal loss
负样本挖掘，损失高的多训练几次
强调小目标检测，下采样多尺寸检测
加入Text-attentional的数据增强方法，再进行训练和测试。
样本进行旋转
扩大尺度
多尺度
**网络：**
修改成宽度网络
修改成ResNetX
其他修改
先用synth800k进行预训练
**融合：**
采用refineNet的融合方式
按不同权重进行融合，分辨率大的可能更重要
将中间层也拉出来做预测，并加入loss中，可调loss权重，与最终结果做融合
多尺度输入，并将不同尺度输入的feature map进行融合，最后进行预测
引入普通规则：将中间谋层拉出来做预测，（统计得到的）宽高比（或其他规则）需满足一定的条件，并加入Loss中，旨在提高准确率。
引入覆盖率概念：从浅层到高层不断做预测，观测预测的结果是否有前面检测小目标后面检测大目标的特性，从而指导应该采用哪些层做预测。
**输出：**
大目标检测单词
小目标检测文本线
![](https://img-blog.csdn.net/20180730153058150?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180730152801823?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180730152947297?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
评测结果：在1万张测试集，准确率:0.75,召回率：0.65

