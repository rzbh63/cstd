
# 人脸比对（1:N） - intflojx的博客 - CSDN博客


置顶2018年07月30日 01:02:57[paulpanwang](https://me.csdn.net/intflojx)阅读数：5433标签：[人脸1:N 																](https://so.csdn.net/so/search/s.do?q=人脸1:N &t=blog)个人分类：[学习笔记																](https://blog.csdn.net/intflojx/article/category/7252345)[项目笔记																](https://blog.csdn.net/intflojx/article/category/7947350)[
							](https://blog.csdn.net/intflojx/article/category/7252345)


**第****1****章****前言**
设计出[人脸1：N](https://blog.csdn.net/intflojx/article/details/81278330)，随着N的增大准确率降低最小的解决方案具有很强的现实意义。[人脸1：N](https://blog.csdn.net/intflojx/article/details/81278330)的框架大致分为：人脸检测、人脸对齐、人脸映射与人脸识别LOSS的设计，结构如下图所示：
![](https://img-blog.csdn.net/20180730010212454?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图1：人脸1：N的主要框架
人脸1：N在学术界有着广泛的研究，对于人脸检测与人脸对齐（MTCNN、TCDCN等）在业界已经有较好的效果，目前的主要性能提升有：DeepFace、DeepID，框架为CNN + Softmax，网络在第一个FC层形成判别力很强的人脸特征，用于人脸识别。对于人脸识别的特征映射来说，并不满足人脸对比的需求；DeepID2, DeepID2+, DeepID3都采用Softmax + Contrastive Loss，使得同类特征的L2距离尽可能小，不同类特征的L2距离大于某个间隔；FaceNet是目前引用量最高的人脸对比方法，提出了Triple Loss，以三元组形式进行优化，获得类内紧凑和类间差异；SphereFace是L-Softmax的改进，归一化了权值W，让训练更加集中在优化深度特征映射和特征向量角度上，降低样本数量不均衡问题。Center Loss为每个类别学习一个中心，并将每个类别的所有特征向量拉向对应类别中心，联合Softmax一起使用；COCO loss，归一化了权值c，归一化了特征f，并乘尺度因子，在LFW上达到99.86%；在损失函数上设计还包括：L2-Softmax、Additive Margin Loss、CosFace、ArcFace等。
本文在FaceNet的框架基础上，对于MTCNN进行改进实现了更精确的人脸对齐结果。同时，对训练样本进行增强，使用人脸随机旋转、随机裁剪、人脸色相、饱和度的变换等扩充样本，进一步降低训练误差。将Triple Loss改为最新的COCO loss，结构风险最小化的正则化因子设为：10^-3，梯度下降采用RMSPROP，mini-batch设置为100，实验环境为：GT1080TI，训练时间：48-55h，在华为训练集上人脸1：N的准确率为：99.92%-99.97%之间。
# 第
# 2章 功能及原理
**2.1****人脸对齐**
对人脸检测与切割并进行对齐与规范化。通过Landmark得到人脸的特征点，通过与正脸比较，学习到单应性矩阵，通过单应性矩阵对人脸图像进行旋转，旋转效果明显优于MTCNN的对齐。
![](https://img-blog.csdn.net/20180730010212393?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180730010212396?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180730010212401?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180730010212403?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
![](https://img-blog.csdn.net/20180730010212409?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180730010212414?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图2：人脸特征点定位结果
![](https://img-blog.csdn.net/20180730010212457?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图3：(a)原始图像 (b) MTCNN对齐结果 (c) 本文对齐结果
**2.1.1****人脸特征点定位**
人脸特征点可以被用来将人脸对齐到平均人脸，这样在对齐之后所有图像中的人脸特征点的位置几乎是相同的。直观上来看，用对齐后的图像训练的人脸识别算法更加有效，这个观点已经被很多论文验证。
本文采用是最新的TCDCN模型，该模型**思想**：通过一些多样的精细的任务，来优化提升特征点定位精度。就是在人脸特征点检测的时候，同时进行多个任务的学习，这些任务包括：性别，是否带眼镜，是否微笑和脸部的姿势。使用这些辅助的属性帮助更好的定位特征点，根据论文结果，这样的确对人脸特征点检测有一定的帮助。该模型**优势**：1）优于现有的方法，特别是在处理一些遮挡和大姿态的面部图像时。2）与现有的state-of-the-art的级联深度模型方法相比，其动态的降低了模型的复杂度。
![](https://img-blog.csdn.net/20180730010212438?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图4：TCDCN的结构
损失函数就是不同任务的损失函数直接相加。而在人脸特征点检测的任务中，不同的任务具有不同的loss，特征点检测是平方和误差，而其它分类任务是交叉熵损失，因此最后的loss就是：
![](https://img-blog.csdn.net/20180730010212432?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
(1)
**2.1.2****单应性矩阵**
平面的单应性即为一个平面到另一个平面的投影映射。若点*Q*到成像仪上的点*q*的映射使用齐次坐标，则单应性（如图13所示）可以表示为：
![](https://img-blog.csdn.net/2018073001021486?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)(2)
其中和![](https://img-blog.csdn.net/20180730010216271?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)，*s*是任意尺度的比例。H是由用于定位观察的物体平面的物理变换和使用摄像机内参数矩阵的投影两部分组成的。
![](https://img-blog.csdn.net/20180730010216277?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180730010216283?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图 5： 平面单应性的示意图
物理变换部分是与观测到的图像平面相关的部分旋转*R*和部分平移*t*的影响之和，可以表示为：
![](https://img-blog.csdn.net/20180730010216294?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)(3)
*M*为摄像机内参数矩阵，即：
![](https://img-blog.csdn.net/20180730010216296?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)(4)
则单应性可以重写为
(5)
又因为单应性是研究一个平面到另一个平面的映射。因此，我们可以令*z*=0，即物体平面上的点用*x*,*y*表示，则可以简化为平面坐标中的![](https://img-blog.csdn.net/20180730010216298?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)，即可以推导为：
(6)
其中，![](https://img-blog.csdn.net/20180730010217538?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)，所以最终的单应性矩阵可以表示为：
(7)
所以，可以通过一张规范化之后的人脸并将其特征点存储起来，之后计算人脸特征点与标准人脸的人脸特征点的单应性矩阵，通过变换关系即可将人脸规范到统一的标准。通常只需要4对点即可对矩阵估计，为了更加准确估计图像的变换关系，将所有人脸的特征点加入计算可以列出68组方程，方程组个数大于带求解的系数，用过RANSAC算法来求解这个超正定方程，每次计算4个点，选取内点个数最多的作为最终的结果。
![](https://img-blog.csdn.net/20180730010219474?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)(8)
实现效果如下所示：
![](https://img-blog.csdn.net/20180730010219497?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图 6：单应性矩阵估计(a) 标准人脸及特征点 (b)-(d):通过变换得到标准人脸
**2.2 Embedding**
按照FaceNet的思路，选取不同的卷积神经框架作为的前级输入，对特征进行L2规范化后，进行特征映射，将特征映射成128维的向量。
![](https://img-blog.csdn.net/20180730010219506?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图 7：FaceNet的框架图
ResNet有效的解决了深度卷积神经网络难训练的问题。这是因为在误差反传的过程中，梯度通常变得越来越小，从而权重的更新量也变小。这个导致远离损失函数的层训练缓慢，随着层数的增加这个现象更加明显。通过对比效果，最终选择图8(d)的结构作为深度结构，并在利用在CASIA上的训练结果作为预训练模型。
![](https://img-blog.csdn.net/20180730010219544?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图8：不同的卷积神经网络框架图
**2.3****相似度计算（****LOSS****选择）**
通过相似度计算函数得到人脸之间的距离，通过交叉验证方法设置最佳阈值将人脸和非人脸分开。“COCO Loss”，其目的同样是：
拉近同类样本的特征（类内方差小）
拉远不同分类样本的特征（类间方差大）
公式为：
![](https://img-blog.csdn.net/20180730010219527?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)(9)
我们会发现该Loss的优化目标是分子越大越好(类内Cos相似度高)，分母越小越好(类间Cos相似度低)。通过可视化不同损失函数，对比不同相似度计算之间的聚类性，不难看出，COCO Loss 的聚类性明显更好，最终选择COCO Loss 作为最终的损失。
![](https://img-blog.csdn.net/20180730010219537?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ludGZsb2p4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
图9：不同的Loss之间的对比
**2.4****梯度下降方法**
由于调整学习率时分母上的变量*s*一直在累加按元素平方的小批量随机梯度，目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解。为了应对这一问题，RMSProp算法对Adagrad做了一点小小的修改。RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均变量*s*，并将其中每个元素初始化为0。给定超参数γ且0≤γ<1， 在每次迭代中，RMSProp首先计算小批量随机梯度g，然后对该梯度按元素平方项g^2做指数加权移动平均。
# 第
# 3章 特色综述
对样本增广、对模型参数进行设置，较高的识别稳定性。
对Loss进行选择，在FaceNet基础上增加识别率。
较好的人脸对齐和预处理效果，环境适应性强。

# 第
# 4章 开发工具及技术
编译环境：Python 3.5 （anaconda3）、Tensorflow 1.2、Scikit-learn、opencv-python、h5py、Matplotlib、Pillow、requests、psutil。
硬件环境：I7-7700K，1080TI，64G内存。
技术：人脸对齐、单应性矩阵估计、Facenet、COCO Loss、梯度下降方法、深度学习（卷积神经网络设计）。
**[下一篇：人脸识别之数据、网络结构、损失函数](https://blog.csdn.net/intflojx/article/details/82377939)**

