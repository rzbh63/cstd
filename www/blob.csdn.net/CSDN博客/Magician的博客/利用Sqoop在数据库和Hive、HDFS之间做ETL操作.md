
# 利用Sqoop在数据库和Hive、HDFS之间做ETL操作 - Magician的博客 - CSDN博客


2018年08月30日 16:13:00[春雨里de太阳](https://me.csdn.net/qq_16633405)阅读数：277所属专栏：[自学大数据之路](https://blog.csdn.net/column/details/18514.html)


[目录：](#目录)[一、利用Sqoop，从Oracle到HDFS](#一利用sqoop从oracle到hdfs)
[二、利用Sqoop，从Oracle到Hive](#二利用sqoop从oracle到hive)
[三、遇到的问题](#三遇到的问题)


# 目录：
## 一、利用Sqoop，从Oracle到HDFS
**第一步：把Oracle驱动拷贝到Sqoop安装路径中的lib文件夹下。**
第二步：切换账户su hdfs
第三步：执行import操作
```python
sqoop
```
```python
import
```
```python
-
```
```python
-
```
```python
connect
```
```python
jdbc:oracle:thin:@IPAddress:databaseName
```
```python
-
```
```python
-
```
```python
username
```
```python
userName
```
```python
-
```
```python
-
```
```python
password
```
```python
password
```
```python
-
```
```python
-
```
```python
table
```
```python
TABLENAME
```
```python
-
```
```python
-
```
```python
target
```
```python
-
```
```python
dir
```
```python
/user/operate
```
```python
-
```
```python
m
```
```python
1
```
```python
-
```
```python
-
```
```python
where
```
```python
"where子句
```
```python
and
```
```python
根据需要添加"
```
说明：
**1、导入的目录若已经存在则要先删除已经存在的目录，否则会报路径以存在的错误提示：**
```python
FileAlreadyExistsException
```
```python
:
```
```python
Output directoryhdfs://master:8020/user/operate already exists
```
删除路径语句：hadoop fs –rmr /user/operate
如果多次导入同一个表中的数据，数据要以append的形式插入到HDFS目录中。
2、**-m 1表示使用几个map任务处理，sqoop默认采用4个map任务，有几个任务在HDFS中执行，结果中就有几个part-m文件;若m值不等于1，则导入的表需要有主键，否则会报错：**
```python
Error
```
```python
during import: No primary
```
```python
key
```
```python
could befound
```
```python
for
```
```python
table KW_CARENLIST. Please specify one
```
```python
with
```
```python
--split-
```
```python
by
```
```python
or
```
```python
perform asequential import
```
```python
with
```
```python
'-m 1'.
```
sqoop是根据–splite-by <字段名>进行分区，–m设置map的数量，sqoop根据不同的splite参数进行切分，然后将切分出来的区域分配到不同的map中。Splite-by的参数类型不同则其切分方法不同，如int型，sqoop会取最大和最小splite-by字段。Sqoop会向关系数据库发送一个命令；select max(id)，min(id) from table，然后会把max、min之间的区间平均分成m份，最后m个并行的map会执行任务。–splite-by认是主键，如果操作的表没有主键导入时会报错。splite-by对非数字类型的字段支持不好，一般用于主键及数字类型的字段（对于非数字类型有可能会导致数据量丢失。如：a，b，c，d，e，a，a，v，f，g）。在实例测试中，要导入的Oracle一张表（200多万条数据）中没有关键字，若以数据表的最后一列（NUMBER类型，值存在null，存在重复值）为splite-by对象，则最终检验导入的数据只有100多万条。
3、Sqoop默认从数据库导入到HDFS的分隔符是逗号，**可用—field –terminated-by 来指定输出文件中的行字段分隔符。**如果导入数据的字段内容中存在分隔符，可以另外指定父、字段包围符和转转义字符。
4、空列的值使用null。Sqoop默认导入的数据格式为文本文件。另可导入其他几种文件格式，如SequenceFile、Avro格式。文本文件不能保存二进制字段（如数据库中类型为VARBINARY的列），且在区分null值和null字符串时可能出现问题。Avro和SequenceFile格式的文件能够为导入的数据提供最精确的表示方式，同时还允许对数据进行压缩，并支持MapReduce并行处理同一文件的不同部分。（不过目前版本还不能将Avro或Sequence文件加载到Hive中，尽管可以手动地将Avro数据文件加载到Hive中）。SequenceFile文件格式的最后一个缺点是它只支持Java语言，而Avro数据文件却可以被很多语言支持。
5、Sqoop中的map数设置原则：一个表的数据抽取不超过3分钟，否则就增加map数。
辅助操作：
查看路径下文件：hadoop fs -ls /user/operate/
打开指定文件：hadoop fs –cat/user/operate/tabe_name
统计文件中行数：hadoop fs –cat /路径/文件名 |wc –l
## 二、利用Sqoop，从Oracle到Hive
访问 Hive数据库：
第一步：进入到hive命令行。
第二步：直接导入Hive数据库。
```python
sqoop
```
```python
import
```
```python
-
```
```python
-
```
```python
connect
```
```python
jdbc:oracle:thin:@IPAddress:1521:databaseName
```
```python
-
```
```python
-
```
```python
username
```
```python
userName
```
```python
-
```
```python
-
```
```python
password
```
```python
password
```
```python
-
```
```python
-
```
```python
table
```
```python
TABLENAME
```
```python
-
```
```python
m
```
```python
1
```
```python
-
```
```python
-
```
```python
hive
```
```python
-
```
```python
import
```
```python
-
```
```python
-
```
```python
hive
```
```python
-
```
```python
database
```
```python
test
```
相比导入到HDFS中，添加了—hive-import 、–hive-database参数。
**–hive-import指定导入方式是导入到hive数据库中**
**–hive-database指定导入的目标数据库test，若无此参数则默认导入到Hive的默认数据库default中**
上面的导入结果是导入到了test.test_table中，**Hive自动创建了一张与Oracle导入表一样的表。也可以自己创建表，指定自己的表名：–hive-table tbName。**
**注意：因Oracle、Hive数据类型的不一致，导入的数据会存在精度减低的问题**。
也可以使用–option-file传入一个文件，使用这种方式可以重用一些配置参数（该方法为测试，看起来不错，做下记录，有兴趣的可以baidu一下）：
```python
sqoop –option-file /user/hive/import
```
```python
.txt
```
```python
–table test
```
```python
.table
```
```python
其中，/user/hive/import
```
```python
.txt
```
```python
文件内容如下：
```
**辅助操作：**
删除数据库：DROP DATABASE IF EXISTS test;
删除数据库表：DROP TBALE test.test_table;
退出hive命令行：exit;
## 三、遇到的问题
在使用Sqoop从Oracle抽数据到Hive表时，有时候会遇到以下报错
```python
Error:
```
```python
java
```
```python
.lang
```
```python
.RuntimeException
```
```python
: java
```
```python
.lang
```
```python
.RuntimeException
```
```python
: java
```
```python
.sql
```
```python
.SQLRecoverableException
```
```python
: IO Error: Connection reset
    at org
```
```python
.apache
```
```python
.sqoop
```
```python
.mapreduce
```
```python
.db
```
```python
.DBInputFormat
```
```python
.setDbConf
```
```python
(DBInputFormat
```
```python
.java
```
```python
:
```
```python
170
```
```python
)
    at org
```
```python
.apache
```
```python
.sqoop
```
```python
.mapreduce
```
```python
.db
```
```python
.DBInputFormat
```
```python
.setConf
```
```python
(DBInputFormat
```
```python
.java
```
```python
:
```
```python
161
```
```python
)
    at org
```
```python
.apache
```
```python
.hadoop
```
```python
.util
```
```python
.ReflectionUtils
```
```python
.setConf
```
```python
(ReflectionUtils
```
```python
.java
```
```python
:
```
```python
73
```
```python
)
    at org
```
```python
.apache
```
```python
.hadoop
```
```python
.util
```
```python
.ReflectionUtils
```
```python
.newInstance
```
```python
(ReflectionUtils
```
```python
.java
```
```python
:
```
```python
133
```
```python
)
    at org
```
```python
.apache
```
```python
.hadoop
```
```python
.mapred
```
```python
.MapTask
```
```python
.runNewMapper
```
```python
(MapTask
```
```python
.java
```
```python
:
```
```python
749
```
```python
)
    at org
```
```python
.apache
```
```python
.hadoop
```
```python
.mapred
```
```python
.MapTask
```
```python
.run
```
```python
(MapTask
```
```python
.java
```
```python
:
```
```python
341
```
```python
)
    at org
```
```python
.apache
```
```python
.hadoop
```
```python
.mapred
```
```python
.YarnChild
```
```python
$2
```
```python
.run
```
```python
(YarnChild
```
```python
.java
```
```python
:
```
```python
164
```
```python
)
    at java
```
```python
.security
```
```python
.AccessController
```
```python
.doPrivileged
```
```python
(Native Method)
    at javax
```
```python
.security
```
```python
.auth
```
```python
.Subject
```
```python
.doAs
```
```python
(Subject
```
```python
.java
```
```python
:
```
```python
422
```
```python
)
    at org
```
```python
.apache
```
```python
.hadoop
```
```python
.security
```
```python
.UserGroupInformation
```
```python
.doAs
```
```python
(UserGroupInformation
```
```python
.java
```
```python
:
```
```python
1714
```
```python
)
    at org
```
```python
.apache
```
```python
.hadoop
```
```python
.mapred
```
```python
.YarnChild
```
```python
.main
```
```python
(YarnChild
```
```python
.java
```
```python
:
```
```python
158
```
```python
)
Caused by: java
```
```python
.lang
```
```python
.RuntimeException
```
```python
: java
```
```python
.sql
```
```python
.SQLRecoverableException
```
```python
: IO Error: Connection reset
    at org
```
```python
.apache
```
```python
.sqoop
```
```python
.mapreduce
```
```python
.db
```
```python
.DBInputFormat
```
```python
.getConnection
```
```python
(DBInputFormat
```
```python
.java
```
```python
:
```
```python
223
```
```python
)
    at org
```
```python
.apache
```
```python
.sqoop
```
```python
.mapreduce
```
```python
.db
```
```python
.DBInputFormat
```
```python
.setDbConf
```
```python
(DBInputFormat
```
```python
.java
```
```python
:
```
```python
168
```
```python
)
    ...
```
```python
10
```
```python
more
```
**解决**
此问题一般是由于缺少一个生成快速随机数的工具，一般可以通过在JRE中的java.security中修改securerandom.source的值为以下内容并重新登陆shell解决问题。
```python
cd
```
```python
$JAVA_HOME
```
```python
/jre/lib/security
vi java.security
securerandom.
```
```python
source
```
```python
=
```
```python
file
```
```python
:/dev/../dev/urandom
```

