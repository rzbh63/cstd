
# 依存分析：基于序列标注的中文依存句法分析模型实现 - lpty的博客 - CSDN博客

2018年02月13日 14:20:34[lpty](https://me.csdn.net/sinat_33741547)阅读数：3301所属专栏：[自然语言基础](https://blog.csdn.net/column/details/22512.html)



## 一、前言
[1、中文依存句法分析](http://blog.csdn.net/sinat_33741547/article/details/79258045)
## 二、实战
### 1、数据源
数据采用清华大学语义依存网络语料作为训练集，同时在开发集上进行测试。
#### （1）语料预处理
原语料库如下：
`1   坚决  坚决  a   ad  _   2   方式  
2   惩治  惩治  v   v   _   0   核心成分    
3   贪污  贪污  v   v   _   7   限定  
4   贿赂  贿赂  n   n   _   3   连接依存    
5   等   等   u   udeng   _   3   连接依存    
6   经济  经济  n   n   _   7   限定  
7   犯罪  犯罪  v   vn  _   2   受事`格式说明：
`1   ID  当前词在句子中的序号，１开始.
2   FORM    当前词语或标点  
3   LEMMA   当前词语（或标点）的原型或词干，在中文中，此列与FORM相同
4   CPOSTAG 当前词语的词性（粗粒度）
5   POSTAG  当前词语的词性（细粒度）
6   FEATS   句法特征，在本次评测中，此列未被使用，全部以下划线代替。
7   HEAD    当前词语的中心词
8   DEPREL  当前词语与中心词的依存关系`对于依存句法分析，本质上可以转换为分类问题，所以将依存句法作为序列标注任务进行解决也是可行的。鉴于依存关系类过多，直接作为分类标签会导致效果不理想，这里需要进行处理。
根据依存文法，决定两个词之间的依存关系主要有两个因素：方向和距离，因此可以将类别标签定义为以下形式：
`[+|-]dPOS`其中，[+|-]表示中心词在句子中的相对坐标轴方向；POS代表中心词具有的词性类别；d表示距离中心词存在与中心词词性相同的词的数量，即距离。
处理后的格式如下：
`坚决  a   ad  1_v
惩治  v   v   0_Root
贪污  v   v   1_v
贿赂  n   n   -1_v
等   u   udeng   -1_v
经济  n   n   1_v
犯罪  v   vn  -2_v`
#### （2）特征选择
这里简单的进行3-gram的抽取，将词性与词语两两进行匹配：
`feature = {"w0": sentences[index][i][0],
            "p0": sentences[index][i][1],
            "w-1": sentences[index][i-1][0] if i != 0 else "BOS",
            "w+1": sentences[index][i+1][0] if i != len(sentences[index])-1 else "EOS",
            "p-1": sentences[index][i-1][1] if i != 0 else "un",
            "p+1": sentences[index][i+1][1] if i != len(sentences[index])-1 else "un"}
 feature["w-1:w0"] = feature["w-1"]+feature["w0"]
 feature["w0:w+1"] = feature["w0"]+feature["w+1"]
 feature["p-1:p0"] = feature["p-1"]+feature["p0"]
 feature["p0:p+1"] = feature["p0"]+feature["p+1"]
 feature["p-1:w0"] = feature["p-1"]+feature["w0"]
 feature["w0:p+1"] = feature["w0"]+feature["p+1"]`
### 2、模型处理
模型方面采用条件随机场，这里不做详细说明，代码如下：
`algorithm = self.config.get('model', 'algorithm')
 c1 = float(self.config.get('model', 'c1'))
 c2 = float(self.config.get('model', 'c2'))
 max_iterations = int(self.config.get('model', 'max_iterations'))
 self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2,
                                   max_iterations=max_iterations, all_possible_transitions=True)`模型参数如下：
`[model]
algorithm = lbfgs
c1 = 0.1
c2 = 0.1
max_iterations = 100
model_path = depparser/data/{}.pkl`
### 3、模型效果
在训练集2W条语料上训练，之后在开发集中进行测试：
`avg / total      0.612     0.645     0.617      3753`从上面可以看到效果不太理想，分析大概存在以下原因：
1）3-gram的窗口太小，句法关系中的两个词如果存在窗口中，是可以被捕捉到，但是本例中取的窗口可能会遗漏很多关系
2）同样在模板特征的设计，这里除了窗口大小，还需要进行其他特征选取
3）语料库本身可能存在一定的错误
4）关于CRF中的标签合法性没有进行约束，维特比算法中模型所有存在的标签都是合法的，实际上本任务中，标签存在合法性的界定，即只能存在一个中心词以及标签的距离不能超过句子长度等等
### 4、参考
`1、《基于序列标注的中文依存句法分析方法》 计峰 邱锡鹏`
## 三、其他
具体源码可以在我github上找到：[https://github.com/lpty/nlp_base](https://github.com/lpty/nlp_base)

