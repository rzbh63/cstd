# 强化学习系列10：无模型的直接策略搜索 - kittyzc的博客 - CSDN博客
2019年01月07日 20:10:40[IE06](https://me.csdn.net/kittyzc)阅读数：76所属专栏：[强化学习系列](https://blog.csdn.net/column/details/33845.html)
# 1. 从值函数到策略函数

前面介绍的方法都是基于值函数$vvv$或者$qqq$来计算的（用定义或者bellman方程），策略根据$π(s)=arg⁡max⁡aq(s,a)\pi(s)=\arg \max_a q(s,a)π(s)=argmaxa​q(s,a)$得到。有时我们可以直接建立状态与策略之间的函数，即$π(s)=fθ(s)\pi(s)=f_{\theta}(s)π(s)=fθ​(s)$，比如说建立一个神经网络，网络参数为$θ\thetaθ$，输入是状态$sss$，输出为每一个行动的概率$ppp$。直接策略搜索的有点是：直接、简单、易收敛。

同样在直接策略搜索的领域内，也分有模型与无模型两类，无模型算法中，确定性策略一般用DDPG方法，随机策略主要是TRPO、A2C等；有模型算法中，主要是使用GPS等算法。这里首先介绍无模型的算法。

详细的介绍可以参考这篇博客：[https://blog.csdn.net/jinzhuojun/article/details/78007628](https://blog.csdn.net/jinzhuojun/article/details/78007628)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190226094528863.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tpdHR5emM=,size_16,color_FFFFFF,t_70)
# 2. 随机策略梯度法

首先简单推导一下策略梯度更新方法：

定义采样策略为$Π\PiΠ$，采样轨迹为$τ\tauτ$，总体回报为$r(τ)r(\tau)r(τ)$，则目标函数为最大化$Jπ=Eτ∼π[r(τ)]J_{\pi}=E_{\tau\sim\pi}[r(\tau)]Jπ​=Eτ∼π​[r(τ)]$，梯度为$∇Jπ=Σ[r(τ)∇π(τ)]=Σ[π(τ)r(τ)∇log⁡π(τ)]=Eτ∼π[r(τ)∇log⁡π(τ)]\nabla J_{\pi}=\Sigma[r(\tau)\nabla \pi(\tau)]=\Sigma[\pi(\tau)r(\tau)\nabla \log\pi(\tau)]=E_{\tau\sim\pi}[r(\tau)\nabla \log\pi(\tau)]∇Jπ​=Σ[r(τ)∇π(τ)]=Σ[π(τ)r(τ)∇logπ(τ)]=Eτ∼π​[r(τ)∇logπ(τ)]$。将$τ\tauτ$展开即可得到上式等于$E[ΣtrtΣt∇log⁡π(at∣st)]E[\Sigma_t r_t\Sigma_t\nabla\log\pi(a_t|s_t)]E[Σt​rt​Σt​∇logπ(at​∣st​)]$。令$G=ΣtrtG=\Sigma_t r_tG=Σt​rt​$，这个公式可以看做是以长期回报$GGG$为加权权重，对所有的($s,as,as,a$)样本进行最大似然估计的公式，非常简洁：$E[G∇logπ]E[G\nabla log\pi]E[G∇logπ]$。

将期望用蒙特卡洛采样法进行替换，用当前策略采样$NNN$条轨迹，并将权重从长期期望改成从当前时间开始的期望值。则$∇Jπ=1NΣiGi∇log⁡π(ai,t∣si,t)\nabla J_{\pi}=\frac{1}{N}\Sigma_{i} G_i\nabla \log\pi(a_{i,t}|s_{i,t})∇Jπ​=N1​Σi​Gi​∇logπ(ai,t​∣si,t​)$ ，然后进行更新：$Δθ=α∇Jπ\Delta{\theta}=\alpha\nabla J_{\pi}Δθ=α∇Jπ​$，其中$α\alphaα$是学习率。

在实际过程中，我们进行交互的次数往往是有限的，学习成本很高，样本不足会造成策略波动太大的问题，业界发展了一些新的方法来处理这个问题。
## 2.1 Actor-Critic方法

在Actor-Critic方法中，我们采用几种措施来降低算法的方差，首先是使用独立的模型代替轨迹的长期回报（比如TD-error，$gt=Σi=1...nγirt+i+v(st+n)−v(st)g_t= \Sigma_{i=1...n}\gamma^i r_{t+i}+v(s_{t+n})-v(s_t)gt​=Σi=1...n​γirt+i​+v(st+n​)−v(st​)$），称为Critic；原来用于行动的策略模型称为Actor。公式变为：$∇Jπ=E[Σtgt∇log⁡π(at∣st)]\nabla J_{\pi}=E[\Sigma_t g_t\nabla\log\pi(a_t|s_t)]∇Jπ​=E[Σt​gt​∇logπ(at​∣st​)]$。

其次，在实际更新时又有异步和同步两种方法。分别称为A3C(Asynchronous Advantage Actor-Critic)和A2C(Advantage Actor-Critic)。OpenAI在官方博客中提到A2C效果比A3C要好。

再者，目标函数中加入策略的熵，以增加不确定性进行一定量的探索。具体来说，就是加上$β∇θH(π(st))\beta\nabla_{\theta}H(\pi(s_t))β∇θ​H(π(st​))$。

Baselines上的案例如下：
```python
import gym
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.vec_env import SubprocVecEnv
from stable_baselines import A2C

# multiprocess environment
n_cpu = 4
env = SubprocVecEnv([lambda: gym.make('CartPole-v1') for i in range(n_cpu)])
model = A2C(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=25000)
obs = env.reset()
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    env.render()
```

## 2.2 TRPO：置信区域策略优化

在上面的算法中，学习率$α\alphaα$是固定的。我们想要自动选择合适的步长避免模型震荡，这里就要用到TRPO方法。TRPO全称trust region policy optimization，其目标是每次更新策略后，回报函数的值不能变差。其核心是如下公式：$Jπ′=Jπ+Eπ′[ΣtγtAπ]J_{\pi&#x27;}=J_{\pi}+E_{\pi&#x27;}[\Sigma_t\gamma^tA_{\pi}]Jπ′​=Jπ​+Eπ′​[Σt​γtAπ​]$。

注意其中$π′\pi&#x27;π′$和$π\piπ$分别代表新策略和旧策略，后面计算期望的时候，用新策略采样，用旧策略计算优势函数$AπA_{\pi}Aπ​$。经过一系列tricky的处理之后，求解函数变为：
$min⁡∇Lx∗(θ−x)\min \nabla L_{x}*(\theta-x)min∇Lx​∗(θ−x)$

s.t. $12(θ−x)TIθ(θ−x)≤δ\frac{1}{2}(\theta-x)^TI_{\theta}(\theta-x)\le\delta21​(θ−x)TIθ​(θ−x)≤δ$

其中$Lx=E[πxπθAθ]L_x=E[\frac{\pi_x}{\pi_{_{\theta}}}A_{\theta}]Lx​=E[πθ​​πx​​Aθ​]$，$IθI_{\theta}Iθ​$为fisher阵，TRPO算法使用共轭梯度法计算fisher阵。

Baselines中的使用方法和A2C类似，把模型名字换掉即可。
## 2.3 GAE：泛化优势估计

GAE(Generalized Advantage Estimation)可以看做是AC方法的拓展。我们把2.1节中的权重再进行替换。我们将n步TD-error记作$GtnG_t^nGtn​$，则新的方法$GtGAE(λ)=Gt1+γλGt2+(γλ)2Gt3+......)=Σi(γλ)iGt+i1G_t^{GAE(\lambda)}=G_t^1+\gamma \lambda G_t^2+(\gamma \lambda )^2G_t^3+......)=\Sigma_i(\gamma \lambda)^iG_{t+i}^1GtGAE(λ)​=Gt1​+γλGt2​+(γλ)2Gt3​+......)=Σi​(γλ)iGt+i1​$。当$λ=0\lambda=0λ=0$时，权重为1阶TD-error；当$λ=1\lambda=1λ=1$时，权重为蒙特卡洛和价值函数之间的差值。

Baselines中的使用方法和A2C和TROP类似，把模型名字换掉即可。

## 2.4 PPO：近似策略优化

PPO(Proximal Policy Optimization)对TRPO方法进行了优化，将置信区域的约束条件改到了目标函数中，$Lx=E[πxπθAθ]L_x=E[\frac{\pi_x}{\pi_{_{\theta}}}A_{\theta}]Lx​=E[πθ​​πx​​Aθ​]$变为：$E[min⁡{πxπθAθ,clip(πxπθ,1−ϵ,1+ϵ)Aθ}]E[ \min \{\frac{\pi_x}{\pi_{_{\theta}}}A_{\theta},clip(\frac{\pi_x}{\pi_{_{\theta}}},1-\epsilon,1+\epsilon)A_{\theta} \}]E[min{πθ​​πx​​Aθ​,clip(πθ​​πx​​,1−ϵ,1+ϵ)Aθ​}]$，相当于给$πxπθ\frac{\pi_x}{\pi_{_{\theta}}}πθ​​πx​​$加了2把锁。此外，PPO还将值函数的目标函数和策略模型的熵加入到了目标中。

# 3. 无模型确定策略

这里的确定策略指的是我们的输出不再是不同行动的概率值，而是直接给出一个具体的行动。

确定策略的信息量比随机策略要少，但是在连续决策空间问题上，只能使用确定策略。比如我们要通过强化学习去得到词汇的32维词向量，每一维都有无穷多的连续取值，我们不可能给出它们每一个概率（除非进行离散化），只能给出最合适的一个词向量值。

## 3.1 DDPG：深度确定性策略梯度法

DDPG以AC为基础，借鉴使用DQN的replay buffer和target network来进行学习。首先，Actor和Critic都有自己的Behavior和Target Network，其次不同于DQN的隔一段时间同步一次，在DDPG中每次都是更新一点点，称为soft方法。




