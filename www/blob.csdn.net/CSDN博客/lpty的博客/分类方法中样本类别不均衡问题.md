
# 分类方法中样本类别不均衡问题 - lpty的博客 - CSDN博客

2017年11月29日 20:45:51[lpty](https://me.csdn.net/sinat_33741547)阅读数：1660



## 一、前言
大部分的分类学习方法都存在一个基本的假设，训练集中不同类别的训练样本数目差不多。如果不同类别的样本数目差距很大，比如正类样本有98个，而负类样本只有2个，这种情况下学习出来的分类器只要一直返回正类的预测结果，那很轻易的就能达到百分之九十八的正确率，但实际上这样的模型并没有什么作用。
## 二、解决方法
### 1、欠抽样（under sampling）
欠抽样，是指减少分类中数目较多的类别数量，来实现样本均衡。最直接的方法就是随机的去掉一些多数类中的样本，但这样可能会丢失一些重要的信息。
比较有代表的欠抽样算法是EasyEnsemble，利用集成学习机制，将多数类中的样本划分为若干个集合供不同的学习器使用，表面看来每个学习器都进行了欠抽样，但整体看来却不会丢失重要信息。
### 2、过抽样（over sampling）
过抽样，是指增加分类中数目较少的类别数量，来实现样本均衡。最直接的方法就是随机的复制一些少数类中的样本，但这样可能会导致严重的过拟合。
比较有代表性的过抽样算法是SMOTE，通过对训练集里的少数类样本进行差值来产生额外的记录。
### 3、权重惩罚
权重惩罚，是指对不同的类别样本赋予不同的权重，比如数量较少的类别拥有更大的权重，再进行建模。

此外还有一些其他的方法，就不一一列举了。

