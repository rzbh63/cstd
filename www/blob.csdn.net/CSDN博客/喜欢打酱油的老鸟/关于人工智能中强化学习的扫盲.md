
# 关于人工智能中强化学习的扫盲 - 喜欢打酱油的老鸟 - CSDN博客


2019年03月15日 11:29:33[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：497


[https://www.toutiao.com/a6666688293163237896/](https://www.toutiao.com/a6666688293163237896/)

# 前言
对一个认知科学家来说，一个最基本的问题是“如何学习新知识？”。回答这样一个问题的idea是，人通过学习获得某种知识，或者拥有某一种技能。而对于机器而言，可以通过学习、训练去完成更多只有人能完成的任务，实现真正的人工智能。
虽然目前无法完全回答这个问题，但是有些事实是很清楚的：即在忽略skill的前提下，可以通过与环境的交互进行学习，这是强化学习相对于其他机器学习非常显著的特点之一。无论人学习驾驶汽车还是婴儿学习走路，这样的学习方式都是基于与环境的交互，从交互中学习是学习和智力理论的基础概念。
![关于人工智能中强化学习的扫盲](http://p1.pstatp.com/large/pgc-image/1aa7bf2c064241b18be04c37792ed6b4)

在维基百科中是这样介绍强化学习的：强化学习(RL)作为机器学习的一个子领域，其灵感来源于心理学中的行为主义理论，即智能体如何在环境给予的奖励或惩罚的刺激下（即与环境的交互中），逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。它强调如何基于环境而行动，以取得最大化的预期利益。通俗的讲：就是根据环境学习一套策略，能够最大化期望奖励。由于它具有普适性而被很多领域进行研究，例如自动驾驶，博弈论、控制论、运筹学、信息论、仿真优化、多主体系统学习、群体智能、统计学以及遗传算法。
# 什么是强化学习？
首先，做一个简单的类比。假如你家有一只宠物，如下图的小狗。
![关于人工智能中强化学习的扫盲](http://p9.pstatp.com/large/pgc-image/21850c5a73c446508128c6f10965a95b)
平常你会带它到附近的公园去玩，但你不是局限于简单的遛狗，而是买了一个犬笛（一种训练响片），训练它来完成一些游戏。比如，通过犬笛让狗坐下，当它完成这个动作之后，你会给它一个奖励（例如一个好吃的）。这实质上是reinforce你的狗以完成一个good action。随着时间的推移，小狗会习惯于这种声音并在每次听到咔哒声时做出respond。通过这样的训练方式，可以训练你的狗在需要时完成一个good action。
现在，在这个类比的例子中，用进行如下替换：
l*宠物（狗）变成一个人造agent*；
l*对狗的奖励变成奖励函数*；
l*一个good action（狗坐下）变成一个result action*；
以上类比的例子，就是强化学习一个简单的“模样”，即是强化学习最典型的一个例子。要在人造agent上实现此功能，需要添加一个反馈循环来强化agent。当所执行的action是正确的，会奖励它；在错误的情况下，会惩罚它。将上图的狗换成猫咪结构图为：
![关于人工智能中强化学习的扫盲](http://p9.pstatp.com/large/pgc-image/a778467ab57b48ea9a4292a704efaea5)
进一步抽象为：
![关于人工智能中强化学习的扫盲](http://p1.pstatp.com/large/pgc-image/83811af630aa47dea7181eea4d10b6c8)
从上图可以一个典型的强化学习模型的结构包含两部分：环境和agent，描述包含：
l agent有限的状态集S，即agent能够处于多少种转态，例如在上面的类比中，狗是agent，人则是environment；那么，转态state则是人通过犬笛发出不同声音，给出不同的指令；
l Agent有限的动作集A，例子中则是狗根据人发出的指令需要完成的动作，比如坐下，趴下等；
l 回报函数R，狗完成不同动作之后，人给予不同的奖励；
l 折扣因子，用来计算累积的回报（reward）r，反映agent在序列决策中考虑未来一段时间回报的折扣；当r=0时，可以认为这个agent“目光短浅”，只考虑了眼前利益；当r接近于1时，可以认为这个学习体“目光长远”，考虑了将来可能带来的整体利益。
这是一个序列化过程，在时刻t，agent基于当前状态St发出动作At，环境做出回应，生成新的状态S(t+1)和对应的回报R(t+1)。需要强调一点的是，状态S和回报R是成对出现的。学习体的目标就是，通过更加明智地执行动作，从而最大化接下来的累计奖励，公式为：
![关于人工智能中强化学习的扫盲](http://p1.pstatp.com/large/pgc-image/ffd994e0635a4848bd7bed0115b27925)
当学习体并不是随机地产生可能的动作，而是经过对过去经历的思考得来的时，我们可以把这样的动作称为策略policy。从数学的角度来讲，策略就是由state到action的映射，它有两种形式，“确定性策略”和“非确定性策略”，前者指给定状态输出唯一的确定性动作，后者指给定状态输出所有动作的概率。一旦确定了策略时，那么学习体在每一个时刻都会按照该策略发出新的动作
# 强化学习的分类
解决强化学习问题，agent可以有多种工具组合，比如通过建立对状态的value估计来解决问题，或者通过直接建立对策略的估计来解决问题。因此，根据agent内包含的“工具”进行分类，可以把agent分为如下三类：
l**仅基于价值函数的Value Based**：在此类agent中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。而这类强化学习又包含基于蒙特卡洛的强化学习，基于时间差分的强化学习等。
l**仅直接基于策略的Policy Based**：这样的agent中行为直接由策略函数产生，agent并不维护一个对各状态价值的估计函数。这类包括基于策略梯度的强化学习，基于置信域策略优化的强化学习，基于确定性策略的强化学习，基于引导策略搜索的强化学习等。
l**演员-评判家形式Actor-Critic**：agent既有价值函数、也有策略函数。两者相互结合解决问题。
此外，根据agent在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类：
l**不基于模型的agent**：这类agent并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。
l**基于模型的agent**：agent尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。
# 与其他机器学习的对比
![关于人工智能中强化学习的扫盲](http://p1.pstatp.com/large/pgc-image/7c7a2731c6634a0481ed81463b55ddb0)
l**监督学习**是一种目标变量是已知的学习，并且在训练期间明确使用该信息（监督），即模型在目标的监督下训练，直接给出预测结果或者未来的输出，主要用于解决分类问题和回归问题。例如，如果想要为手写数字构建分类模型，则输入将是图像集（训练数据），目标变量将是分配给这些图像的标签，即0-9的类。
l**无监督学习**是对由目标未知的输入数据组成的数据集进行推断。最常见的无监督学习方法是聚类分析，用于探索、分析数据，以发现隐藏的模式或数据分组。其评价是定性或不明确的，不能进行预测。
l**强化学习**是在给定某种情况/环境的情况下，机器决定采取什么动作，以便最大化奖励。 监督和强化学习之间的区别在于奖励信号，它简单地告诉agent采取的行动（输入）是好还是坏。它没有告诉agent什么是最好的行动。 在这种类型的学习中，既没有训练数据也没有目标变量。
# 强化学习的一些应用
强化学习凭借其在序列决策问题上的优势，加之深度学习的巨大成功，使得强化学习在智能决策问题上表现出强劲的势头，有望解决通用人工智能。而且，强化学习有着广泛的使用领域。
**PC Games**
强化学习广泛应用于诸如刺客信条（Assasin’s Creed），国际象棋等PC游戏中，程序可以游戏者的表现改变它们的动作和方法。例如2016年3月，阿尔法围棋与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜。
![关于人工智能中强化学习的扫盲](http://p3.pstatp.com/large/pgc-image/02a9ab80011448f8bb2312db6678ac1d)
2017年5月，人类棋手“群殴”AlphaGo的大戏开始上演。由时越、芈昱廷、唐韦星、陈耀烨和周睿羊5位世界冠军组成中国围棋“天团”，仍没能抵挡住AlphaGo 。
![关于人工智能中强化学习的扫盲](http://p1.pstatp.com/large/pgc-image/cc8fad9fb3a440029ebb7874f9c5c1ea)
从AlphaGo Fan到AlphaGo zero，基于深度强化学习的网络结构对于序列决策的能力越来越强，因而在围棋这种决策非常复杂的游戏中能够战胜并超越人类棋手。这样的例子还有很多，如2019年1月25日，AlphaStar在《星际争霸2》人机大战直播节目中，以10-1的战绩轻松战胜2018 WCS Circuit排名13、神族最强10人之一的MaNa。
**机器人**
机器人本身就可以作为一个智能体（agent），因此强化学习在机器人上的使用，是一个非常贴切的例子。例如想要机器人模仿人的行为，找到从房子中的一个地方移动到另一个地方，而不会碰到障碍物的最佳路线。因此，重要的是定义分数，遇到障碍并得到负分（惩罚），避免障碍并获得正分（奖励）。而它覆盖的距离越远，奖励就越多。目标是在每种情况下最大化整体感知分数。除此之外，利用强化学习学习机器狗跳跃姿势时的局部线性驱动器参数，学习双轮机器人的平衡参数等等。
![关于人工智能中强化学习的扫盲](http://p9.pstatp.com/large/pgc-image/97eb2e32b9224f529f8b706fa822f955)
**库存管理**
通过构建强化学习算法，以减少库存的运输时间，以及检索仓库中的产品，以优化空间利用率和仓库运营。
**化学**
强化学习也可用于优化化学反应。基于强化学习模型优于其他先进的算法，并在“Optimizing Chemical Reactions with Deep Reinforcement Learning”一文中推广到不同的潜在机制。该应用表现出如何在相对稳定的环境中减少耗时和反复试验。
![关于人工智能中强化学习的扫盲](http://p1.pstatp.com/large/pgc-image/a53981bb71584a539ae3d79790cb37b5)
强化学习的应用领域还有自动驾驶（自动驾驶载具）、推荐系统（阿里巴巴黄皮书（商品推荐），广告投放）、问答系统、智能电网（电网负荷调试，调度）、通信网络（动态路由，流量分配）等等。

