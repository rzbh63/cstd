
# 神经网络和机器学习基础入门分享 - 杨秀璋的专栏 - CSDN博客

2015年11月03日 05:16:35[Eastmount](https://me.csdn.net/Eastmount)阅读数：10515所属专栏：[知识图谱、web数据挖掘及NLP](https://blog.csdn.net/column/details/eastmount-kgdmnlp.html)



最近在做知识图谱实体对齐和属性对齐中，简单用了下Word2vec谷歌开源代码。Word2vec是一个将单词表征成向量的形式，它可以把文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度。
Word2vec采用CBOW（Continuous Bag-Of-Words Model，连续词袋模型）和Skip-Gram（Continuous Skip-GramModel）两种模型，涉及到神经网络和深度学习的一些知识。故这周给学弟和同学们分享PPT的主题就是《神经网络是个什么东东？》，参考了很多资料并简单讲述了机器学习和神经网络的入门知识。
希望文章对你有所帮助，文章中分享了很多参考资料和最后对吴老的赞扬~

## 一. 机器学习入门介绍
以前转载过"数盟"的一篇关于机器学习入门介绍的文章，这里简单引入它的几张图片作为引入。参考原文地址和转载地址：
[机器学习科普文章：“一文读懂机器学习，大数据/自然语言处理/算法全有了”](http://www.thebigdata.cn/JieJueFangAn/13080.html)
[http://blog.csdn.net/eastmount/article/details/43673209](http://blog.csdn.net/eastmount/article/details/43673209)
机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning（简称ML）的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器“学习”的技术。但是计算机是死的，怎么可能像人类一样“学习”呢？
下图是非常形象的将机器学习的过程与人类对历史经验归纳的过程做个比对：
**机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。**
![](https://img-blog.csdn.net/20151102205153985)

常见的例子包括房价与面积、预测患病概率等等，详见斯坦福NG教授课程。
下图是机器学习所牵扯的一些相关范围的学科与研究领域。
![](https://img-blog.csdn.net/20151102211134141)

简单的等价划分如下：
**模式识别=机器学习:**两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。
**数据挖掘=机器学习+数据库:**从数据中挖出金子，以及将废弃的数据转化为价值。
**计算机视觉=图像处理+机器学习:**图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。例如百度识图、手写字符识别、车牌识别等等应用。
**统计学习≈机器学习:**统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可认为统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。区别在于统计学习者重点关注的是统计模型的发展与优化，偏数 学，而机器学习者更关注的是能够解决问题，偏实践。
**语音识别=语音处理+机器学习:**语音识别就是音频处理技术与机器学习的结合。一般会结合自然语言处理的相关技术，目前的相关应用有苹果的语音助手siri。
**自然语言处理=文本处理+机器学习:**让机器理解人类的语言，NLP中大量使用了编译原理相关的技术，例如词法分析、语法分析、语义理解等。
自然语言处理作为唯一由人类自身创造的符号，一直是机器学习界不断 研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。
Deep Learning （深度学习）算法已经在图像和音频领域取得了惊人的成果，但是在 NLP 领域中尚未见到如此激动人心的结果。有一种说法是：
**语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语音和图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。**
而将词用“词向量”的方式表示可谓是将 Deep Learning 算法引入 NLP 领域的一个核心技术。大多数宣称用了 Deep Learning 的论文，其中往往也用了词向量。显然Word2vec也引入了词向量。参考licstar文章：
[Deep Learning in NLP （一）词向量和语言模型](http://licstar.net/archives/328)机器学习的方法很多，其中经典的方法包括：回归算法、神经网络、支持向量机SVM、聚类算法、降维算法、推荐算法、朴素贝叶斯、决策树等。详见这两篇文章：
[常见的机器学习&数据挖掘知识点 - 作者：一只鸟的天空](http://blog.csdn.net/heyongluoyao8/article/details/47840255)
[[转载] 机器学习面试之算法思想简单梳理](http://blog.csdn.net/eastmount/article/details/42421289)

## 二. 神经网络入门介绍
该部分主要通过白话文的方式讲述神经网络，其中主要转载吴老的文章。链接：
[
吴祖增前辈：神经网络入门(连载之一)](http://blog.csdn.net/zzwu/article/details/574931)
[
吴祖增前辈：神经网络入门(连载之二)](http://blog.csdn.net/zzwu/article/details/575050)
斯坦福机器学习视频NG教授[https://class.coursera.org/ml/class/index](https://class.coursera.org/ml/class/index)
书籍《游戏开发中的人工智能》、《游戏编程中的人工智能技术》
神经网络(也称之为人工神经网络，ANN)算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，携着“深度学习”之势，神经网络重装归来，重新成为最强大的机器学习算法之一。
![](https://img-blog.csdn.net/20151103023058400)
人工神经网络（artificial neural network，缩写ANN），是一种模仿生物神经网络的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结进行计算。其来源于生物，故吴老先先讲述了生物神经网络的基础知识，从而进行引入。

![](https://img-blog.csdn.net/20151103023724578)

**神经细胞通过轴突将信号传递给其他的神经细胞，通过树突向各个方向接受信号。**
神经细胞利用电-化学过程交换信号。输入信号来自另一些神经细胞。这些神经细胞的轴突末梢（也就是终端）和本神经细胞的树突相遇形成突触（synapse），信号就从树突上的突触进入本细胞。
信号在大脑中实际怎样传输是一个相当复杂的过程，但就我们而言，重要的是把它看成和现代的计算机一样，利用一系列的0和1来进行操作。就是说，大脑的神经细胞也只有两种状态：兴奋（fire）和不兴奋（即抑制）。
![](https://img-blog.csdn.net/20151103024805255)
神经细胞利用一种我们还不知道的方法，把所有从树突突触上进来的信号进行相加，如果全部信号的总和超过某个阀值，就会激发神经细胞进入兴奋（fire）状态，这时就会有一个电信号通过轴突发送出去给其他神经细胞。如果信号总和没有达到阀值，神经细胞就不会兴奋起来。这样的解释有点过分简单化，但已能满足我们的目的。
![](https://img-blog.csdn.net/20151103025739605)
由于人脑具有一下几个特点：
**1.能实现无监督的学习**
大脑能够自己进行学习，而不需要导师的监督教导。如果一个神经细胞在一段时间内受到高频率的刺激，则它和输入信号的神经细胞之间的连接强度就会按某种过程改变，使得该神经细胞下一次受到激励时更容易兴奋。
**2.对损伤有冗余性(tolerance)**
大脑即使有很大一部分受到了损伤, 它仍然能够执行复杂的工作。
**3.处理信息的效率极高**
神经细胞之间电-化学信号的传递，与一台数字计算机中CPU的数据传输相比，速度是非常慢的，但因神经细胞采用了并行的工作方式，使得大脑能够同时处理大量的数据。例如，大脑视觉皮层在处理通过我们的视网膜输入的一幅图象信号时，大约只要100ms的时间就能完成，眼睛并发执行。
**4.善于归纳推广**
大脑和数字计算机不同，它极擅长的事情之一就是模式识别，并能根据已熟悉信息进行归纳推广(generlize)。例如，我们能够阅读他人所写的手稿上的文字，即使我们以前从来没见过他所写的东西。
**5.它是有意识的**
![](https://img-blog.csdn.net/20151103032002251)
如下图所示，它表示的是一个人工神经细胞。其中：
**输入****(Input****)；****权重****(Weight)****：左边五个灰色圆底字母****w****代表浮点数；****激励函数****(Activation
 Function)****：大圆，所有经过权重调整后的输入加起来，形成单个的激励值；输出(Output)：神经细胞的输出。**
![](https://img-blog.csdn.net/20151103033007229)
进入人工神经细胞的每一个input(输入)都与一个权重w相联系，正是这些权重将决定神经网络的整体活跃性。假设权重为-1和1之间的一个随机数，权重可正可负（激发和抑制作用）。当输入信号进入神经细胞时，它们的值将与它们对应的权重相乘，作为图中大圆的输入。如果激励值超过某个阀值（假设阀值为1.0），就会产生一个值为1的信号输出；如果激励值小于阀值1.0，则输出一个0。这是人工神经细胞激励函数的一种最简单的类型。涉及的数学知识如下图所示：
![](https://img-blog.csdn.net/20151103033932054)
如果最后计算的结果激励值大于阈值1.0，则神经细胞就输出1；如果激励值小于阈值则输出0。这和一个生物神经细胞的兴奋状态或抑制状态是等价的。下面图是通过神经网络实现逻辑表达式与运算：（参考NG斯坦福机器学习讲义）
![](https://img-blog.csdn.net/20151103034617548)

可以看到x1和x2变量作为神经网络的输入，当它们取不同的0或1值时，其结果通过sigmod函数计算的值是不同的。它模拟了整个AND运算。
![](https://img-blog.csdn.net/20151103035354873)
该图中神经网络共有三层 ( 注输入层不是神经细胞，神经细胞只有两层 )：
**输入层中的每个输入都馈送到了隐藏层，作为该层每一个神经细胞的输入；然后，从隐藏层的每个神经细胞的输出都连到了它下一层（即输出层）的每一个神经细胞。**
注意：
1.图中仅仅画了一个隐藏层，作为前馈网络，一般地可以有任意多个隐藏层。但在对付你将处理的大多数问题时一层通常是足够的。
2.事实上，有一些问题甚至根本不需要任何隐藏单元，你只要把那些输入直接连结到输出神经细胞就行了。
3.每一层实际都可以有任何数目的神经细胞，这完全取决于要解决的问题的复杂性。但神经细胞数目愈多，网络的工作速度也就愈低，网络的规模总是要求保持尽可能的小。
![](https://img-blog.csdn.net/20151103040150028)
神经网络体系创建成功后，它必须接受训练来认出数字4，方法：
1.先把神经网络的所有权重初始化为任意值；
2.然后给他一系列输入代表面板不同配置的输入，对每种输入配置，检查它的输出是什么，并调整相应的权重；
3.如果我们送给网络的输入模式不是4，则我们知道网络应该输出一个0。因此每个非4字符时，网络权重应进行调整，使得它的输出趋向于0；当代表4的模式输送给网络时，则应把权重调整到使其输出趋向于1；
4.我们可以进一步识别0到9的所有数字或字母，其本质是手写识别的工作原理。
5.最后，网络不单能认识已经训练的笔迹，还显示了它有显著的归纳和推广能力。
正是这种归纳推广能力，使得神经网络已经成为能够用于无数应用的一种无价的工具，从人脸识别、医学诊断，直到跑马赛的预测，另外还有电脑游戏中的bot（作为游戏角色的机器人）的导航，或者硬件的robot（真正的机器人）的导航。
![](https://img-blog.csdn.net/20151103041044137)
![](https://img-blog.csdn.net/20151103041200953)
上图会演示神经网络在图像识别领域的一个著名应用，这个程序叫做LeNet，是一个基于多个隐层构建的神经网络。通过LeNet可以识别多种手写数字，并且达到很高的识别精度与拥有较好的鲁棒性。LeNet的发明人是机器学习的大牛Yann LeCun（目前google）。
右下方的方形中显示的是输入计算机的图像，方形上方的红色字样“answer”后面显示的是计算机的输出。左边的三条竖直的图像列显示的是神经网络中三个隐藏层的输出，可以看出，随着层次的不断深入，越深的层次处理的细节越低，例如层3基本处理的都已经是线的细节了。
推荐我自己非常喜欢的&牛逼的CSDN机器学习"一只鸟的天空"博主的一篇文章：
[当今世界最NB的25位大数据科学家](http://blog.csdn.net/heyongluoyao8/article/details/48598169)
这种类型的训练称作有监督的学习（supervised learnig），用来训练的数据称为训练集（training set）。调整权重可以采用许多不同的方法。对本类问题最常用的方法就是反向传播（backpropagation，简称backprop或BP）方法，即BP神经网络。
你自己可以去学习另外的一种训练方式，即根本不需要任何导师来监督的训练，或称无监督学习（unsupervised learnig）。
下图是神经网络的简单回顾与总结：

![](https://img-blog.csdn.net/20151103042300580)
最后给大家看一个利用神经网络对图片进行分类的例子：过程就不详细论述了，图片很清晰，对人、汽车、摩托车、卡车进行图片识别，而具体的隐藏层函数需要大家去深入研究，我自己研究得也很浅显，抱歉~
![](https://img-blog.csdn.net/20151103042724702)
参考资料包括NG教授的课程讲义和CSDN一位很厉害的女博主Rachel-Zhang：
[
Stanford机器学习---第五讲. 神经网络的学习 Neural Networks learning](http://blog.csdn.net/abcjennifer/article/details/7758797)

![](https://img-blog.csdn.net/20151103042908475)



## 三. 监督学习和无监督学习
因为给刚入学的学弟们讲的PPT，所以也简单讲述了监督学习和无监督学习的区别。下图是刘斌《Web数据挖掘》书的整体框架，当年我讲过。
![](https://img-blog.csdn.net/20151103043444553)
在给它们解释监督学习和无监督学习的过程中，主要通过[知乎](http://www.zhihu.com/question/23194489)的五个问题进行讲解。
**第一个问题：什么是学习(Learning)？**
一个成语就可以概括：举一反三 。以高考为例，高考的题目在上考场前我们未必做过，但在高三做过很多题目，懂得解题方法，因此考场上也能算出答案。
机器学习的思路类似：我们能不能利用一些训练数据(已经做过的题)使机器能够利用它们(解题方法)分析未知数据(高考题目)。
**第二个问题：最普遍也是最简单的机器学习算法？**
分类(Classification)：输入的训练数据有特征(feature)，有标签(label)。学习的本质就是找到特征和标签间的关系(mapping)。这样当有特征而无标签的未知数据输入时，我们可以通过已有的关系得到未知数据的标签。
**第三个问题：上述分类所有训练数据都有标签，如果没有呢？**
所有训练数据都有标签则为有监督学习(Supervised Learning)，如果数据没有标签则是无监督学习(Unsupervised Learning)，也即聚类(Clustering)。但有监督学习并非全是分类还有回归(Regression)。
无监督学习本身的特点使其难以得到如分类一样近乎完美的结果。这也正如我们在高中做题，答案(标签)是非常重要的，假设两个完全相同的人进入高中，一个正常学习，另一人做的所有题目都没有答案，那么想必第一个人高考会发挥更好，第二个人会发疯。
**第四个问题：既然分类如此之好，聚类如此之不靠谱，那为何我们还可以容忍聚类的存在？**
在实际应用中，标签的获取常常需要极大的人工工作量，有时甚至非常困难。例如在自然语言处理NLP中，Penn Chinese Treebank在2年里只完成了4000句话的标签。
**第五个问题：难道有监督学习和无监督学习就是非黑即白的关系吗？有没有灰呢？**
Good idea。灰是存在的。二者的中间带就是半监督学习(semi-supervised learning)。对于半监督学习，其训练数据的一部分是有标签的，另一部分没有标签，而没标签数据的数量常常极大于有标签数据数量(这也是符合现实情况的)。
![](https://img-blog.csdn.net/20151103044250748)



## 四. 总结
这篇文章主要是一篇基础介绍神经网络和机器学习的入门文章，同时参考了很多资料，主要是自己分享的PPT给学弟的新知识。同时由于我自己也只是入门，所以很多高深的东西也在学习，在此需要向上面文章中的个位大牛虚心学习。但作为入门文章，感觉还是不错的~
最近发生的事情太多太多，其中包括拒绝了一些互联网公司或航天院的offer、面试，一心准备回家乡贵州的一些大学教书。怎么说呢？贵州那里确实承载了太多的东西，有亲情、有梦想，更有一些根深蒂固的东西。从小就在校园长大，父母都是老师，家住校园，到了高中也独自去到了省会贵阳住校三年，大学更是来到了北理，异地他乡的我，一呆就是六年。从小玩到大的姐姐也成为了初中老师，似乎我这一生与学校挂钩后，就再也脱不了关系了。
这并没有什么情怀，没有什么了不起，更没有什么高大上。更多的是自己从小一直希望成为一名教师吧！尽管父母都不愿意让孩子再从事这一行业，但我已毅然决定。确实，我自己也舍弃了很东西，如每月的高工资福利、发布一款产品后的欣喜若狂等等，但同时有舍才有得，希望自己能在大学中获得一些东西吧！还是《当幸福来敲门》那句话：
在你的人生路上，总有很多人说你这也不行，那也不行，梦想是你自己的；有梦想就需要学会自己去保护它。
别拿着青春的幌子，浪费自己的年轻时光。我一直都喜欢深夜写文章，深夜是思考的好时间，更是寂寞的好时光。一个人行走在这个世界，很多路都需要自己去孤独的前行，需要让自己的内心强大起来。但每每写完一篇博客或想到当老师上课，都能让我心灵为之一颤。这就够了~
下面是我看到吴祖增老师80多岁高龄后，依然坚持写作的感慨！见笑了~
侠之为大，为国为民。行止无定，随遇而安。心安乐处，便是深安乐处。
![](https://img-blog.csdn.net/20151103050714104)
最后希望自己能够找到一所大学，成为一名教师！同时也希望文章对你有所帮助~
(By:Eastmount
 2015-11-03 深夜5点[http://blog.csdn.net/eastmount/](http://blog.csdn.net/eastmount/))


