
# 深度学习笔记 - 喜欢打酱油的老鸟 - CSDN博客


2018年08月07日 12:23:56[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：219


[https://blog.csdn.net/github_39655029/article/details/81430298](https://blog.csdn.net/github_39655029/article/details/81430298)

> TensorFlow
TensorFlow不仅是一个实现机器学习算法的接口，也是一种框架，也可用于线性回归、逻辑回归、随机森林等算法；
TensorFlow使用数据流图来规划计算流程，每个运算操作作为一个节点node，节点之间的连接称为边，边中流动的数据称为张量，故而得名TensorFlow，预算操作可以有自己的属性，但必须被预先设置，或者能在创建计算图时被推断出来；
TensorFlow有一套为节点分配设备的策略，这是一个简单的贪婪策略，不能确保找到全局最优解，但可以快速找到一个不错的节点运算分配方案；
故障出现的两种情况：一是信息从发送节点传输到接受节点失败时，而是周期性的worker心跳检测失败时；
TensorFlow提供的加速神经网络训练的并行计算模式：数据并行：通过将一个mini-batch的数据放在不同设备上计算没实现梯度计算的并行化，计算性能损耗非常小，同步的方式优点是没有梯度干扰，缺点是容错性差，异步的方式优点是有一定容错性，但因为梯度干扰，导致利用效率下降；
模型并行：将计算图的不同部分放在不同设备上运算；
流水线并行：将计算做成流水线，在一个设备上连续并行执行，提高设备利用率；

> 卷积神经网络CNN
CNN具有极强泛化性，最大的特点在于卷积的权值共享结构，能大幅较少神经网络的参数量，防止过拟合的同时降低了神经网络模型的复杂度；
CNN每个卷基层中对数据的操作：图像通过多个不同卷积核的滤波，加以偏置，提取出局部特征，每个卷积核映射出一个新的2D图像；
将卷积核的滤波结果进行非线性的激活函数处理，常为ReLU函数；
对激活结果进行池化操作(即降采样)，一般采用最大池化，保留最显著特征，提升模型的畸变容忍能力；
卷积核的大小即为卷积核拥有的参数多少；
采用局部连接的方式，参数量得到了缩减；
卷积的好处是不管图片尺寸如何，我们需要训练的权值数量只和卷积核大小、卷积核数量有关，可以用极少的参数量处理任意大小的图片，虽然训练的参数下降了，但隐含节点数量未下降，隐含节点数量只与卷积的步长相关；
CNN要点：局部连接：降低参数量，减轻过拟合，降低训练复杂度；
权值共享：降低参数量，减轻过拟合，赋予对平移的容忍性；
池化层中的降采样：降低输出参数量，赋予轻度形变的容忍性，调高模型的泛化能力；
LeNet5的特性：每个卷基层包含三个部分：卷积、池化、非线性激活函数；
使用卷积提取空间特性；
降采样的平均池化层；
双曲正切或S型激活函数；
MLP(多层神经网络)作为最后的分类器；
层与层之间的稀疏连接减少计算复杂度；
LeNet5有三个卷积层、一个全连接层和一个高斯连接层；第一个卷积层6个卷积核，尺寸5*5，共(5*5+1)*6 = 156个参数
第二个卷积层16个卷积核；
第三个卷积层120个卷积核；
全连接层84个隐含节点，激活函数Sigmoid；
VGGNet-16网络结构主要分为6部分，前5段为卷积网络，最后一段为全连接网络；第一段：两个卷积层和一个最大池化层，卷积核大小3*3，卷积核数量64，步长1*1，第一个卷积层输入尺寸224*224*3，输出尺寸224*224*64，第二个输入输出尺寸均为224*224*64，池化层2*2，输出尺寸112*112*64；
第二段：和第一段相似，输出通道数变为128，卷积网络输出尺寸56*56*128，池化层保持不变；
第三段：三个卷积层和一个最大池化层，输出通道变为256，输出尺寸28*28*256；
第四段：和第三段相似，输出通道变为512，通过最大池化将图片缩为14*14；
第五段：和第四段相似，池化层尺寸2*2，步长为2*2，输出尺寸7*7*512；
第六段：将第五段输出结果进行扁平化，连接一个隐含节点数为4096的全连接层，激活函数为ReLU；

> R-CNN
检测系统三个模块：生成类别无关区域提案；
从每个区域提取固定长度特征向量的大型CNN；
一组特定类别的线性SVM；
需要训练数据的三个阶段：CNN微调；
检测器SVM训练；
检测框回归训练；
引入CNN来分类目标候选框，有很高的目标检测精度，但有明显缺点：训练过程是多级流水线；
训练在时间和空间的开销上极大；
目标检测速度很慢，因为为每个目标候选框进行CNN正向传递，不共享计算；

> Fast R-CNN
训练VGG16网络比SPP-Net快3倍，测试速度快10倍，比R-CNN训练快9倍，测试时间快213倍，有13个卷积层和3个fc层；
目标检测难点：大量候选目标位置(提案)需要处理；
候选框只提供粗略定位，必须对其精细化以实现精确定位；
优点：比R-CNN和SPPnet有更高的目标检测精度mAP；
训练是使用多任务损失的但阶段训练；
训练可以更新所有网络层参数；
不需要磁盘空间缓存特征；
网络架构流程：输入图像和多个感兴趣区域ROI，传送到全卷积网络，经池化到固定大小的特征图中，然后通过全连接层FC映射到特征向量，网络对每个ROI具有两个输出向量：Softmax概率和每类检测框回归偏移量；
> Faster R-CNN
Faste R-CNN实现了接近实时检测的速率，但忽略了生成区域提案框的时间，Faster R-CNN算法通过将RPN网络集成到目标检测网络中共享卷积层，缩减了生成区域提案框的时间，计算提案框的边界成本小；
RPN是一种全卷积网络FCN，可以针对生成检测提案框的任务端到端训练；
RPN中引入新“锚点”作为多尺度和纵横比的参考，避免了枚举多个尺度或纵横比得图像或卷积；
为统一RPN和Fast R-CNN网络，提出一种训练方案：保持提案框固定，微调区域提案和微调目标检测之间交替进行；
组成模块：提出区域提案的CNN网络；
使用区域提案的Fast R-CNN检测器；
RPN将一个任意大小的图像作为输入，输出矩形目标提案框的集合，每个框由一个objectness得分；
为生成区域提案框，在最后一个共享的卷积层输出的卷积特征映射上滑动小网络，网络连接到输入卷积特征映射的n*n的空间窗口，每个滑动窗口映射到一个低维向量上，向量在输出给两个同级的全连接的层：检测框回归层reg和检测框分类层cls；
多尺度预测方式：基于图像/特征金字塔：以多尺度调整大小，为每个尺度计算特征图，有效却耗时；
在特征图上使用多尺度的滑动窗口；
具有共享特征的网络的解决方案：交替训练；
近似联合训练；
非近似联合训练；
cls检测框分类层得分是排名最高的提案框准确的原因；

