
# 过拟合、欠拟合与正则化 - 喜欢打酱油的老鸟 - CSDN博客


2019年03月11日 08:25:59[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：410标签：[过拟合																](https://so.csdn.net/so/search/s.do?q=过拟合&t=blog)[欠拟合																](https://so.csdn.net/so/search/s.do?q=欠拟合&t=blog)[正则化																](https://so.csdn.net/so/search/s.do?q=正则化&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=欠拟合&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=过拟合&t=blog)个人分类：[人工智能																](https://blog.csdn.net/weixin_42137700/article/category/7820233)
[
																								](https://so.csdn.net/so/search/s.do?q=过拟合&t=blog)


[https://www.toutiao.com/a6666213114146456072/](https://www.toutiao.com/a6666213114146456072/)
在机器学习模型的训练过程中，经常会发生过拟合（overfitting）、欠拟合（underfitting）的现象。那这二者究竟为何物呢？
过拟合，通俗点说，就是我们的模型对数据模拟的太好了，训练集中的数据几乎都被完美预测。有人就说了，我预测的完美也有错吗？是的，你有错。因为我们的目标并不是看你训练集中的数据预测得有多好，而是要看在测试集中的表现。也就是说，把这个模型放到新环境中，测试预测效果。同时，我们的训练集中数据有噪声，如果你连噪声都完美预测，那么放到测试集中，模型的表现一定不是很好。专业点说，就是模型的泛化能力差。
欠拟合，正好跟过拟合相反，我们的模型在训练集中表现的太差了，几乎很少能预测正确的。放到测试集中同样不会有很好的效果。
那出现这两种情况该如何解决呢？对于欠拟合来说，我们可以增加模型的复杂度，增加数据量等等手段。而对于过拟合呢，今天的主角——正则化（regularization）就派上用场了。
也就是说，对于我们之前的线性回归模型，给它加一个正则化项，它就不那么容易发生过拟合。根据正则化项的不同，一般分为Ridge Regression（岭回归）和Lasso Regression（拉索回归）。我们先来看应用广泛的岭回归。
岭回归的代价函数如下：
![过拟合、欠拟合与正则化](http://p3.pstatp.com/large/pgc-image/e232fbf02ffc4df7b71437802305f8c4)

从上式可以看出，正则化项其实就是参数w的平方和再乘以个系数λ。对于加了正则化项的代价函数来说，我们要想求得最小值，从而得到w的解，还是要对J(w)求导。
在没加正则化项之前，J(w)的导数可化简为（参考之前的线性回归模型推导）：
![过拟合、欠拟合与正则化](http://p9.pstatp.com/large/pgc-image/d75dc0e5961c4c778cdbead50bb9dec8)

而正则化项的导数为λ**w**/m，把这两个加到一起，就得到
![过拟合、欠拟合与正则化](http://p1.pstatp.com/large/pgc-image/ff087bd73c8048e19408b40c6945b0ee)

令其等于0，得到：
![过拟合、欠拟合与正则化](http://p1.pstatp.com/large/pgc-image/91184ff4cf044cfea28b1d5194cddcb2)

注意到，w的系数一个为矩阵，一个为标量λ，我们想将w提出来，该怎么办呢？事实上，只要让λ乘以单位矩阵就可以了：
![过拟合、欠拟合与正则化](http://p1.pstatp.com/large/pgc-image/198b9ec59a0d46a8873d91120c3c4ab3)

之前我们在线性回归推导到这一步时，曾说到w的矩阵系数可能不可逆。但现在我们添加了λI这一项后，就保证了w的矩阵系数一定是可逆的。所以在等式两边同时乘以矩阵系数的逆，则可求出w为：
![过拟合、欠拟合与正则化](http://p1.pstatp.com/large/pgc-image/c05b3981400442be978dfaf4d28d1a71)

这是我们利用求导直接得到了参数w的值，当然我们也可以用梯度下降法来进行计算，只需要在梯度下降公式后面加上‘λm’这一项即可，这里就不再推导了。
推导完之后，我们再来回顾下岭回归的代价函数，重点观察它的正则化项，思考为什么加入它之后就能避免过拟合了呢？
注意我们的参数λ，如果它比较大，那要想J(w)取小值，那么系数w就必须减小，这就降低了模型的复杂度，过拟合现象得以缓解。但λ也不能过大，过大会导致系数被“惩罚”得很厉害，模型反而会过于简单，可能欠拟合；同时，λ也不能过小，当λ趋近于0的时候，相当于我们没有添加正则化项，同样不能缓解过拟合。
最后我们来简单了解下Lasso回归，与岭回归想比，它的正则化项就是参数w的绝对值的和，用公式表示为：
![过拟合、欠拟合与正则化](http://p9.pstatp.com/large/pgc-image/cae71b7612b84f88b27cd9881dde45d4)

如果我们用梯度下降法，求得J(w)的梯度为：
![过拟合、欠拟合与正则化](http://p1.pstatp.com/large/pgc-image/2f53105dede84d53b35ea6fbdc284254)

其中，sign(w)叫做符号函数。它的含义是：当w>0时，sign(w)=1；当w<0时，sign(w)=-1；当w=0时，sign(w)=0。
最后，我们把Lasso回归叫做L1正则化，岭回归叫做L2正则化。因为它们的正则化项分别是L1范数和L2范数。L-p范数的定义如下：
![过拟合、欠拟合与正则化](http://p3.pstatp.com/large/pgc-image/4c512ae3ee1b44b68fc03cd8468835c4)

这就是今天的全部内容，下一篇我们会来介绍分类模型，敬请期待。

