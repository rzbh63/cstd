# 百面机器学习--Hulu

 

关注

 0.2 2018.12.20 11:22 字数 267 阅读 344评论 0喜欢 3

以下内容为转发Hulu公众号的链接，更全内容可查看书籍

- 0 [序](https://mp.weixin.qq.com/s/xkwtTuTA6oXAs09By_TvVg)
- 1 [模型评估](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430219&idx=1&sn=fa1690a94ae62c089ef249da900053b4&chksm=8b0048cabc77c1dca7624cb722eea4801b36c5d7577bdd4a23afe6647823bc3acd02d5c50df4&scene=21#wechat_redirect)
- 2 [SVM模型](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430230&idx=1&sn=b0597b35705105507cbc0c07cff3a358&chksm=8b0048d7bc77c1c1dca0d942d91b61b17039018f0973ccd97fd51f6113aaf2660a982d0fcc96&scene=21#wechat_redirect)
- 3 [优化简介](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430247&idx=1&sn=fd2ce2a046ac740449db342968d3cfd5&chksm=8b0048e6bc77c1f0d8094ff06f04e5f0ee6b72b2529682dbd2e35a52c813d74f7be1911585d7&scene=21#wechat_redirect)
- 4 [采样](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430319&idx=1&sn=2e22a6371e30929b4aeacb33565e5184&chksm=8b0048aebc77c1b83b8dd5b25fa6dfd759bc1a3fdc069827d9de2e591615b6d41bddc867bf36&scene=21#wechat_redirect)
- 5 [余弦距离](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430368&idx=1&sn=79060c02ac1b9b9b575bf30802ae4e86&chksm=8b004961bc77c07739f06a37f35599eafb28073b6f2ff53b19b2a10b09a09662bac6291ccf71&scene=21#wechat_redirect)
- 6 [PCA算法](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430435&idx=1&sn=f55f0ad0b5025076f8b9cbb248737f75&chksm=8b004922bc77c034b92a97387d791ac350f643130da03534dfadfe28a162c8fab98441e0a7b9&scene=21#wechat_redirect)
- 7 [非监督学习算法与评估](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430467&idx=1&sn=291f17682a82aa62448d4758859dc225&chksm=8b0049c2bc77c0d46793ecd0ccefd3631031d58c3e0e3071bfe1043f5f22426842bc9f34fead&scene=21#wechat_redirect)
- 8 [强化学习（一）](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430479&idx=1&sn=a779edc5adf724893fa089e810c161f5&chksm=8b0049cebc77c0d887b52f06742488c13a352dc9cfd5ee6a61cad3442cd54f2b76f5e0364455&scene=21#wechat_redirect)，[强化学习（二）](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430482&idx=1&sn=92cab5c70f77716bc8142ccab16a5b54&chksm=8b0049d3bc77c0c5051ebf0a484592bfd9d575a4ef413115ca9e240b5e9a18a5f394a5181c31&scene=21#wechat_redirect)
- 9 [循环神经网络](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430489&idx=1&sn=2a97f49c27afa5179dc4b7dcdf697ebf&chksm=8b0049d8bc77c0ce46844334f8c6e1300d80a1bb0743124a2ee515440b57b3ecf5a53923e8a7&scene=21#wechat_redirect)
- 10 [LSTM](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430538&idx=1&sn=0c63c199e8eb7f15a9d6c9e122c25d3b&chksm=8b00498bbc77c09ddbefb338717b7199fb68bea4dcaaf2a17064423e885e1eb88ded6a569b52&scene=21#wechat_redirect)
- 11 [Seq2Seq](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430548&idx=1&sn=10c57c88ab576196d212368242f564b2&chksm=8b004995bc77c083b106c0fd13e765bfca8e5576acbe3af646f17afd546f41bf71c571a3d9d3&scene=21#wechat_redirect)
- 12 [注意力机制](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430556&idx=1&sn=a71b24c63d1d5645601e89c37dc5e6d9&chksm=8b00499dbc77c08b03f69c96257ae72b402d58deda79e10b629c083944732ad58bbc5d83dad5&scene=21#wechat_redirect)
- 13 [集成学习](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430592&idx=1&sn=7932da416c87cc7f861810651eddc45c&chksm=8b004a41bc77c357166fa888aaef1b27e75ff711878a5e6c62a4eaf5f0f7075c85f0d24376a0&scene=21#wechat_redirect)
- 14 [如何对高斯分布进行采样](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430608&idx=1&sn=ac2bf320fe8359b4f7742d6a7892269b&chksm=8b004a51bc77c347f48c0ab34777c281b2d3d61c0a421f53edd422af3b2538a05c572441ad6d&scene=21#wechat_redirect)
- 15 [多层感知机与布尔函数](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430644&idx=1&sn=0bed7bdcc92be95310fc50bc326b4d3e&chksm=8b004a75bc77c36322f364f8579a783f2cada2449cfd1dd1eab4d2b655e7430e74826ff07531&scene=21#wechat_redirect)
- 16 [经典优化算法](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430674&idx=1&sn=a34b828fced35e3f8d9385754d7653f8&chksm=8b004a13bc77c3050f959a610f7725f52c0cbcd3995dad1dcebb796c436f0c6879b0308ec8dc&scene=21#wechat_redirect)
- 17 [随机梯度下降算法之经典变种](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430694&idx=1&sn=41652491c23de310aa4aadcdd26ff563&chksm=8b004a27bc77c33192c67a6fa8a97cc67285ef3e4c1d9240b82a29b793d8775775aa07952ab3&scene=21#wechat_redirect)
- 18 [SVM—核函数与松弛变量](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430721&idx=1&sn=8d02f40ae6bf725c6aeaefaf275843e3&chksm=8b004ac0bc77c3d6640e1e55bf9596e7fac9e74deb41a78c9f363af15f9475f98e75ccb8bd9d&scene=21#wechat_redirect)
- 19 [主题模型](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430742&idx=1&sn=22218a480a99451ee787e5db14e325b7&chksm=8b004ad7bc77c3c1177b66e16fe91abd79bda260851fcdf6ff1f88618ddfb9cb73593b64b68d&scene=21#wechat_redirect)
- 20 [PCA最小平方误差理论](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430757&idx=1&sn=ee4d6f0b037ecd273929dca8901bf96f&chksm=8b004ae4bc77c3f2b49248612199ea420c7a87fd63f553146d83e1a2b17012c5ffa6993416aa&scene=21#wechat_redirect)
- 21 [分类、排序、回归模型的评估](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430790&idx=1&sn=597b5d19c8d2141b6108f8f420395eaa&chksm=8b004a87bc77c39172055c84d670ba5095ad74202df34cbbf19b33b5d117bd923c1ffa360aaf&scene=21#wechat_redirect)
- 22 [特征工程—结构化数据](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430794&idx=1&sn=5ea4c6f5d2bd54ab15665620ef59a0bd&chksm=8b004a8bbc77c39de7727ea2abf2c97a1780db16ca76a73e5175919a25d7741af77443969dc4&scene=21#wechat_redirect)
- 23 [神经网络训练中的批量归一化](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430800&idx=1&sn=59b8fb01995f9ba37e62dcae0280ba1f&chksm=8b004a91bc77c387eb36b66e3ee2ce3599caf6e257b843fc0aed034cc586cb00a722687cebdd&scene=21#wechat_redirect)
- 24 [随机梯度下降法](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430814&idx=1&sn=95b912e8a4d828908dba5f340a49ece8&chksm=8b004a9fbc77c3890933d3bb36b503dfa5e4b650024d79b0145709e2abfc472c9394ee05569c&scene=21#wechat_redirect)
- 25 [初识生成式对抗网络（GANs）](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430833&idx=1&sn=216dfeab7eec10a7a47ca2ce8a145216&chksm=8b004ab0bc77c3a65bb6519d8fcc3a1b9a31b046d26c5b199e40e4764a87b42dfc985e6ab284&scene=21#wechat_redirect)
- 26 [隐马尔科夫模型](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430837&idx=1&sn=6e8427a3f35946c126bb41f0cbd42ca0&chksm=8b004ab4bc77c3a2f2676ff65afe6c7b7f266082612a31edc7d220f7aa272401c9da0f9eae4e&scene=21#wechat_redirect)
- 27 [自组织映射神经网络](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430844&idx=1&sn=69e9363a24a0c8eb26d49c239f628f55&chksm=8b004abdbc77c3ab5fcba3fbe35d8abc89bfc93266a9ee064dcb94611f8a3fd91b57acf6304c&scene=21#wechat_redirect)
- 28 [概率图模型](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430868&idx=1&sn=60ec3d8e08a6f8a2edeec7ab089282fe&chksm=8b004b55bc77c243695ce4f49cd43bbcbba7b307b837c9acc082061a32218bc2ddcede5d4ac6&scene=21#wechat_redirect)
- 29 [WGANs：抓住低维的幽灵](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430896&idx=1&sn=639d62e9c89492b22cb46679f9b4e1cc&chksm=8b004b71bc77c267e8384d5b64730e639e4667e5d23dd4a42e75ab16588b7a5e18ae9188695b&scene=21#wechat_redirect)
- 30 [常见的采样方法](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430899&idx=1&sn=3b2f60df6ec181c12cad53f5eba99dee&chksm=8b004b72bc77c2644696fdfc09e3c4bf5541e4b4dc75e5e834a628fddb49be31c75f598a9c52&scene=21#wechat_redirect)





序言 | 诸葛越



2018招聘季正如火如荼地进行，而今年最热门的职业，就是“算法工程师”。



“人工智能”，“机器学习”，“深度学习”，“建模”，“卷积神经网络”，这些不仅仅是关键字，不仅仅是茶余饭后的谈资，它们会像“数据结构”，“排序”和“链表”一样，成为软件工程师必备技能。



AlphaZero是自学习的，这个自学习的程序是算法研究员和算法工程师写的。成为一个出色的算法工程师，走在技术和时代的前沿，是很多程序员的理想工作。



大家知道，Hulu是一家国际领先的视频平台，提供优质电影，电视剧点播节目，和ESPN, FOX, CNN, HBO 等各大电视网络的直播节目，在美国有数千万付费用户。Hulu 技术架构最为先进的一点就是人工智能和机器学习算法的广泛应用：在个性化内容推荐，搜索，视频内容理解，广告预测和定向，安全检测，决策支持，甚至视频编辑和客服系统。



在过去的几年中，Hulu面试了数百位人工智能和机器学习相关的研究员(Researcher)和算法工程师(Research Engineer)。除了定期的机器学习专题研讨，和我们以前给同学们做过的大数据及机器学习公开课，Hulu最近也在内部开设了深度学习课程。



在这些工作、面试、和开课过程中，我们收集到不少机器学习相关的有趣的题目和答案。我们希望通过我们的微信公众号把这些内容分享给有兴趣的读者，让更多的人练习和掌握机器学习相关的知识，共同建造一个开放的兴趣社区。



人工智能是个很宽泛的领域，而机器学习算法也可以用不同的维度去分类。因为机器学习算法往往需要比较深的背景知识，所以我们在发送问题和解答之前，也会预先发送对这个领域的简单介绍。



我们将要提供的内容包括很多方面，除了传统机器学习方法(SVM, LR, Clustering等)，还有数据处理 (Data Processing)，模型评估(Model Evaluation)，采样算法(Sampling)，优化算法(Optimization)，神经网络(Neural Network)，增强学习(Reinforcement Learning)等等，也会涉猎学术界讨论中的新领域和新算法。我们会尽量注解一个题目需要的背景知识，以帮助读者的容易理解。



为了比较有趣，我们会每一期预先告知下一期的内容，或者下一个题目。希望广大读者和我们互动，一起来解题，讨论，给我们鼓励也指出我们的错误和不足。欢迎大家给我们提供更多的素材，一起把这个系列做好。









**【模型评估】**



**引言**



“没有测量，就没有科学。”这是科学家门捷列夫的名言。在计算机科学中，特别是在机器学习的领域，对模型的测量和评估同样至关重要。只有选择与问题相匹配的评估方法，我们才能够快速的发现在模型选择和训练过程中可能出现的问题，迭代地对模型进行优化。

模型评估主要分为离线评估和在线评估两个阶段，针对分类、排序、回归、序列预测等不同类型的机器学习问题，评价指标的选择也有所不同，是否知道每种评估指标的精确定义，如何有针对性的选择合适的评估指标，是否能够根据评估指标的反馈进行模型调整，是机器学习模型评估阶段的关键问题，也是一名合格的算法工程师应具备的基本功。  

在所有的模型评估指标中，ROC曲线和AUC值无疑是最常用的指标之一，我们模型评估的第一道面试题也是基于ROC曲线提出的。





**问题**



**在一个二值分类问题中，**

**如何绘制模型的ROC曲线并计算相应的AUC值？**

**ROC曲线相比PR(Precision-Recall)曲线有什么优点？**





**【问题背景】**

二值分类器（binaryclassifier）是机器学习领域中最常见也是应用最为广泛的分类器。评价二值分类器的指标很多，比如precision，recall，F-score，PR曲线等，相比较而言，基于ROC曲线的AUC值有很多突出的优点（本题第二问会做相关解释），经常作为评估二值分类器最重要的指标之一。下面我们详细解答一下这道面试题。



**【解答】**

ROC曲线的全称是theReceiver Operating Characteristic曲线，中文名为“受试者工作特征曲线”。AUC全称是Area under the Curve，曲线下面积，这里的“曲线”一般就是指ROC曲线。所以这道面试题第一问的重点就绘制出ROC曲线，基于ROC曲线计算面积的方法是显然的。

ROC曲线的横坐标为falsepositive rate（FPR），中文一般称为假阳性率，纵坐标为truepositive rate（TPR），中文一般称为真阳性率。FPR和TPR的计算方法如下：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLbYdnAyYahRbNIbrgAuGEnPSgxSicKcRBKlDPNrn1l4w6dMoltLpCuzHIG3agOia9gg9uO2dABK5pQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上式中，P指的是所有样本中，真正的正样本的数量，N指的是真正的负样本的数量。TP指的是P个正样本中，分类器把其中TP个预测为正。FP指的是，N个负样本中，分类器把其中FP个样本误判成了正样本。

只看定义确实有点绕，为了更直观的说明这个问题，我们举一个医院诊断病人的例子。假设有10位疑似癌症患者，其中有3位很不幸确实患了癌症（P=3），另外7位不是癌症患者（N=7）。医院对这10位疑似患者做了诊断，也确实发现了3位患了癌症，但这3位患者中，只有2位是真正的患者（TP=2）。那么真阳性率TPR=TP/P=2/3。而对于7位非癌症患者来说，有一位很不幸被误诊为癌症患者（FP=1），那么假阳性率FPR=FP/N=1/7。对于“医院”这个分类器来说，这组分类结果就对应ROC曲线上的一个点（1/7,2/3）。那么ROC曲线上的其他点都是怎样生成的呢？

事实上，ROC曲线是通过不断移动分类器的“截断点”来生成所有关键点的，我们通过下面的例子来解释“截断点”的概念。

在一个二值分类问题中，我们构建的模型输出一般都是预测样本为正例的概率。假设我们的测试集中有20个样本，模型的输出如下图所示，图中第一列为样本序号，Class为样本的真实标签，Score为模型输出的样本为正的预测概率，样本按照预测概率从高到低排序。



![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLbYdnAyYahRbNIbrgAuGEnHgXVcl8dw5ibm8jjxUhH5OGkhjCwkE8dWxgoZWyV3DZY6QZLib6fNaEw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*图片来自Fawcett,Tom. "An introduction to ROC analysis." Pattern recognition letters27.8 (2006): 861-874.*



在预测样本为正负例的时候，我们需要指定一个阈值。大于等于这个阈值的样本都会预测为正样本。比如指定0.9为阈值，那么只有第一个样本会被预测为正例，其他全部都是负例。上面所说的“截断点”指的就是区分正负预测结果的阈值。我们动态调整这个截断点，从最高的得分开始（实际是从正无穷开始开始，对应着ROC曲线的零点），逐渐调整到最低得分，每一个截断点都会对应一个FPR和TPR，在ROC图上绘制出每个截断点对应的位置，再连接每个点即得到最终的ROC曲线。

就本例来说，当截断点选择为正无穷时，模型把全部样本预测为负例，那么FP和TP必然都为0，FPR和TPR也都为0，显然，曲线的第一个点就是(0,0)。当把截断点调整为0.9时，模型预测1号样本为正样本，并且该样本确实是正样本，因此，TP=1，20个样本中，所有正例数量为P=10，故TPR=TP/P=1/10；这里没有预测错的正样本，即FP=0，负样本总数N=10，故FPR=FP/N=0/10=0。对应着ROC图上的点(0,0.1)，依次调整截断点，直到画出全部的关键点，再连接关键点即得到了下图所示的ROC曲线。





![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLbYdnAyYahRbNIbrgAuGEnKIMbAEfROWJOiapOQuCenmhYytwkML4vrIncqhLiameR9ujSR96T64WA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*图片来自Fawcett, Tom. "An introduction to ROCanalysis." Pattern recognition letters 27.8 (2006): 861-874.*



其实，绘制ROC曲线有一个更直观的方法，我们首先根据样本标签统计出正负样本的数量，假设正样本数量为P，负样本数量为N。下一步就把横轴的刻度间隔设置为1/N，纵轴的刻度间隔设置为1/P。再根据模型预测结果将样本进行排序后，从高到低遍历样本，与此同时从零点开始绘制ROC曲线，每遇到一个正样本就沿纵轴正方向绘制一个单位长度的曲线，每遇到一个负样本就延横轴正方向绘制一个单位长度的曲线，直到遍历完所有样本，曲线也最终终结在(1,1)这个点。

有了ROC曲线，计算AUC只要沿着横轴做积分就可以了。由于ROC曲线一般都处于y=x这条直线的上方（如果不是的话，只要把模型预测的概率反转成1-p就可以得到一个更好的分类器），所以AUC的取值一般在0.5和1之间。显然，AUC越大，分类器越可能把真正的正样本排在前面，分类器性能越好。





**ROC曲线相比PR(Precision-Recall)曲线有什么优点？**



除了ROC曲线外，还有很多评估分类模型的指标，PR曲线也是经常使用的指标之一。与ROC曲线不同的是，PR曲线的横坐标是Recall，纵坐标是Precision。

ROC曲线相比PR曲线有一个非常好的特性：就是当正负样本分布发生变化的时候，ROC曲线的形状能够基本保持不变。而PR曲线的形状会发生较剧烈的变化。





![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLbYdnAyYahRbNIbrgAuGEniaqeOibr39Yiab04eHCvmbBNkbL1jBkoARfAmmf9U28lUJRJE5cR4uBvw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*图片来自Fawcett, Tom. "An introduction to ROCanalysis." Pattern recognition letters 27.8 (2006): 861-874.*



举例来说，上图是ROC曲线和PR曲线的一张对比图，左边两幅是ROC曲线，右边两幅是PR曲线。下面两幅图是将测试集的负样本数量增加10倍后的结果。可以明显看出，PR曲线发生了明显的变化，而ROC曲线形状基本不变。这就让ROC曲线能够尽量屏蔽测试集选择带来的干扰，更加客观的衡量模型本身的性能。

这样的优点有什么实际意义呢？因为在很多实际问题中，正负样本数量往往很不均衡，比如计算广告领域经常涉及的转化率模型，正样本的数量往往是负样本数量的1/1000甚至1/10000，这个时候选择不同的测试集，PR曲线的变化就会非常大，这显然不利于评估模型本身的性能。而ROC曲线则能够更加稳定的反应模型本身的好坏，适用的场景更多，广泛适用于排序、推荐、广告等领域。





**【SVM模型】**



**引言**

SVM（Support Vector Machine, 支持向量机）是众多监督式学习方法中十分出色的一种，几乎所有的讲述经典机器学习方法的教材都会介绍。以它为模型，可以考察机器学习各个方面的知识，也是面试题目中常见的模型，SVM的第一道面试题就是考察模型推导的基础知识。



**问题**



**在空间上线性可分的两类点，**

**分别向SVM分类的超平面上做投影，**

**这些点在超平面上的投影仍然是线性可分的吗？**

**能否证明你的观点？**



*背景知识：数学基础，机器学习基础，SVM理论推导*



**解答与分析**

首先分析题意，线性可分的两类点，即指通过一个超平面可以将两类点完全分开，如左图所示，假设蓝色的超平面（对于二维空间来说，一维的线即为超平面）为SVM算法计算得出的分类平面，那么红绿两类的点就被它完全分开。我们的问题是将红绿两色的点，向蓝色平面上做如右图所示投影，可得在超平面上红绿两色的点，问题即为投影后的点仍然是线性可分的吗？



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIpDn7ibyZpf9FazaOBGrtRjurV1SmcQIhzr2X688RlLok6SB9BzWHKr85GMVftQpfPhcmISr18UTQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这个问题初看起来第一感觉是并不是线性可分的，反例也很好构造，设想只有两个点每个点各属于一类的情况，那么SVM的分类超平面就在两点连线的中垂线上，那么两点关于超平面的投影落在了平面上的同一点自然无法线性可分。实际上对于任意线性可分的两组点，它们在SVM分类的超平面上的投影都是线性不可分的，那么这个结论怎么证明呢？



我们在下面的叙述中以二维情况进行讨论，对于高维空间的推广也是成立的。先考虑SVM分类中只有支持向量的情况，使用反证法，假设存在一个SVM分类结果的超平面，所有支持向量在这个超平面上的投影依然线性可分。那么这个超平面的分类结果如下图所示，使用初等几何知识不难发现图中A,B两点连线的中垂线所组成的超平面蓝色虚线是相较于蓝色实线超平面更优的解，且两组点在新的超平面下线性不可分。而我们之前假设蓝色实线超平面为最有的解，由此推出矛盾。



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIpDn7ibyZpf9FazaOBGrtRjia5LY45JzkL975GfxYt8U9icKWJN6iaeQMT8LGt1w7C9pwnwowKkN3mUg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)





但我们的证明目前还有不严谨之处，即我们假设了仅有支持向量的情况，会不会在超平面的变换过程支持向量发生了改变，原先的非支持向量和支持向量发生了转化呢？下面我们就来证明SVM的分类结果仅依赖于支持向量。考虑SVM推导中的KKT条件：



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIpDn7ibyZpf9FazaOBGrtRj5LoFgiaMt0nXoX4raCx5GuibDHz86KGMOamMxrr734rwv1huUWctnKhg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

结合3和4两个条件不难发现gi(*)<0时，必有i*=0，将这一结果与拉格朗日对偶优化问题的公式相比较：



![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIpDn7ibyZpf9FazaOBGrtRj7xmJN4InSLwn4Pw8DkATxGiaPrQvDwIyheUczLerIU4v2AkluEBjSQw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIpDn7ibyZpf9FazaOBGrtRjqjMR5pnW1Tr7bY1icehEuHEZWBt66fkUz8rUT0Wag18px2LYO68SDsg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到，除支持向量外，其他非支持向量的系数均为0，所以SVM的分类结果与仅使用支持向量的分类结果一致，这也是SVM有极高的运行效率的关键之一。将这一结论代回我们的假设中，可知去掉非支持向量并不影响SVM的分类结果，故此证明成立。



实际上，该问题也可以通过凸优化理论中的超平面分离定理（Separating Hyperplane Theorem）更加轻巧地解决。该定理是在凸优化理论中极为重要，定理的定义是：对于不相交的两个凸集，存在一个超平面，将两个凸集分离，并且该超平面为两个凸集上最短距离两点连线的中垂线。



我们可以考虑线性可分的这两类点的凸包，不难发现，SVM所求得的超平面为两凸包上最短距离两点连线的中垂线，由超平面分离定理可得，其为定理中两类点的凸包的超平面。而两个凸包中距离最短的两点只有两种可能，为样本点或在两个样本点的连线上。分情况两边均为样本点，两边均在样本点的连线上，一边为样本点一边在样本点的连线上三种情况简单讨论即可发现，无论哪种情况两类点的投影均是线性不可分的。



对于面试者来说，能通过对SVM的推导给出前一种结论即可，如果熟悉凸优化理论，也可以根据后一种思路来作答。



**拓展阅读**

SVM的公式推导过程: 

http://cs229.stanford.edu/notes/cs229-notes3.pdf

对偶问题与KKT条件: 

http://stanford.edu/class/ee364a/lectures/duality.pdf

超平面分离定理: 

http://www.princeton.edu/~amirali/Public/Teaching/ORF523/S16/ORF523_S16_Lec5_gh.pdf





**【优化简介】**



**引言**

优化是应用数学的一个分支，也是机器学习的核心组成部分。有文章[1] 认为，

机器学习算法 =模型表征+ 模型评估+ 优化算法

其中优化算法所做的事情就是在模型假设空间中找到模型评估指标最好的模型。不同的模型假设、不同的评估指标，对应的优化算法都不尽相同，例如经典的SVM（线性分类模型+ 最大间隔）、逻辑回归（线性分类模型+ 交叉熵）、CART（决策树模型+ 基尼纯度）等。

随着大数据和深度学习的迅猛发展，在实际应用中我们面临的大多是大规模、高度非凸的优化问题，这给传统的基于全量数据、凸优化的优化理论带来巨大的挑战。如何设计适用于新场景的高效、准确的优化算法成为近年来的研究热点。虽然优化是一门能够追溯到拉格朗日、欧拉的“古老”学科，但是大部分能够用于训练深度神经网络的优化算法直到最近几年才被提出，例如Adam算法[2] 等。

目前大部分机器学习工具已经内置了常用的优化算法，所以我们实际只需要一行代码即可完成调用，省去了实现的烦恼。但鉴于优化算法在机器学习中的重要作用，我们认为了解优化算法的原理还是很有必要的，也许在解决实际问题的过程中你也能提出更好的求解算法呢。

我们从一道简单的题开始这段激动人心的优化之旅吧。



**“梯度验证”**



**场景描述**

为了求解一个优化问题，最重要的操作是计算目标函数的梯度。在一些机器学习的应用中，例如深度神经网络，目标函数的梯度公式非常复杂，需要验证自己写出的实现代码是否正确。





**问题描述**

假设你需要求解优化问题



![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIpDn7ibyZpf9FazaOBGrtRjeEJIiaqA8FNBvO2lDCZtTU7erClQFxfBJS7iaSyR4ACpMYsaua8aW9SA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

并且用代码实现了求目标函数值和求目标函数梯度的功能。问如何利用求目标函数值的功能来验证求目标函数梯度的功能是否正确？



**先验知识**

线性代数，微积分



**解答和分析**



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKvZ4n6IlIzicoShLrt2VbQa7LsBLibFKTC0qbvU4ZBOw7mMQGJklejEsOOa44nQkr9rf04NdT7Y5NQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKvZ4n6IlIzicoShLrt2VbQa67wEiabOsA3MEqjIAicGCsGSSTGU2n4aMiaD2KYw32F9VASSAsTyHqShQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKvZ4n6IlIzicoShLrt2VbQabSkzrrH02mhVFh85ChBfhKVxWVdhv5DU91VbWCPzXfpgr668g5vFQA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**参考文献**

[1] Domingos, Pedro. "A few useful things to know aboutmachine learning." Communications of the ACM 55.10 (2012): 78-87.

[2] Kingma, Diederik, and Jimmy Ba. "Adam:A method for stochastic optimization." arXiv preprint arXiv:1412.6980(2014).







**【采样】**



**引言**

古人有云：“知秋一叶，尝鼎一脔”，其中蕴含的就是采样思想。采样，就是根据特定的概率分布产生对应的样本点。对于一些简单的分布（如均匀分布、高斯分布），­很多编程语言里面都有直接的采样函数。然而，即使是这种简单的分布，其采样过程也并不是显而易见，需要精心设计；对于比较复杂的分布，往往并没有直接的采样函数可供调用，这时候就需要其它更加复杂的采样方法。因此，对采样方法的深入理解是很有必要的。

采样在机器学习中有着重要的应用，它可以将复杂的分布简化为离散的样本点，可以对样本集进行调整（重采样）以更好地适应后续的模型学习，或者用于随机模拟以进行复杂的模型求解。

在“采样”这个系列中，我们会通过不同的问题与解答，来展现机器学习中的采样知识，包括采样方法（直接采样、接受/拒绝采样、重要性采样、MonteCarlo采样等）、采样的应用，以及采样在一些算法中的具体实现。今天，我们以采样在训练数据不均衡时的应用来开始我们的“采样”系列。



**“不均衡样本集的处理”**



**场景描述**

在训练二分类模型时，经常会遇到正负样本不均衡的问题，例如医疗诊断、网络入侵检测、信用卡反诈骗等。对于很多分类算法，如果直接采用不均衡的样本集来进行训练学习，会存在一些问题。例如，如果正负样本比例达到1:99，则分类器简单地将所有样本都判为负样本就能达到99%的正确率，显然这并不是我们想要的，我们想让分类器在正样本和负样本上都有足够的准确率和召回率。

 

**问题**

对于二分类问题，当训练集中正负样本非常不均衡时，如何处理数据以更好地训练分类模型？ 



背景知识：机器学习，概率统计



**解答与分析**

为什么很多分类模型在训练数据不均衡时会出现问题？本质原因是模型在训练时优化的目标函数和人们在测试时使用的评价标准不一致。这种“不一致”可能是由于训练数据的样本分布与测试时期望的样本分布不一致，例如，在训练时优化的是在整个训练集（正负样本比例可能是1：99）上的正确率，而测试时可能想要模型在正样本和负样本上的平均正确率尽可能大（实际上是期望正负样本比例为1:1）；也可能是训练阶段不同类别的权重（重要性）与测试阶段不一致，例如训练时认为所有样本的贡献是相等的，而测试时 False Positive和False Negative有着不同的代价。



根据上述分析，一般可以从两个角度来处理样本不均衡问题：



**1）基于数据的方法：**主要是对数据进行重采样，使原本不均衡的样本变得均衡。



![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJJfaXPfJp05pkctibq7nziam0DQzibvHBBzDMn2q882wE7aZWYNCPuZkCY47RgXPjeSAk389qZ4mTyw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



直接的随机采样虽然可以使样本集变得均衡，但会带来一些问题：过采样对少数类样本进行了多次复制，扩大了数据规模，增加了模型训练的复杂度，同时也容易造成过拟合；欠采样会丢弃一些样本，可能会损失部分有用信息，造成模型只学到了整体模式的一部分。





![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJJfaXPfJp05pkctibq7nziamEj7PhRfXZdRyjss2cHGR8SXChicCpceA5ibXQ6oCEJE1uIXM2CWYic9zQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



SMOTE算法为每个少数类样本合成相同数量的新样本，这可能会增大类间重叠度，并且会生成一些不能提供有益信息的样本。为此出现Borderline-SMOTE、ADASYN等改进算法：Borderline-SMOTE只给那些处在分类边界上的少数类样本合成新样本，而ADASYN则给不同的少数类样本合成不同个数的新样本。此外，还可以采用一些数据清理方法（如基于TomekLinks）来进一步降低合成样本带来的类间重叠，以得到更加well-defined的类簇，从而更好地训练分类器。



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJJfaXPfJp05pkctibq7nziam5UwlB4IFfgsTvzU0KkTiaoFBETHjUfRAhz3tH6rT4ySQonpJvvXeZPw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

SMOTE算法



![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJJfaXPfJp05pkctibq7nziamZicswBCKIgolavCuoYq66q5eHOfDNEhpo6ibwBW8c5abH1OMdOIJZdsQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



在实际应用中，具体的采样操作可能并不总是如上述几个算法一样，但基本思路很多时候还是一致的。例如，基于聚类的采样方法，利用数据的类簇信息来指导过采样/欠采样操作；经常用到的数据扩充 (data augmentation) 方法也是一种过采样，对少数类样本进行一些噪音扰动或变换（如图像数据集中对图片进行裁剪、翻转、旋转、加光照等）以构造出新的样本；而Hard Negative Mining则是一种欠采样，把比较难的样本抽出来用于迭代分类器。



**2）基于算法的方法：**在样本不均衡时，也可以通过改变模型训练时的目标函数（如代价敏感学习中不同类别有不同的权重）来矫正这种不平衡性；当样本数目极其不均衡时，也可以将问题转化为one-class learning / anomalydetection。本节主要关注采样，不再细述这些方法（我们会在其它章节的陆续推送相关知识点）。



**扩展与总结**

在实际面试时，这道题还有很多可扩展的知识点。例如，模型在不均衡样本集上的评价标准（可以参考“Hulu机器学习问题与解答系列”的第一弹：模型评估）；不同样本量（绝对数值）下如何选择合适的处理方法（考虑正负样本比例为1:100和1000：100000的区别）；代价敏感学习和采样方法的区别、联系以及效果对比等。



参考文献

[1] H. He and E. A. Garcia, "Learning from Imbalanced Data," in IEEE Transactions on Knowledge and Data Engineering, vol. 21, no. 9, pp. 1263-1284, Sept. 2009.



**【余弦距离】**



**场景描述**

在机器学习中，我们常将特征表示为向量的形式。在近年来，将高维稀疏的研究对象表示为低维分布的向量形式，极大地提高了各类模型的效果。在分析两个向量之间的相似性时，常使用余弦相似度来表示。余弦相似度的取值范围是[-1,1]，相同的两个向量之间的相似度为1。如果希望得到类似于距离的表示，将1减余弦相似度即为余弦距离。余弦距离的取值范围为[0,2]，相同的两个向量余弦距离为0。



**问题**

\1. 结合你的学习和研究经历，探讨为什么要在一些场景使用余弦相似度而不是欧式距离？

\2. 余弦距离是否是一个严格定义的距离？



先验知识：基本数学



**解答与分析**

**问题1**: 该题考查对余弦相似度和欧氏距离的理解和区分，以及对它们的使用经验。

余弦相似度关注的是两个向量之间的角度关系，而并不关心它们的绝对大小，其取值范围是[-1,1]。当我们比较两个长度差距很大、但内容相近的文本的相似度时，如果使用词频或词向量作为特征，那么欧式距离会很大；而如果使用余弦相似度，其夹角很小，因而相似度很高。

此外，在文本，图像，视频等领域，研究的对象特征维度往往很高，余弦相似度在高维的情况下，依然保持相同时为1，正交时为0，相反时为-1的性质；而欧式距离的数值受到维度的影响，范围不固定，并且其含义会比较模糊。

在一些场景例如word2vec中，其向量的模长是固定的，此时欧氏距离与余弦相似度有着单调的关系，即欧氏距离:



![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

在此场景下，如果选择距离最小/相似度最大的近邻，那么使用余弦相似度和欧氏距离，其结果是相同的。

这道题同时要考察被试者的研究和学习经历，初学者可以从两者的定义开始拓展讨论，余弦相似度更适用于向量模长不相等的场景中；研究者如果是NLP背景的，可以结合词、文本的各种表示，以及如何求其相似度等问题进行探讨；如果是CV背景的，可以结合HOG特征的使用、人脸识别等场景进行探讨。总而言之，特定的度量方法适用于什么样的问题，需要在学习和研究中多总结和思考，这样不仅仅对面试有帮助，在遇到新的问题时，也可以活学活用。



**问题2**: 该题考察对距离的定义的理解，以及简单的反证和推导。

余弦距离不是严格定义的距离。

在一个集合中，如果每一对元素均可唯一确定一个实数，使得三条距离公理（正定性，对称性，三角不等式）成立，则该实数就可称为这对元素之间的距离。

余弦距离满足正定性和对称性，但是不满足三角不等式。对于向量A，B：

1: 正定性



![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIwDiaK4EHyLcaCnmIBA8qOicCC2ep5LTviaNmGGesyibpeqiavHibJv41R9lic8S46BGiahiaVeXv4zStvGmg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



2: 对称性



![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIwDiaK4EHyLcaCnmIBA8qOicuE5luU9iasdXIJ8pq9wBPFoCriaAUktOEQwVvoYX0gQ5Qoia1wic88zzZg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



3: 三角不等式

下面给出一个反例：

给定A=(1,0), B=(1,1), C=(0,1)  那么:



![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIwDiaK4EHyLcaCnmIBA8qOicVo7U0nwkppgWRkQW9ExRoq0ngMw8I5NexgFGY6Pat2D7wZciaqj2cdA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



假如面试的时候紧张，一时想不到反例，该怎么办呢？

此时可以思考余弦距离和欧式距离的关系，从题1中，我们知道单位圆上欧式距离和余弦距离满足：



![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIwDiaK4EHyLcaCnmIBA8qOicqX0KPJz4B3xghXFKshmjgjYGXicDGWBv6rlyleUWmxLpsGeNtXfHdBg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

显然在单位圆上，余弦距离和欧氏距离的范围都是[0,2]。我们已知欧式距离是一个合法的距离，而余弦距离与欧式距离有二次关系，那么自然不满足三角不等式。可以假设A与B、B与C非常近，其欧式距离为极小量u；此时A、B、C虽然在圆弧上，但近似在一条直线上，所以A与C的欧式距离接近于2u。因此，A与B、 B与C的余弦距离为1/2u2；A与C的余弦距离接近于2u2，大于A与B、B与C的余弦距离之和。



假如遇到这种类型的题该怎么办呢？首先，大多数应试者未必能熟记距离三公理，这没有关系，应该主动和面试官沟通合法距离的定义。然后，正定性和对称性的证明，只是给出含糊的表述诸如‘显然满足’是不好的，应该给出一些推导。最后，三角不等式的证明/证伪中，不应表述为‘我觉得满足/不满足’，而是应该积极分析给定三个点的情况，或者推导其和欧式距离的关系，这样哪怕一时找不到反例而误认为其是合法距离，也比‘觉得不满足’这样蒙对正确的答案要好。

笔者首次注意到余弦距离不符合三角不等式，是在研究电视剧的标签时，发现在通过影视语料库训练出的词向量中，comedy（喜剧）和funny，funny和happy的余弦距离都很近，小于0.3，然而comedy和happy的余弦距离却高达0.7。这一现象明显不符合距离的定义，引起了我们的注意和讨论，经过思考和推导，得出了上述结论。



在机器学习领域，被俗称为距离，却不满足三条距离公理的不仅仅有余弦距离。KL距离（又被称为KL散度、相对熵）也是一例，它常被用于计算两个分布之间的差异，不满足对称性和三角不等式。









**【降维】**



**引言**

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKL5icMkvfU4A5xMxdxwvibpEIfCOkN1CO0iapibVAQIVpaDKTic7R26sAsdpYgynrFibdYFppoGTkOQib3w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

宇宙，是时间和空间的总和。时间是一维的，而空间的维度，众说纷纭，至今没有定论。弦理论说是9维，霍金所认同M理论则认为是10维。它们解释说人类所能感知的三维以外的维度都被卷曲在了很小的空间尺度内。当然，谈及这些并不是为了推销《三体》系列读物，更不是引导读者探索宇宙真谛，甚至怀疑人生本质，而是为了引出今天机器学习课堂主题——降维。

机器学习中的数据维数与现实世界的空间维度本同末离。在机器学习中，数据通常需要被表示成向量形式以输入模型进行训练。但众所周知，对高维向量进行处理和分析时，会极大消耗系统资源，甚至产生维度灾难。例如在CV（计算机视觉）领域中将一幅100x100的RGB图像提取像素特征，维度将达到30000；在NLP（自然语言处理）领域中建立<文档-词>特征矩阵，也动辄产生几万维的特征向量。因此，进行降维，即用一个低维度的向量表示原始高维度的特征就显得尤为重要。试想，如果宇宙真如M理论所说，每个天体的位置都由一个十维坐标来描述，应该没有一个正常人能想象出其中的空间构造。但当我们把这些星球投影到一个二维平面，整个宇宙便会像上面的银河系一样直观起来。

常见的降维方法主要有主成分分析（PCA）、线性判别分析（LDA）、等距映射（Isomap）、局部线性嵌入（LLE）、拉普拉斯特征映射（LE）、局部保留投影（LPP）等。这些方法又可以按照线性/非线性，监督/非监督，全局/局部，进行不同划分。其中 PCA作为最经典的方法，至今已有100多年的历史，它属于一种线性、非监督、全局的降维算法。我们今天就来回顾一下这经久不衰的百年经典。



**“PCA”**



**场景描述**

在机器学习领域中，我们对原始数据进行特征提取，有时会得到比较高维的特征向量。在这些向量所处的高维空间中，包含很多的冗余和噪声。我们希望通过降维的方式来寻找数据内部的特性，从而提升特征表达能力，降低训练复杂度。PCA（主成分分析）作为降维中最经典的方法，是面试中经常被问到的问题。



**问题**

PCA的原理及目标函数

PCA的求解方法



背景知识：线性代数



**解答与分析**

PCA(principal components analysis)， 即主成分分析，旨在找到数据中的主成分，并利用这些主成分表征原始数据，从而达到降维的目的。举一个简单的例子，在三维空间中有一系列数据点，这些点分布在一个过原点的平面上。如果我们用自然坐标系x, y, z这三个轴来表示数据，需要使用三个维度，而实际上这些点只出现在一个二维平面上，如果我们通过坐标系旋转使得数据所在平面与x, y平面重合，那么我们就可以通过x’, y’两个维度表达原始数据，并且没有任何损失，这样就完成了数据的降维，而x’, y’两个轴所包含的信息就是我们要找到的主成分。

但在高维空间中，我们往往不能像刚才这样直观地想象出数据的分布形式，也就更难精确地找到主成分对应的轴是哪些。不妨，我们先从最简单的二维数据来看看PCA究竟是如何工作的。



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKL5icMkvfU4A5xMxdxwvibpEp0d95ibVx58afEDDT9GskMUQjH2prVYvOibdBXCBvhpia9NTVK0yP5iaOQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKL5icMkvfU4A5xMxdxwvibpEucibJ0ZwZKE4tuZBju1SdqEWedeqpPNXwP8aaBS6GsndiaAa3lM0Mvwg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



上图（左）是二维空间中经过中心化的一组数据，我们很容易看出主成分所在的轴（以下称为主轴）的大致方向，即右图中绿线所处的轴。因为在绿线所处的轴上，数据分布的更为分散，这也意味着数据在这个方向上方差更大。在信号处理领域中我们认为信号具有较大方差，噪声具有较小方差，信号与噪声之比称为信噪比，信噪比越大意味着数据的质量越好。由此我们不难引出PCA的目标，即最大化投影方差，也就是让数据在主轴上投影的方差最大。

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKL5icMkvfU4A5xMxdxwvibpEgMF5iaDFYk2PUMbAItGH1TZRrKibDy25n3W2V5mVJrhibBib576hBog7AQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKL5icMkvfU4A5xMxdxwvibpEmTrJrCybMt01ibG1QsM57aD1SZTYEGxeQoMZbzYtxNvP523hKbQMticA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKL5icMkvfU4A5xMxdxwvibpECuic5lPGXC2RO1kRv4RAZIz7XqWzicNCN3aISbIfXT3Xhm7xQxhNuP4A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



熟悉线性代数的读者马上就会发现，原来，**x**投影后的方差就是协方差矩阵的特征值。我们要找到最大的方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应特征向量。次佳投影方向位于最佳投影方向的正交空间中，是第二大特征值对应的特征向量，以此类推。至此，我们得到了PCA的求解方法：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKL5icMkvfU4A5xMxdxwvibpEvo3pgrGic3SIIxK6Oo3ic7bK6HlicswII8icW8zL8t69XqB0XSBYh9df3A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKL5icMkvfU4A5xMxdxwvibpEVMwnYzc9ohymIYlkcMtZDrTEOfMFcDiaUiaCaZV22vTbUaiaf8ZXgibwZQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**总结与扩展**

至此，我们从最大化投影方差的角度解释了PCA的原理、目标函数和求解方法，其实PCA还可以用其他思路（比如最小回归误差的角度）进行分析，得到新的目标函数，但最终会发现其对应的原理和求解方法与本文中的是等价的。另外，由于PCA是一种线性降维方法，虽然经典，但其具有一定局限性。我们可以通过核映射对PCA进行扩展得到KPCA方法，也可以通过流形映射的降维方法（如Isomap、LLE、LE等）对一些PCA效果不好的复杂数据集进行非线性降维操作。这些方法都会在之后的推送中有所涉及，敬请期待。





**【非监督学习算法与评估】**



**场景描述**

人具有很强的归纳思考能力，善于从一大堆碎片似的事实或者数据中寻找普遍规律，并得到具有逻辑性的结论。以用户观看视频的行为为例，可以存在多种直观的归纳方式，比如从观看内容的角度看，有喜欢看动画片的，有喜欢看偶像剧的，有喜欢看科幻片的等等；从使用设备的角度看，有喜欢在台式电脑上观看的，有喜欢手机或者平板便携式设备上观看的，还有喜欢在电视等大屏幕上观看的；从使用习惯上看，有喜欢傍晚观看的，有喜欢中午时段观看的，有每天都观看的用户，也有只在周末观看的用户，等等。对所有用户进行有效的分组对于理解用户并推荐给用户合适的内容是重要的。通常这类问题没有观测数据的标签或者分组信息，需要通过算法模型来寻求数据内在的结构和模式 。



**问题**

以聚类算法为例，假设没有外部标签数据，如何区分两个无监督学习（聚类）算法性的优劣呢？



背景知识：非监督学习，常见聚类算法



**解答与分析**

场景描述中的例子就是一个典型的聚类问题，从中可以看出，数据的聚类依赖于需求的定义，同时也依赖于分类数据的特征度量以及数据相似性的方法。相比于监督式学习，非监督学习通常没有正确答案，算法模型的设计直接影响最终的输出和性能，需要通过多次迭代的方法寻找模型的最优的参数。因此了解常见数据簇的特点和常见聚类算法的特点，对寻求评估不同聚类算法性能的方法有很大的帮助。



常见数据簇的特点：

- 以中心定义的数据簇：这类数据集合倾向于球形分布，通常中心被定义为质心，即此数据簇中所有点的平均值。集合中的数据到中心的距离相比到其它簇中心的距离更近；
- 以密度定义的数据簇：这类数据集合呈现和周围数据簇明显不同的密度，或稠密或稀疏。当数据簇不规则或互相盘绕，并且有噪声和离群点时，常常使用基于密度的簇定义；
- 以连通定义的数据簇：这类数据集合中的数据点和数据点之间有连接关系，整个数据簇表现为图结构，该定义对不规则形状或者缠绕的的数据簇有效；
- 以概念定义的数据簇：这类数据集合中的所有数据点具有某种共同性质。



常见的聚类算法的特点：

- 划分聚类：将数据对象划分成互不重叠的数据簇，其中每个数据点恰在一个数据簇中；
- 层次聚类：数据簇可以具有子簇，具有多个（嵌套）子簇的数据簇可以表示为树状结构；
- 模糊聚类：每个数据点均以0~1的隶属权值属于某个数据簇；
- 完全/不完全聚类：是否对所有数据点都指派一个数据簇。



由于数据以及需求的多样性，没有一种算法能够适应所有的数据类型、簇和应用，似乎每种情况都可能需要一种不同的评估度量。例如，K均值聚类通常需要用SSE (Sum of  Square Error) 来评估，但是基于密度的数据簇可以不必是球形，SSE则完全失效。在许多情况下，判断聚类算法结果的好坏最终强烈依赖主观解释。尽管如此，聚类算法的评估还是必须的，它是聚类分析中重要部分之一 。



对聚类算法优劣的评估通常可以总结为对以下五个方面的分析：

1. 辨识数据中是否存在非随机簇结构的能力；
2. 辨识数据中正确数据簇的能力；
3. 评估数据被正确聚类的能力；
4. 辨识两个数据簇之间优劣的能力；
5. 评估与客观数据集之间的差异；



假设存在外部标注数据的支持，那么第5点将转化为监督学习的问题，直接度量聚类算法发现的聚类结构与标注数据的结构匹配程度即可。假设不存在外部标注数据，基于以上所列1~4点，可以如下图1~5所示，测试聚类算法对不同类型数据簇的聚类能力：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCg2fPfLa8Xg9ELLpburRM2Hxw0Jfdcf2GOQrsdiaj8P1zaMAK2ibA7e72g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图1. 观察误差是否随聚类类别数量的增加而单调变化



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCgg2pzjrJZ5IiakTTCIRu3OqZ7tVEWIeNv7Jfable5ONptvSQJE1hPqiaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图2. 观察误差对聚类结果的影响



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCgT9oyEKTuQLl0CIFom79MHYhs857rD8eD73VDuIbA6URCIybjmfzibbg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图3. 观察近邻数据簇的聚类准确性 



![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

图4. 观察聚类算法在处理较大的数据密度差异时的性能



![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCgfNbDhvMibG9bwX5hkf8St1FAsr1KlRpj66LI78gcibBibLwicYS7fQpMIA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图5. 观察处理不同大小数据种类时的聚类准确度



**扩展问题**

在以上的回答中介绍了五种评估两个聚类算法性能优劣的，那么具体而言有哪些常见的指标可以用来计算和辨识聚类算法优劣呢？给出几种可能的数据簇形态，定义评估指标可以展现面试者实际解决和分析问题的能力。事实上测量指标可以有很多种，以下列出了三个在数据紧凑性或数据簇可分离程度上的度量，更多指标则请参考文献[1]，具体描述如下：



- 均方根标准偏差 (RMSSTD)，衡量聚类同质性：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCgPAoaFfvkRgKiariaWQmuLyroicoUjx0jV37micAc5W2J7GMaGPQCkNjXbQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



- R方 (R-Square)，衡量聚类差异度：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCgpCLsrJh7WNl7ue1WGVBicpgD3SGibUCdzQIRU6U7df7Gcxwibwxbcjicicw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



- 改进的Hubert Γ统计，通过数据对的不一致性来评估聚类的差异：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCgCv9cDNiaTh0NHsObzpaXySm2ibsdCKLVRcXUe0YWicyibyI6DRzA5Hn8gQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



这其中：

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCgIic2REDV7eauhT3ZNwhM7x2J9tWvKqSFmcz7mz9Nv08COtL1LFKXYFA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJHUmy2lcNC8xxJgKiaHXiaCgS9pMbE2Rw2d3MFbpSoorTkRXcTriculIRlTyN9ZKNsU4NZiajHASZskw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)





------



**下一题预告**

**【强化学习】**



**场景描述**

游戏是强化学习最有代表性也是最合适的应用领域之一，其几乎涵盖了强化学习所有的要素，例如环境：游戏本身的状态，动作：用户操作，机器人：程序，回馈：得分、输赢等。通过输入原始像素来玩视频游戏，是人工智能成熟的标志之一。雅达利（Atari）是20世纪七八十年代红极一时的电脑游戏，类似于国内的红白机游戏，但是画面元素要更简单一些。它的模拟器相对成熟简单，使用雅达利游戏来测试强化学习，是非常合适的。应用场景可以描述为：在离散的时间轴上，每个时刻你可以得到当前的游戏画面，选择向游戏机发出一个指令（上下左右，开火等），然后得到一个回馈（reward）。由于基于原始像素的强化学习对应的状态空间巨大，没有办法直接使用传统的方法。于是，2013年DeepMind提出了深度强化学习模型，开始了深度学习和强化学习的结合[2]。

传统的强化学习主要使用Q-learning，而深度强化学习也使用Q-learning为基本框架，把Q-learning的对应步骤改为深度形式，并引入了一些技巧，例如经验重放（experience replay）来加快收敛以及提高泛化能力。



**问题描述**

什么是深度强化学习，它和传统的强化学习有什么不同，如何用它来玩视频游戏？





参考文献：

[1] Liu Y C, Li Z M, Xiong H, et al. Understanding of internal clustering validation measures. In: Proceedings of IEEE ICDM'10, Sydney, 2010. 911-916.

[2] Antonoglou, I., Graves, A., Kavukcuoglu, K., Mnih, V., Riedmiller, M.A., Silver, D., & Wierstra, D. (2013). Playing Atari with Deep Reinforcement Learning. CoRR, abs/1312.5602.







**【强化学习】**



*为帮助大家更好地理解本期课题，作者将首先介绍强化学习的基本概念。**解答与分析请关注明日推送。*



**“强化学习的基本概念”**

强化学习近年来在机器学习领域越来越火，也受到了越来越多人的关注。强化学习是一个20世纪80年代兴起的，受行为心理学启发而来的一个机器学习领域，它关注身处某个环境中的决策器通过采取行动获得最大化的累积收益。和传统的监督学习不同，在强化学习中，并不直接给决策器的输出打分。相反，决策器只能得到一个间接的反馈，而无法获得一个正确的输入/输出对，它需要在不断的尝试中优化自己的策略以获得更高的收益。从广义上说，大部分涉及动态系统的决策学习过程都可以看成是一种强化学习。它的应用非常广泛，包括博弈论、控制论、优化等多个不同领域。这两年，AlphaGo及其升级版横空出世，彻底改变了围棋这一古老的竞技领域，在业界引起很大震惊，其核心就是强化学习。与未来科技发展密切相关的机器人领域，也是强化学习的用武之地，从机器人行走，到自动驾驶，处处都有强化学习的身影。



强化学习的基本场景可以用下图来描述（图来自于wiki），有环境（Environment）、机器人（Agent），状态（State），动作（Action），奖励（Reward）等几个基本概念。一个机器人在环境中会发出各种动作，环境会接收到动作，引起自身状态的变迁，同时给机器人以奖励。机器人的目标就是使用一些策略，发出合适的动作，最大化自身的收益。





![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLDUWNjKsNVmnAb6sjcQibB9VpDIlCvDXJziamljyXd1ocyPiaLPJhdCR3SnPu7Cw1rOjMuaJUc7HtPw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



整个场景一般可以描述为一个马尔科夫决策过程（Markov decision process, MDP），这个过程的几个要素如下（这里我们以离散时间的马尔科夫决策过程为例，整个过程也是离散时间随机控制过程）：



动作（action），所有可能发出动作的集合记作A（可能是无限的）；

状态（state），所有状态的集合记作S；

奖励（reward），机器人可能收到的奖励，一般是一个实数；

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLDUWNjKsNVmnAb6sjcQibB9xF1I1rSzv4ayGl6CRQpbfrkFAmIhqWdiaAv2t60u4WC7mQFFuM7xkpA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLDUWNjKsNVmnAb6sjcQibB9sx05N0tHtCqf3fTGveFfoCSqr8KicicdiaID6Nxfck6MylkKjfiaW3MCfQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



强化学习的核心任务是，学习一个从状态空间S到动作空间A的映射，最大化累积受益。常用的强化学习算法有Q学习（Q-Learning），策略梯度（Policy gradient），以及演员评判家算法（Actor-critic）等。







**【循环神经网络】**



**场景描述**

循环神经网络（Recurrent Neural Network）是一种主流的深度学习模型，最早在20世纪80年代被提出 ，目的是建模序列化的数据。我们知道，传统的前馈神经网络一般的输入都是一个定长的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。而RNN通过将神经元串行起来处理序列化的数据，比如文本的词序列、音频流和视频流序列等。由于每个神经元能用它的内部变量保存之前输入的序列信息，使得整个序列可以被浓缩成抽象的表示，并可以据此进行分类或生成新的序列。近年来，得益于计算能力的大幅提升和网络设计的改进（LSTM、GRU、Attention机制等），RNN在很多领域取得了突破性的进展。比如机器翻译、序列标注、图像描述、视频推荐、智能聊天机器人、自动作词作曲等，给我们的日常生活带来了不少便利和乐趣。



**问题描述**

1. 什么是循环神经网络？如何用它产生文本表示？
2. RNN为什么会出现梯度的消失或爆炸？有什么样的改进方案？



**解答与分析**

**1. 什么是循环神经网络？如何用它产生文本表示？**

传统的前馈神经网络，包括卷积神经网络（Convolutional Neural Network, CNN）在内，一般都是接受一个定长的向量作为输入。比如在做文本分类时，我们可以将一篇文章所对应的TF-IDF（Term Frequency-Inverse Document Frequency）向量作为前馈神经网络的输入，其中TF-IDF向量的维度是词汇表的大小。显而易见，这样的表示实际上丢失了输入的文本序列中每个单词的顺序。在用卷积神经网络对文本进行建模时，我们可以输入变长的字符串或者单词串，然后通过滑动窗口+Pooling的方式将原先的输入转换成一个固定长度的向量表示；这样做可以捕捉到原文本中的一些局部特征，但是两个单词之间的长距离依赖关系还是很难被学习到。



RNN（Recurrent Neural Network，循环神经网络）的目的便是处理像文本这样变长并且有序的输入序列。它模拟了人阅读一篇文章的顺序，从前到后阅读文章中的每一个单词，并且将前面阅读到的有用信息编码到状态变量中去，从而拥有了一定的记忆能力，可以更好地理解之后的文本。下图展示了一个典型RNN的网络结构[1]：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKXpTXbibDdLMHtvfUlJUUJAmlduOHiblbd9x5jiammhaeFhDdNPU8Zicib4ic7QZN8r1ZNuxXG7M27fJEg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKXpTXbibDdLMHtvfUlJUUJAGavblXnLPYZrxo5tBl3JdnMJkCn9BbNExQzG57Fn09L6zd8oJIdZYw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中f和g为激活函数，U为输入层到隐含层的权重矩阵，W为隐含层从上一时刻到下一时刻状态转移的权重矩阵，在文本分类任务中（如上图），f可以选取Tanh或者ReLU函数，g可以采用Softmax函数，更多关于激活函数的细节可以参见[2]。



**2. RNN为什么会出现梯度的消失或爆炸？有什么样的改进方案？**

RNN模型的求解可以采用BPTT（Back Propagation Through Time）算法实现 ，实际上是反向传播（Back Propagation）算法的简单变种；如果将RNN按时间展开成T层的前馈神经网络来理解，就和普通的反向传播算法没有什么区别了。RNN的设计初衷之一就是能够捕获长距离输入之间的依赖。从结构上来看，RNN也绝对能够做到这一点。然而实践发现，使用BPTT算法学习的RNN并不能成功捕捉到长距离的依赖关系，这一现象可以用梯度消失来解释。传统的RNN梯度可以表示成连乘的形式：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKXpTXbibDdLMHtvfUlJUUJAJUa8XX8jFuJBzqIFynZTrFsGZIwdtCIgrzVVYjTtSYWfqibzibxhV4Rg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKXpTXbibDdLMHtvfUlJUUJAbDuJVOfNArTE4HcjcvyL0dNHQjNM4KAuzR5zn2sOrPIUibLOMU52iaOg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKXpTXbibDdLMHtvfUlJUUJA91psyxobmWKhojocKZ16MIkNvNrqkUzicbia1Ie05n30HV4W2hnvwjhA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

由于预测的误差是沿着神经网络的每一层反向传播的，因此当Jacobian矩阵的最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸（gradient explosion）；反之，若Jacobian矩阵的最大特征值小于1，梯度的大小会呈指数缩小，即产生了梯度消失（gradient vanishing）。对于普通的前馈网络来说，梯度消失导致无法通过加深网络层次来改善神经网络的预测效果，因为无论如何加深网络，只有靠近输出的若干层才真正起到的学习的作用。对于RNN来说，这导致模型很难学习到输入序列中的长距离依赖关系。



梯度爆炸的问题可以通过梯度裁剪(Gradient Clipping)来缓解，也就是当梯度的范式大于某个给定值时，对梯度进行等比收缩；而梯度消失问题相对比较棘手，需要对模型本身进行改进。 ResNet[3]是对前馈神经网络的改进，通过残差学习的方式缓解了梯度消失的现象，从而使得我们能够学习到更深层的网络表示；而对于RNN来说，LSTM(Long short-term memory)[4]及其变种GRU(Gated recurrent unit)[5]等模型通过加入门控机制(Gate)，很大程度上改善了梯度消失所带来的损失。关于ResNet和LSTM的细节会在其他章节介绍，敬请期待。



参考文献：

[1] Liu, Pengfei, Xipeng Qiu, and Xuanjing Huang. "Recurrent neural network for text classification with multi-task learning." arXiv preprint arXiv:1605.05101 (2016).

[2] https://en.wikipedia.org/wiki/Activation_function

[3] He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.

[4] Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 9.8 (1997): 1735-1780.

[5] Chung, Junyoung, et al. "Empirical evaluation of gated recurrent neural networks on sequence modeling." arXiv preprint arXiv:1412.3555 (2014).





**【LSTM】**



**场景描述**

俗话说，前事不忘，后事之师，各种带有记忆功能的网络是近来深度学习研究和实践的一个重要领域。由于RNN有着梯度弥散（vanishing gradient）和梯度爆炸（explosion gradient）等问题，难以学习长期的依赖，在遇到重要的信息时，又难以清空之前的记忆，因此在实际任务中的效果往往并不理想。

LSTM是Long Short-Term Memory(长短期记忆网络)的简称。作为RNN的最知名和成功的扩展，LSTM可以对有价值的信息进行长期记忆，并在遇到新的重要信息时，及时遗忘过去的记忆，减小了RNN的学习难度。它在语音识别，语音建模，机器翻译，图像描述生成，命名实体识别等各类问题中，取得了巨大的成功。



**问题描述**

LSTM是如何实现长短期记忆功能的？它的各模块分别使用了什么激活函数，可以使用别的激活函数么？



*背景知识假设：基本的深度学习知识。*

*该类问题的被试者：对RNN有一定的使用经验，或在自然语言理解、序列建模等领域有一定的经历。*



**解答与分析**

**1. LSTM是如何实现长短期记忆功能的？**

该问题需要被试者对LSTM的结构有清晰的了解，并理解其原理。在回答的过程中，应当结合其结构图或更新的计算公式进行讨论，LSTM的结构图示例如下：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

图片来源：http://colah.github.io/posts/2015-08-Understanding-LSTMs/

与传统的RNN比较，LSTM仍然是基于xt和ht-1来计算ht，只不过对内部的结构进行了更加精心的设计，加入了三个阀门（输入门i, 遗忘门f，输出门o）和一个内部记忆单元c。输入门控制当前计算的新状态以多大程度更新到记忆单元中；遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉；输出门控制当前的输出有多大程度上取决于当前的记忆单元。

经典的LSTM中，第t步的更新计算公式如下：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJF5w6Qg8nDGKdMMt4icmIVcibWerWCYnhxPWpicUP3mt7ZwyIjQX9LqsyxZKmcKtlJMHnOOicubibFIrg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中输入门it是通过输入xt和上一步的隐含层输出ht-1进行线性变换，再经过Sigmoid激活函数得到。输入门的结果是向量，其中每个元素是0到1之间的实数，用于控制各维度流过阀门的信息量；Wi，Ui两个矩阵和向量bi为输入门的系数，是在训练过程中需要学习得到的。遗忘门ft和输出门ot的计算方式与输入门类似，它们有各自的系数W，U和b。与传统RNN不同的是，从上一个记忆单元的状态ct-1到当前的状态ct的转移不完全取决于由tanh激活函数计算得到状态，而是由输入门和遗忘门来共同控制。

在一个训练好的网络中，当输入的序列中没有重要信息时，LSTM的遗忘门的值接近于1，输入门的值接近于0，此时过去的记忆会被保存，从而实现了长期记忆功能；当输入的序列中出现了重要的信息时，LSTM应当把其存入记忆中，此时其输入门的值会接近于1；当输入的序列中出现了重要信息，且该信息意味着之前的记忆不再重要时，此时输入门的值接近1， 遗忘门的值接近于0，这样旧的记忆被遗忘，新的重要信息被记忆。经过这样的刻意设计，可以使得整个网络更容易学习到序列之间的长期依赖。



**2. LSTM里各模块分别使用什么激活函数，可以使用别的激活函数么？**

LSTM中，遗忘门、输入门和输出门使用Sigmoid函数作为激活函数；在生成候选记忆时，使用双曲正切函数tanh作为激活函数 。Sigmoid函数的输出在0～1之间，符合门控的物理定义，且当输入较大／较小时，其输出会非常接近1／0，从而保证该门开／关。在生成候选记忆时使用tanh函数，是因为其输出在-1～1之间，这与大多数场景下特征的分布是0中心的相吻合。此外，tanh函数在输入为0附近相比Sigmoid函数有更大的梯度，通常使模型收敛更快。

LSTM的激活函数也不是一成不变的。例如在原始的LSTM中[1]，使用的激活函数是Sigmoid函数的变种，h(x)=2*sigmoid(x)-1，g(x)=4*sigmoid(x)-2，这两个函数的范围分别是[-1,1]和[-2,2]。并且在原始的LSTM中，只有输入门和输出门，没有遗忘门，其中输入x经过输入门后是直接与记忆相加的，所以输入门控g(x)的值是0中心的。后来经过大量的研究和实践，发现增加遗忘门对LSTM的性能有很大的提升[4]，并且h(x)使用tanh比2*sigmoid(x)-1要好，所以现代的LSTM采用Sigmoid和tanh作为激活函数。事实上在门控中，使用Sigmoid函数是几乎所有现代神经网络模块的常见选择：例如在GRU[2]和注意力机制中，也广泛使用Sigmoid作为门控的激活函数。因为门控的结果要与被控制的信息流按元素相乘，所以门控的激活函数通常是会饱和的函数。

此外，在一些对计算能力有限制的情景，诸如可穿戴设备中。由于Sigmoid函数中求指数需要一定的计算， 此时会使用hard gate，让门控输出为0或1的离散值：即当输入小于阈值时，门控输出为0，大于阈值时，输出为1。从而在性能下降不显著的情况下，减小计算量。

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJF5w6Qg8nDGKdMMt4icmIVczicgichZl3xDQEtrltfT7Iq58cl3l5nrFKgV1hCc0DmiaJOUDqcAgpkBw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

总而言之，LSTM经历了20年的发展，其核心思想一脉相承，但各个组件都发生了很多演化。了解其发展历程和常见变种，可以让我们在实际工作和研究中，结合问题选择最佳的LSTM模块。灵活思考并知其所以然，而不是死背网络的结构和公式，也有助于在面试中取得更好的表现。



参考文献：

[1] Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 9.8 (1997): 1735-1780.

[2] Chung, Junyoung, et al. "Empirical evaluation of gated recurrent neural networks on sequence modeling." arXiv preprint arXiv:1412.3555 (2014).

[3] Gers, Felix A., and Jürgen Schmidhuber. "Recurrent nets that time and count."  IJCNN 2000

[4] Gers, Felix A., Jürgen Schmidhuber, and Fred Cummins. "Learning to forget: Continual prediction with LSTM." (1999): 850-855.







**【Seq2Seq】**



**场景描述**

作为生物体，我们的视觉和听觉会不断地获得带有序列的声音和图像信号，并交由大脑理解；同时我们在说话、打字、开车等过程中，也在不断地输出序列的声音、文字、操作等信号。在互联网公司日常要处理的数据中，也有很多是以序列形式存在的，例如文本、语音、视频、点击流等等。因此如何更好的对序列进行建模，一向是研究的要点。

2013年来，随着深度学习的发展，Seq2Seq（sequence to sequence，序列到序列）框架在机器翻译领域被大量采用，随后迅速影响到了上述的各领域，应用在了各类序列数据上。



**问题描述**

\1. Seq2Seq的框架是什么？有哪些优点？

\2. Seq2Seq在解码时，有哪些常用的方法？



*背景知识假设：基本的深度学习知识。*

*该类问题的被试者：对RNN有一定的使用经验，或在自然语言理解、序列建模等领域有一定的经历。*



**解答与分析**

**1. Seq2Seq的框架是什么？有哪些优点？可以结合具体的应用场景进行分析。**

在Seq2Seq框架提出之前，深度神经网络在图像分类等问题上取得了非常好的效果。在其擅长解决的问题中，输入和输出通常都可以表示为固定长度的向量，如果长度稍有变化，会使用补零等操作。然而许多重要的问题，例如机器翻译、语音识别、自动对话等，表示成序列后，其长度事先并不知道。因此如何突破先前深度神经网络的局限，使其可以适应这些场景，成为了13年以来的研究热点，Seq2Seq框架应运而生[1][2]。

Seq2Seq框架的核心思想是：通过深度神经网络将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码输入与解码输出两个环节构成。在经典的实现中，编码器和解码器分别由一个RNN构成，其选择有传统RNN、LSTM、GRU等，这两个RNN是共同训练的。

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJ6qMaHBmIrjkjvxnuEibLk8m310Ie2H8pV3y1HVOXbQfCwRLQ1RMCmvichpbYIkZF051EXOoAmoLXA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上图[3]是Seq2Seq应用于机器翻译时的例子，输入的序列是一个源语言的句子，有三个单词A、B、C，编码器依次读入A、B、C和结尾符<EOS>。 在解码的第一步，解码器读入编码器的最终状态，生成第一个目标语言的词W；第二步读入第一步的输出W，生成第二个词X；如此循环，直至输出结尾符<EOS>。输出的序列W、X、Y、Z就是翻译后目标语言的句子。

在文本摘要时，输入的序列是长句子或段落，输出的序列是摘要短句。在图像描述生成时，输入是图像经过视觉网络的特征，输出的序列是图像的描述短句。在语音识别时，输入的序列是音频信号，输出的序列是识别出的文本。这些场景中，编码器或解码器会依据场景有不同的设计，大家在面试时可以结合自己的项目经历展开探讨。



**2. Seq2Seq在解码时，有哪些常用的方法？**

Seq2Seq最核心的部分是其解码部分，大量的改进也是在解码环节衍生的，因此面试时也常常问到。Seq2Seq最基础的解码方法是贪心法，即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果，直到结束。贪心法的计算代价低，适合作为基础实现（baseline），并与其他方法相比较。很显然贪心法获得的是一个局部最优解，由于实际问题的复杂性，该方法往往并不能取得最领先的效果。

Beam search是常见的改进算法，它是一种启发式的算法。该方法会保存beam size（后面简写为b）个当前的较佳选择，然后解码时每一步根据保存的选择进行下一步扩展和排序，接着选择前b个保存，循环进行，直到结束时选择最佳的一个作为解码的结果。下图为b为2的示例：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJ6qMaHBmIrjkjvxnuEibLk84hFOodZLicxiazHOdGNQ5EskaWgFA03ibdROYTic0lCcT3geKRUmzcGoFA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在该例中，当前已经有解码得到的第一个词的两个选项：I和My。然后，将I和My输入到解码器，得到一系列候选的序列诸如I decided、My decision、 I thought等。最后，从后续序列中选择最优的两个，作为前两个词的两个选项。很显然，如果b取1，那么会退化为前述的贪心法。随着b的增大，其搜索的空间增大，获得的效果会提高，但需要的计算量也会增大。在实际的应用如机器翻译、文本摘要中，b往往会选择一个适中的范围如8～12。

解码时使用堆叠的RNN、增加dropout机制、与编码器之间建立残差连接等，也是常见的改进措施。大家在研究和工作中，可以依据使用场景，多查阅文献和技术文档，有针对的选择和实践。

在解码环节，另一个重要的改进是注意力机制[4]，它的引入使得解码时，每一步可以有针对地关注与当前有关的编码结果，从而减小了编码器输出表示的学习难度，也更容易学到长期的依赖关系。注意力机制会在下一节做更加深入的探讨，敬请期待。此外，解码时还可以采用记忆网络（memory network）[5]等，从外界获取知识。



参考文献：

[1] Auli, Michael, et al. "Joint Language and Translation Modeling with Recurrent Neural Networks." EMNLP. 2013.

[2] Cho, Kyunghyun, et al. "Learning phrase representations using RNN encoder-decoder for statistical machine translation." EMNLP. 2014.

[3] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. "Sequence to sequence learning with neural networks." NIPS. 2014.

[4] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate." ICLR. 2015.

[5] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. "End-to-end memory networks." NIPS. 2015.









**【注意力机制】**



**场景描述**

作为生物体，我们的视觉和听觉会不断地获得带有序列的声音和图像信号，并交由大脑理解；同时我们在说话、打字、开车等过程中，也在不断地输出序列的声音、文字、操作等信号。在互联网公司日常要处理的数据中，也有很多是以序列形式存在的，例如文本、语音、视频、点击流等。因此如何更好的对序列进行建模，一向是研究的要点。

为了解决这些问题，注意力机制（attention mechanism）被引入Seq2Seq模型中。注意力机制模仿人在生成序列时的行为，当解码序列的某个输出时，重点关注输入序列中和当前位置的输出相关的部分，缓解了Seq2Seq模型的上述问题。作为一种通用的思想，注意力机制在包括Seq2Seq在内的多种场景中都得到了应用。



**问题描述**

RNN的Seq2Seq模型中引入了注意力机制（attention mechanism）是为了解决什么问题？在机器翻译的Seq2Seq模型中使用注意力机制建模时，为什么选用了双向的RNN模型？



*背景知识假设：基本的深度学习知识。*

*该类问题的被试者：对RNN有一定的使用经验，或在自然语言理解、序列建模等领域有一定的经历。*



**解答与分析**

我们已经介绍了Seq2Seq模型以及LSTM。在实际任务，例如机器翻译中，使用Seq2Seq模型，通常会先使用一个RNN作为编码器，将输入序列（源语言句子的词向量序列）编码成为一个向量表示，然后再使用一个RNN模型作为解码器，从编码器得到的向量表示里解码得到输出序列（目标语言句子的词序列）。

Seq2Seq模型中， 当前隐状态以及上一个输出词决定了当前输出词，即:

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其中f, g是非线性变换，通常是多层神经网络；yi是输出序列中的一个词，si是对应的隐状态。

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJpQEQia5BqyAZ52o0iaO7bEibk8hZbJVIgEsNFdd0TLwAofGYqClr1uOEqUl2zQsVd3BOwEM9E77spA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Figure 1. Seq2Seq模型

在实际使用中，会发现随着输入序列的增长，模型的性能发生了显著下降。这是因为编码时将输入序列的全部信息压缩到一个向量表示中，随着序列增长，句子越前面的词的信息的丢失就越严重。试想翻译一个有100个词的句子，需要将整个句子全部词的语义信息编码在一个向量中，而在解码时，目标语言的第一个词大概率是和源语言的第一个词相对应的，这就意味着第一步的解码就需要考虑100步之前的信息。建模时的一个小技巧是将源语言句子逆序输入，或者将其重复两遍输入来训练模型，这可以得到一定的性能提升。使用LSTM能够在一定程度上缓解这个问题，但在实践中对于过长的序列仍然难以有很好的表现。

同时，Seq2Seq模型的输出序列中，常常会丢失或者重复部分输入序列的信息。这是因为在解码时，对当前词及对应的源语言词的上下文信息和位置信息也在编码解码过程中丢失了。

Seq2Seq模型中引入注意力机制就是为了解决这些问题。

在注意力机制中，仍然可以用普通的RNN对输入序列进行编码，得到隐状态h1, h2 ... hT，但是在解码时，每一个输出词都依赖于前一个隐状态以及输入序列每一个对应的隐状态：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJpQEQia5BqyAZ52o0iaO7bEibp8cCic8s5bz0hrAsrhQdlkxOnia8he3z0NibJlsq3YVuFhpU7gKAXw2gQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中语境向量ci是输入序列全部隐状态h1, h2 ... hT的一个加权和：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJpQEQia5BqyAZ52o0iaO7bEibgXyu3euZGrLAOg3f9AIWtK3VUnialsTQIbRy2uflYmCmnwgEG6YKq4w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

权重参数，即注意力权重αij并不是一个固定权重，而是由另一个神经网络计算得到：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJpQEQia5BqyAZ52o0iaO7bEibnQ6icmibSDOndoa5ASScKWZKGQ4sjgUPZt9cI7YhdGOx0aA6kiboenK0A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

神经网络α将上一个输出序列隐状态si-1和输入序列隐状态hj作为输入，计算出一个xj, yi对齐的值eij，再归一化得到权重αij。

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJpQEQia5BqyAZ52o0iaO7bEibyl1ynIpYgBuFOfZABHOOOSibbJBsjpmpbE8sHaAwb1ZRqD44OqQDWZA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Figure 2. 双向RNN的注意力机制模型

我们可以对此给出一个直观的理解：在生成一个输出词时，会考虑每一个输入词和当前输出词的对齐关系，对齐越好的词，会有越大的权重，对生成当前输出词的影响也就越大。下图给出了翻译时注意力机制的权重分布，在互为翻译的词上会有最大的权重。

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJpQEQia5BqyAZ52o0iaO7bEibw7iaHEMhPwuTFgke6E3kW3qqRGj88ggK1U9ckKZlOfIcoicnevJpwd0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Figure 3. 注意力机制的权重分布

在机器翻译这样一个典型的序列到序列模型里，生成一个输出词yj时，会用到第i个输入词对应的隐状态hi以及对应的attention权重αij，如果只使用一个方向的RNN网络来计算隐状态，那么hi只包含了x0到xi的信息，相当于在αij这里丢失了xi后面的词的信息。而使用双向RNN进行建模，第i个输入词对应的隐状态包含了

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJpQEQia5BqyAZ52o0iaO7bEibwibnlH8O0yvBRBtgibqZfRKkrtia1VXZaUTZQJJxQanloiaRoibtyW1MXPg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

前者编码了x0到xi的信息，后者编码了xi及之后所有词的信息，防止了前后文信息的丢失。

注意力机制是一种思想，可以有多种不同的实现方式，在Seq2Seq以外的场景也有不少应用，下图展示了在图像描述文本生成任务中的结果，可以看到在生成对应词时，图片上对应物体的部分有较大的注意力权重。

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJpQEQia5BqyAZ52o0iaO7bEibY2E81TnugYiaRRhdo8UkGlTJFeTNeK3huXK4jfH12yTmbqthwzPBaqg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Figure 4. 注意力机制在图片描述文本生成中的应用



参考文献：

[1] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate." 

[2] Xu, Kelvin, et al. "Show, attend and tell: Neural image caption generation with visual attention."

[3] Rocktäschel, Tim, et al. "Reasoning about entailment with neural attention."







**【集成学习】**



**场景描述**

之前几篇中我们介绍了具体的机器学习模型和算法，集成学习(ensemble learning)更多的是一种组合策略，将多个机器学习模型结合起来，可以称为元算法(meta-algorithm)。

面对一个机器学习问题，通常有两种策略，一种是研发人员尝试各种模型，选择其中表现最好的模型做重点调参优化。这种策略类似于奥运会比赛，通过强强竞争来选拔最优的运动员，并逐步提高成绩。另一种重要的策略是集各家之长，如同贤明的君主广泛的听取众多谋臣的建议，然后综合考虑，得到最终决策。后一种策略的核心，是将多个分类器的结果集成为一个统一的决策。使用这类策略的机器学习方法统称为集成学习。其中的每个单独的分类器称为基分类器。

集成学习可以大致分为两类：

1. Boosting：这类方法训练基分类器时采用串行的方法，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。
2. Bagging：这类方法基分类器之间无强依赖，可以并行。其中很著名的算法之一是基于决策树基分类器的随机森林(Random Forest)。为了让基分类器之间互相独立，将训练集分为若干子集（当训练样本数量较少时，子集之间可能有交叠）。

基分类器有时又称为弱分类器，因为基分类器的错误率要大于集成后的分类器。基分类器的错误，是偏差(Bias)和方差(Variance)两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不能收敛到一个比较小的值。方差则是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。

Boosting方法通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。Bagging方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。假设每个基分类器出错的概率都是相互独立的，在某个测试样本上，用简单多数的投票方法来集成结果，超过半数基分类器都出错的概率会小于每个单独的基分类器的出错概率。一个Bagging的简单示例如下图：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrI5ibAicWaCHflRsrdxLfvkxZvw6kWTsfNJeibHLWB74w2NaNA2ZPaujkm8t357cOcXK6ttA91tviaH6A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**问题描述**

\1. 集成学习有哪些基本步骤？ 

\2. 常用的基分类器是什么？



**解答与分析**

**1. 集成学习有哪些基本步骤？请举两个集成学习的例子。**

集成学习一般可分为以下三个步骤：

\1. 找到误差互相独立的基分类器；

\2. 训练基分类器；

\3. 合并基分类器的结果。

合并基分类器的方法有voting和stacking两种。前者是用投票的方式，将获得最多选票的结果作为最终的结果。后者是用串行的方式，把前一个基分类器的结果输出到下一个分类器，将所有基分类器的输出结果相加作为最终的输出。

以Adaboost为例，它基分类器的训练和合并的基本步骤如下：

**训练基分类器：**

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrI5ibAicWaCHflRsrdxLfvkxZSUzsofW7cpibgiano6pbc6yJZyNY1DWiawe4uloiaeKibGERdwjjcebmFIw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**合并基分类器：**

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrI5ibAicWaCHflRsrdxLfvkxZJbU7ib0LlW0UArYV32kTLb8lKz8nciao5vTcUT1XABumVhBKrBXfcUVg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

另一个例子是GBDT(Gradient Boosted Decision Tree)：

GBDT的基本思想是，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。

我们以一个视频网站的用户为例，为了将广告匹配到指定年龄性别的用户，视频网站需要对每个用户的性别/年龄做出预测。在这个问题中，每个样本是一个已知性别/年龄的用户，而特征则包括这个人访问的时长、时段、观看的视频的类型等。

例如用户A的真实年龄是25岁，但第一棵决策树的预测年龄是22岁，差了3岁，即残差为3。那么在第二棵树里我们把A的年龄设为3岁去学习，如果第二棵树能把A分到3岁的叶子节点，那两棵树的结果相加就可以得到A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在-2岁的残差，第三棵树里A的年龄就变成-2岁，继续学。这里使用残差继续学习，就是Gradient Boosting的意义。



**2. 常用的基分类器是什么？可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？**

最常用的基分类器是决策树，这是因为：

\1. 决策树可以较为方便的将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重。

\2. 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。

随机森林属于Bagging类的集成学习。Bagging的主要好处是集成后的分类器的方差，比基分类器的方差小。Bagging所采用的基分类器，最好是本身对样本分布较为敏感的（即所谓unstable分类器）。这样Bagging才能有用武之地。线性分类器或者K-近邻都是较为稳定的分类器，本身方差就不大，所以以它们为基分类器使用Bagging并不能在原有基分类器的基础上，获得更好的表现。甚至可能因为Bagging的采样，而导致他们在训练中更难收敛，而增大了集成分类器的偏差。







**【如何对高斯分布进行采样】**



**场景描述**

高斯分布，又称正态分布，是一个在数学、物理及工程领域都非常重要的概率分布。在实际应用中，我们经常需要对高斯分布进行采样。虽然在很多编程语言中，直接调用一个函数就可以生成高斯分布随机数，但了解其中的具体算法能够加深我们对相关概率统计知识的理解；此外，高斯分布的采样方法有多种，通过展示不同的采样方法在高斯分布上的具体操作以及性能对比，我们会对这些采样方法有更直观的印象。



**问题描述**

如果让你来实现一个高斯分布随机数生成器，你会怎么做？



背景知识：概率统计



**解答与分析**

首先，假设随机变量*z*服从标准正态分布*N*(0,1)，令

*x = σ·z + μ*

则*x*服从均值为*μ*、方差为*σ²*的高斯分布*N*(*μ*, *σ*²)。因此，任意高斯分布都可以由标准正态分布通过拉伸和平移得到，所以这里我们只考虑标准正态分布的采样。另外，几乎所有的采样方法都是以均匀分布随机数作为基本操作，因此这里假设我们已经有均匀分布随机数生成器了。均匀分布随机数一般用线性同余法来生成（伪随机数），具体参见文献[1]。

常见的采样方法有逆变换法 (Inverse Transform Method)、拒绝采样法 (Rejection Sampling)、重要性采样及其重采样 (Importance Sampling, Sampling-Importance-Resampling)、马尔科夫蒙特卡洛采样法 (Markov Chain Monte Carlo) 等。具体到高斯分布，我们需要如何采样呢？ 



如果直接用**逆变换法**，基本操作如下：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9Ionat273oic98iclBpTn5Gw6WKAP6Xd7gZlz75FDj0LcRuDOCjAmjibyvA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



上述逆变换法需要求解erf(*x*)的逆函数，这并不是一个初等函数，没有显式解，计算起来比较麻烦。为了避免这种非初等函数的求逆操作，**Box-Muller算法**采用如下解决方案：既然单个高斯分布的累计分布函数不好求逆，那么两个独立的高斯分布的联合分布呢？假设*x*, *y*是两个服从标准正态分布的独立随机变量，它们的联合概率密度为：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9I2nyTzb774P4CjQxCiaicD5vn2YKcB3aFRqyicYBG0DEIBTjqiawYN0DvicQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

考虑(*x*, *y*)在圆盘{(*x*, *y*) | *x*² *+ y*² ≤ *R*²}上的概率：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9IRx1pUKn8LdHL7oQwPICrQY16OdMIKGWZOibXqtnpvPblA19bDOib4hlg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

通过极坐标变换将(*x*, *y*)转化为(*r*, *θ*)，可以很容易求得上述二重积分：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9IV4nDc1ria9xeVh6VZAZYliaYklCsDck0Ny8Cg7siaZSibXKG4qEvgQWB3A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里*F*(*R*)可以看成是极坐标中*r*的累积分布函数。由于*F*(*R*)的计算公式比较简单，逆函数也很容易求得，所以可以利用逆变换法来对*r*进行采样；对于*θ*，在[0, 2*π*]上进行均匀采样即可。这样就得到了(*r*, *θ*)，经过坐标变化即可得到符合标准正态分布的(*x*, *y*)。具体采样过程如下：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9IH89FGDvRvZ8T204sUCntEyW7KfwjXXre4xwHYCa38Hdic1ThmiclYn7A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



Box–Muller算法由于需要计算三角函数，相对来说还是比较耗时，而**Marsaglia polar method**则避开了三角函数的计算，因而更快，其具体采样操作如下：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9Iia5KjkcFWvtK9cqqpM2CacC6RtPfZpqHpUu2Zy3iap4ApU0ZEgkUYdxg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



除了逆变化法，我们还可以利用**拒绝采样法**，选择一个比较好计算累积分布逆函数的参考分布来覆盖当前正态分布（可以乘以一个常数倍），进而转化为对参考分布的采样以及对样本点的拒绝/接收操作。考虑到高斯分布的特性，这里可以用指数分布来作为参考分布。指数分布的累积分布及其逆函数都比较容易求解。由于指数分布的样本空间为*x*≥0，而标准正态分布的样本空间为(-∞, +∞)，因此还需要利用正态分布的对称性来在半坐标轴和全坐标轴之间转化。具体来说，取*λ*=1的指数分布作为参考分布，

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9IWDr214PBSIdkyZXj2vypnmsmBdyw0lCpwKK9Ff4WjXEle1qBO205uw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

实际应用时，*M*需要尽可能小，这样每次的接受概率大，采样效率更高。因此，可以取

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

因此，具体的采样过程如下：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9Iwm3lA3n6iakqiclgYt0E0nxxBYDsQbCUvy9t5xoYNtz630JRkk93FRVw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



拒绝采样法的效率取决于接受概率的大小：参考分布与目标分布越接近，则采样效率越高。有没有更高效的拒绝采样算法呢？这就是**Ziggurat算法**，该算法本质也是拒绝采样，但采用多个阶梯矩形来逼近目标分布（如图1所示）。Ziggurat算法虽然看起来稍微繁琐，但实现起来并不复杂，操作也非常高效，其具体采样步骤可以参考文献[2]。

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKaXjygnhxfzN1ibrkjoAO9IFhaWzlN8th3VWf6PSlcQnk5bv1f2LibZGaoYFU6Y8pbibEaoa1L9YjLA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图1  Ziggurat算法

（图片来源于Numerical Computing with MATLAB第九章Random Numbers）



**总结与扩展**

高斯分布的采样方法还有很多，我们只列举了几种最常见的。具体面试时，候选人不需要回答所有的方法，知道其中一两种即可，面试官可以针对这一两种方法深入提问，如理论证明、优缺点、性能等。如果候选人没有思路，面试官可以引导其回忆那些通用的采样方法，如何将那些策略用到高斯分布这个具体案例上。另外，本题还可以适当扩展，把一般的高斯分布换成截尾高斯分布 (Truncated Gaussian Distribution) ，如何采样？如果是高维随机变量，拒绝采样法会存在什么问题？怎么解决呢？



参考文献：

[1] Linear congruential generator,

https://en.wikipedia.org/wiki/Linear_congruential_generator

[2] Ziggurat algorithm,

https://en.wikipedia.org/wiki/Ziggurat_algorithm





**【多层感知机与布尔函数】**



**场景描述**

神经网络概念的诞生很大程度上受到了神经科学的启发。生物学研究表明，大脑皮层的感知与计算功能是通过分多层实现的，例如视觉图像，首先光信号进入大脑皮层的V1区，即初级视皮层，之后依次通过V2层，V4层，即纹外皮层，进入下颞叶参与物体识别。深度神经网络，除了其模拟人脑功能的多层结构，最大的优势在于能够以紧凑简洁的方式来表达比浅层网络更复杂的函数集合（这里的“简洁”可定义为隐层单元的数目与输入单元的数目呈多项式关系）我们的问题将从一个简单的例子引出，已知神经网络中每个节点都可以进行“逻辑与／或／非”的运算，如何构造一个多层感知机 (Multi-Layer Perceptron, MLP) 网络实现n个输入比特的奇偶校验码（任意布尔函数）？



**问题描述**

1. 如何用多层感知机实现一个异或逻辑（仅考虑二元输入）？
2. 如果只使用一个隐层，需要多少隐节点能够实现包含n元输入的任意布尔函数？
3. 上面的问题中，由单隐层变为多隐层，需要多少节点？
4. 合理配置后所需的最少网络层数是多少？



背景知识：数理逻辑、深度学习



**解答与分析**

**1. 如何用多层感知机实现一个异或逻辑（仅考虑二元输入）？**

如下图所示（可有其他解法）：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)



**2. 如果只使用一个隐层，需要多少隐节点能够实现包含n元输入的任意布尔函数？**

包含n元输入的任意布尔函数可以唯一表示为“析取范式 (Disjunctive Normal Form, DNF)”（由有限个简单合取式构成的析取式）的形式。先看一个简单的例子：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGeFqPz5hIMOB4ZDK5NoCyeAeUtr3icLgRzZ8POA7oVhpGTrdsKxkJfZXg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

由于每个隐节点可以表示析取范式中的一个简单合取式，所以该函数可由包含六个隐节点的三层感知机实现，如下图：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGeuuwDqyTqKj9WXdicET2zn5O7prFDeuEfsJOKa5EZF9IqL51esVUeekA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们可以使用卡诺图表示析取式，即用网格表示真值表，当输入的合取式值为1时，则填充相应的网格。卡诺图中相邻的填色区域可以进行规约，以达到化简布尔函数的目的，如下图所示，七个填色网格最终可规约为三个合取式，故该函数可由包含三个隐节点的三层感知机实现：



![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGeiclnXxE07MRHJvicFnqmx32Wkj4QuiawdHC9WgP6WCxgPwTicQX7fPRHkQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGeVkNNVibIzia3Zslzg7iaga1PT3HjJpbfuNByibmVb8BxDQzj6OuBnl1yibw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

于是我们的问题可转化为，寻找“最大不可规约的”n元析取范式DNF，也等价于最大不可规约的卡诺图，直观上，我们只需间隔填充网格即可实现，其表示的布尔函数恰为n元输入的异或操作，如图：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGeYApKEoWR92PRlibRhUZNuW9YiaSg2f27rUfDwm65AxT6LQvsf34DxJ3Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此，n元布尔函数的析取范式最多包含2(n-1)个合取式，对于单隐层的MLP，需要2(n-1)个隐节点可以实现。



**3. 上面的问题中，由单隐层变为多隐层，构造一个n元异或函数需要多少节点？**

考虑二元输入的情况，需要三个节点可完成一次异或操作；对于四元输入，包含三次异或操作，需要3×3＝9个节点即可完成；而对于六元输入，包含五次异或操作，需要3×5＝15个节点…依此类推，n元异或函数需要3(n－1)个节点（包括最终输出节点）。网络的构造方式可参考下图：

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGeWyczbTWBP6ydbybajsMFq7WxJlFT6V3VMH4TEDNpwvKmALt9n80eSQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGegicBQHFDKdsxIMMcKGy2g7L4ib3IzEvwE0cH9J0sYLhDHsQ869c54jKQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGecNmIN8SdjyF6LicnqLsMMibKwH1tN34HZcNEL020aD5UFrT2rKn7BJSQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGeOswqgAnEbxNh7xf1ZFzqFBxicU79WtL2Iia21LfoEnoibsRraGLqpyOXA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们可以发现，多隐层结构可以将隐节点的数目从指数级O(2(n-1))直接减少至线性级O(3(n﹣1))！



**4. 合理配置后所需的最少网络层数是多少？**

根据二分思想，每层节点两两分组进行异或运算，需要两个隐层操作完成，故合理配置后需要的网络层数为2㏒2(*N*)。



------



**下一题预告**

**【经典优化算法****】**



**场景描述**

针对我们遇到的各类优化问题，研究者们提出了多种有各自适用场景的求解算法，并逐渐发展出了有严格理论支撑的研究领域——凸优化[1]。在这众多的算法中，有几种经典的优化算法是值得被牢记的，了解它们的适用场景有助于我们在面对新的优化问题时有求解思路。



**问题描述**

有一道无约束优化问题摆在你面前

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKFOC9wOlvwIIibXtuBIusGeqBe37VcoeUHoo0Lwb4gSq2VwJJEiaEJCYnCFSQriavbKAJCEqsNsQveA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中目标函数 *L*(·) 是光滑的。请问求解该问题的优化算法有哪些？它们的适用场景是什么？



参考材料与文献：

11-785 CMU Deep Learning Course (Fall 2017)

http://deeplearning.cs.cmu.edu/slides/lec2.universal.pdf

[1] Boyd, Stephen, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.







**【经典优化算法】**



**场景描述**

针对我们遇到的各类优化问题，研究者们提出了多种有各自适用场景的求解算法，并逐渐发展出了有严格理论支撑的研究领域——凸优化[1]。在这众多的算法中，有几种经典的优化算法是值得被牢记的，了解它们的适用场景有助于我们在面对新的优化问题时有求解思路。



**问题描述**

有一道无约束优化问题摆在你面前

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3uay8CO3E2zAuWhHAzIWYxiaKbrEE5icONFQb3QkytFMIxB1GNibII4LCwtA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中目标函数 *L*(·) 是光滑的。请问求解该问题的优化算法有哪些？它们的适用场景是什么？



先验知识：微积分、线性代数、凸优化基本概念



**解答与分析**

经典的优化算法可以分为两大类：直接法和迭代法。

直接法，顾名思义，就是能够直接给出优化问题最优解的方法。这个方法听起来非常厉害的样子，但它不是万能的。直接法要求目标函数满足两个条件。*第一个条件是，L(·)是凸函数。*什么是凸函数呢？它的严格定义可以参见文献[1]的第3章：对任意*x*和*y*，任意0≤*λ*≤1，都成立

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

一个直观的解释是，任取函数曲面上的两点连接成线段，该线段的任意一点都不会处于函数曲面的下方，示意图如下

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3uaGrfoSAaTFwTfKwCCm31vaGymfCy1iaxxJ4kOLM0gmLrgMnVLcwKtnZg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



*任取函数曲面上的两点连接成线段，*

*该线段的任意一点都不会处于函数曲面的下方*

若 *L*(·) 是凸函数，则文献[1]第140页的一个结论是，*θ**是优化问题最优解的充分必要条件是 *L*(·) 在*θ**处的梯度为0，即

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3ualj9VDhUfXymC4r0vkmtLKUJ5yHwnDag6k7tP3ic0rbXoz9uDfgNCS1A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此，为了能够直接求解出*θ**，*第二个条件是，上式有闭式解。*同时满足这两个条件的经典例子是岭回归（ridge regression），其中目标函数为

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3ua8sPTHJq14wxJWNuoQEWXurICBvHgTibiaUMaTdZsU3ja81tAChGbUz8Q/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

稍加推导就能得到最优解为（要试着自己推导哟）

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3uaxsqY39ICJ7VIHCR7kwngxFWticLseXdMGB3Xk3gHBeh9p1dZM7AYBibw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

直接法的这两个要求限制了它的广泛应用，因此在很多实际问题中，我们会采用迭代法。这类方法迭代地修正对最优解的估计，即假设当前的估计为*θt*，我们希望求解优化问题

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3uaNiawOpEKTHnbH9JvjejrvIhiaZMibytLLibFrXL2xHW7jzgv5PF9ibKnkSw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从而得到更好的估计*θt+1*＝*θt*﹢*δt*。迭代法又可以分为两类，一阶法和二阶法。

一阶法对函数 *L*(*θt*﹢*δ*) 做一阶泰勒展开，得到近似式

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3uafYPU3xPufZ1qn6Iqicp8ibqnJv7iclSCyjib1ibeExuicsy4PHap9crNoCRw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

由于该近似式仅在*δ*较小时才比较准确，我们可以求解带*l**2*正则的近似优化问题

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3ua8XGB5hB890YspD9OaVnDWFQxsdNouZskgaHsyrstiaqdVhPiaQI8wl3A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此一阶法的迭代更新公式是

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

一阶法也称为梯度下减法，梯度是目标函数的一阶信息。

二阶法对函数 *L*(*θt*﹢*δ*) 做二阶泰勒展开，得到近似式

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其中

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

是函数 *L*(·) 在*θt*处的Hessian矩阵。我们可以求解近似优化问题

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

从而得到二阶法的迭代更新公式

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

二阶法也称为牛顿法，Hessian矩阵是目标函数的二阶信息。二阶法的收敛速度一般要远快于一阶法，但是在高维情况下Hessian矩阵求逆的计算复杂度更大，而且当目标函数非凸时，二阶法有可能会收敛到鞍点（saddle point）。



**扩展阅读**

​      俄罗斯著名数学家Yurii Nesterov于1983年提出了对一阶法的加速算法[2]，该算法的收敛速率能够达到一阶法收敛速率的理论界。针对二阶法矩阵求逆的计算复杂度过高的问题，Charles George Broyden，Roger Fletcher，Donald Goldfarb和David Shanno四人于1970年分别独立提出了后来被称为BFGS的伪牛顿算法[3-6]，1989年扩展为低存储的L-BFGS算法[7]。

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrKHwAHky7S6KDLqxGg6j3uaD2iabdcHozNEhXfLqWyzDfPqQDK2fEuicUOibxKB0FaEFib7cZvtcvckTA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Charles George Broyden，Roger Fletcher，Donald Goldfarb和David Shanno的合影



参考文献：

[1] Boyd, Stephen, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

[2] Nesterov, Yurii. "A method of solving a convex programming problem with convergence rate O (1/k2)." Soviet Mathematics Doklady. Vol. 27. No. 2. 1983.

[3] Broyden, Charles G. "The convergence of a class of double-rank minimization algorithms: 2. The new algorithm." IMA journal of applied mathematics 6.3 (1970): 222-231.

[4] Fletcher, Roger. "A new approach to variable metric algorithms." The computer journal 13.3 (1970): 317-322.

[5] Goldfarb, Donald. "A family of variable-metric methods derived by variational means." Mathematics of computation 24.109 (1970): 23-26.

[6] Shanno, David F. "Conditioning of quasi-Newton methods for function minimization." Mathematics of computation 24.111 (1970): 647-656.

[7] Liu, Dong C., and Jorge Nocedal. "On the limited memory BFGS method for large scale optimization." Mathematical programming 45.1 (1989): 503-528.







**【随机梯度下降算法之经典变种】**



**场景描述**

提到Deep Learning中的优化方法，人们都会想到Stochastic Gradient Descent (SGD)，但是SGD并不是理想的万金油，反而有时会成为一个坑。当你设计出一个deep neural network时，如果只知道用SGD来训练，不少情况下你得到一个很差的训练结果，于是你放弃继续在这个深度模型上投入精力。但可能的原因是，SGD在优化过程中失效了，导致你丧失了一次新发现的机会。



**问题描述**

Deep Learning中最常用的优化方法是SGD，但是SGD有时候会失效，无法给出满意的训练结果，这是为什么？为了改进SGD，研究者都做了哪些改动，提出了哪些SGD变种，它们各有哪些特点？



*背景知识假设：Gradient Descent Method,* 

*Stochastic Gradient Descent Method*



**解答与分析**

**（1）SGD失效的原因——摸着石头下山**

为了回答第一个问题，我们先做一个形象的比喻：想象一下你是一个下山的人，眼睛很好，能看清自己所在位置的坡度，那么沿着坡向下走，最终你会走到山底。现在你被蒙上双眼，只能凭脚底踩石头的感觉判断当前位置的坡度，精确性大大下降。有时候你认为的坡，实际上可能不是坡，走上一段时间发现没有下山，或者曲曲折折走了好多弯路才下山。

我们回到SGD，传统的Gradient Descent（也称Batch Gradient Descent）是带着眼睛下山，而SGD是蒙着眼睛下山。Gradient Descent的每一步，为了获取准确的梯度，把整个训练集载入模型中计算，时间花费和内存开销非常大，无法用于实际中大数据集和大模型的场景。SGD放弃了对梯度准确性的追求，每步仅仅随机采样少量样本来计算梯度，计算速度快，内存开销小，但是由于每步接受的信息量有限，对梯度的估计常常出现偏差，造成目标函数曲线收敛得很不稳定，伴有剧烈波动，甚至有时出现不收敛的情况。图1展示了GD与SGD在优化过程中的参数轨迹，可以看到GD稳定地逼近最低点，而SGD曲曲折折简直是“黄河十八弯”。

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MJcSN4Y3MXzvgFxE1Fph3R04Qr1vfXmQKzczV5jicX1P7oNcrxcAGBBw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MlvCwibcVQibT9Jq1BJria1EWdqpmOs5mSN3YrK8Lia9LD2PSJjluJ5gEicA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图1 GD与SGD的参数优化轨迹

进一步地，有人会说deep learning优化问题本身就很难，有太多局部最优点的陷阱。没错，这个陷阱对SGD和GD都是普遍存在的。但对SGD来说，可怕的不是局部最优点，而是两类地形——山谷和鞍点[1]。山谷顾名思义就是狭长的山间小道，左右两边是峭壁；鞍点的形状像是一个马鞍，一个方向上两头翘，另一个方向上两头垂，而中心区域是一片近乎水平的平地。

为什么SGD最害怕遇上这两类地形呢？在山谷中，准确的梯度方向是沿山道向下，稍有偏离就会撞向山壁，而SGD粗糙的梯度估计使得它在两山壁间来回反弹震荡，不能沿山道方向迅速下降，导致收敛不稳定和收敛速度慢；在鞍点处，SGD走入一片平坦之地（此时离最低点还很远，故也称plateau），想象一下蒙着双眼只凭借脚底感觉坡度，如果坡度很明显那么不精确也能估计出下山的大致方向，但是如果坡度不明显那么很可能走错方向，同样在几近零的梯度区域，对梯度微小变化无法准确察觉的SGD蒙圈了，结果停滞下来。



**（2）解决之道——惯性保持和环境感知**

SGD本质上是采用迭代方式更新参数，每次迭代在当前位置的基础上，沿着某一方向迈一小步抵达下一位置，然后在下一位置接着这么做。SGD的更新公式是：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其中当前估计的梯度﹣*gt*表示步子的方向，学习速率*η*控制步子的大小。改造SGD仍然基于这个更新形式。

**变种1：Momentum**

为了解决SGD的山谷震荡和鞍点停滞的问题，我们做一个简单的思维实验。想象一下纸团在山谷和鞍点处的运动轨迹，在山谷中纸团受重力作用沿山道滚下，两边是不规则的山壁，纸团不可避免地撞在山壁，由于质量小受山壁弹力的干扰大，从一侧山壁反弹回来撞向另一侧山壁，结果来回震荡地滚下；当纸团来到鞍点的一片平坦之地时，还是由于质量小，速度很快减为零。纸团的情况和SGD遇到的问题简直如出一辙。直觉上，如果换成一个铁球，当沿山谷滚下时，会不容易受到途中旁力的干扰，轨迹更稳更直；当来到鞍点中心处，在惯性作用下继续前行，从而有机会冲出这片平坦的陷阱。因此，我们有了Momentum的方法[2]，更新公式为

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MZugffUOp2BuKw5LYD8WMsMwlKUXQPKCRQhpu0CkbKIvvEFPRDNGaPg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71Md1DT5u2zUtk0Iibhq2zcNkOwPmXEaQBnCloRLk9VQ4FRl7oicLqJQhibQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

前进步子﹣*vt*由两部分组成：(1) 学习速率*η*和当前估计梯度*gt*，(2) 衰减下的前一次步子*vt*－1。这里，惯性就体现在对前次步子信息的重利用上。拿中学物理作个类比，当前梯度就好比当前时刻受力产生的加速度，前一次步子好比前一时刻的速度，当前步子好比当前时刻的速度，为了计算当前时刻的速度，我们应当考虑前一时刻速度和当前加速度共同作用的结果，因此*vt*直接依赖于*vt*－1和*gt*，而不是仅有*gt*。另外，衰减系数*γ*扮演了阻力的作用。

中学物理还告诉我们，刻画惯性的物理量是动量，这也是算法名字的由来。沿山谷滚下的铁球，会受到两个方向上的力：沿坡道向下的力和与左右山壁碰撞的弹力。向下的力稳定不变，产生的动量不断累积，速度越来越快；左右的弹力总是在不停切换，动量累积的结果是相互抵消，自然减弱了球的来回震荡。因此，与SGD相比，Momentum的收敛速度快，收敛曲线稳定。

**变种2：AdaGrad**

惯性的获得是基于历史信息的。那么，除了从过去的步伐中获得一股子向前冲的劲儿，我们还能获得什么？我们期待获得对周围环境的感知，即使蒙上双眼，依靠前几次迈步的感觉，我们也应该能判断出一些信息，比如这个方向总是坑坑洼洼的，那个方向可能很平坦。

具体到SGD中，对环境的感知是指在参数空间中，对不同参数方向上的经验性判断，确定这个参数的自适应学习速率，即更新不同参数的步子大小应是不同的。在一些任务中，如文本处理中训练word embeddings参数，有的words频繁出现，有的则极少出现，数据的稀疏导致相应参数的梯度稀疏，即不频繁出现words的参数大多数情况梯度为零，使得这些参数被更新的频率很低，因此我们希望更新它们的步子大些，而对频繁更新的参数，更新的步子小些。AdaGrad[2]采用过往梯度平方和

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MgicBTxVueKPYn24LPn05pdb5qwQ4kUvpr5w6c5TRv8E1T77FRU40JKQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

的方式来衡量不同参数的梯度稀疏性，和越小表明越稀疏。AdaGrad的更新公式是：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MMm7QrkBpfjIz4hEBUibdenTPJwHpmPYIbQf6IHfQbOvGiaHIPb6hLFvg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中*θt*＋1,i表示*θt*＋1的第i个参数。另外，分母中和的形式实现了退火的过程，这是很多优化技术中常见的策略，意味着随着时间推移，学习速率

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71Mwd1JJjVv8GEicC10T3Gic4ALf7NvGS4dGYw97Kq1r2kUvO5kb5FTskvQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

越来越小，从而保证了优化的最终收敛。

**变种3：Adam**

Adam方法[4]将惯性保持和环境感知这两个优点集于一身。一方面，Adam记录梯度的first moment，即过往梯度与当前梯度的平均，这体现了惯性保持；另一方面，Adam还记录梯度的second moment，即过往梯度平方与当前梯度平方的平均，这类似AdaGrad，体现了环境感知，为不同参数产生自适应的学习速率。First moment和second moment求平均的思想类似滑动窗口内求平均的做法，关注当前梯度和近一段时间内梯度的平均值，时间久远的梯度对当前平均的贡献呈指数衰减，具体采用指数衰退平均(exponential decay average)技术，计算公式为：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MAw5NL9DAznQPibfAUfa3pMZaF8YgfSkb9GgWC3YXoUicb7PMibLC7gR0w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中*β*1, *β*2为衰减系数。

如何理解first moment和second moment呢？First moment相当于估计![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MP17Nu1HjrQr0K3VDxohfdwOFDItY2fEYuUcglhdelp7ToqmnhjZkhg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，由于当下梯度*gt*是随机采样估计的结果，比起*gt*我们更关心它在统计意义上的期望；second moment相当于估计![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MnjmCibM6xIj1k0GOOs1Iw63YGfPlVR97IoQdM7BQ8WhF4s24VA3Q8fA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，这点与AdaGrad不同，不是![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MqFk6icm8nDUyZSicxWKDqiaYAicU8JnMWNVibiajw44P6JBceWyIzuuwqF1w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)从开始到现在的和而是它的期望。它们的物理意义是：当‖*mt*‖大*vt*大时，梯度大且稳定，表明遇到一个明显的大坡，前进方向明确；当‖*mt*‖趋零*vt*大时，梯度不稳定，可能遇到一个峡谷，容易引起反弹震荡；当‖*mt*‖大*vt*趋零时，这种情况不可能出现；当‖*mt*‖趋零*vt*趋零时，梯度趋零，可能到达局部最低点，也可能走到一片坡度极缓的平地，此时要避免陷入plateau。另外，Adam还考虑了*mt*, *vt*在零初始值情况下的偏置矫正。Adam的更新公式为：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71MBQsG5BAzVoPK2CXgj7CVbOlibpspjHlqaicI91OPJgdSJRXZH50nplOw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**扩展阅读**

除了上述三种SGD变种，研究者还提出了其他方法：

\1. Nesterov Accelerated Gradient：扩展了Momentum方法，顺着惯性方向，计算未来可能位置处的梯度而非当前位置的梯度，这个“提前量”的设计让算法有了对前方环境预判的能力。

\2. AdaDelta和RMSProp：这两个方法非常类似，是对AdaGrad的改进。AdaGrad采用所有过往梯度平方和的平方根做分母，分母随时间单调递增，产生的自适应学习速率随时间衰减的速度过于激进，因此AdaDelta和RMSProp采用指数衰退平均的计算方法，用过往梯度的均值代替它们的和。

\3. AdaMax：基于Adam的一个变种，对梯度平方的处理由指数衰退平均改为指数衰退求max。

\4. Nadam：可看成Nesterov Accelerated Gradient版的Adam。



参考文献：

[1] Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. arXiv, pages 1–14, 2014

[2] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks: the official journal of the International Neural Network Society, 12(1):145–151, 1999

[3] John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121–2159, 2011

[4] Diederik P. Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. InternationalConference on Learning Representations, pages 1–13, 2015.





**【SVM – 核函数与松弛变量】**



**场景描述**

当我们在SVM中处理线性不可分的数据时，核函数可以对数据进行映射，从而使得原问题在某种度量下具有更为可分的相似度，而通过引入松弛变量，我们可以放弃一些离群点的精确分类来使分类平面不受太大的影响。将这两种技术与SVM结合起来，正是SVM分类器简洁而强大的原因之一。



**问题描述**

1. 一个使用高斯核

   ![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJnmBEbz1WQ3gR0pKoNF71McIOkx0IRMxIZ7yMoc9H5ZicSICO7UoE1PGiaiae6Dpv9jSsSmcoqp6Mgg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   训练的SVM（Support Vector Machine）中，试证明若给定训练集中不存在两个点在同一位置，则存在一组参数{*α*1, ... *αm*, *b*}以及参数γ使得该SVM的训练误差为0。

2. 若我们使用问题1中得到的参数γ训练一个不加入松弛变量的SVM，是否能保证得到的SVM，仍有训练误差为0的结果，试说明你的观点。

3. 若我们使用SMO（Sequential Minimal Optimization）算法来训练一个带有松弛变量的SVM，并且惩罚因子C为任意事先不知道的常数，我们是否仍能得到训练误差为0的结果，试说明你的观点。



先验知识：SVM训练过程、核函数、SMO算法



**解答与分析**

**1.**

根据SVM的原理，我们可以将SVM的预测公式可写为下式：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其中{(*x*(1), *y*(1)), …, (*x*(*m*), *y*(*m*))}为训练样本，而{*α*1, …, *αm*, *b*}以及高斯核参数γ则为训练样本的参数，根据题意我们可以得到对于任意的i≠j 我们有‖*x*(*i*)﹣*x*(*j*)‖≥*ε*，我们可以直接对任意i，取*α**i*＝1，*b*＝0，则有

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIbQBSaaoUCeQ1yeS6nooibQzpKqWs0ickGPloCdqrpoe6lQID2WNUFepotxGDIwxF9gOSosDiaIud3w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

将任意*x*(*j*)代入则有

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIbQBSaaoUCeQ1yeS6nooibQE0cNN12U5RuuicSgQFhcmib0SYLFHQgAVSmLnsOfsERf8CKBs6Az5t8Q/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

注意到*y*(*i*)∈{1, ﹣1}

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIbQBSaaoUCeQ1yeS6nooibQiaQcRx7FuFlmZOsXcoLjltO109KUia6LpxmpkVqdPED2zLARFKQltexg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

由题意知‖*x*(*i*)﹣*x*(*j*)‖≥*ε*，取γ＝ε/㏒1/2*m*

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIbQBSaaoUCeQ1yeS6nooibQufkpumvblGfky7eFOVsUeGVZDyzopffNP9RqYvtSSHcchKYooWHa2g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

故有

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIbQBSaaoUCeQ1yeS6nooibQKmFEcvpibGGAibCnMicxV9ChRyIjBD83YN1974ybWAaMm2RRYOojsqKMw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可知对于任意*x*(*j*)，预测结果与样本的距离不超过1，则训练误差为0。



**2.**

我们能得到训练误差为0的分类器，我们仅需要证明解存在即可。考虑SVM推导中的限制*y*(*i*)(*w**T**x*(*i*)﹢*b*)≥1，与上一问相同，我们取b＝0，那么则有*y*(*i*)·*f*(*x*(*j*))＞0，由上问，我们有

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

所以一个可行解在将所有*α**i*取到足够大时（这里改变*α**i*的取值并不会影响上一问的结论），我们可得到*y*(*i*)(*w**T**x*(*i*)﹢*b*)≥1，则得到一个可行解，那么最优解的训练误差仍为0。



**3.**

我们的分类器并不一定能得到0训练误差，因为我们的优化目标改变了，并不再是训练误差最小，考虑我们优化的结果实际上包含两项

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIbQBSaaoUCeQ1yeS6nooibQhwovtiak1MBnfTen44npNyzZzUnf2KQlGyzBAMPJ1hnejbhxu93ONibw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可知当我们的参数C选取较小的值时，我们就可以得出后一正则项将占据优化的较大比重，那么一个带有训练误差，但是参数较小的点将成为更优的结果，例如当C取0时，*w*也可取0即可达到优化目标，但是显然这样我们的训练误差不一定能达到0。



------



**下一题预告**

**【主题模型】**



**场景描述**

基于Bag-Of-Words（或N-gram）的文本表示模型有一个明显的缺陷，就是无法识别出不同的词（或词组）具有相同主题的情况。我们需要一种技术能够将具有相同主题的词（或词组）映射到同一维度上去，于是产生了主题模型(Topic Model)。主题模型是一种特殊的概率图模型。想象一下我们如何判定两个不同的词具有相同的主题呢？这两个词可能有更高的概率出现在同一主题的文档中；换句话说，给定某一主题，这两个词的产生概率都是比较高的，而另一些不太相关的词产生的概率则是较低的。假设有K个主题，我们可以把任意文章表示成一个K维的主题向量，其中向量的每一维代表一个主题，权重代表这篇文章属于该主题的概率。主题模型所解决的事情，就是从语料库中发现有代表性的主题（得到每个主题上面词的分布），并且计算出每篇文章对应着哪些主题。这样具有相似主题的文章拥有相似的主题向量表示，从而能够更好地表示文章的语义，提高文本分类、信息检索等应用的效果。



**问题描述**

\1. 常见的主题模型有哪些？试介绍其原理。

\2. 如何确定LDA模型中的主题个数？







可以写成： 

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJmDztvgYrJ7iaAOrD34dKkruREyicicFuERd5RspA0Or0BK5mGOF1jf2I5nYWasuH5MMYqh6NBcpHcA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在这里我们做一个简化，假设给定主题z的条件下，生成词w的概率是与特定的文章无关的，则公式可以简化为：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJmDztvgYrJ7iaAOrD34dKkrG9F3ubfaR5icuSxWyEovP9URJMTV5YthBQUbccAoNsnDXfJ084KYArg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

整个语料库中的文本生成概率可以用以下公式表示，我们称之为似然函数（Likelihood Function）：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJmDztvgYrJ7iaAOrD34dKkr2JrJySz81c9KbpfU96PuRVqoq3e1NY8Rrn2UyzKEVsEGH9NHeAjcibQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中*p*(*d*m, *w*n)是在第m篇文章中，第n个单词为*w*n的概率，与上文中*p*(*w*|*d*)的含义是相同的，只是换了一种符号表达。*n*(*d*m, *w*n)表示单词*wn*在文章*d*m中出现的次数。

于是，对数似然函数可以写成：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJmDztvgYrJ7iaAOrD34dKkrvKjRszBkFop6WPW2NfubPWI0g6dOfpZzTsK8KKwXmrBaBoVlaPGRvQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在上面的公式中，定义在文章上的主题分布*p*(*z**k*|*d**m*)和定义在主题上的词分布*p*(*w**n*|*z**k*)是待估计的参数 。我们需要找到最优的参数，使得整个语料库的对数似然函数最大化。由于参数中包含的*z**k*是隐含变量（即无法直接观测到的变量），因此无法用最大似然估计直接求解，可以利用EM（Expectation-Maximization）算法来解决。



(2) LDA

LDA(Latent Dirichlet Allocation)[2]可以看作是pLSA的贝叶斯版本，其文本生成过程与pLSA基本相同，不同的是为主题分布和词分布分别加了狄利克雷(Direchlet)先验。为什么要加入狄利克雷先验呢？这就要从频率学派和贝叶斯学派的区别说起。pLSA采用的是频率派思想，将每篇文章对应的主题分布*p*(*z**k*|*d**m*)和每个主题对应的词分布*p*(*w**n*|*z**k*)看成确定的未知常数，并可以求解出来；而LDA采用的是贝叶斯学派的思想，认为待估计的参数（主题分布和词分布）不再是一个固定的常数，而是服从一定分布的随机变量。这个分布符合一定的先验概率分布（即Dirichlet分布），并且在观察到样本信息之后，可以对先验分布进行修正，从而得到后验分布。LDA之所以选择Dirichlet分布做为先验分布，是因为它为多项式分布的共轭先验概率分布，后验概率依然服从Dirichlet分布，这样做可以为计算带来便利。LDA的图模型表示如下：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其中α，β分别为两个Dirichlet分布的超参数，为人工设定。语料库的生成过程如下。

对文本库中的每一篇文档*d**m*：



![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJmDztvgYrJ7iaAOrD34dKkrhRib2ic2QPZuuACk33kaFCakia5rdpYuepmia7l2CQ1Jb1dS62m7RicMiaHw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



这里主题分布*θm*以及词分布![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJmDztvgYrJ7iaAOrD34dKkrOsJgeLUcuQdwXqdE7P8tuTYKwDaoicPfCl8fEzv0rITEheEjAqDW0zw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是待估计的参数，可以用吉布斯采样（Gibbs Sampling）[4]求解其期望。具体做法为，首先随机给定每个词的主题，然后在其它变量固定的情况下，根据转移概率抽样生成每个词的新主题。对于每个词来说，转移概率可以理解为：给定文章中的所有词以及除自身以外其它所有词的主题，在此条件下该词对应各个新主题的概率。最后，经过反复迭代，我们可以根据收敛后的采样结果计算主题分布和词分布的期望。



**2. 如何确定LDA模型中的主题个数?**

在LDA中，主题的个数K是一个预先指定的超参数。对于模型超参数的选择，实践中的做法一般是将全部数据集分成训练集、验证集、和测试集3部分，然后利用验证集对超参数进行选择。例如，在确定LDA的主题个数时，我们可以随机选取60%的文档组成训练集，另外20%的文档组成验证集，剩下20%的文档组成测试集。我们在训练时尝试多组超参数的取值，并在验证集上检验哪一组超参数所对应的模型取得了最好的效果。最终，在验证集上效果最好的一组超参数和其对应的模型将被选定，并在测试集上进行测试。

为了衡量LDA模型在验证集和测试集上的效果，我们需要寻找一个合适的评估指标。一个常用的评估指标是困惑度（perplexity）。在文档集合D上，模型的困惑度被定义为：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJmDztvgYrJ7iaAOrD34dKkrvoyWKfSqkzVmmFguPsaa0hr1HGlJKSjg6gKW0WGfAGoaP08Gib3nfmQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中M为文档的总数，**w**d为文档d中单词所组成的词袋向量，p(**w**d)为模型所预测的文档d的生成概率，Nd为文档d中单词的总数。

一开始，随着主题个数的增多，模型在训练集和验证集的困惑度呈下降趋势，但是当主题数目足够大的时候，会出现过拟合，导致困惑度指标在训练集上继续下降但在验证集上反而增长。这时，我们可以取困惑度极小值点所对应的主题个数作为超参数。实践中，困惑度的极小值点可能出现在主题数目非常大的时候，然而实际应用并不能承受如此大的主题数目，这时就需要在实际应用中合理的主题数目范围内进行选择，比如选择合理范围内困惑度的下降明显变慢（拐点）的时候。

另外一种方法是在LDA基础之上融入分层狄利克雷过程（Hierarchical Dirichlet Process，HDP）[3]，构成一种非参数主题模型HDP-LDA。非参数主题模型的好处是不需要预先指定主题的个数，模型可以随着文档数目的变化而自动对主题个数进行调整；它的缺点是在LDA基础上融入HDP之后使得整个概率图模型更加复杂， 训练速度也更加缓慢，因此在实际应用中还是经常采用第一种方法确定合适的主题数目。



参考文献：

[1] Hofmann, Thomas. "Probabilistic latent semantic analysis." Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1999.

[2] Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "Latent dirichlet allocation." Journal of machine Learning research3.Jan (2003): 993-1022.

[3] Teh, Yee W., et al. "Sharing clusters among related groups: Hierarchical Dirichlet processes." Advances in neural information processing systems. 2005.

[4] George, Edward I., and Robert E. McCulloch. "Variable selection via Gibbs sampling." Journal of the American Statistical Association 88.423 (1993): 881-889.







**【PCA 最小平方误差理论】**



**场景描述**

经历了强化学习、深度学习、集成学习一轮轮面试题的洗礼，我们是否还记得心底对宇宙，对世界本源的敬畏与探索之心？时间回溯到40多天前，我们曾经从宇宙空间出发，讨论维度，从维度引到机器学习，由PCA探寻降维之道，传送门：[Hulu机器学习与问答系列第六弹-PCA算法](http://mp.weixin.qq.com/s?__biz=MzA5NzQyNTcxMA==&mid=2656430435&idx=1&sn=f55f0ad0b5025076f8b9cbb248737f75&chksm=8b004922bc77c034b92a97387d791ac350f643130da03534dfadfe28a162c8fab98441e0a7b9&scene=21#wechat_redirect)。彼日，我们从最大方差的角度解释了PCA的原理、目标函数和求解方法。今夕，我们将从最小平方误差之路，再次通向PCA思想之核心。



**问题描述**

观察到其实PCA求解的是最佳投影方向，即一条直线，这与数学中线性回归问题的目标不谋而合，能否从回归的角度定义PCA的目标并相应地求解问题呢？



背景知识：线性代数



**解答与分析**

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzvMZichsUgkKJPyXrKxAVOIzib4na2VlibkibQsd1BlwmnUOVwCtQpj47NQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们还是考虑二维空间这些样本点，最大方差角度求解的是一条直线，使得样本点投影到这条直线上的方差最大。从求解直线的思路出发，很容易联想到数学中的线性回归问题，其目标也是求解一个线性函数使得对应直线能够更好地拟合样本点集合。如果我们从这个角度定义PCA的目标，那么问题就会转化为一个回归问题。

顺着这个思路，在高维空间中，我们实际上是要找到一个d维超平面，使得数据点到这个超平面的距离平方和最小。对于一维的情况，超平面退化为直线，即把样本点投影到最佳直线，最小化的就是所有点到直线的距离平方之和，如下图所示。

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzBiba3RDGRChJG3I2BogM23iaHGMc1eQcyawjGSMY4tthYa5VsdKricfaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzwsWEJRkmVwHNSt9Zic2cNWw4jBJrZAxZhco0nAMwGt4pvicqB3icXEXFQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzbvjA1wujtvRNv1eEthK7UGjEPB4ZqOyzj59VAcPnzAbfM6W22PUp7Q/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

第一项**x***k**T**x**k*与我们选取的**W**无关，是个常数。我们利用刚才求出的投影向量表示将第二项和第三项分别继续展开  

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzzZ2MHrI1DIzMbAvUQmyTJGmCDulvgt38qcA5VcJs1pZgQ0DHzzEAnw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中**ω***iT****x**k*和**ω***jT****x**k*表示投影长度，都是数字。且i≠j时，**ω***i**T***ω***j*＝0，因此上式的交叉项中只剩下*d*项。 

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzm1gGsSaZF3BzBTvBe8ibvXUhKE7o0czFjuDvy1PiaNNcsTs6DsVv4yKg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzsgkOpWibIJonqk3tnlkibQjl2BnXABNjHW6vk25uS1ISTgOgHG126XIA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们要最小化的式子即对所有的k求和，可以写成

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzqBGjnqPP6OIuUYHzicS10SJVFnjA2RHfImicB3bQv5tpthMa2pKUb43w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果我们对**W**中的*d*个基**ω***1*, **ω***2*, ..., **ω***d*依次求解，就会发现和上一节中方法完全等价。比如当*d＝*1时，我们实际求解的问题是

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLEhEtibzt1GRoIRjuzibCrXzovxDfmicEnvlmaddxlMMCj8S4p8PToPLtIywRHK0juOqejppLEJicrxA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这个最佳直线**ω**与最大方差法求解的最佳投影方向一致，即协方差矩阵的最大特征值所对应的特征向量，差别仅是协方差矩阵∑的一个倍数，以及一个常数偏差，但这并不影响我们对最大值的优化。



**总结与扩展**

至此，我们从最小平方误差的角度解释了PCA的原理、目标函数和求解方法，不难发现，这与最大方差角度殊途同归，从不同的目标函数出发，得到相同的求解方法。







**【分类、排序、回归模型的评估】**



**场景描述**

在模型评估过程中，分类问题、排序问题、回归问题往往需要使用不同的评估指标进行评估。但在诸多的评估指标中，大部分指标只能片面的反映模型一部分的能力，如果不能合理的综合运用评估指标，不仅不能发现模型本身的问题，甚至会得出错误的结论。下面以hulu的业务为背景，假想了几个模型评估的场景，看看大家能否管中窥豹，发现指标选择或者模型本身的问题。



**问题描述**

1. 准确率（Accuracy）的局限
2. 精确率（Precision）和召回率（Recall）的权衡
3. 均方根误差（Root Mean Square Error，RMSE）的“意外”



*知识点：*

*准确率（Accuracy）* *精确率（Precision）*

*召回率（Recall） 均方根误差（Root Mean Square Error, RMSE）*



**解答与分析**

**1. 准确率（Accuracy）的局限**

hulu的奢侈品广告主们希望把广告定向投放给奢侈品用户，hulu通过第三方的DMP（Data Management Platform，数据管理平台）拿到了一部分奢侈品用户的数据，并以此为训练集和测试集训练奢侈品用户的分类模型；该模型的分类准确率超过了95%，但在实际广告投放过程中，该模型还是把大部分广告投给了非奢侈品用户，有可能是什么原因造成的？

*难度：1星*

在解答该问题之前，我们先明确一下分类准确率（Accuracy）的定义——准确率是指分类正确的样本个数，占总样本个数的比例。

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJpcCITxXVg3a0NykYghQpCMkc0fcssyYVT2paibwExIepzJQsviaU8atqqs0OM7GGXft1wrTOEIOgg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中*ncorrect*为被正确分类的样本个数，*ntotal*为总样本的个数。

准确率是分类问题最简单也是最直观的评价指标，但准确率存在明显的缺陷，即当样本所属类别的比例非常不均衡时，样本占比大的分类往往成为影响准确率的最主要因素。比如负样本占99%，那么分类器把所有样本预测为负样本也可以获得99%的准确率。

明确这一点，我们这道题也就迎刃而解了。因为奢侈品用户显然只占hulu全体用户的一小部分，模型的整体分类准确率高，不代表着对奢侈品用户的分类准确率高。在线上投放过程中，我们只会对模型判定的“奢侈品用户“进行投放，因此对“奢侈品用户”判定的准确率不够高的问题被放大了。为解决这个问题，使用平均准确率（每个类别下的样本准确率的算术平均）来进行模型评估是更为有效的指标。

事实上，这道题是一道比较开放的试题，需要面试者根据问题的现象去一步步地排查问题。标准答案不限于指标选择的问题，即使评估指标选择对了，仍会存在模型过拟合或欠拟合，测试集和训练集划分，线下评估与线上测试样本分布存在差异等等一系列的问题。但评估指标选择的问题是最容易被发现也是最可能影响评估结果的因素。



**2. 精确率（precision）与召回率（recall）的权衡**

hulu提供视频的模糊搜索功能，搜索排序模型返回的top 5结果的精确率（precision）非常高，但在实际的使用过程中，用户却还是经常找不到想要找的视频，特别是一些比较冷门的剧集，这有可能是哪个环节出了问题？

*难度：1星*

要回答这个问题，我们需要首先明确两个概念，精确率（precision）和召回率（recall）。

精确率（precision）：分类正确的正样本个数占分类器判定为正样本的样本个数的比例。

召回率（recall）：分类正确的正样本个数占真正的正样本个数的比例。

在排序模型中，由于没有一个准确的阈值把结果判定为正负样本，所以往往使用Top N返回结果的Precision和Recall值来衡量排序模型的性能。即我们认为排序模型返回的Top N的结果就是模型判定的正样本，计算Precision@N，和Recall@N。

Precision和Recall是矛盾统一的两个指标，为了提高精确率，需要分类器尽量在“更有把握时”才把样本预测为正样本，但此时分类器往往会因为过于保守的选择而漏掉很多“没有把握”的正样本，导致召回率降低。

回到问题中来，问题给出Precision@5的结果非常好，也就是说排序模型Top 5的返回值的质量是很高的，但在实际使用过程中，用户为了找一些冷门的视频，往往会寻找排在较靠后的结果，甚至翻页去查找目标视频，但根据题目，用户经常找不到想要的视频，这说明模型没有把相关的视频都找出来呈现给用户，显然，问题出在召回率上，如果相关结果有100个的话，即使Precision@5达到了100%，Recall@5也仅仅是5%。在评价Precision的时候，我们是否应该同时看Recall的指标？进一步，是否应该选取不同的Top N进行观察？再进一步，是否应该选取更高阶的评估指标能够更全面的反映模型在Precision和Recall两方面的表现？

答案显然都是肯定的，为了综合评估一个排序模型的好坏，我们不仅要看模型在不同top N下的Precision@N和Recall@N，而且最好能据此画出Precision-Recall曲线，这里我们简单介绍一下P-R曲线的绘制方法。

P-R曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说，其P-R曲线上的某一个点代表着在某一个正样本阈值（大于该阈值模型预测为正样本，小于该阈值预测为负样本）下所对应的召回率和精确率。而整条P-R曲线是通过从最高到最低滑动正样本阈值生成的。如下图图b所示，其中实线代表模型insts模型的P-R曲线，虚线代表insts2模型的P-R曲线。横轴接近0的点代表着正样本阈值最大时模型的精确率和召回率。如图可见，在召回率接近0时，insts模型的精确率是0.9，而insts2模型的精确率是1。说明insts2模型得分前几位的样本全部是真正的正样本，而insts模型即使是得分最高的几个样本也存在预测错误的情况。而随着召回率的增加，精确率整体上有所下降，在召回率为1时，insts模型的精确率反而超过了insts2模型，这充分说明了我们只用一个点的精确率和召回率结果是不能全面衡量模型性能的，只有通过P-R曲线的整体表现，才能够对模型进行更为全面的评估。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJpcCITxXVg3a0NykYghQpCiamsmgTfstSiaoBUS512TQ7enpPdmlTiaKb8eP0BpUFZOFWTgKKlgC8mw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

（图片来自 Fawcett, Tom. "An introduction to ROC analysis." Pattern recognition letters27.8 (2006): 861-874.）

除此之外，F1 score和ROC曲线也是能够综合地反映一个排序模型的性能，F1-score是精准率和召回率的调和平均值，定义如下：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJpcCITxXVg3a0NykYghQpCdMia8I9UaN9N3rRyk8Vx5zZQd6YfPds3LRIVEZ76jgDza2oOYIRK2Hg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

ROC曲线我们在前面的文章中做过详细的介绍，感兴趣的同学可以翻一下公众号的历史文章。



**3. 平方根误差的“意外”**

hulu作为一家流媒体公司，拥有众多美剧资源，预测每部美剧的流量趋势对于广告投放、用户增长都是非常重要。我们希望构建一个回归模型来预测某部美剧的流量趋势，但无论采用何种回归模型，我们得到的RMSE（Root Mean Square Error，平方根误差）指标都非常高，然而事实上，模型在95%的时间区间内的预测误差都小于1%，取得了相当不错的预测结果，那么造成RMSE指标居高不下的最可能的原因是什么？

*难度：1星*

大家知道，RMSE（Root Mean Square Error，均方根误差）是经常用来衡量一个回归模型好坏的。但按照题目的叙述，RMSE这个指标却失效了。我们首先看一下RMSE的计算方式：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其中，yi是第i个样本点的真实值，![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)是第i个样本点的预测值，n是样本点的个数。

一般情况下，RMSE能够很好的反映回归模型预测值与真实值的偏离程度，但当实际问题中存在别偏离程度非常大的离群点时，即使是极个别的点，也会让RMSE指标变得很差。

回到问题中来，模型在95%的时间区间内的预测误差都小于1%，显然大部分时间区间内模型效果都是非常优秀的。但RMSE效果却一直很差，很可能是由于在剩余5%的时间区间内存在非常严重的离群点。事实上，在流量预估这个实际问题中，噪声点确实是很容易产生的，对于特别小流量的美剧，刚上映的美剧，或者获奖的美剧，甚至一些相关社交媒体突发事件带来的流量，都会成为离群点产生的原因。

那么有什么解决方法呢？这里有三个角度，第一个角度是如果我们认定这些离群点是“噪声点”的话，我们就需要在数据预处理的阶段就把这些噪声点过滤掉；第二个角度，如果我们不认为这些离群点是噪声点的话，我们其实是需要进一步提高模型的预测能力，将离群点产生的机制建模进去。关于流量预估模型如何改进这个问题是一个宏大的话题，我们就不展开讨论了；第三个角度，我们希望找一个更合适评估该模型的指标，关于这个问题，其实是存在比RMSE鲁棒性更好的指标的，比如MAPE（Mean Absolute Percent Error平均绝对百分比误差），定义如下：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJpcCITxXVg3a0NykYghQpCc1X48pLRHEAsreuNiajIx46nwibDpHLDwCtWcdxvUkAyZDtFRAgiatozw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

相比RMSE，MAPE相当于把每个点的误差进行了归一化，消除了个别离群点带来的绝对误差的影响。



**总结与扩展**

本篇文章，我们基于三个假想的hulu的应用场景，主要说明了评估指标选择的重要性。每个评估指标都有其价值，但是如果只从单一的评估指标出发去评价模型，往往会得出片面甚至错误的结论。只有通过一组互补的评价指标去验证试验结果，我们才能够更好的发现并解决模型存在的问题，从而更好的解决实际业务场景的问题。







**【特征工程—结构化数据】**



**场景描述**

特征工程是指结合问题寻找有效的特征并进行处理成适合模型的输入形式。机器学习中有句经典的话叫做“Garbage in, garbage out”，意思是如果输入的数据是垃圾，那么得到的结果也是垃圾。可以看出模型成败的关键并不仅仅取决于模型的选取，还取决于我们是否有根据特定的问题找到了行之有效的输入。常用的数据可以分为结构化数据和非结构化数据，其中：结构化数据可以看成关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型；非结构化数据主要包括文本数据和图像数据，所有信息都是混在一起的，并没有清晰的类别定义，并且每条数据的大小都是不一样的。



**问题描述**

\1. 为什么需要对数值类型的特征做归一化？

\2. 怎样处理类别型特征？

\3. 怎样处理高维组合特征？

\4. 怎样有效地找到组合特征？



**解答与分析**

**1. 为什么需要对数值类型的特征做归一化？**

对数值类型的特征做归一化(normalization)可以将所有的特征都统一到一个大致相同的区间内。最常用的归一化是z-score normalization，它会将特征变换映射到以均值为0、标准差为1的正态分布上。准确的来说，假设原始特征的均值为*μ*、标准差为*σ*，那么z-score normalization定义为：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

为什么通常需要对数值型做归一化呢？我们可以借助随机梯度下降来说明归一化的重要性。假设有两种数值型特征，*x*1的取值范围为[0, 10]，*x*2的取值范围为[0, 3]，可以构造一个目标函数符合以下等值图：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLqPHPkoPcbgv4KUiaIItgM0zgt9lNicreAGLJZStvyXvc9k1mTyc0L67cr3a7DUKpBNibsGRR8pBhiaw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在学习速率相同的情况下，*x*1的更新速度会大于*x*2，需要更多的迭代才能找到最优值[1]。如果将*x*1和*x*2归一化到相同的区间后，则优化目标的等值图会变成右图的圆形，*x*1和*x*2的更新速度会比较一致，能够更快的找到最优值。

那么归一化适用于哪些模型，又对哪些模型是不适用的呢？首先，通过梯度下降法求解的模型是需要归一化的，包括包括线性回归、logistic regression、支持向量机(Support Vector Machine)、神经网络(Neuro Network)。但对于决策树模型则是不适用的，以C4.5为例，决策树在进行节点分裂时主要依据的是x >= threshold 和 x < threshold的信息增益比，而信息增益比跟x是否经过归一化是无关的，因为归一化并不会改变样本在x上的相对顺序。



**2. 怎样处理类别型特征？**

类别型特征(categorical feature)主要是指性别(男、女)、血型(A、B、AB、O)等类似的在有限选项内取值的特征。通常类别型特征原始输入都是字符串形式，只有决策树等少数模型能直接处理字符串形式的输入。对于Logistic Regression、线性支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。

本节主要介绍三种常用的转换方法：Ordinal Encoding、One-hot Encoding、Binary Encoding。

Ordinal Encoding通常用于处理类别间具有大小关系的数据，例如成绩可以分为Low、Medium、High三档，并且存在High > Medium > Low的排序关系。Ordinal Encoding会按照大小关系对类别型特征赋予一个数值ID，例如High表示为3、Medium表示为2、Low表示为1，转换后依然保留了大小关系。

One-hot Encoding通常用于处理类别间不具有大小关系的特征，以血型为例，血型一共有四个取值(A、B、AB、O)，在One-hot Encoding后血型会变成一个4维稀疏向量：A型血表示为(1, 0, 0, 0)，B型血表示为(0, 1, 0, 0)，AB型表示为(0, 0, 1, 0)，O型血表示为(0, 0, 0, 1)。对于类别取值较多的情况下使用One-hot Encoding需要注意以下问题：

- 使用稀疏向量来节省空间：在One-hot Encoding下，只有某一维取值为1，其他位置取值均为0。因此可以用向量的稀疏表示来有效节省空间，并且目前大部分的算法都会实现接受稀疏向量形式的输入。

- 配合特征选择来降低维度：高维度特征会带来两方面的问题：(1)在K近邻算法中，高纬空间下两点之间的距离很难得到有效的衡量; (2) 在Logistic Regression中，模型的参数的数量会随着维度的增高而增加，容易引起过拟合问题; (3) 通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。

最后介绍Binary Encoding，该方法主要分为两步：用Ordinal Encoding给每个类别赋予一个ID，然后将ID对应的二进制编码作为结果。以血型A、B、AB、O为例，Binary Encoding的过程如下图所示：A型血的Ordinal Encoding为1，二进制表示为(0, 0, 1)；B型血的Ordinal Encoding为2，二进制表示为(0, 1, 0)，以此类推可以得到AB型血和O型血的二进制表示。可以看出，Binary Encoding本质上是利用二进制编码对ID进行哈希，最终得到了0/1向量维数要少于One-hot Encoding，节省了空间。

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

除了本章介绍的Encoding方法外，有兴趣的读者还可以参照[2]了解其他的编码方式，包括Helmert Contrast、Sum Contrast、Polynomial Contrast、Backward Difference Contrast。



**3. 怎样处理高维组合特征？**

为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合成高阶特征，构成交互特征（Interaction Feature）。以广告点击预估问题为例，如图1所示，原始数据有语言和类型两种离散特征。为了提高拟合能力，语言和类型可以组成二阶特征，如图2所示：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLqPHPkoPcbgv4KUiaIItgM0WqzY4LE4k9ib5gNVOMmAwVPxtvWJkd6qbicE1zFGcdXKuDA5icR67vzVA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

△ 图1

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLqPHPkoPcbgv4KUiaIItgM0ibQ4DtpugublAPQw4mI9gc5otm1542qaDF0Ao1dic0EibdoItL73XJqow/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

△ 图2

以Logistic Regression为例，假设数据的特征向量**X** = (x_1, x_2, …, x_k)，我们有

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLqPHPkoPcbgv4KUiaIItgM0IfrfWEl3YSqbADHNiawpRhQhPWnibKsduEHK9LSqnxEgy2ssd9ia6dnibw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其*wij*的维度等于|*xi*| * |*xj*|。在上面的广告点击预测问题当中，w的维度是4（2x2，语言取值为中文或者英文、 类型的取值为电影或者电视剧）。

在上面广告预测的问题看起来特征组合是没有任何问题的，但当引入ID类型的特征时，问题就出现了。以推荐问题为例，原始数据如图3所示，用户ID和物品ID组合成新的特征后的数据如图4所示：

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLqPHPkoPcbgv4KUiaIItgM06N9iaHjiaQ2JQEschreXWt3bhjehYdVWl723FeHgTN2CJFbQ7lIJDicew/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

△ 图3

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

△ 图4

假设用户的数量为m、物品的数量为n，那么需要学习的参数的规模为m × n。在互联网环境下，用户数量和物品数量都可以到达千万量级，几乎无法学习m × n的参数规模。在这种情况下，一种行之有效的方法是将用户和物品分别用k维的低维向量表示(k << m, k << n)，

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLqPHPkoPcbgv4KUiaIItgM0jjicPdcundVqQSNYVz5ic5qhTLQF0pUqEXs0u9d6ppxSicVkibib4dGavtA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中*wij*＝*xi'* · *xj'*，在这里*xi'*和*xj'*分别表示对应的低维向量。在上面的推荐问题中，需要学习的参数的规模为m × k + n × k。熟悉推荐算法的同学可以看出来这等于矩阵分解，希望这篇文章提供了另一个理解矩阵分解的思路。



**4. 怎样有效地找到组合特征？**

在上节中我们介绍了如何利用降维方法来减少两个高维特征进行组合需要学习的参数。但是在很多实际数据当中，我们常常需要面对多种高维特征。如果简单的两两组合，参数过多、容易过拟合的问题依然存在，并且并不是所有的特征组合都是有意义地。因此，我们迫切需要一种有效地方法来帮助我们找到哪些特征应该进行组合。

本节介绍介绍一种基于决策树的特征组合寻找方法[3]。以点击预测问题为例，假设原始输入特征包含年龄、性别、用户类型（试用期、付费）、物品类型（护肤、食品等）四个方面的信息，并且假设我们根据原始输入和标签（点击与否）构造出了两个决策树，那么每一条从根节点到叶子节的路径都可以看成一种特征组合的方式。具体来说，我们可以将“年龄<=35”且“性别=女”看成一个特征组合，将“年龄<=35”且“物品类别＝护肤”看出是一个特征组合，将“用户类型＝付费”且“物品类型＝食品”看成是一种特征组合，将“用户类型＝付费”且“年龄<=40”看成是一种特征组合。假设我们有两个样本如下表所示，那么第一行可以编码为(1, 1, 0, 0)，因为既满足“年龄<=35”且“性别=女”，也满足“年龄<=35”且“物品类别＝护肤”。同理可以看出第二个样本可以编码为(0, 0, 1, 1)，因为既满足“用户类型＝付费”且“物品类型＝食品”，又满足“用户类型＝付费”且“年龄<=40”。

![img](http://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLqPHPkoPcbgv4KUiaIItgM0UgJnlNib9qwCNjb30pCo1VU5NsZhhMXufNsnaBKiaYtiaF1UMu1gQPqFg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](http://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLqPHPkoPcbgv4KUiaIItgM0nbeCalTTlmia4lv1PMBd3lbOnrUeGFz1h2tyBicjNx50CMPz9kdJlgjQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

最后，那么给定原始输入该如何有效构造多棵决策树呢？在这里我们采用梯度提升决策树 (gradient boosting decision tree)，该方法地思想是每次都在之前构建的决策树的残差(residual)上构建下一棵决策树，对GDBT感兴趣的读者可以参考原始文献[4]。



参考文献：

[1] https://www.coursera.org/learn/machine-learning

[2] http://contrib.scikit-learn.org/categorical-encoding/index.html

[3] Practical Lessons from Predicting Clicks on Ads at Facebook, ADKDD’14

[4] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189–1232, 1999.







**【神经网络训练中的批量归一化】**



**场景描述**

深度神经网络的训练中涉及诸多手调参数，如学习率，权重衰减系数，Dropout比例等，这些参数的选择会显著影响模型最终的训练效果。批量归一化(Batch Normalization, BN)方法从数据分布入手，有效减弱了这些复杂参数对网络训练产生的影响，在加速训练收敛的同时也提升了网络的泛化能力。



**问题描述**

1. BN基本动机与原理是什么？
2. BN的具体实现中怎样恢复前一层学习到的特征分布？
3. 简述BN在卷积神经网络中如何使用？



*背景知识：统计学习，深度学习*



**解答与分析**

**1. BN的基本动机与原理是什么？**

神经网络训练过程的本质是学习数据分布，训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。

然而随着网络训练的进行，每个隐层的参数变化使得后一层的输入发生变化，从而每一批(batch)训练数据的分布也随之改变，致使网络在每次迭代中都需要拟合不同的数据分布，增大了训练的复杂度以及过拟合的风险。

Batch Normalization方法是针对每一批数据，在网络的每一层输入之前增加归一化处理(均值为0，标准差为1)，将所有批数据强制在统一的数据分布下，即对该层的任意一个神经元(不妨假设为第k维) ![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==) 采用如下公式：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)



**2. BN的具体实现中怎样恢复前一层学习到的特征分布？**

按照上式直接进行归一化会导致上层学习到的数据分布发生变化，以sigmoid激活函数为例，BN之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了之前学习到的特征分布。为了恢复原始数据分布，具体实现中引入了变换重构以及可学习参数γ，β：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJA8TxiaFib7bqOq6V8CO4JUDhj2RkgY51B13v6GM0WkSsA4ibPZyXsL8D6f3EKQ1ciclLibwwjFhgcLsg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJA8TxiaFib7bqOq6V8CO4JUDV4WwceAYHDoteibBMuHsl9p5CIyDicLnQos7hOKHnWO2nnicfI1payUpQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

分别为输入数据分布的方差和偏差，对于一般的网络，不采用BN操作时，这两个参数高度依赖前面网络的学习到的连接权重(对应复杂的非线性)。而在BN操作中提取出来后，γ，β变成了该层的学习参数，与之前网络层的参数无关，从而更加有利于优化的过程。

完整的Batch Normalization网络层的前向传导过程公式如下：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJA8TxiaFib7bqOq6V8CO4JUDlU7xwdfkrIRzReSKk75BJjtM4SpTXM2vfH8WvMaqLMJ4xvlQLeLX5g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**3. 简述BN在卷积神经网络中如何使用？**

卷积神经网络中每一层操作后得到一系列特征图(feature maps)，卷积层上的BN同样使用类似权值共享的策略，将每张特征图做为一个处理单元，即全连接网络中的单个神经元，进行归一化操作。

具体实现中，假设网络训练中每个batch包含*b*个样本，特征图个数为*f*，特征图的宽高分别为*w, h*，那么，每个特征图所对应的全部神经元个数为*b \* w \* h*，利用这些神经元我们可得到一组学习参数γ，β用于对该特征图进行BN操作。详细过程可参考问题2中的公式，其中*m*＝*b \* w \* h*，为BN操作中的mini-batch。



------



**下一题预告**

**【随机梯度下降法】**



**场景描述**

深度学习得以在近几年迅速占领工业界和学术界的高地，重要原因之一是数据量的爆炸式增长。如下图所示，随着数据量的增长，传统机器学习算法的性能会进入平台期，而深度学习算法因其强大的表示能力，性能得以持续增长，甚至在一些任务上超越人类。因此有人戏称，“得数据者得天下”。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJA8TxiaFib7bqOq6V8CO4JUDnUDstwicXzCbZAFjV6t7r5VI3zQRQicEl0hc46k8D7icvkHT905ITclxg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

经典的优化方法，例如梯度下降法，每次迭代更新需要用到所有的训练数据，这给求解大数据、大规模的优化问题带来了挑战。掌握基于大量训练数据求解模型的方法，对于掌握机器学习，尤其是深度学习至关重要。



**问题描述**

针对训练数据量过大的问题，当前有哪些优化求解算法？







**【随机梯度下降法】**



**场景描述**

深度学习得以在近几年迅速占领工业界和学术界的高地，重要原因之一是数据量的爆炸式增长。如下图所示，随着数据量的增长，传统机器学习算法的性能会进入平台期，而深度学习算法因其强大的表示能力，性能得以持续增长，甚至在一些任务上超越人类。因此有人戏称，“得数据者得天下”。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6icb9xoxEribiaYG5Az2lCCHxibHHbtrnTBoYHZALS2uibVQscxJUQZu0GVA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

经典的优化方法，例如梯度下降法，每次迭代更新需要用到所有的训练数据，这给求解大数据、大规模的优化问题带来了挑战。掌握基于大量训练数据求解模型的方法，对于掌握机器学习，尤其是深度学习至关重要。



**问题描述**

针对训练数据量过大的问题，当前有哪些优化求解算法？



*先验知识：概率论、梯度下降法*



**解答与分析**

在机器学习中，优化问题的目标函数通常可以表示成

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其中*θ*是待优化的模型参数，*x*是模型输入，*f*(*x, θ*)是模型的实际输出，*y*是模型的目标输出，*L*(·,·)刻画了模型在数据(*x, y*)上的损失，*P*data表示数据的分布，E表示期望。*J*(*θ*)刻画了当参数为*θ*时，模型在所有数据上的平均损失，我们希望能够找到使得平均损失最小的模型参数，也就是求解优化问题

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6H8FlicVR02zn5rLp7dOpvxYKgAy8AWS5afxH0ia11UdWmZMJzTFWSUNg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为了求解该问题，梯度下降法的迭代更新公式为

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy69c7QnU4Z01M16Bl3F4SgibML805Rfk0tKMXzGhT0zicOxI5c6iaiaJ3V8g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中*α*＞0是步长。若采用所有训练数据的平均损失来近似目标函数及其梯度，即

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6bjCacAvQJx1wIw3vicbIRKIhHkuBCtHzBXyzEa7BNn2VlW9J7ic5eIFg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中*M*表示训练数据的个数，则对模型参数的单次更新需要遍历所有的训练数据，这在*M*很大时是不可取的。

为了解决该问题，随机梯度下降法（stochastic gradient descent, SGD）采用单个训练数据的损失近似平均损失，即

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6mltrOw8CW7G3o1cXNlynFmcjzRZOZjNbrxHIicqYhW2icZDyD9Jc96MQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此，随机梯度下降法用单个训练数据即可对模型参数进行一次更新，大大加快了收敛速率。该方法也非常适用于数据源源不断到来的在线场景。

为了降低梯度的方差从而使得算法的收敛更加稳定，也为了充分利用高度优化的矩阵运算操作，实际中我们会同时处理若干训练数据，该方法被称为小批量梯度下降法（mini-batch gradient descent）。假设需要处理*m*个训练数据{*x**i1, yi1, …, xim, yim*}，则目标函数及其梯度为

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6JrdWcGTiaZc1LStv8XCU5fkWRq61pB0IBPyibzY4COz29voESffBEsQw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

对于小批量梯度下降法，有三点需要注意的地方：

1. 如何选取参数*m*？在不同的应用中，最优的*m*通常会不一样，需要通过调参选取。一般*m*取2的幂次时能充分利用矩阵运算操作，所以我们可以在2的幂次中挑选最优的取值，例如64，128，256，512等。
2. 如何挑选*m*个训练数据？为了避免数据的特定顺序给算法收敛带来的影响，一般会在每次遍历训练数据之前，先对所有的数据进行随机排序，然后顺序挑选*m*个训练数据进行训练，直至遍历完所有的数据。
3. 如何选取学习速率*α*？为了加快收敛速率同时提高求解精度，通常会选取递减的学习速率方案。算法一开始能够以较快的速率收敛到最优解附近，再以较小的速率精细调整最优解。最优的学习速率方案也通常需要调参才能得到。

综上，我们通常采用小批量梯度下降法解决训练数据量过大的问题，每次迭代更新只需要处理*m*个训练数据即可，其中*m*是一个远小于总数据量*M*的常数，能够大大加快收敛速率。



------



**下一题预告**

**【初识生成式对抗网络（GANs）】**



**场景描述**

2014年的一天，Goodfellow与好友相约到酒吧聊天。也许平日里工作压力太大，脑细胞已耗尽了创作的激情，在酒吧的片刻放松催生了一个绝妙的学术点子，然后就有了GANs的传说。GANs全称为生成式对抗网络，是一个训练生成模型的新框架。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6vnG2WRkefaYZhkA1ySFicz5eT1BImSeRXMqRlWC45eibY82wL3okJRow/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

GANs自提出之日起，就迅速风靡深度学习的各个角落，GANs的变种更是雨后春笋般进入人们的视野，诸如：WGAN、InfoGAN、f-GANs、BiGAN、DCGAN、IRGAN等等。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6SdfMINlQZfpicqrz9kttVvrOlicBhetHNGKsXYma3RvNCfUVsQOje63Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

GANs之火，就连任何初入深度学习的新手都能略说一二。GANs刚提出时没有华丽的数学推演，描绘出的是一幅魅力感极强的故事画面，恰好契合了东方文化中太极图的深刻含义——万物在相生相克中演化，听起来很有意思。想象GANs框架是一幅太极图，“太极生两仪”，这里“两仪”就是生成器和判别器，生成器负责“生”，判别器负责“灭”，这一生一灭间有了万物。具体说来，生成器在初始混沌中孕育有形万物，判别器甄别过滤有形万物，扮演一种末日大审判的角色。回到严谨的学术语言上，生成器从一个先验分布中采得随机信号，经过神经网络的“妙手”转换，得到一个模拟真实数据的样本；判别器既接收来自生成器的模拟样本，也接收来自实际数据集的真实样本，我们不告诉判别器这个样本是哪里来的，需要它判断样本的来源。判别器试图区分这两类样本，生成器则试图造出迷惑判别器的模拟样本，两者自然构成一对“冤家”，置身于一种对抗的环境。然而，对抗不是目的，在对抗中让双方能力各有所长才是目的，理想情况下最终达到一种平衡，使得双方的能力以臻完美，彼此都没有了更进一步的空间。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6ibS52sya833n0R5dKIU5xnlNc4DgBz9WFCxqKibj2tk4pGRQe8PDp7RA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**问题描述**

关于GANs，从基本理论到具体模型再到实验设计，我们依次思考三个问题：

（1）GANs可看作一个双人minimax游戏，请给出游戏的value function。我们知道在理想情况下最终会达到一个纳什均衡点，此时生成器表示为*G**，判别器表示为*D**，请给出解(*G\*, D**)和value function的值；在未达到均衡时，我们将生成器*G*固定，去寻找当前下最优的判别器*D**G**，请给出*D**G**和此时的value function。至此的答案都很容易在原论文中找到，这里进一步发问，倘若固定*D*，我们将*G*优化到底，那么解*G**D**和此时的value function是什么？

（2）发明GANs的初衷是为了更好地对概率生成模型作估计，我们知道在应用传统概率生成模型（如：马尔科夫场、贝叶斯网）时会涉及大量难以完成的概率推断计算，GANs是如何避开这类计算的？

（3）实验中训练GANs的过程会如描述的那么完美吗，求解*G*的最小化目标函数

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJMUBpLBJ6Fib9dmj9V0ibxy6yDyVGu5xBINTaMWDaD1iapF5jWq5mx9AqtcB3GI1UribIUYaGb1orjAA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在训练中会遇到什么问题，你有什么解决方案？









**【初识生成式对抗网络（GANs）】**



**场景描述**

2014年的一天，Goodfellow与好友相约到酒吧聊天。也许平日里工作压力太大，脑细胞已耗尽了创作的激情，在酒吧的片刻放松催生了一个绝妙的学术点子，然后就有了GANs的传说。GANs全称为生成式对抗网络，是一个训练生成模型的新框架。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUib6hnfKtCQAZDECuyo5daahFxpmAfdiaZmkrKbiaBnicc0WHHpsvvwZzQqA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

GANs自提出之日起，就迅速风靡深度学习的各个角落，GANs的变种更是雨后春笋般进入人们的视野，诸如：WGAN、InfoGAN、f-GANs、BiGAN、DCGAN、IRGAN等等。

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

GANs之火，就连任何初入深度学习的新手都能略说一二。GANs刚提出时没有华丽的数学推演，描绘出的是一幅魅力感极强的故事画面，恰好契合了东方文化中太极图的深刻含义——万物在相生相克中演化，听起来很有意思。想象GANs框架是一幅太极图，“太极生两仪”，这里“两仪”就是生成器和判别器，生成器负责“生”，判别器负责“灭”，这一生一灭间有了万物。具体说来，生成器在初始混沌中孕育有形万物，判别器甄别过滤有形万物，扮演一种末日大审判的角色。回到严谨的学术语言上，生成器从一个先验分布中采得随机信号，经过神经网络的“妙手”转换，得到一个模拟真实数据的样本；判别器既接收来自生成器的模拟样本，也接收来自实际数据集的真实样本，我们不告诉判别器这个样本是哪里来的，需要它判断样本的来源。判别器试图区分这两类样本，生成器则试图造出迷惑判别器的模拟样本，两者自然构成一对“冤家”，置身于一种对抗的环境。然而，对抗不是目的，在对抗中让双方能力各有所长才是目的，理想情况下最终达到一种平衡，使得双方的能力以臻完美，彼此都没有了更进一步的空间。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibYjWhARv6GOCZFp0nY57CVpDoLTV6xjkrgNmrgyiaaeQ2scndqV036rA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**问题描述**

关于GANs，从基本理论到具体模型再到实验设计，我们依次思考三个问题：

（1）GANs可看作一个双人minimax游戏，请给出游戏的value function。我们知道在理想情况下最终会达到一个纳什均衡点，此时生成器表示为*G**，判别器表示为*D**，请给出解(*G\*, D**)和value function的值；在未达到均衡时，我们将生成器*G*固定，去寻找当前下最优的判别器*D**G**，请给出*D**G**和此时的value function。至此的答案都很容易在原论文中找到，这里进一步发问，倘若固定*D*，我们将*G*优化到底，那么解*G**D**和此时的value function是什么？

（2）发明GANs的初衷是为了更好地对概率生成模型作估计，我们知道在应用传统概率生成模型（如：马尔科夫场、贝叶斯网）时会涉及大量难以完成的概率推断计算，GANs是如何避开这类计算的？

（3）实验中训练GANs的过程会如描述的那么完美吗，求解的最小化目标函数

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUib3ia4U57Zo3ERn2scP74Vbic4gd8Ejb2dUm88SLF867ffNAMfcSd6VdVA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在训练中会遇到什么问题，你有什么解决方案？





**解答与分析**

**（1）**

在minimax游戏里，判别器的目标是将来自实际数据集的样本识别为真实样本，同时将来自生成器的样本识别为模拟样本。简单地看，这是一个二分类问题，损失函数自然是negative log-likelihood，也称为categorical cross-entropy loss或cross-entropy loss，即：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibQOtw79MuAibCHfIrtrhtejINEX7t3Oia540BgycexRsP2CQq4d4abMFQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中，*D*(*x*)表示判别器*D*预测*x*为真实样本的概率，*p*(data|*x*)和*p*(g|*x*)分别表示*x*为真实样本和模拟样本的后验概率。另外，*p*(*x*) = *P*src(*s* = data)*P*(*x*|data) + *P*src(*s* = g)*P*(*x*|g), 这里的*P*(*x*|data)即*p*data(*x*)表示从实际数据集获取样本*x*的概率，*P*(*x*|g)即*p*g(*x*)表示从生成器生成样本*x*的概率。由于假定实际数据集的和生成器的样本各占一半，即*P*src(*s* = data) *=* *P*src(*s* = g) = 1/2，我们可以得到

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUib2slHfj7JauMZibNFiawSlA3UEf0dszLqJC6pBDLHZlT7dLUrtl9QkiaOg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此，判别器最小化*L(D*)的过程可以化作最大化如下value function：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

同时，作为另一方的生成器*G*最小化该value function，故这个minimax游戏可表示为：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibVCL4Tr4syZzt9Rb7R8tHZ0H4cqDNJiadQUIQOJRuQgadnwwIVteQzYA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibU70H34tbEjAR1BucLMfsibUjICjQ2mic5VtfmYeqsKyibLKAYdTsu7rYg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们发现在优化G的过程中，最小化value function本质是在最小化生成器样本分布*p*g与真实样本分布*p*data的Jensen-Shannon divergence *JSD*(*p*data||*p*g)。

进一步考虑最终达到的均衡点，*JSD*(*p*data||*p*g)的最小值在*p*data = *p*g取到零，故最优解*G**满足*x* = *G**(*z*)~*p*data(*x*)，*D**满足*D**(*x*)≡1/2，此时value function的值为*V*(*G**, *D**) =﹣㏒4。

进一步，在训练时如果给定*D*求解当下最优的*G*，我们可以得到什么？

我们不妨假设*G'*表示前一步的生成器，给出的*D*是*G'*下的最优判别器![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)，即

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

那么，当前*G*的最小化目标变为

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibINyu5TicB10rmibHBP4GibA0SOqCxKJe5rGew23OwGoicdicWdLZqeoUbyA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**（2）**

传统概率生成模型要定义一个描述概率分布的表达式*P*(*X*)，通常是一个联合概率分布的密度函数*P*(*X*1, *X*2,…, *XN*)，然后基于此表达式做最大似然估计。这个过程少不了做概率推断计算，如：计算边缘概率*P*(*Xi*)，计算条件概率*P*(*Xi*|*Xj*)，计算作分母的partition function等。当随机变量很多时，概率模型会变得十分复杂，做概率计算变得非常困难，即使做大量近似计算，效果常不尽人意。而GANs在刻画概率生成模型时，并不对概率密度函数*P*(*X*)直接建模，而是通过直接制造样本*X*，间接地体现出分布*P*(*X*)，就是说我们实际上看不到*P*(*X*)的一个表达式。那么怎么做呢？

我们知道，如果有两个随机变量*Z*和*X*，且它们之间存在某种映射关系，*X* = *f*(*Z*)，那么它们各自的概率分布*P**X*(*X*)和*P**Z*(*Z*)也存在某种映射关系。当*Z*, *X*∈R都是一维随机变量时，

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibNw9OicNxQjxsQxgjs70uCnWmlLlumZqUYic4PHNf6wnatuTDD4Ps294g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

当*Z*, *X*是高维随机变量时，导数变成Jacobian矩阵，为*P**X* = *J PZ*。因此，已知*Z*的分布，我们对随机变量之间的转换函数*f*直接建模，就唯一确定了*X*的分布。

这样，不仅避开了大量复杂的概率计算，而且给了*f*更大的发挥空间，我们可以用神经网络来刻画*f*。我们知道，近些年神经网络领域大踏步向前发展，涌现出一批新技术来优化网络结构，除了经典的CNN和RNN结构，ReLu激活函数、Batch Normalization、Dropout等，都可以自由地添加到生成器的网络中，大大增强了生成器的表达能力。



**（3）**

在实际训练中，早期阶段的生成器*G*很差，生成的模拟样本很容易被判别器*D*识别，使得*D*回传给的*G*梯度非常非常小，达不到训练*G*的目的，这个现象称为优化饱和。为什么会出现这种现象呢？回答这个问题前，我们将判别器*D*的sigmoid输出层的前一层记为*o*，那么*D*(*x*)可表示成*D*(*x*) = sigmod(*o*(*x*))，当输入的是一个真实样本时*x*~*p*data，当输入的是一个模拟样本时*x* = *G*(*z*;*θg*), *z*~*pz*。我们看判别器*D*的导数形式

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

训练生成器的loss项的导数形式为

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibghC050kSiaM9odUb7PP07K11vThKSEGynxsYAicgGJ4BULNc5WbJntEw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibrM9rJRFgsYbgFKJowPDF0jboicAPr63VtF2LX8QYNTy2LMvlkPPnS5g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

此时生成器获得的导数基本为零，这说明判别器强大后对生成器的帮助反而变得微乎其微。怎么办呢？

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJB6TIo6b5jZyQ5PgCJUPUibUwuVYiaUqnRFsdJs37cOBan2Ve7OmSjA4Jj80nPSEObiciaQZv9rGd61A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)









**【隐马尔科夫模型】**



**场景描述**

序列标注（sequence labeling）是对一个序列的每个元素给出标签的机器学习任务。序列标注模型被应用于文本处理相关领域，包括中文分词、词性标注、语义角色标注、命名实体识别、语音识别等。我们前面已经提到过用RNN等深度学习模型解决序列标注问题，接下来我们还将回顾序列标注的一系列经典模型。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLwWkjNsVSF9bT3lo7wIFMxe2w1fQWddMzxtxzqOGoDEhTkZiatB5Nl0nvs73PrR7JhaKp4E31nTpQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**问题描述**

简述如何对中文分词问题用隐马尔科夫模型进行建模，给定一批语料，如何对模型进行训练。



**解答与分析**

*背景：隐马尔科夫模型是机器学习中一个经典的生成式模型，描述了由隐状态的马尔科夫链随机生成观测状态序列的过程，常用于解决序列标注问题，在自然语言处理、语音识别等领域有广泛的应用。*

在谈隐马尔科夫模型之前，先简单谈一谈马尔科夫过程。马尔科夫过程是满足无后效性的随机过程。假设一个随机过程中，tn时刻的状态xn的条件分布，仅仅与其前一个状态xn-1有关，即*P*(*xn*|*x*1, *x*2 … *xn*-1)＝*P*(*xn*|*xn*-1)，则将其称为马尔科夫过程，时间和状态的取值都是离散的马尔科夫过程也称为马尔科夫链。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLwWkjNsVSF9bT3lo7wIFMxNPnPmvjgibGcwiblAqxv8sCCjLXU4XVk8Ie6O7zPHiaTD3eFkLfvqm5nQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

隐马尔科夫模型是对含有未知参数（隐状态）的马尔科夫链进行建模的生成模型。在简单的马尔科夫模型中，所有状态对于观测者都是可见的，因此在马尔科夫模型里仅仅包括状态间的转移概率，而在隐马尔科夫模型中，隐状态xi对于观测者而言是不可见的，观测者能观测到的只有每个隐状态xi对应的输出yi，而观测状态yi的概率分布仅仅取决于对应的隐状态xi。在隐马尔科夫模型中，参数包括了隐状态间的转移概率、隐状态到观测状态的输出概率、隐状态*x*的取值空间、观测状态*y*的取值空间以及初试状态的概率分布。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLwWkjNsVSF9bT3lo7wIFMxe2w1fQWddMzxtxzqOGoDEhTkZiatB5Nl0nvs73PrR7JhaKp4E31nTpQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

下面我们用一个简单的例子来说明隐马尔科夫模型的建模过程。

假设现在我们有3个不同的葫芦，每个葫芦里有好药坏药若干，现在我们从这3个葫芦中按以下规则倒出药来：

1. 随机挑选一个葫芦
2. 从葫芦里倒出一颗药，记录是好药还是坏药后将药放回
3. 从当前葫芦依照一定的概率转移到下一个葫芦
4. 重复步骤2-3

在整个过程中，我们并不知道每次拿到的是哪一个葫芦。

用隐马尔科夫模型来描述以上过程，隐状态就是当前是哪一个葫芦，隐状态的取值空间为{葫芦1，葫芦2，葫芦3}，观测状态的取值空间为{好药，坏药}，初始状态的概率分布就是第1步随机挑选葫芦的概率分布，隐状态间的转移概率就是从当前葫芦转移到下一个葫芦的概率，而隐状态到观测状态的输出概率就是每个葫芦里好药和坏药的概率。记录下来的药的顺序就是观测状态的序列，而每次拿到的葫芦的顺序就是隐状态的序列。

隐马尔科夫模型包括三个基本问题：概率计算问题、预测问题、学习问题。

概率计算问题：已知模型的所有参数，计算观测序列Y出现的概率。概率计算问题可使用前向和后向算法求解。

预测问题：已知模型所有参数和观测序列Y，计算最可能的隐状态序列X。可使用维特比算法来求解最可能的状态序列，维特比算法是一个经典的动态规划算法。

学习问题：已知观测序列Y，求解使得该观测序列概率最大的模型参数。 使用Baum-Welch算法进行参数的学习，Baum-Welch算法是期望最大化算法（EM algorithm）的一个特例。

上面提到的问题和算法在此不多做介绍，感兴趣的读者可以查阅相关资料。下面回到我们开头的问题。隐马尔科夫模型通常用来解决序列标注问题，因此我们也可以将分词问题转化为一个序列标注问题来进行建模。

例如可以对中文句子中的每个字做以下标注：B表示一个词的开头一个字，E表示一个词的结尾一个字，M表示一个词中间的字，S表示一个单字词，则隐状态的取值空间为{B, E, M, S}，同时对隐状态的转移概率可以给出一些先验知识：B和M后面只能是M或者E，S和E后面只能是B或者S。而每个字就是模型中的观测状态，取值空间为语料中的所有中文字。

完成建模之后，使用语料进行训练可以分有监督训练和无监督训练。有监督训练即对语料进行标注，相当于得到了语料的所有隐状态信息，可以用简单的计数法来对模型进行极大似然估计。无监督训练可以用上文提到的Baum-Welch算法。







**【自组织映射神经网络】**



**场景描述**

自组织映射算法是无监督学习方法中一类重要方法，可以用作聚类、高维可视化、数据压缩、特征提取等多种用途。在深度神经网络大为流行的今天，谈及自组织映射神经网络（Self-Organizing Map, SOM）依然是一件非常有意义的事情，这主要是由于自组织映射神经网络融入了大量人脑神经元的信号处理机制的特点。具体而言，在人脑的感知通道上，神经元的组织是有序排列的；同时，大脑皮层会对外界特定时空信息的输入在特定区域产生兴奋，而且相类似的外界信息输入产生对应兴奋的大脑皮层区域也连续映像的。此外，在生物神经系统中，还存在着一种“侧抑制”现象，这种抑制作用会使神经细胞之间出现竞争，其结果是某些获胜，而另一些则失败，表现形式是获胜神经细胞兴奋，失败神经细胞抑制。自组织神经网络就是对上述生物神经系统功能的一种人工神经网络模拟，该模型由芬兰赫尔辛基大学教授Teuvo Kohonen于1981年提出，因此有时也称为Kohonen网络。



**问题描述**

自组织映射神经网络是如何工作的？它有什么样的特点？



**解答与分析**

自组织映射算法是无监督学习方法中一类重要方法，可以用作聚类、高维可视化、数据压缩、特征提取等多种用途。在深度神经网络大为流行的今天，谈及自组织映射神经网络依然是一件非常有意义的事情，这主要是由于自组织映射神经网络融入了大量人脑神经元的信号处理机制的特点。该模型由芬兰赫尔辛基大学教授Teuvo Kohonen于1981年提出，因此有时也称为Kohonen网络。

生物学研究表明，在人脑的感知通道上，神经元组织是有序排列的；同时，大脑皮层会对外界特定时空信息的输入在特定区域产生兴奋，而且相类似的外界信息输入产生对应兴奋的大脑皮层区域也连续映像的。例如，生物视网膜中有许多特定的细胞对特定的图形比较敏感，当视网膜中有若干个接收单元同时受特定模式刺激时，就使大脑皮层中的特定神经元开始兴奋，且输入模式接近时与之对应的兴奋神经元也接近；在听觉通道上，神经元在结构排列上与频率的关系十分密切，对于某个频率，特定的神经元具有最大的响应，位置相邻的神经元具有相近的频率特征，而远离的神经元具有的频率特征差别也较大。大脑皮层中神经元的这种响应特点不是先天安排好的，而是通过后天的学习自组织形成的。

在生物神经系统中，还存在着一种侧抑制现象，即一个神经细胞兴奋以后，会对周围其它神经细胞产生抑制作用。这种抑制作用会使神经细胞之间出现竞争，其结果是某些获胜，而另一些则失败。表现形式是获胜神经细胞兴奋，失败神经细胞抑制。自组织神经网络就是对上述生物神经系统功能的一种人工神经网络模拟。

自组织映射神经网络本质上是一个两层的神经网络，包含输入层和竞争层（输出层）。输入层模拟感知外界输入信息的视网膜，输出层模拟做出响应的大脑皮层。竞争层中神经元的个数通常是聚类的个数，代表每一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在竞争层中找到一个和它最匹配的节点，称为它的激活节点，也叫“winning neuron”；紧接着用随机梯度下降法更新激活节点的参数；同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。这种竞争可以通过神经元之间的横向抑制连接（负反馈路径）来实现。SOM的竞争层节点是有拓扑关系的。这个拓扑关系依据需求确定，如果想要一维的模型，那么隐藏节点可以是“一维线阵”；如果想要二维的拓扑关系，那么就行成一个“二维平面阵”，也如图（1）所示，也有更高维度的拓扑关系的，比如“三维栅格阵”，但并不常见。



![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLfxSibfZxGYAicGEA0Lg4EceKttwUQ4X04udPg6iagibiaFbJtDGOeic1Bw4Ls7ibhTFiaicQicBepKPbcvUPg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLfxSibfZxGYAicGEA0Lg4Ece9f8rWxFjqSLibxtD9aWicrJBobGWC5NVlTbtYHgW9tvAjNjSiaal5CxgQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图1. SOM常见的两种网络结构



自组织映射神经网络的自组织学习过程也可以归纳为以下四个子过程，其中假设输入空间是*D*维，输入模式为：*x*＝{*xi*, *i*＝1, …, *D*}，输入单元*i*和神经元*j*之间在计算层的连接权重为：*w*＝{*wi,j*, *j*＝1, …, *N*, *i*＝1, …, *D*}，其中*N*是神经元的总数：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrLfxSibfZxGYAicGEA0Lg4Eceld8x0OdsbgUMeQFz8mMAP2MVbkFU5ZZibByyAr9oGl4zbF5icy89iaZIg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

基于自组织映射神经网络，通常会引发以下常见的几个问题：

- SOM具有什么样的显著特点，为什么？

  保序映射。SOM网络可以将任意维输入模式在输出层映射为一维或者二维图形，并保持拓扑结构不变。这种拓扑映射使得“输出层神经元的空间位置对应于输入空间的特定域或特征”。由其学习过程可以看出，每个学习权重更新的效果等同于将获胜的神经元及其邻居的权向量w*i*向输入向量*x*移动，同时对该过程的迭代进行会使得网络的拓扑有序。

- 怎样设计自组织映射神经网络并设定网络训练参数？

  自组织映射神经网络设计和参数主要包括以下几个部分：

  **输出层神经元数量设定**：和训练集样本的类别数相关。若不清楚类别数量，则尽可能先设定较多的节点数，以便较好的映射样本的拓扑结构，如果分类过细再酌情减少输出节点。这样可能会带来少量从未更新过权值的 “死节点”，但此问题一般可通过重新初始化权值得到解决。

  **输出层节点排列的设计**：输出层的节点排列成哪种形式取决于实际应用的需要，排列形式应尽量直观反映出实际问题的物理意义。例如，对于一般的分类问题，一个输出节点节能代表一个模式类，用一维线阵意义明确结构简单；对于颜色空间或者旅行路径类的问题，二维平面则比较直观。

  **权值初始化问题**：可以随机初始化，但尽量使权值的初始位置与输入样本的大概分布区域充分重合，避免出现大量的初始“死节点”。一种简单易行的方法是从训练集中随机抽取m个输入样本作为初始权值。

  **拓扑邻域的设计**：拓扑领域设计原则是使领域不断缩小，这样输出平面上相邻神经元对应的权向量之间既有区别又有相当的相似性，从而保证当获胜节点对某一类模式产生最大响应时，其领域节点也能产生较大响应。领域的形状可以是正方形、六边形或者菱形。优势领域的大小用领域的半径表示，通常凭借经验来选择。

  **学习率的设计**：学习率是一个递减的函数，可以结合拓扑邻域的更新一起考虑，也可分开。在训练开始时，学习率可以选取较大的值，之后以较快的速度下降，这样有利于很快捕捉到输入向量的大致结构，然后学习率在较小的值上缓降至0值，这样可以精细地调整权值使之符合输入空间的样本分布结构。

- 自组织映射神经网络中的竞争学习是怎样实现的？

  网络的竞争层各神经元竞争对输入模式的响应机会，获胜的神经元将使得相关的各权重向更加有利于它竞争的方向调整，即以获胜神经元为中心，对近邻的神经元表现出兴奋性侧反馈，而对远邻的神经元表现出抑制性侧反馈，近邻者互相激励，远邻者相互抑制。近邻和远邻均有一定的范围，对更远邻的神经元则表现弱激励的作用。这种交互作用的方式以曲线可视化则类似于“墨西哥帽”，如图2所示。



![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrLfxSibfZxGYAicGEA0Lg4EcegHEwzJcECEt47Fth6Zt1Ot7Qnkib0YAwBice15XL0TRfluoceSwnBhQw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图2. 神经元的激励交互方式



- SOM对比K-means有何不同点？

  （1）K-Means需要事先定下类的个数，也就是K的值； SOM则不用，隐藏层中的某些节点可以没有任何输入数据属于它。所以，K-Means受初始化的影响要比较大。

  （2）K-means为每个输入数据找到一个最相似的类后，只更新这个类的参数；SOM则会更新临近的节点。所以，K-mean受noise data的影响比较大，SOM的准确性可能会比k-means低（因为也更新了临近节点）；

  （3） SOM的可视化比较好，具有优雅的拓扑关系图。











**【概率图模型】**



**场景描述**

概率图模型（Probabilistic Graphical Model）主要分为两种（如图1所示），即贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）；其中贝叶斯概率图模型可以用一个有向图结构表示，马尔可夫概率图模型可以表示成一个无向图的网络结构。概率图模型适用于对实体间的依赖关系进行建模，有向边用来建模单向的依赖，无向边用来建模双方的相互依赖关系。概率图模型包括朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等，在诸多机器学习场景中有着广泛的应用。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmnNTaw3kDHWiaxZ5y4ic1corvaTibF4HcibCIeJibkhKzasBhfHQ5ufREs9Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图1



**问题描述**

1. 能否写出图1中贝叶斯网络的联合概率分布？
2. 写出图1所示的马尔可夫网络的联合概率分布。
3. 解释朴素贝叶斯模型的原理，并给出概率图模型表示。
4. 解释最大熵模型的原理，并给出概率图模型表示。



*知识点：概率图、贝叶斯网络、马尔可夫网络*



**解答与分析**

**1. 能否写出图1中贝叶斯网络的联合概率分布？**

如图所示的贝叶斯网络中，在给定A的条件下B和C是条件独立的，因此：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmXVj8Yn4XYyP8EqQtcicMPKlCMfAcnSJp4DpzITHAGQdT8ne0cjKAWJg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在给定B和C的条件下A和D是条件独立的，因此：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmuD3WZaHL3CA9bDt4Lu87cUonqA6YYdQUXh4MmgcaFD7TicgIx56o7bQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

于是有：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmEp0AlOjBUQ3VNoNlwoVVzwc5bjkxVlJQpBlt1Vt6yC2BjE80ru1Ibg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**2. 写出图1所示的马尔可夫网络的联合概率分布。**

在马尔可夫网络中，联合概率分布的定义为：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmaXktpEV1wD8bxKdZB94mEs22215acRlMKJic6FUvEibNStppV48rwTEg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmncIcYXicuNtv0L6qe2R27tl1giaibt4xxI7dXwzjL86HWsM4XzWXic5pQA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmkTMdlV68SoAPDTJGuBR9syhOpco2fwzibf3CPQnZaSicy9obFJFK4wLQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

对于图中所有节点*x*＝{*x1, x2, …, xn*}所构成的一个子集，如果在这个子集中，任意两点之间都存在边相联，则这个子集中的所有节点构成了一个团。如果在这个子集中加入任意其他节点，都不能构成一个团，则称这样的子集构成了一个最大团。

在图1所示的网络结构中，我们可以看到（A，B），（A，C），（B，D），（C，D）构成团，同时也是最大团。因此联合概率分布可以表示如下：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

如果采用如上定义的指数函数作为势函数，则有：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmtt2fyRb1zzUrxhNOMMlxibetWuzIskXMBibqv2ezaroZAAgvYera3ibrA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**3. 解释朴素贝叶斯模型的原理，并给出概率图模型表示。**

朴素贝叶斯模型通过预测指定样本属于特定类别的概率P(yi|**x**)来预测该样本的所属类别。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmRVFfeSoNGq7L6g5FkaLVGZrKtTyic0vqia7xicpB100wCIW4axDU7Hlmg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

P(yi|**x**)可以写成如下的形式：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmNDu0ia75VqfbfhHVpiaDrEdia3UUic54dyfOkwiaM5EBT8E0GdXzfK2bDQQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中**x**＝(*x1, x2, …, xn*)为样本对应的特征向量，*P*(**x**)为样本的先验概率。对于特定的样本**x**和任意类别yi，*P*(**x**)的取值均相同，并不会影响P(yi|**x**)取值的相对大小，因此在计算中可以被忽略。并且假设特征*x1, x2, …, xn*相互独立，可以得到：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmVkXEdbzf9HibZArHPs7WA1qLLZ23vnduTnLgVBTkq1bibWjYqyYEhwYg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中P(x1|yi), P(x2|yi), …, P(xn|yi)，以及*P*(yi)可以通过训练样本统计得到。可以看到后验概率P(xj|yi)的取值决定了分类的结果，并且任意特征xj都由yi的取值所影响。因此概率图模型可以用下图表示：

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibm624PW56lpQPIYsnpSZGv5nHJFnmmGXcz4cFbyUU3THicIo7QIibFNfgA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

注：上图的表示为盘式记法。盘式记法是一种简洁的概率图模型表示方法，如果变量y同时对*x1, x2, …,**xN*这N个变量产生影响，则可以简记成如图的形式。



**4. 解释最大熵模型的原理，并给出概率图模型表示。**

信息是指人们对事物理解的不确定性的降低或消除，而熵就是这种不确定性的度量，熵越大，不确定性也就越大。最大熵原理是概率模型学习的一个准则，指导思想是在满足约束条件的模型集合中选取熵最大的模型，即不确定性最大的模型。在平时生活中，我们也会有意无意地使用最大熵的准则，例如人们常说的鸡蛋不能放在一个篮子里，就是指在事情具有不确定性的时候，我们倾向于尝试它的多种可能性，从而降低结果的风险。同时，在我们摸清了事情背后的某种规律之后，我们可以加入一个约束，将不符合规律约束的情况排除，在剩下的可能性中去寻找使得熵最大的决策。

假设离散随机变量X的分布是P(X)，则关于分布P的熵定义为：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

可以看出当X服从均匀分布时对应的熵最大，也就是不确定性最高。

给定离散随机变量X和Y上的条件概率分布P(Y|X)，定义在条件概率分布上的条件熵为：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其中![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)为样本在训练数据集上的经验分布，即X的各个取值在样本中出现的频率统计。

最大熵模型就是要学习到合适的分布P(Y|X)，使得条件熵H(P)的取值最大。在我们对训练数据集一无所知的情况下，最大熵模型认为P(Y|X)是符合均匀分布的。那么当我们有了训练数据集之后呢？我们希望从中找到一些规律，从而消除一些不确定性，这时就需要用到特征函数f(x,y)。特征函数f描述了输入x和输出y之间的一个规律，例如当x=y时，f(x,y)等于一个比较大的正数。为了使学习到的模型P(Y|X)能够正确捕捉训练数据集中的这一规律（特征），我们加入一个约束，使得特征函数f(x,y)关于经验分布![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmp9lkeFkpgpEibKNJvXRTFujIADOlSsVQHSicFBqKaZdd6GChxoWApNuA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的期望值与关于模型P(Y|X)和经验分布![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibm3zViaL5JEeK4cibnlUHhMJcnlMB3jLS6IUluPZsFxPXeI5aTIkg1UCYA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的期望值相等，即

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibm35libepmffsMTDre4zd9KVzLNibBsd6C2icbvoNq0n9cJqlEy71El6guw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmSoSelAMEGUGrxgCrOqdICSa9gGxJ7WLOcSrMusGB6LvvtY0iaBe6f4w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

求解之后可以得到最大熵模型的表达形式：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmA0puGEY21dJMrjFoOAmCqJE62ic8uibHiaBWribDL83xPU5V1oztUZDdsQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

最终，最大熵模型归结为学习最佳的参数**w**，使得Pw(y|x)最大化。从概率图模型的角度理解，我们可以看到Pw(y|x)的表达形式非常类似于势函数为指数函数的马尔可夫网络，其中变量X和Y构成了一个最大团，如下图所示：

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIkRl615ChIbiaLFahQHA4ibmLibdwoWlicLu6r84M3ic2nojGibLo7fa1FF7RQkBf44Nvlib7GTlLTzTcmw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)









**【WGANs：抓住低维的幽灵】**



**场景描述**

看过《三体3》的朋友，一定听说过“降维打击”这个概念，像拍苍蝇一样把敌人拍扁。其实，低维不见得一点好处都没有。想象猫和老鼠这部动画的一个镜头，老鼠Jerry被它的劲敌Tom猫一路追赶，突然Jerry发现墙上挂了很多照片，其中一张的背景是海边浴场，沙滩上有密密麻麻的很多人，Jerry一下子跳了进去，混在人群中消失了，Tom怎么也找不到Jerry。 三维的Jerry变成了一个二维的Jerry，躲过了Tom。一个新的问题是：Jerry对于原三维世界来说是否还存在？ 极限情况下，如果这张照片足够薄，没有厚度，那么它就在一个二维平面里，不占任何体积，体积为零的东西不就等于没有吗！拓展到高维空间中，这个体积叫测度，无论N维空间的N有多大，在N+1维空间中测度就是零，就像二维平面在三维空间中一样。因此，一个低维空间的物体，在高维空间中忽略不计。对生活在高维世界的人来说，低维空间是那么无足轻重，像一层纱，如一个幽灵，似有似无，是一个隐去的世界。

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

2017年，一个训练生成对抗网络的新方法——WGAN被提出。在此之前，GANs已提出三年，吸引了很多研究者来使用它。原理上，大家都觉得GANs的思路实在太巧妙，理解起来一点都不复杂，很符合人们的直觉，万物不都是在相互制约和对抗中逐渐演化升级吗。理论上，Goodfellow在2014年提出GANs时，已经给出GANs的最优性证明，证明GANs本质上是在最小化生成分布与真实数据分布的Jensen-Shannon Divergence，当算法收敛时生成器刻画的分布就是真实数据的分布。但是，实际使用中发现很多解释不清的问题，生成器的训练会很不稳定。生成器这只Tom猫，很难抓住真实数据分布这只老鼠Jerry。



**问题描述**

请思考：原GANs中存在哪些问题，会成为制约模型训练效果的瓶颈；WGAN针对这些问题做了哪些改进； WGAN算法的具体步骤；并写出WGAN的伪代码。



*知识点：JS距离、坍缩模式、Wasserstein距离、*

*1-Lipschitz函数*



**解答与分析**

**1. GANs的陷阱：请回答原GANs中存在哪些问题，成为了制约模型训练效果的瓶颈。**

难度：★★★

GANs的判别器试图区分真实样本和生成的模拟样本。Goodfellow在论文中指出，训练判别器，实际是在度量生成器分布和真实数据分布的Jensen-Shannon Divergence，也称JS距离； 训练生成器，是在减小这个JS距离。这是我们想要的，即使我们不清楚形成真实数据的背后机制，还是可以用一个模拟生成过程去替代之，只要它们的数据分布一致。

但是实验中发现，训练好生成器是一件很困难的事，生成器很不稳定，常出现坍缩模式(Collapse Mode)。什么是坍缩模式？拿图片举例，反复生成一些相近或相同的图片，多样化太差。生成器似乎将图片记下，没有更高级的泛化，更没有造新图的能力，好比一个笨小孩被填鸭灌输了知识，只会死记硬背，没有真正理解，不会活学活用，更无创新能力。

为什么会这样？既然训练生成器基于JS距离，猜测问题根源可能与JS距离有关。高维空间中不是每点都能表达一个样本（如一张图片），空间大部分是多余的，真实数据蜷缩在低维子空间的流形（即高维曲面）上，因为维度低，所占空间体积几乎为零，就像一张极其薄的纸飘在三维空间，不仔细看很难发现。考虑生成器分布与真实数据分布的JS距离，即两个Kullback-Leibler (KL)距离的平均

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJrDmgsibCz3QDibennakCchrVDSDC3v2CDNDw8ycmkSvEovb5lqunXhDA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

初始的生成器，由于参数随机初始化，与其说是一个样本生成器，不如说是高维空间点的生成器，点广泛分布在高维空间中。打个比方，生成器将一张大网布满整个空间，“兵力”有限，网布得越大，每个点附近的兵力就越少。想象一下，当这张网穿过低维子空间时，所剩的“兵”几乎为零，成了一个“盲区”，如果真实数据全都分布在这，就对生成器“隐身”了，成了“漏网之鱼”。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJSiats3GEGRD6mgbiaq1s81SOVPDSjC0Wq0aLEXZ0SaGFcsRSehHRfDGA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

回到公式，看第一个KL距离：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJMFCblCkRkQiazzLGhr1xfU2xFTP7fxRhbMQ2rabIOticoWwfOicFDqS9A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

高维空间绝大部分见不到真实数据，处处为零，对KL距离的贡献为零；即使在真实数据蜷缩的低维空间，高维空间会忽略低维空间的体积，概率上讲测度为零。KL距离就成了：∫ ㏒2·pr(x)dμ(x)＝㏒2。

再看第二个KL距离：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJ4UicEgG5YNF6NaqjCusgaSia5gPahoWBLicuWQ6bvlhCOgO1lnn8cpNhA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

同理KL距离也为：∫ ㏒2·pg(x)dμ(x)＝㏒2。因此，JS距离为㏒2，一个常量。无论生成器怎么“布网”，怎么训练，JS距离不变，对生成器的梯度为零。训练神经网络是基于梯度下降的，用梯度一次次更新模型参数，如果梯度总是零，训练还怎么进行。



**2. 破解陷阱的武器：请回答WGAN针对前面问题做了哪些改进，以及什么是Wasserstein距离。**

难度：★★★★

直觉告诉我们：不要让生成器傻傻地在高维空间布网，让它直接到低维空间“抓”真实数据。道理是这样，但是高维空间中藏着无数的低维子空间，怎么找到目标子空间呢？站在大厦顶层，环眺四周，你可以迅速定位远处的山峦和高塔，却很难知晓一个个楼宇间办公室里的事情。你需要线索，而不是简单撒网。处在高维空间，对抗隐秘的低维空间，不能再用粗暴简陋的方法，需要有特殊武器，这就是Wasserstein距离，也称推土机距离（Earth Mover distance）：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJic6qpwso9ZUX0oqUeKu3zGNVvvhjeG9LcBd9mGuqKTIGjmSqiaZIDwLQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJNOf3MCx8h7co7VFicLaZdzTaXExYwohiclCnjIicLQXRRHwJaLr6wV6Gw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

怎么理解这个公式？想象你有一个很大的院子，院子里几处坑坑洼洼需要填平，四个墙角都有一堆沙子， 沙子总量正好填平所有坑。搬运沙子很费力，你想知道有没有一种方案，使得花的力气最少。直觉上，每个坑都选择最近的沙堆，搬运的距离最短，但是这里面有个问题，如果最近的沙堆用完了怎么办，或者填完坑后近处还剩好多沙子，或者坑到几个沙堆的距离一样，我们需要设计一个系统的方案，通盘考虑这些问题。最佳方案是上面目标函数的最优解。可以看到，沙子分布给定，坑分布给定，我们关心搬运沙子的整体损耗，而不关心每粒沙子的具体摆放，在损耗不变的情况下，沙子摆放可能有很多选择。对应上面的公式，当你选择一对(*x*, *y*)时，表示把*x*处的一些沙子搬到*y*处的坑，可能搬部分沙子，也可能搬全部沙子，可能只把坑填一部分，也可能都填满了。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJHANV47reJUlwATgmTurWpyEnOK60GDrS1YjP4UjE3LZlUx7uxz1tiaQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为什么Wasserstein距离能克服JS距离解决不了的问题？理论上的解释很复杂，要证明当生成器分布随参数*θ*变化而连续变化时，生成器分布与真实分布的Wasserstein距离，也随*θ*变化而连续变化，并且几乎处处可导，而JS距离不保证随*θ*变化而连续变化。

通俗的解释，接着“布网”的比喻，现在生成器不再“布网”，改成“定位追踪”了，不管真实分布藏在哪个低维子空间里，生成器都能感知它在哪，因为生成器只要将自身分布稍作变化，就会改变它到真实分布的推土机距离，而JS距离是不敏感的，无论生成器怎么变化，JS距离都是一个常数。因此，使用推土机距离，能有效锁定低维子空间中的真实数据分布。



**3. WGAN之道：请回答怎样具体应用Wasserstein距离实现WGAN算法。**

难度：★★★★★

一群大小老鼠开会，得出结论：如果在猫脖上系一铃铛，每次它靠近时都能被及时发现，那多好！唯一的问题是：谁来系这个铃铛？现在，我们知道了推土机距离这款武器，那么怎么计算这个距离？推土机距离的公式太难求解。幸运的是，它有一个孪生兄弟，和它有相同的值，这就是Wasserstein距离的对偶式：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJUcLficUTIkSUI6OlWSd7ezx6b61kZVgczPdYgrXAoff4aRvb1ricZdNQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

细心的你会发现，这里的*f*与*D*不同，前者要满足||*f*||*L*≤1，即1-Lipschitz函数，后者是一个Sigmoid函数。要求在寻找最优函数时，一定要考虑个“界”，如果没有限制，函数值会无限大或无限小。Sigmoid函数的值有天然的界，而1-Lipschitz不是限制函数值的界，而是限制函数导数的界，使得函数在每点上的变化率不能无限大。神经网络里如何体现1-Lipschitz或K-Lipschitz呢？WGAN的作者思路很巧妙，在一个前向神经网络里，输入经过多次线性变换和非线性激活函数得到输出，输出对输入的梯度，绝大部分都是由线性操作所乘的权重矩阵贡献的，因此约束每个权重矩阵的大小，可以约束网络输出对输入的梯度大小。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJ121YRxIArrHIPZFhKm4LnkxJoHo5icGOLaD9UXN4WU3Nwrt5AcSGgoA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

判别器在这里换了一个名字，叫评分器(Critic)，目标函数由区分样本来源，变成为样本打分，越像真实样本分数越高，否则越低，有点类似SVM里margin的概念。打个龟兔赛跑的比方，评分器是兔子，生成器是乌龟，评分器的目标是甩掉乌龟，让二者的距离（或margin）越来越大，生成器的目标是追上兔子。严肃一点讲，训练评分器就是计算生成器分布与真实分布的Wasserstein距离；给定评分器，训练生成器就是在缩小这个距离。因此，算法中要计算Wasserstein距离对生成器参数*θ*的梯度，

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrIBgPFcclVLF3TA0N63GVsJB3htibbxFX1SLkQIHlTBnKGoJrYncKJEIPibpa1qKTJ4KQDI8ZAcJomw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

再通过梯度下降法更新参数，让Wasserstein距离变小。



扩展阅读：

\1. Martin Arjovsky, Soumith Chintala, Léon Bottou, *Wasserstein GAN*, 2017

\2. Martin Arjovsky, Léon Bottou, *Towards Principled Methods for Training Generative Adversarial Networks*, 2017













**【常见的采样方法】**



**场景描述**

对于一个随机变量，我们通常用概率密度函数来刻画该变量的概率分布特性。具体来说，给定随机变量的一个取值，我们可以根据概率密度函数来计算该值对应的概率（密度）；反过来，也可以根据概率密度函数提供的概率分布信息来生成随机变量的一个取值，这就是采样。因此，从某种意义上来说，采样是概率密度函数的逆向应用。与根据概率密度函数计算样本点对应的概率值不同，采样过程（即根据概率分布来选取样本点）往往没有那么直接，通常需要依据待采样分布的具体特点来选择合适的采样策略 。 在“采样”章节的前两个小问题中，我们展示了采样的一个具体应用（不均衡样本集的处理），以及针对特定分布（高斯分布）而特别设计的采样方法；接下来，我们来关注一些通用的采样方法和采样策略。



**问题描述**

抛开那些针对特定分布而精心设计的采样方法外，说一些你所知道的通用采样方法或采样策略，简单描述它们的主要思想以及具体操作步骤。



*背景知识：概率与统计、采样*



**解答与分析**

在之前采样章节的推送中我们提到，几乎所有的采样方法都是以均匀分布的采样作为基本操作。均匀分布随机数一般用线性同余法来产生（伪随机数），这里不再赘述，我们假设已经可以生成 [0,1] 上的均匀分布随机数。

对于一些简单的分布，可以直接用基于均匀采样的方法来产生样本点（如有限离散分布可以用轮盘赌算法来采样）；然而，很多分布一般不好直接进行采样，此时可以考虑函数变换法。一般地，如果随机变量*x*和*u*存在变换关系*u*＝*φ*(*x*)，则它们的概率密度函数有如下关系：*p*(*u*)|*φ*'(*x*)|＝*p*(*x*)。因此，如果从目标分布*p*(*x*)中不好采样*x*，可以构造一个变换*u*＝*φ*(*x*)，使得从变换后的分布*p*(*u*)中采样*u*比较容易，这样可以通过先对*u*进行采样然后通过反函数*x*＝*φ*-1(*u*)来间接得到*x*。如果是高维空间的随机向量，则上述|*φ*'(*x*)|对应Jacobian行列式。

特别地，在上述函数变换法中，如果变换关系*φ*(·)是累积分布函数的话，则得到所谓的**逆变换采样法** **(Inverse Transform Sampling)**：假设待采样的目标分布的概率密度函数为*p*(*x*)，它的累积分布函数为

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjz63QaT9aJLg5N3OKGmtJOLulRX1xmYmJo2peduiaLBhkdyTDseT9Sicg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

按如下过程进行采样：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjuFmgic8YpSdaOZvVCYfBMb4MecynviazvzNnSTkibgxoVaib4ovyibphA1g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

根据函数变换法，上述采样过程得到的*xi*服从*p*(*x*)分布。图1是逆变换采样法的示意图。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjauc8XmdiboicSItyibDHLfxeK18u2tnc8Wq81D7ToLoQxJXbj6R2q49lA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图1  逆变换采样法示意图



如果待采样的目标分布的累积分布函数的逆函数无法求解或者不容易计算，则不适用于逆变换采样法。此时可以构造一个容易采样的参考分布，先对参考分布进行采样，然后对得到的样本进行一定的后处理操作，使得最终的样本服从目标分布。常见的拒绝采样、重要性采样，就属于这类采样算法，下面分别简单介绍它们的具体采样过程。



**拒绝采样 (Rejection sampling)**，又叫接受/拒绝采样 (Accept-Reject Sampling)。对于目标分布*p*(*x*)，选取一个容易采样的参考分布*q*(*x*)，使得*p*(*x*)≤*M*·*q*(*x*)，则可以按如下过程进行采样：

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

通过简单的推导，可以知道最终得到的*xi*服从目标分布*p*(*x*)。如图2(A)所示，拒绝采样法的关键是为目标分布*p*(*x*)选取一个合适的包络函数*M*·*q*(*x*)：包络函数越紧，每次采样时样本被接受的概率越大，采样效率越高。在实际应用中，有时候寻找解析形式的*q*(*x*)比较困难，因此延伸出了**自适应拒绝采样 (Adaptive Rejection Sampling)**，在目标分布是对数凹函数时，用分段线性函数来覆盖目标分布的对数㏑*p*(*x*)，如图2(B)所示，这里不再细述。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjzAYLiaFzl2XaQumtMUJzcIRvp9rUIplkxRvVARp32gFTxg3nWhkETicQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图2  (A) 拒绝采样示意图；(B) 自适应拒绝采样法



**重要性采样 (Importance Sampling)**，其实是用于计算函数*f*(*x*)在目标分布*p*(*x*)上的积分，即

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjVWrI6gZo4RSiavnfcBBCCjpLzXibjtEIBreKL5Kgw0Awia01jrkvApJ9w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里*w*(*x*)可以看成是样本*x*的重要性权重。由此，可以先从参考*q*(*x*)分布中抽取*N*个样本{*xi*}，然后利用如下公式来估计*I*[*f*]：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjtUsyNkBGAwJ8tpgvaLzERVAvDXNtDPlLNiaPI85ichkqMBNdQ5lFdGvw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



如果不需要计算函数积分，只想对目标分布*p*(*x*)进行采样，则可以用**重要性重采样 (Sampling-Importance Re-sampling, SIR)**，即在从参考分布*q*(*x*)抽取*N*个样本{*xi*}后，按照它们对应的重要性权重{*w(**xi*)}对这些样本进行重新采样（这是一个简单的针对有限离散分布的采样），最终得到的样本服从目标分布*p*(*x*)。

![img](https://mmbiz.qpic.cn/mmbiz_png/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjChaibEbj1rHbI4UHroP3PfmlYS9CL5wsPqCDKNe1vU1bR9ovzcanjfg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图3  重要性采样示意图



在实际应用中，如果是高维空间的随机向量，拒绝采样/重要性重采样经常难以寻找合适的参考分布，采样效率低下（样本的接受概率小或重要性权重低），此时可以考虑马尔科夫蒙特卡洛采样法 (Markov Chain Monte Carlo, MCMC) 。MCMC基本思想是：针对待采样的目标分布，构造一个马尔科夫链，使得该马尔科夫链是收敛的，并且最终的稳态分布就是我们要采样的目标分布。实际操作中，核心点是构造合适的马尔科夫链，不同的马尔科夫链对应着不同的MCMC算法，常见的有Metropolis-Hastings算法和Gibbs算法。在后续的微信推送中，我们会有专门针对MCMC的问答小节，这里只简单介绍Metropolis-Hastings算法和Gibbs算法的具体操作步骤。



**Metropolis-Hastings (MH) 采样**：对于目标分布*p*(*x*)，首先选择一个容易采样的参考条件分布*q*(*x**|*x*)，并令

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjh0OOMooSeZpRomeQSRX2dJLVu9upsO8ticP4zlr3xrpVdmSl7icxztCA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

然后根据如下过程进行采样：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yj5PqmWjvaPicXmfDWQCxCWtVuXB5NjEo3qXLg5Axj4H1It7fYt23YICA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以证明，上述过程得到的样本序列{…, *x*(*t-*1), *x*(*t*), …}最终会收敛到目标分布*p*(*x*)。



**Gibbs采样**：Gibbs采样的核心思想是每次只对样本的一个维度进行采样和更新。对于目标分布*p*(*x*)，其中*x*＝(*x*1, *x*2,…, *xd*)是高维向量，按如下过程进行采样：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/QicE4QDr3FrJZ1pTpr0k5eehnfTvgt6yjdbDvamXdpibKeTWhiaX8Jlc6Wkn0t24dlfOXm6MbGhDmCMOoR0pnk2oA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

同样可以证明，上述过程得到的样本序列{…, *x*(*t-*1), *x*(*t*), …}会收敛到目标分布*p*(*x*)。另外，步骤2中对样本每个维度的抽样和更新操作，不是必须按下标顺序进行的，可以是随机顺序 。



**扩展与总结**

上述解答中我们只是列举了几个最常用的采样算法，简单介绍了它们的具体操作。在实际面试时，可以让面试者选择其最熟悉的某个采样算法来回答，展开来问一下该算法的理论证明、优缺点，以及相关扩展等。











