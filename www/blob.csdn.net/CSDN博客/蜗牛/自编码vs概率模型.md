
# 自编码vs概率模型 - 蜗牛 - CSDN博客


2015年05月09日 12:20:14[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：991


## ML算法的概率解释
我最喜欢的理论学习论文是用概率框架来解释启发式学习算法，并且我发现他们正在做一些深刻而有意义的事情。被训练成一个贝叶斯，我认为深刻的东西通常是指统计推断或拟合统计模型。一个实例是K-means算法。K-均值作为聚类算法在直观上很容易理解。但是，当我们观察到它实际上是高斯混合模型上期望最大化的一个特例时才真正的理解它做了什么。某种特殊情况下的解释让我们更好地明白算法的期望行为，也使我们能够对可能会失败的情况做预测，并扩展到它处理不好的情况。
下面是我最喜欢论文的一些例子：
Sam Roweis和Zoubin Ghahramani的“线性高斯模型的统一回顾”可能是我一直以来最爱的。现在，它看起来微不足道，但是当它被公布时，将是一篇有影响力的论文，该论文连接和解释了主成分分析，独立成分分析，卡尔曼滤波，HMMS，混合建模和其他类似于同一家族模型的特殊情况。我觉得本文对将生成模型和无监督学习作为一个整体的想法做出了很大的贡献。 Zoubin相关的无监督学习笔记仍然是了解这些技术的经典资源。
Rich Turner和Maneesh Sahani的“缓慢特征分析的最大似然解释”做的事情非常相似。它可能没有Sam和Zoubin的论文影响力大，但有一个非常相似的味道- 它从神经网络的文献中采取缓慢特征分析，并用与PCA或卡尔曼滤波相同的线性高斯模型框架来重新解释它。
我们做同样事情的尝试（shameless plug）记录在我与David Duvenaud的论文“最优加权羊群效应是贝叶斯正交的“，在文章中我们用贝叶斯正交的框架重新解释了内核羊群效应，从而给出启发式方法的一个概率基础。
## 我最喜欢的论文
这是我最喜欢的论文：”广义的去噪自动编码器作为生成模型“。这里是我对这篇论文和这个研究内容的直观理解。这与实际结果有点不同，就像数学和类比不是无懈可击的，但我仍然认为这种解释比较好：
通过训练深度神经网络，从被噪声污染版本Zi中重构单个数据点Xi就是去噪自动编码（DAE）的工作。它们通常具有低维中间层或“瓶颈”层，这迫使这层欠完全的表示来捕捉数据中更多的信号和规律，而忽略了不相关的维度和噪音。 DAE是成功的，因为它把无监督学习（解释数据分布p（x））的难题转换到监督问题（从z中预测x），我们可以用目前的方法很容易地解决。
对我来说自编码似乎是一个黑客，我没有一个好的方法来解释他们。但事实证明，我们可以将去噪自编码理解为伪似然学习的一个特例。这样考虑：不要想着拟合概率模型p（x；o）的数据x，而是你学习数据的一个联合概率分布p（x，z；o）和它的噪音污染版本z。噪声污染是由我们人为引入的，服从一个污染分布Pnoise。如果你学会了联合模型p（x，z；o），这也意味着得到了一个生成模型![这里写图片描述](https://img-blog.csdn.net/20150509020051094)[ ](https://img-blog.csdn.net/20150509020051094)
为了拟合联合模型到观察（xi，zi），我们将使用能匹配以下伪似然得分规则的得分作为目标函数：
![这里写图片描述](https://img-blog.csdn.net/20150509020455983)[ ](https://img-blog.csdn.net/20150509020455983)
![这里写图片描述](https://img-blog.csdn.net/20150509020609272)[ ](https://img-blog.csdn.net/20150509020609272)
我们知道，伪似然是非常合适的得分规则，因此如果我们最小化损失函数*l*，我们得到o的一致估计。所以，到现在为止，我们没有做逃避的事情，一直是统计模型与数据的拟合。
现在，我们实际上知道了z|x的真实分布是Pnoise，因此，我们可以限制我们模型的类别以便p（z|x；o）=Pnoise，这样一来，目标函数的第二项变成了一个常数，所以我们留下的正好是去噪自编码的训练规则。我们从得分匹配中推导出自动编码，这与估计统计模型是一致的。
这是非常打马虎眼的，但是这就是它的意思：去噪自编码训练可以被解释为伪似然学习，前提条件是：如果自编码神经网络表示为pDAE（x|z；o）并且我们用pnoise来产生噪声，那么存在一个联合概率分布p（x，z；o）使得
![这里写图片描述](https://img-blog.csdn.net/20150509114554946)[ ](https://img-blog.csdn.net/20150509114554946)
这种观察的重要性在于更喜欢概率模型和得分匹配（如最大似然）的机器学习研究员现在不太可能不理会深度自编码。
说实在的，我仍然有点把深度学习作为一种实用的黑客。它是一种简单的方式，抛开问题的计算能力反而是更明确的给先验/已知的不变式建模，例如：图模型。但即便如此，我现在更想看自编码的无监督学习，不只是作为一个实际有用的黑客，也作为一个聪明的训练方法。我现在对它有很好的理解，以及如何让它适应我喜欢框架。这将很容易让它加入到最喜欢的论文列表。
ps：关于文中的论文会在以后上传，敬请期待哦！
[
						](https://img-blog.csdn.net/20150509114554946)
[
	](https://img-blog.csdn.net/20150509114554946)
