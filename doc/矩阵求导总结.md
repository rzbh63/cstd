# 矩阵求导总结



[知乎：如何理解矩阵对矩阵求导](https://www.zhihu.com/question/39523290)

[affine/linear(仿射/线性)变换函数详解及全连接层反向传播的梯度求导](https://blog.csdn.net/oBrightLamp/article/details/84333111)

[简书：机器学习-矩阵求导](https://www.jianshu.com/p/e818917ffd9d)

[5分钟学会矩阵求导](https://uqer.io/v3/community/share/596da6e8f83a2100527016b0)

[矩阵求导计算法则 例题](http://blog.sina.com.cn/s/blog_4a033b090100pwjq.html)

[矩阵求导公式](http://www.cnblogs.com/grandyang/p/4010890.html)

[矩阵求导总结](https://yq.aliyun.com/articles/411340)

[Optimizing RNN performance](https://svail.github.io/rnn_perf/)

 

[反向传播原理 & 卷积层backward实现<一>](https://zhuanlan.zhihu.com/p/33802329)

[反向传播之六：CNN 卷积层反向传播](https://zhuanlan.zhihu.com/p/40951745)

[深度学习 CNN卷积神经网络 LeNet-5详解](https://blog.csdn.net/happyorg/article/details/78274066)

[深度学习与求导链式法则](https://zhuanlan.zhihu.com/p/37082439)

 

★★★★★

[刘建平：卷积神经网络(CNN)反向传播算法](https://www.cnblogs.com/pinard/p/6494810.html)

[Python实现卷积神经网络】：推导卷积层的W,b的梯度](https://blog.csdn.net/weixin_37251044/article/details/81910932)

[零基础入门深度学习(三)：卷积神经网络](https://dbaplus.cn/news-21-712-1.html)

[Python教程-numpy.pad](https://blog.csdn.net/hustqb/article/details/77726660)







# 如何理解矩阵对矩阵求导？

我在矩阵理论里面没有找到这个。按照wikipedia上的定义，可以把矩阵对矩阵的导数写成一个超矩阵，但是我不懂得如何运用这个超矩阵。 具体到实际问题，因…显示全部

关注者

1,255

被浏览

111,293

关注问题写回答

5 条评论

分享









#### 14 个回答

默认排序

[![猪猪专业户](https://pic3.zhimg.com/v2-b58b3bb75d9aa70ccff4b5896269d88e_xs.jpg)](https://www.zhihu.com/people/yuangao)

[猪猪专业户](https://www.zhihu.com/people/yuangao)

Applied Mathematics PhD

301 人赞同了该回答

如果题主学过泛函分析，可能会更容易理解矩阵对矩阵的求导。

**定义：**假设![X](https://www.zhihu.com/equation?tex=X)和![Y](https://www.zhihu.com/equation?tex=Y)为赋范向量空间，![F: X\rightarrow Y](https://www.zhihu.com/equation?tex=F%3A+X%5Crightarrow+Y)是一个映射，那么![F](https://www.zhihu.com/equation?tex=F)在![x_0 \in X](https://www.zhihu.com/equation?tex=x_0+%5Cin+X)可导的意思是说存在一个有界线性算子![L \in \mathcal{L}(X, Y)](https://www.zhihu.com/equation?tex=L+%5Cin+%5Cmathcal%7BL%7D%28X%2C+Y%29)，使得对于任意的![\epsilon > 0](https://www.zhihu.com/equation?tex=%5Cepsilon+%3E+0)都存在![\delta > 0](https://www.zhihu.com/equation?tex=%5Cdelta+%3E+0)，对于满足![x \in X \backslash \{x_0\}, \|x - x_0\| < \delta](https://www.zhihu.com/equation?tex=x+%5Cin+X+%5Cbackslash+%5C%7Bx_0%5C%7D%2C+%5C%7Cx+-+x_0%5C%7C+%3C+%5Cdelta)的![x](https://www.zhihu.com/equation?tex=x)都有![\frac{\|F(x) - F(x_0) - L(x - x_0)\|}{\|x - x_0\|} < \epsilon](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5C%7CF%28x%29+-+F%28x_0%29+-+L%28x+-+x_0%29%5C%7C%7D%7B%5C%7Cx+-+x_0%5C%7C%7D+%3C+%5Cepsilon).我们称![L](https://www.zhihu.com/equation?tex=L)为![F](https://www.zhihu.com/equation?tex=F)在![x_0](https://www.zhihu.com/equation?tex=x_0)点的导数。

取一些特殊情况，比如当![X = \mathbb R^n, Y = \mathbb R](https://www.zhihu.com/equation?tex=X+%3D+%5Cmathbb+R%5En%2C+Y+%3D+%5Cmathbb+R)的时候![L](https://www.zhihu.com/equation?tex=L)就被称作梯度；当![X = \mathbb{R}^n, Y = \mathbb{R}^m](https://www.zhihu.com/equation?tex=X+%3D+%5Cmathbb%7BR%7D%5En%2C+Y+%3D+%5Cmathbb%7BR%7D%5Em)的时候![L](https://www.zhihu.com/equation?tex=L)被称作雅可比，等等。从这个一般化的定义出发的好处是，我们可以更好的理解矩阵到矩阵映射的"导数"，甚至是从一个函数空间到另一个函数空间的“导数"。

以上定义有一个等价的表述，往往计算起来更方便：对于距离![x_0](https://www.zhihu.com/equation?tex=x_0)足够近的点![x](https://www.zhihu.com/equation?tex=x)，即![\lim_{x \rightarrow x_0}\frac{o(\|x-x_0\|)}{\|x-x_0\|} = 0](https://www.zhihu.com/equation?tex=%5Clim_%7Bx+%5Crightarrow+x_0%7D%5Cfrac%7Bo%28%5C%7Cx-x_0%5C%7C%29%7D%7B%5C%7Cx-x_0%5C%7C%7D+%3D+0)，有
![F(x) = F(x_0) + L(x - x_0) + o(\|x - x_0\|).](https://www.zhihu.com/equation?tex=F%28x%29+%3D+F%28x_0%29+%2B+L%28x+-+x_0%29+%2B+o%28%5C%7Cx+-+x_0%5C%7C%29.)
（注：此处![L(x-x_0)](https://www.zhihu.com/equation?tex=L%28x-x_0%29)应该理解为线性算子![L](https://www.zhihu.com/equation?tex=L)在![x - x_0](https://www.zhihu.com/equation?tex=x+-+x_0)这个点的值，而不是![L](https://www.zhihu.com/equation?tex=L)乘以![x-x_0](https://www.zhihu.com/equation?tex=x-x_0)。不过在有限维空间所有线性算子都可以用矩阵表述，![L](https://www.zhihu.com/equation?tex=L)在![x - x_0](https://www.zhihu.com/equation?tex=x+-+x_0)这个点的值便正好可以表述为矩阵与向量的乘积！这个notation正好巧妙的一致。）

**例子：**假设![F(X) = X^TX](https://www.zhihu.com/equation?tex=F%28X%29+%3D+X%5ETX)是一个![\mathbb{R}^{m\times n} \rightarrow \mathbb{ S}^n](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes+n%7D+%5Crightarrow+%5Cmathbb%7B+S%7D%5En)的映射，其中![\mathbb{S }^n](https://www.zhihu.com/equation?tex=%5Cmathbb%7BS+%7D%5En)为n维对称阵的空间。那么![F](https://www.zhihu.com/equation?tex=F)的导数![L](https://www.zhihu.com/equation?tex=L)就应该是![\mathbb{R}^{m\times n} \rightarrow \mathbb{ S}^n](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes+n%7D+%5Crightarrow+%5Cmathbb%7B+S%7D%5En)的一个有界线性算子。![L](https://www.zhihu.com/equation?tex=L)究竟是什么样可以从定义出发计算：

![\begin{align} &F(X+\Delta X) - F(X) \\ =& (X+\Delta X)^T(X+\Delta X) - X^TX\\ =& X^T\Delta X + \Delta X^TX + o(\|\Delta X\|) \end{align}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26F%28X%2B%5CDelta+X%29+-+F%28X%29+%5C%5C+%3D%26+%28X%2B%5CDelta+X%29%5ET%28X%2B%5CDelta+X%29+-+X%5ETX%5C%5C+%3D%26+X%5ET%5CDelta+X+%2B+%5CDelta+X%5ETX+%2B+o%28%5C%7C%5CDelta+X%5C%7C%29+%5Cend%7Balign%7D)

所以我们有![L(\Delta X) = X^T\Delta X + \Delta X^TX](https://www.zhihu.com/equation?tex=L%28%5CDelta+X%29+%3D+X%5ET%5CDelta+X+%2B+%5CDelta+X%5ETX)，这个就是![F](https://www.zhihu.com/equation?tex=F)在![X](https://www.zhihu.com/equation?tex=X)点的导数。![L](https://www.zhihu.com/equation?tex=L)这个函数(有界线性算子)可以用张量来表述，这里就不详细说了。

**例子：**最小二乘问题![f(x) = \frac{1}{2}\|Ax-b\|_2^2](https://www.zhihu.com/equation?tex=f%28x%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5C%7CAx-b%5C%7C_2%5E2)，![f](https://www.zhihu.com/equation?tex=f)是一个![\mathbb R^n \rightarrow \mathbb R](https://www.zhihu.com/equation?tex=%5Cmathbb+R%5En+%5Crightarrow+%5Cmathbb+R)的映射。

![\begin{align} &f(x+\Delta x) - f(x) \\ =& \frac{1}{2}\|A(x+\Delta x) - b\|^2 - \frac{1}{2}\|Ax - b\|^2\\ =& \frac{1}{2}\|Ax - b + A\Delta x\|^2 - \frac{1}{2}\|Ax - b\|^2\\ =& (Ax - b)^TA\Delta x + o(\|\Delta x\|) \end{align}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26f%28x%2B%5CDelta+x%29+-+f%28x%29+%5C%5C+%3D%26+%5Cfrac%7B1%7D%7B2%7D%5C%7CA%28x%2B%5CDelta+x%29+-+b%5C%7C%5E2+-+%5Cfrac%7B1%7D%7B2%7D%5C%7CAx+-+b%5C%7C%5E2%5C%5C+%3D%26+%5Cfrac%7B1%7D%7B2%7D%5C%7CAx+-+b+%2B+A%5CDelta+x%5C%7C%5E2+-+%5Cfrac%7B1%7D%7B2%7D%5C%7CAx+-+b%5C%7C%5E2%5C%5C+%3D%26+%28Ax+-+b%29%5ETA%5CDelta+x+%2B+o%28%5C%7C%5CDelta+x%5C%7C%29+%5Cend%7Balign%7D)

所以我们有![L(\Delta x) = (Ax - b)^TA\Delta x](https://www.zhihu.com/equation?tex=L%28%5CDelta+x%29+%3D+%28Ax+-+b%29%5ETA%5CDelta+x)，这个就是![f](https://www.zhihu.com/equation?tex=f)在![x](https://www.zhihu.com/equation?tex=x)点的导数。在这种情况下，![L](https://www.zhihu.com/equation?tex=L)这个有界线性算子可以用梯度来表述(recall Riesz表示定理)：
![L(\Delta x) = \langle \nabla f(x), \Delta x \rangle = \langle A^T(Ax - b), \Delta x \rangle = (Ax-b)^TA\Delta x](https://www.zhihu.com/equation?tex=L%28%5CDelta+x%29+%3D+%5Clangle+%5Cnabla+f%28x%29%2C+%5CDelta+x+%5Crangle+%3D+%5Clangle+A%5ET%28Ax+-+b%29%2C+%5CDelta+x+%5Crangle+%3D+%28Ax-b%29%5ETA%5CDelta+x)
所以梯度![\nabla f(x) = A^T(Ax - b)](https://www.zhihu.com/equation?tex=%5Cnabla+f%28x%29+%3D+A%5ET%28Ax+-+b%29)。

**例子：**单层神经网络![f(W) = \frac{1}{2}\|\sigma(Wx) - y\|^2_2](https://www.zhihu.com/equation?tex=f%28W%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx%29+-+y%5C%7C%5E2_2)，![f](https://www.zhihu.com/equation?tex=f)是一个![\mathbb{R}^{m \times n } \rightarrow \mathbb{ R }](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n+%7D+%5Crightarrow+%5Cmathbb%7B+R+%7D)的映射。这里![\sigma: \mathbb{ R}^m \rightarrow \mathbb{ R}^m](https://www.zhihu.com/equation?tex=%5Csigma%3A+%5Cmathbb%7B+R%7D%5Em+%5Crightarrow+%5Cmathbb%7B+R%7D%5Em)是一个elementwise的logistic function。算起来

![\begin{align} & f(W+\Delta W) - f(W) \\ =& \frac{1}{2}\|\sigma(Wx + \Delta Wx) - y\|_2^2 - \frac{1}{2}\|\sigma(Wx) - y\|_2^2\\ =& \frac{1}{2}\|\sigma(Wx) + \sigma(Wx)\odot (\mathbf 1_m - \sigma(Wx)) \odot \Delta W x + o(\|\Delta W\|) - y\|_2^2 - \frac{1}{2}\|\sigma(Wx) - y\|_2^2\\ =& (\sigma(Wx) - y)^T(\sigma(Wx)\odot (\mathbf 1_m - \sigma(Wx)) \odot \Delta W x) + o(\|\Delta W\|) \end{align}](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26+f%28W%2B%5CDelta+W%29+-+f%28W%29+%5C%5C+%3D%26+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx+%2B+%5CDelta+Wx%29+-+y%5C%7C_2%5E2+-+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx%29+-+y%5C%7C_2%5E2%5C%5C+%3D%26+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx%29+%2B+%5Csigma%28Wx%29%5Codot+%28%5Cmathbf+1_m+-+%5Csigma%28Wx%29%29+%5Codot+%5CDelta+W+x+%2B+o%28%5C%7C%5CDelta+W%5C%7C%29+-+y%5C%7C_2%5E2+-+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx%29+-+y%5C%7C_2%5E2%5C%5C+%3D%26+%28%5Csigma%28Wx%29+-+y%29%5ET%28%5Csigma%28Wx%29%5Codot+%28%5Cmathbf+1_m+-+%5Csigma%28Wx%29%29+%5Codot+%5CDelta+W+x%29+%2B+o%28%5C%7C%5CDelta+W%5C%7C%29+%5Cend%7Balign%7D)

其中![\odot](https://www.zhihu.com/equation?tex=%5Codot)为Hadamard乘积（elementwise乘积），![\mathbf 1_m](https://www.zhihu.com/equation?tex=%5Cmathbf+1_m)为长度为m的元素均为1的向量。这里我使用了一维logistic函数的导数公式。所以
![L(\Delta W) = (\sigma(Wx) - y)^T(\sigma(Wx)\odot (\mathbf 1_m - \sigma(Wx)) \odot \Delta W x)](https://www.zhihu.com/equation?tex=L%28%5CDelta+W%29+%3D+%28%5Csigma%28Wx%29+-+y%29%5ET%28%5Csigma%28Wx%29%5Codot+%28%5Cmathbf+1_m+-+%5Csigma%28Wx%29%29+%5Codot+%5CDelta+W+x%29)。
注：这个例子的倒数第二步到最后一步的计算影射了微积分中的一个重要的思想——链式法则(chain rule)。链式法则能够成立的本质是![ao(\|x\|)+bo(\|x\|) = o(\|x\|)](https://www.zhihu.com/equation?tex=ao%28%5C%7Cx%5C%7C%29%2Bbo%28%5C%7Cx%5C%7C%29+%3D+o%28%5C%7Cx%5C%7C%29)和![o(\|x\|)o(\|x\|) = o(\|x\|^2)](https://www.zhihu.com/equation?tex=o%28%5C%7Cx%5C%7C%29o%28%5C%7Cx%5C%7C%29+%3D+o%28%5C%7Cx%5C%7C%5E2%29)。

最后，由于![\mathbb{R}^{m \times n}](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D)和![\mathbb{R}^{mn}](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bmn%7D)是同构的，所以可以通过vectorization把矩阵映射到![\mathbb{R}^{mn}](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bmn%7D)中再进行计算，见 

[@SS Wang](https://www.zhihu.com/people/8c3a8eefd51943127bbed4a0eb92e2c4)

 

的答案。

