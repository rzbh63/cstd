
# 神经网络Tips 和Tricks - 我和我追逐的梦~~~ - CSDN博客


置顶2015年10月31日 22:32:24[一只鸟的天空](https://me.csdn.net/heyongluoyao8)阅读数：13079标签：[神经网络																](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)[结构																](https://so.csdn.net/so/search/s.do?q=结构&t=blog)[参数																](https://so.csdn.net/so/search/s.do?q=参数&t=blog)[
							](https://so.csdn.net/so/search/s.do?q=结构&t=blog)[
																					](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)个人分类：[数据挖掘与机器学习																](https://blog.csdn.net/heyongluoyao8/article/category/2220409)
[
																								](https://so.csdn.net/so/search/s.do?q=神经网络&t=blog)


原文地址：一只鸟的天空，[http://blog.csdn.net/heyongluoyao8/article/details/49537427](http://blog.csdn.net/heyongluoyao8/article/details/49537427)
## 神经网络Tips 和Tricks
### 神经网络层数
ANN一般有三层或四层，包含一到两个隐含层。每层有10~1000个神经元。实验神经网络可能有5层甚至6层，包含3或4个隐含层，有数百万个神经元，但大多数实际应用仅有3层，因为每增加一层，计算量将呈呈指数级上升。
### 隐藏层单元个数
隐藏层单元个数是网络的一个超参数，它依赖于数据集。笼统地讲，数据集分布越复杂，那么网络需要有更强大的拟合能力，而网络的拟合能力是由隐藏层的单元个数决定的，个数越多，能力越强，因此需要更多的隐藏层节点数，一旦隐藏层节点增加，那么参数就会大量增加，那么网络的学习时间就会大量增多。因此需要取得一个平衡，
这里有几个经验公式：

$$
n_h = \sqrt{n_i+n_o}+\alpha
$$

$$
n_h = \log_2{n_i}+\alpha
$$

$$
n_h = \sqrt{n_i*n_o}+\alpha
$$
其中$n_i, n_h, n_o$分别输入层、隐藏层、输出层的节点个数，$\alpha$取$1~10$之间的常数。
但一般都是多试几个值，哪一个效果好就选取哪一个。当然也可以使用一些优化算法（如遗传算法、粒子群优化算法）或者启发式规则来确定。
### 隐藏层单元的激活函数
两个最常用非线性的激活函数是sigmoid和tanh函数，一般建议选择tanh函数（实践中也表明），因为tanh函数关于原点对称的，其输出是0均值，并作为下一层的输入，而零均值是下一层的输入所期望的。在实践中表明tanh具有更好的收敛性。
### 损失函数
损失函数即代价函数，也即模型训练中的优化目标，优化的方向便是使得损失函数最小。一般使用均方误差（MSE）或者softmax代价函数：

$$
MSE=\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^2}{n}
$$

$$
softmax=-\frac{\sum_{i=1}^{n}\sum_{j=1}^{k}1{\{y^{(i)}=j \} log\frac{e^{\theta_j^Tx^{(i)}}}{\sum_l^k e^{\theta_l^Tx^{(i)}}}  }}{n}
$$

### 初始参数
在对网络的参数进行初始化时，需要使得参数足够小并处于原点附近，使得激活函数在近似线性范围，这时候梯度就最大，使得在反向传播过程中的梯度消失（gradient vanish）问题得到减轻，特别是在深度学习中。一般经验为，对于tanh激活函数：$uniform[-\frac{\sqrt{6}}{\sqrt{n_{i}+n_{o}}},\frac{\sqrt{6}}{\sqrt{n_{i}+n_{o}}}]$，对于sigmoid激活函数：$uniform[-4*\frac{\sqrt{6}}{\sqrt{n_{i}+n_{o}}},4*\frac{\sqrt{6}}{\sqrt{n_{i}+n_{o}}}]$，其中$n_i$与$n_o$分别是前一层（该层的输入层）与当前层的节点数，如在三层网络结构中，输入层节点数为$n_i$，隐藏层的节点数为$n_h$，输出层的节点数为$n_o$，那么输入层到隐藏层的连接权重矩阵的初始化为~$U(-\frac{\sqrt{6}}{\sqrt{n_{i}+n_{h}}},\frac{\sqrt{6}}{\sqrt{n_{i}+n_{h}}})$，隐藏层到输出层的连接权值矩阵的初始化为~$U(-\frac{\sqrt{6}}{\sqrt{n_{h}+n_{o}}},\frac{\sqrt{6}}{\sqrt{n_{h}+n_{o}}})$。
### 学习速率
选择一个好的学习速率是非常有必要的，因为如果学习速率过小，那么收敛慢，则需要更多的迭代，意味着需要更多的时间。如果学习速率过大，那么会引起在极值点附近震荡。但是最好的学习速率是很难找到的，需要进行多次试验选择一个实验中最好的学习速率，但是一旦换了一批数据，则可能这个学习速率又没有其它的好了。因此如何确定一个好的学习速率是一件较困难的事。最简单的方法便是，每次试验选取一个恒定的学习速率，然后进行多次试验，从这些试验的学习速率中选择一个最优的。一般规则是按照对数进行网格搜索，从初始值按照对数衰减，如$(10^{-1}, 10^{-2}...)$。
随着迭代的次数衰减学习速率是个不错的方法，即随着迭代的进行步子不断减少，在极值点远处大步向前，到达极致点附近时，小步逼近，反正在极致点附近振荡，这样平衡了收敛时间与振荡。一个简单的做法便是$\frac{\alpha_0}{1+d * t}$，其中$\alpha_0$是初始学习速率（可以使用前面的对数网格搜索来选择），$d$是衰减常数，用来控制学习速率衰减快慢的，一般其是一个较小的正数，如$10^{-3}$或者更小，$t$是第几次迭代。
当然更好的做法是为每一个连接权值选择不同的学习速率，以自适应分类器的误差。
### 是否需要加factor
在进行权值更新的时候，我们可以加上上次的更新量（不乘以学习速率的量）乘以一个factor因子，来控制权值的更新。可以提到收敛的速度。一般取factor为：$10^{-1}, 10^{-2}...$
### 正则参数
L1/L2正则一般选择的典型参数值$\lambda$为$10^{-2}, 10^{-3}...$。
未完待续……

