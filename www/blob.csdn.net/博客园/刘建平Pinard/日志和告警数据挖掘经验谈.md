
# 日志和告警数据挖掘经验谈 - 刘建平Pinard - 博客园






# [日志和告警数据挖掘经验谈](https://www.cnblogs.com/pinard/p/6039099.html)
最近参与了了一个日志和告警的数据挖掘项目，里面用到的一些思路在这里和大家做一个分享。
项目的需求是收集的客户系统一个月300G左右的的日志和告警数据做一个整理，主要是归类(Grouping)和关联(Correlation)，从而得到告警和日志的一些统计关系，这些统计结果可以给一线支持人员参考。
得到的数据主要分为两部分，一部分是告警的历史数据，这部分数据很少，只有50M左右，剩下的全部都是日志数据。日志数据大概有50多种不同类型，对应系统中不同的模块。每种类型的文件每天产生一个日志文件，所以总数大概是1500个左右的日志文件。文件大概都是这样的：A_2016-04-15.log, B_2016-04-15.log, ..., A_2016-05-14.log, B_2016-05-14.log。每个文件在10M-1G之间不等。
# 1. 日志的模式挖掘
通过查看日志，发现所有的log每一行基本都是类似这样的Pattern:
YYYY-MM-DD hh:mm:ss [模块名] [具体日志]
每类日志的模块名都是一样的，基本可以忽略。有价值的就是时间戳和具体日志。
而且可以发现，很多日志只是极少部分动态内容不同，在代码中属于同一个位置的输出，这些数据后面我们会分为一类数据。比如：
2016-04-26 00:30:38.795 55637   ResourceManager Free ram (MB): 244736
2016-04-26 00:34:38.795 55637   ResourceManager Free ram (MB): 244748
有某些类型日志每个时段都有出现，咨询后得知基本没有任何分析价值，这些日志后面我们会加入黑名单，不加分析。
# 2. 日志的归类
由于每类日志都有30个文件，每个文件基本都有100万行，我们的第一步工作就是去除上面提到的无用日志。去掉无用日志后，我们要分析的日志大概减少了30%。
接着我们要做的就是每一行的日志进行归类（Grouping）。这里有很多的方法可以选择，比如K-means，但是我们这么多的日志，很难去定义一个合适的K。经过一番尝试后我们放弃了K-means。但是K-means的思想还是可以用的。最后我们使用的是启发式的方法来归类。
首先定下的基本思路是： 对于每一类文件，我们分别做归类，最后再一起和告警文件做关联（Crrelation）。我们作了不同类别文件的日志肯定不在一类的假定。
对于每一类文件的每一行日志，我们我们通过对具体日志的字符串的相似度进行归类，算法如下：
1）初始化将最终类别数组设置为空，类别数组的每一行的格式是 [index] [类别里第一次出现的具体日志内容] [该类日志出现的所有时间形成的数组]
2）初始化字符串相似度阈值，相似度超过阈值的字符串即为一类。项目里面我们相似度阈值取80%。
3）初始化归类的时间间隔，在一个时间间隔内的相似日志仅仅记录一次时间。也就是说如果某类日志已经有这段时间的记录，再次在这段时间出现的类似日志将会被忽略。取的过大，后面关联时精确度降低，取的过小，后面关联时计算量会很大。项目里我们取10分钟作为日志间隔。也就是一天划分成了24*6个时间间隔。
4）对于某一种类别， 对于每一行的具体日志我们去和该类别的最终类别数组的每一行的具体日志做相似度比较：
a) 如果和最终类别里的某行具体日志的字符串的相似度超过了阈值，则这两个字符串即归为一类，仅仅把这个要分析的具体日志的时间点存入该类别，停止该行日志的分析。
b) 如果和最终类别里的任何一行具体日志的字符串的相似度都低于阈值。则我们发现了一个新的类别。在最终类别里加入一行记录。并把该日志的时间间隔对应的点作为该类别的时间数组的第一条时间记录。
5） 对于所有其他的类别，分别执行上面的第4步。得到所有类别的最终类别数组。最终我们的50多个类别数组一共只剩下100多M，每个数组平均有100多种类别。
这个算法产生的类别数组中每一行是这样的内容：
1*ResourceManager Free ram (MB): 244736*[[2016-04-26 00:30],[2016-04-26 10:40], ...]
上面的算法中，我们用到了字符串相似度算法。这里我们用到是python的字符串下相似度算法库：python-Levenshtein。计算相似度我们用了python-Levenshtein库的ratio函数，即莱文斯坦比。如果大家对python-Levenshtein的字符串相似度计算有兴趣，可以参考python-Levenshtein的官方文档：[https://pypi.python.org/pypi/python-Levenshtein/0.12.0\#id1](https://pypi.python.org/pypi/python-Levenshtein/0.12.0#id1)

# 3. 日志和告警的关联
现在我们有了50多种日志的类别数据，每个类别也有在时间分布上的数据，同时，回到告警，每个告警也有在时间分布上的数据。现在我们可以在时间维度上做关联算法。
我们的日志类别数组和告警在时间维度一共有30*24*6=4320个点。我们的目标是找到和每个告警在时间维度上关联度比较高的一组日志。这里我们采用的是基于余弦相似度的算法。我们选择了所有的和告警在时间维度上相似度超过80%的日志类别。这些类别作为最终的统计结果作为我们输出的一部分。
# 4. 告警和告警的关联
这部分工作主要是研究告警和告警之间的统计关系。主要是基于统计的在时间维度上的父子关系。
由于告警数据较少，我们将时间间隔精确到1分钟。对于每一种告警，我们检查在该告警和其他告警在时间维度上的关系。我们检查3种情况。
第一种情况是在相同时间间隔出现的兄弟告警和该告警的统计关系，我们选择在时间维度上和该告警相似度超过80%的所有告警，这些告警和该告警有时间上同步的关系，也就是这些告警统计上总是和该告警同时出现。
第二种情况是在该告警出现前一分钟内的所有父亲告警和该告警的关系，我们选择在时间维度上和该告警相似度超过80%的所有告警，这些告警和该告警有时间上先后的关系，也就是这些告警统计上总是在该告警之前出现。
第三种情况是在该告警出现后一分钟内的所有儿子告警和该告警的关系，我们选择在时间维度上和该告警相似度超过80%的所有告警，这些告警和该告警有时间上先后的关系，也就是这些告警统计上总是在该告警之后出现。
以上就是对日志和告警数据挖掘的项目经验总结，希望对大家有所启发。
（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）





