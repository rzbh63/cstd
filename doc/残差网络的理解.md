# 残差网络的理解

2017年12月20日 14:20:57 [向阳+](https://me.csdn.net/m0_37407756) 阅读数：4356



 版权声明：本文为博主原创文章，未经博主允许不得转载。	https://blog.csdn.net/m0_37407756/article/details/78852742

[知乎](https://www.baidu.com/s?wd=%E7%9F%A5%E4%B9%8E&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)上有一个解释说，基础形式实际上类似于差分放大，很有道理。

假如： F'(5)=5.1 从5到5.1的变化率是极低的，因为5的基数太大，所以可以进行拆分：

H(5)=5.1, H(5)=5+F(5), F(5)=0.1 此时由于在拆分项中0变成0.1，

这个变化率就达到了10%，便放大了这种效果。

一般理论上认为网络层数越多，提取细节与抽象能力越丰富，然而实际层数迭代多了，

会出现梯度爆炸或梯度弥散 ，这可以通过通过正则化解决。但即使解决了这个问题，

仍然会有退化问题，即层数多了后，出现准确率反而下降的情况。



这里有一个变化的技巧就是，在 H(x) = F(x) + x 时，当F(x)=0时， 显然H(x) = x，

如果f(x)越来越趋近于0，则h(x)越来越趋进x。所以可以构造这样的形式：

 ![道家阴符派博客--残差网络--神经网络](http://blog.yinfupai.com/wp-content/uploads/cf.png)

所以，x可以直接跳两层作为输入，因为h(x)=x，f(x)这里当然不会等于0，

但是可以通过relu使得矩阵中尽量多的地方变成0，从而实现残差放大。

深度残差网络在2015的ILSVRC比赛中获得取得第一的成绩，ICLR2016上也是重点议题之一。

它主要思想很简单，就是在标准的前馈卷积网络上，加一个跳跃绕过一些层的连接。



每绕过一层就产生一个残差块(residual block)，卷积层预测加输入张量的残差。

普通的深度前馈网络难以优化。除了深度，所加层也使得training和validation的错误率增加，即使用上了batch normalization也是如此。

残差神经网络由于存在shorcut connections，网络间的数据流通更为顺畅。残差网络结构的解决方案是，增加卷积层输出求和的捷径连接。



实验表明，残差网络更容易优化，并且能够通过增加相当的深度来提高准确率。
核心是解决了增加深度带来的副作用（退化问题），这样能够通过单纯地增加网络深度，来提高网络性能。

**网络的深度为什么重要？**

因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。

并且，越深的网络提取的特征越抽象，越具有语义信息。

**为什么不能简单地增加网络层数？**



- 对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。

对于该问题的解决方法是

正则化初始化

和

中间的正则化层（Batch Normalization）

，这样的话可以训练几十层的网络。







虽然通过上述方法能够训练了，但是又会出现另一个问题，就是**退化问题**，网络层数增加，

但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。

退化问题说明了深度网络不能很简单地被很好地优化。 作者通过实验：通过浅层网络+ y=x 等同映射构造深层模型，结果深层模型并没有比浅层网络有等同或更低的错误率，

推断退化问题可能是因为深层的网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数。

怎么解决退化问题？

 

深度残差网络。



如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那现在要解决的就是学习恒等映射函数了。

但是直接让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难，这可能就是深层网络难以训练的原因。

但是，如果把网络设计为H(x) = F(x) + x。

我们可以转换为学习一个残差函数F(x) = H(x) - x. 只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。

其他的参考解释



（1）上边的已整理

F是求和前网络映射，H是从输入到求和后的网络映射。

比如把5映射到5.1，那么引入残差前是F'(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。

这里的F'和F都表示网络参数映射，**引入残差后的映射对输出的变化更敏感**。

比如s输出从5.1变到5.2，映射F'的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。

明显后者输出变化对权重的调整作用更大，所以效果更好。

残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器.

（2）

```
至于为何shortcut的输入时X，而不是X/2或是其他形式。
kaiming大神的另一篇文章[2]中探讨了这个问题，对以下6种结构的残差结构进行实验比较，shortcut是X/2的就是第二种，
结果发现还是第一种效果好啊（摊手）
部分参考：http://www.jianshu.com/p/e58437f39f65
```

