# 分布式理论



# [分布式理论(一) —— CAP 定理](https://www.cnblogs.com/stateis0/p/9062121.html)





目录：

1. 什么是 CAP 定理
2. 为什么只能 3 选 2
3. 能不能解决 3 选 2 的问题
4. 引用

## 1. 什么是 CAP 定理

2000 年的时候，Eric Brewer 教授提出了 CAP 猜想，2年后，被 Seth Gilbert 和 Nancy Lynch 从理论上证明了猜想的可能性，从此，CAP 理论正式在学术上成为了分布式计算领域的公认定理。并深深的影响了分布式计算的发展。

CAP 理论告诉我们，一个分布式系统不可能同时满足一致性（C:Consistency)，可用性（A: Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中的2个。

| 选项                    | 描述                                                         |
| ----------------------- | ------------------------------------------------------------ |
| C（Consistence）        | **一致性**，指数据在多个副本之间能够保持一致的特性（严格的一致性）。 |
| A（Availability）       | **可用性**，指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据。 |
| P（Network partitioning | **分区容错性**，分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障。 |

什么是分区？

> 在分布式系统中，不同的节点分布在不同的子网络中，由于一些特殊的原因，这些子节点之间出现了网络不通的状态，但他们的内部子网络是正常的。从而导致了整个系统的环境被切分成了若干个孤立的区域。这就是分区。

## 2. 为什么只能 3 选 2

为什么只能 3 选 2？

![图片来自网络](https://upload-images.jianshu.io/upload_images/4236553-1a91100e8c2a887a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

首先问，能不能同时满足这三个条件？

假设有一个系统如下：

![img](https://upload-images.jianshu.io/upload_images/4236553-573b18642806420a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

整个系统由两个节点配合组成，之间通过网络通信，当节点 A 进行更新数据库操作的时候，需要同时更新节点 B 的数据库（这是一个原子的操作）。

上面这个系统怎么满足 CAP 呢？C：当节点A更新的时候，节点B也要更新，A：必须保证两个节点都是可用的，P：当节点 A,B 出现了网络分区，必须保证对外可用。

可见，根本完成不了，只要出现了网络分区，A 就无法满足，因为节点 A 根本连接不上节点 B。如果强行满足 C 原子性，就必须停止服务运行，从而放弃可用性 C。

所以，最多满足两个条件：

| 组 合 | 分析结果                                                     |
| ----- | ------------------------------------------------------------ |
| CA    | 满足原子和可用，放弃分区容错。说白了，就是一个整体的应用。   |
| CP    | 满足原子和分区容错，也就是说，要放弃可用。当系统被分区，为了保证原子性，必须放弃可用性，让服务停用。 |
| AP    | 满足可用性和分区容错，当出现分区，同时为了保证可用性，必须让节点继续对外服务，这样必然导致失去原子性。 |

## 3. 能不能解决 3 选 2 的问题

难道真的没有办法解决这个问题吗？

CAP 理论已经提出了 13 年，也许可以做些改变。

仔细想想，分区是百分之百出现的吗？如果不出现分区，那么就能够同时满足 CAP。如果出现了分区，可以根据策略进行调整。比如 C 不必使用那么强的一致性，可以先将数据存起来，稍后再更新，实现所谓的 “最终一致性”。

这个思路又是一个庞大的问题，同时也引出了第二个理论 Base 理论，我们将在后面的文章中详细介绍。

## 4. 引用

1. [《从 Paxos 到 Zookeeper —— 分布式一致性原理实践》倪超](https://item.jd.com/11622772.html)
2. [CAP理论十二年回顾："规则"变了](http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed)
3. [维基百科CAP定理](https://zh.wikipedia.org/wiki/CAP%E5%AE%9A%E7%90%86)





# [分布式理论(二)——Base 理论](https://www.cnblogs.com/stateis0/p/9062123.html)





## 前言

在前文 [分布式理论(一) —— CAP 定理](https://www.jianshu.com/p/4118718658ac) 中，我们说，CAP 不可能同时满足，而分区容错是对于分布式系统而言，是必须的。最后，我们说，如果系统能够同时实现 CAP 是再好不过的了，所以出现了 BASE 理论，今天就来讲讲 Base 理论。

## 1. 什么是 Base 理论

> BASE：全称：Basically Available(基本可用)，Soft state（软状态）,和 Eventually consistent（最终一致性）三个短语的缩写，来自 ebay 的架构师提出。

Base 理论是对 CAP 中一致性和可用性权衡的结果，其来源于对大型互联网分布式实践的总结，是基于 CAP 定理逐步演化而来的。其核心思想是：

> 既是无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。

## 2. Basically Available(基本可用)

什么是基本可用呢？假设系统，出现了不可预知的故障，但还是能用，相比较正常的系统而言：

1. 响应时间上的损失：正常情况下的搜索引擎 0.5 秒即返回给用户结果，而**基本可用**的搜索引擎可以在 1 秒作用返回结果。
2. 功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单，但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。

## 3. Soft state（软状态）

什么是软状态呢？相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种 “硬状态”。

软状态指的是：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。

## 4. Eventually consistent（最终一致性）

这个比较好理解了哈。

上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性。从而达到数据的最终一致性。这个时间期限取决于网络延时，系统负载，数据复制方案设计等等因素。

稍微官方一点的说法就是：

> 系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问最终都能够获取到最新的值。

而在实际工程实践中，**最终一致性分为 5 种：**

**1. 因果一致性（Causal consistency）**

指的是：如果节点 A 在更新完某个数据后通知了节点 B，那么节点 B 之后对该数据的访问和修改都是基于 A 更新后的值。于此同时，和节点 A 无因果关系的节点 C 的数据访问则没有这样的限制。

**2. 读己之所写（Read your writes）**

这种就很简单了，节点 A 更新一个数据后，它自身总是能访问到自身更新过的最新值，而不会看到旧值。其实也算一种因果一致性。

**3. 会话一致性（Session consistency）**

会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现 “读己之所写” 的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。

**4. 单调读一致性（Monotonic read consistency）**

单调读一致性是指如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值。

**5. 单调写一致性（Monotonic write consistency）**

指一个系统要能够保证来自同一个节点的写操作被顺序的执行。

然而，在实际的实践中，这 5 种系统往往会结合使用，以构建一个具有最终一致性的分布式系统。实际上，不只是分布式系统使用最终一致性，关系型数据库在某个功能上，也是使用最终一致性的，比如备份，数据库的复制过程是需要时间的，这个复制过程中，业务读取到的值就是旧的。当然，最终还是达成了数据一致性。这也算是一个最终一致性的经典案例。

## 5. 总结

总的来说，BASE 理论面向的是大型高可用可扩展的分布式系统，和传统事务的 ACID 是**相反的**，它完全不同于 ACID 的强一致性模型，而是**通过牺牲强一致性**来获得可用性，并允许数据在一段时间是不一致的。



# [分布式理论（三）—— 一致性协议之 2PC](https://www.cnblogs.com/stateis0/p/9062126.html)





## 前言

为了使系统尽量能够达到 CAP，于是有了 BASE 协议，而 BASE 协议是在可用性和一致性之间做的取舍和妥协。

人们往往需要在系统的可用性和数据一致性之间反复的权衡。于是呢，就产生我们标题中的一致性协议，而且还不止一个呢。

为了解决分布式问题，涌现了很多经典的算法和协议，最著名的就是二阶段提交协议，三阶段提交协议，Paxos 算法。

本文重点介绍二阶段提交协议，简称 2PC。

## 1. 什么是 2PC

在分布式系统中，会有多个机器节点，因此需要一个 “协调者” ，而各个节点就是 “参与者”，协调者统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点就是 “参与者”。

协调者最终决定这些参与者是否要把事务真正进行提交。正式基于这个思想，有了二阶段提交和 三阶段提交。

2PC ，不是 2 个 pc 机的意思，而是 Two-Phase Commit 。可以认为是一种算法，也可以认为是一种协议，主要目的就是为了保证分布式系统数据的一致性。

协议说明：顾名思义，二阶段提交就是讲事务的提交过程分成了两个阶段来进行处理。流程如下：

## 2. 2PC 阶段一

##### 1. 事务询问

协调者向所有的参与者询问，是否准备好了执行事务，并开始等待各参与者的响应。

##### 2. 执行事务

各参与者节点执行事务操作，并将 Undo 和 Redo 信息记入事务日志中

##### 3. 各参与者向协调者反馈事务询问的响应

如果参与者成功执行了事务操作，那么就反馈给协调者 Yes 响应，表示事务可以执行；如果参与者没有成功执行事务，就返回 No 给协调者，表示事务不可以执行。

从上面可以感觉到，这个一个 所谓的 “投票阶段”，什么意思呢？所有的节点都投票决定是否执行事务操作。

## 3. 2PC 阶段二

在阶段二中，会根据阶段一的投票结果执行 2 种操作：执行事务提交，中断事务。

**执行事务提交步骤如下：**

1. 发送提交请求：协调者向所有参与者发出 commit 请求。
2. 事务提交：参与者收到 commit 请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资源。
3. 反馈事务提交结果：参与者在完成事务提交之后，向协调者发送 Ack 信息。
4. 协调者接收到所有参与者反馈的 Ack 信息后，完成事务。

**中断事务步骤如下：**

1. 发送回滚请求：协调者向所有参与者发出 Rollback 请求。
2. 事务回滚：参与者接收到 Rollback 请求后，会利用其在阶段一种记录的 Undo 信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。
3. 反馈事务回滚结果：参与者在完成事务回滚之后，想协调者发送 Ack 信息。
4. 中断事务：协调者接收到所有参与者反馈的 Ack 信息后，完成事务中断。

从上面的逻辑可以看出，二阶段提交就做了2个事情：投票，执行。

> 核心是对每个事务都采用先尝试后提交的处理方式，因此也可以将二阶段提交看成一个强一致性的算法。

整个事务的执行过程如图（可能不是太直观。。。。）

![image.png](https://upload-images.jianshu.io/upload_images/4236553-4ca515b5cd0c4fef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 4. 优点缺点

优点：原理简单，实现方便
缺点：同步阻塞，单点问题，数据不一致，过于保守

1. 同步阻塞：

在二阶段提交的过程中，所有的节点都在等待其他节点的响应，无法进行其他操作。这种同步阻塞极大的限制了分布式系统的性能。

1. 单点问题：

协调者在整个二阶段提交过程中很重要，如果协调者在提交阶段出现问题，那么整个流程将无法运转，更重要的是：其他参与者将会处于一直锁定事务资源的状态中，而无法继续完成事务操作。

1. 数据不一致：

假设当协调者向所有的参与者发送 commti 请求之后，发生了局部网络异常或者是协调者在尚未发送完所有 commit 请求之前自身发生了崩溃，导致最终只有部分参与者收到了 commit 请求。这将导致严重的数据不一致问题。

1. 过于保守：

如果在二阶段提交的提交询问阶段中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息的化，这时协调者只能依靠其自身的超时机制来判断是否需要中断事务，显然，这种策略过于保守。换句话说，**二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败**。

## 5. 总结

由于 Base 理论需要在一致性和可用性方面做出权衡，因此涌现了很多关于一致性的算法或者说协议，这些协议设计的目的，就是能让分布式系统能够在可用性和一致性之间取得一个很好的平衡，能够基本可用。 比如 2PC，two-phase commit，分为两个阶段提交一个事务：投票，执行。通过协调者和各个参与者的配合，实现一致性协议。

当然，他也是有去缺点的，比如同步阻塞的时候性能较低，协调者的单点问题，网络故障可能引起的数据不一致的问题，执行策略过于保守的问题等等。

这些问题，将在另一个算法 3PC 中解决。我们将在下一篇文章中详细说明。

good luck！！！









# [分布式理论（四）—— 一致性协议之 3PC](https://www.cnblogs.com/stateis0/p/9062128.html)



## 前言

我们说为了实现 BASE 理论，需要在可用性和一致性之间找到一个合适的一致性理论，于是，我们在上篇文章中了解了 2PC 理论，也就是两阶段提交，二阶段提交原理简单，实现方便，但是缺点则是同步阻塞，单点问题，数据不一致，过于保守。

而为了弥补二阶段提交的缺点，研究者们在他的基础上，提出了三阶段提交。

## 1. 什么是三阶段提交

3PC，全称 “three phase commit”，是 2PC 的改进版，其将 2PC 的 “提交事务请求” 过程一分为二。

回忆一下 2PC 的过程：

![image.png](https://upload-images.jianshu.io/upload_images/4236553-9eade5b9337b3866.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

也就是说，3PC 将阶段一 "提交事务请求" 分成了2部分，总共形成了 3 个部分：

1. CanCommit
2. PreCommit
3. do Commit

如下图所示：

![3PC](https://upload-images.jianshu.io/upload_images/4236553-3dc30375a30462c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 2. 阶段一：CanCommit

第一个阶段： CanCommit

1. 事务询问：协调者向所有的参与者发送一个包含事务内容的 canCommit 请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应。
2. 各参与者向协调者反馈事务询问的响应：参与者接收来自协调者的 canCommit 请求，如果参与者认为自己可以顺利执行事务，就返回 Yes，否则反馈 No 响应。

## 3. 阶段 二：PreCommit

协调者在得到所有参与者的响应之后，会根据结果执行2种操作：执行事务预提交，或者中断事务。

###### 1. 执行事务预提交分为 3 个步骤：

- 发送预提交请求：协调者向所有参与者节点发出 preCommit 的请求，并进入 prepared 状态。
- 事务预提交：参与者受到 preCommit 请求后，会执行事务操作，对应 2PC 中的 “执行事务”，也会 Undo 和 Redo 信息记录到事务日志中。
- 各参与者向协调者反馈事务执行的结果：如果参与者成功执行了事务，就反馈 Ack 响应，同时等待指令：提交（commit） 或终止（abor）。

###### 2. 中断事务也分为2个步骤：

- 发送中断请求：协调者向所有参与者节点发出 abort 请求 。
- 中断事务：参与者如果收到 abort 请求或者超时了，都会中断事务。

## 4. 阶段三：do Commit

该阶段做真正的提交，同样也会出现两种情况：

###### 1. 执行提交

- 发送提交请求：进入这一阶段，如果协调者正常工作，并且接收到了所有协调者的 Ack 响应，那么协调者将从 “预提交” 状态变为 “提交” 状态，并向所有的参与者发送 doCommit 请求 。
- 事务提交：参与者收到 doCommit 请求后，会正式执行事务提交操作，并在完成之后释放在整个事务执行期间占用的事务资源。
- 反馈事务提交结果：参与者完成事务提交后，向协调者发送 Ack 消息。
- 完成事务：协调者接收到所有参与者反馈的 Ack 消息后，完成事务。

###### 2. 中断事务

假设有任何参与者反馈了 no 响应，或者超时了，就中断事务。

- 发送中断请求：协调者向所有的参与者节点发送 abort 请求。
- 事务回滚：参与者接收到 abort 请求后，会利用其在二阶段记录的 undo 信息来执行事务回滚操作，并在完成回滚之后释放整个事务执行期间占用的资源。
- 反馈事务回滚结果：参与者在完成事务回滚之后，想协调者发送 Ack 消息。
- 中断事务：协调者接收到所有的 Ack 消息后，中断事务。

注意：一旦进入阶段三，可能会出现 2 种故障：

1. 协调者出现问题
2. 协调者和参与者之间的网络故障

一段出现了任一一种情况，最终都会导致参与者无法收到 doCommit 请求或者 abort 请求，针对这种情况，参与者都会在等待超时之后，继续进行事务提交。

## 5. 总结：

优点：相比较 2PC，最大的优点是减少了参与者的阻塞范围（第一个阶段是不阻塞的），并且能够在单点故障后继续达成一致（2PC 在提交阶段会出现此问题，而 3PC 会根据协调者的状态进行回滚或者提交）。

缺点：如果参与者收到了 preCommit 消息后，出现了网络分区，那么参与者等待超时后，都会进行事务的提交，这必然会出现事务不一致的问题。





# [分布式理论(五)—— 一致性算法 Paxos](https://www.cnblogs.com/stateis0/p/9062130.html)



![img](https://upload-images.jianshu.io/upload_images/4236553-f8ab2a736e321f7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 前言

Paxos 算法如同我们标题大图：世界上只有一种一致性算法，就是 Paxos。出自一位 google 大神之口。

同时，Paxos 也是出名的晦涩难懂，推理过程极其复杂。楼主在尝试理解 Paxos 算法的过程中历经挫折。

今天，楼主不会讲推理过程，因为就算是尝试使用大白话来讲，也非常的难懂。当然更不会讲数学公式。

而是从一个普通 Java 程序员的角度来理解 Paxos 算法。

## 1. 什么是 Paxos 算法

Paxos 算法由图灵奖获得者 Leslie Lamport 于 1990 年提出的一种基于消息传递且具有高度容错的特性的一致性算法。

来看看大师的样貌：

![Leslie Lamport（莱斯利·兰波特）](https://upload-images.jianshu.io/upload_images/4236553-d3dc929240b8f0e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

标准的程序员。。。。

Paxos 有点类似我们之前说的 2PC，3PC，但是解决了他们俩的各种硬伤。该算法在很多大厂都得到了工程实践，比如阿里的 OceanBase 的分布式数据库，底层就是使用的 paxos 算法。再比如 Google 的 chubby 分布式锁也是用的这个算法。可见该算法在分布式系统中的地位，甚至于，paxos 就是分布式一致性的代名词。

## 2. Paxos 解决了什么问题

那么它解决了什么问题呢？
答：解决了一致性问题。

什么是 consensus （一致性）问题？

在一个分布式系统中，有一组的 process，每个 process 都可以提出一个 value，consensus 算法就是用来从这些 values 里选定一个最终 value。如果没有 value 被提出来，那么就没有 value 被选中；如果有1个 value 被选中，那么所有的 process 都应该被通知到。

我们假设一种情况，在一个集群环境中，要求所有机器上的状态是一致的，其中有2台机器想修改某个状态，机器 A 想把状态改为 A，机器 B 想把状态改为 B，那么到底听谁的呢？

有人说，可以像 2PC，3PC 一样引入一个协调者，谁先到，听谁的。

![img](https://upload-images.jianshu.io/upload_images/4236553-138fec3412382850.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但是如果，协调者宕机了呢？

所以需要对协调者也做备份，也要做集群。这时候，问题来了，这么多协调者，听谁的呢？

![image.png](https://upload-images.jianshu.io/upload_images/4236553-8cdcb216bb7a10b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Paxos 算法就是为了解决这个问题而生的！！！ 牛不？

下面，楼主会尝试用自己的语言配合图，来解释使用 Paxos 算法来解决这样一个类似问题的过程。如果解释的不好，还请见谅。但楼主不会去写任何的算法推导过程，如果各位想看，文末有 Paxos 论文链接，和很多大牛写的推导过程。

开始吧！

## 3. Paxos 例子说明

楼主这个例子来自中文维基百科，但楼主为了形象化，辅以图片解释，但愿不会让人更迷糊。

#### 例子：

在 Paxos 岛上，有A1, A2, A3, A4, A5 5位议员，就税率问题进行决议。我们假设几个场景来解释：

#### 场景 1.

假设 A1 说：税率应该是 10%。而此时只有他一个人提这个建议。如下图：

![img](https://upload-images.jianshu.io/upload_images/4236553-a8942ad847e30fe4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

很完美，没有任何人和他竞争提案，他的这个提案毫无阻挠的通过了。A2 - A5 都会回应他：`我们收到了你的提案，等待最终的批准。`而 A1 在收到 2 份回复后，就可以发布最终的决议：`税率定位 10%，不用再讨论了。`

这里有个注意的地方就是：为什么收到了 2 份回复就可以确定提案了呢？
答：因为包括他自己，就达到 3 个人了，`少数服从多数。`
如果各位听说过鸽笼原理/抽屉原理，就明白个大概了。有人说，`鸽笼原理/抽屉原理就是 Paxos 的核心思想。`

#### 场景 2：

现在我们假设在 A1 提出 10% 税率提案的同时, A5 决定将税率定为 20%，如果这个提案要通过侍从送到其他议员的案头，A1 的草案将由 4 位侍从送到 A2-A5 那里。但是侍从不靠谱（代表分布式环境不靠谱），负责 A2 和 A3 的侍从顺利送达，而负责 A4 和 A5 的侍从则开溜了！

而 A5 的草案则送到了 A4 和 A3 的手中。

![img](https://upload-images.jianshu.io/upload_images/4236553-16ed1e944c9a3164.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

现在，A1 ，A2，A3 收到了 A1 的提案，A3，A4， A5 收到 A5 的提案，按照 Paxos 的协议，A1，A2，A4，A5 4个侍从将接受他们的提案，侍从拿着回复：`我已收到你的提案，等待最终批准` 回到提案者那里。

而 A3 的行为将决定批准哪一个。

当 A3 同时收到了 A1 和 A5 的请求，该如何抉择呢？不同的抉择将会导致不同的结果。

有 3 种情况，我们分析一下：

#### 场景2：情况一

假设 A1 的提案先送到 A3 那里，并且 A3 接受了该提案并回复了侍从。这样，A1 加上 A2 加上 A3，构成了多数派，成功确定了税率为 10%。 而 A5 的侍从由于路上喝酒喝多了，晚到了一天，等他到了，税率已经确定了，A3 回复 A5：`兄弟，你来的太晚了，税率已经定好了，不用折腾了，听 A1 的吧`。

如下图：

![img](https://upload-images.jianshu.io/upload_images/4236553-8225ccdd35b12857.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 场景2：情况二

依然假设 A1 的提案先送到 A3 处，但是这次 A5 的侍从不是放假了，只是中途耽搁了一会。这次, A3 依然会将"接受"回复给 A1 .但是在决议成型之前它又收到了 A5 的提案。这时协议根据 A5 的身份地位有两种处理方式，但结果相同。

1. 当 A5 地位很高，例如 CEO，就回复 A5：`我已收到您的提案，等待最终批准，但是您之前有人提出将税率定为10%,请明察。`
2. 当 A5 没地位，普通码农一个，直接不回复。等待 A1 广播：`税率定为 10% 啦！！！`

如下图：

![img](https://upload-images.jianshu.io/upload_images/4236553-e78d51fcb7950709.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 场景2：情况三

在这个情况中，我们将看见，根据提案的时间及提案者的权势决定`是否应答`是有意义的。在这里，时间和提案者的权势就构成了给提案编号的依据。这样的编号符合"任何两个提案之间构成偏序"的要求。

A1 和 A5 同样提出上述提案，这时 A1 可以正常联系 A2 和 A3，A5 也可以正常联系这两个人。这次 A2 先收到 A1 的提案; A3 则先收到 A5 的提案。而 A5 `更有地位`。

在这种情况下，已经回答 A1 的 A2 发现有比 A1 更有权势的 A5 提出了税率 20% 的新提案，于是回复A5说：`我已收到您的提案，等待最终批准。`

而回复 A5 的 A3 发现新的提案者A1是个小人物，`没地位不予应答`。

此时，A5 得到了 A2，A3 的回复，于是 A5 说：`税率定为 20%，别再讨论了`。

那 A4 呢？ A4 由于睡过头了，迷迷糊糊的说：`现有的税率是什么? 如果没有决定，则建议将其定为 15%.`

这个时候，其他的议员就告诉他：`哥们，已经定为 20% 了，别折腾了。洗洗继续睡吧`。

整个过程如下图：

![img](https://upload-images.jianshu.io/upload_images/4236553-6d4bb868d27fc6ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 4. 总结

从上面的例子可以看出：这个 Paxos 协议/算法 就是少数服从多数，标准的 [鸽笼原理/抽屉原理](https://baike.baidu.com/item/%E6%8A%BD%E5%B1%89%E5%8E%9F%E7%90%86/233776?fr=aladdin&fromid=8942185&fromtitle=%E9%B8%BD%E7%AC%BC%E5%8E%9F%E7%90%86)，同时，还会根据议员的身份来判断是否需要应答，这个身份其实就是一个编号，是为了防止出现活性导致死循环。

注意：这一切都是在没有 `拜占庭将军` 问题的基础上建立的，即消息不会被篡改（因为分布式大多在局域网中）。

Paxos 的目标：保证最终有一个提案会被选定，当提案被选定后，其他议员最终也能获取到被选定的提案。

Paxos 协议用来解决的问题可以用一句话来简化： 将所有节点都写入同一个值，且被写入后不再更改。

## 引用

[如何浅显易懂地解说 Paxos 的算法？](https://www.zhihu.com/question/19787937)
[图解 Paxos 一致性协议](http://blog.xiaohansong.com/2016/09/30/Paxos/)
[分布式系列文章——Paxos算法原理与推导](http://www.cnblogs.com/linbingdong/p/6253479.html)
[Paxos算法 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/Paxos%E7%AE%97%E6%B3%95)
[Paxos Made Simple【翻译】](https://wenku.baidu.com/view/bf8e118fa0116c175f0e4853.html?from=search)
[The Part-Time Parliament(Paxos算法中文翻译)](https://wenku.baidu.com/view/87276e1dfad6195f312ba6d7.html)







# [分布式理论(六)—— Raft 算法](https://www.cnblogs.com/stateis0/p/9062131.html)



## 前言

我们之前讲述了 Paxos 一致性算法，虽然楼主尝试用最简单的算法来阐述，但仍然还是有点绕。楼主最初怀疑自己太笨，后来才直到，该算法的晦涩难懂不是只有我一个人这么认为，而是国际公认！

所以 Paxos 算法在 1990 就发表出来，但却得不到运用。真正的名声大噪还是在兰伯特使用 “更简单” 的方式重写了一篇论文才开始。

这些和今天说的 Raft 有什么关系呢？

答：Raft 也是一个一致性算法，和 Paxos 目标相同。但他还有另一个名字：易于理解的一致性算法。

也就是说，他的目标就是成为一个易于理解的一致性算法。以替代 Paxos 的晦涩难懂。

那我们就开始讲讲 Raft 算法吧！

## 1. 什么是 Raft 算法

首先说什么是 Raft 算法：**Raft 是一种为了管理复制日志的一致性算法。**

什么是一致性呢？
Raft 的论文这么说的：一致性算法允许一组机器像一个整体一样工作，即使其中一些机器出现故障也能够继续工作下去。

这里的`一致性`针对分布式系统。

什么是管理日志呢？
一致性算法是从复制状态机的背景下提出的，复制状态机通常都是`基于复制日志`实现的，这个日志可以理解为一个比喻，相当于一个指令。

关于状态机的描述：

> 多个节点上，从相同的初始状态开始，执行相同的一串命令，产生相同的最终状态。实际上，与其说是一致，其实可以泛化为分布式的两个节点状态存在某种约束。
> 复制状态机通常都是基于复制日志实现的，**保证复制日志相同就是一致性算法的工作了。**
> 典型应用就是一个独立的的复制状态机去管理领导选举和存储配置信息并且在领导人宕机的情况下也要存活下来。比如 Chubby 和 ZooKeeper。

对于 Raft 更重要的应该是 `易于理解`。从 Raft 的论文题目就可以看出：`In Search of an Understandable Consensus Algorithm (Extended Version)`。这里的易于理解是相对于 Paxos 的，在他的论文中，和 Paxos 做了大量针对 `易于理解` 的对比和统计测试。

从楼主阅读论文的过程中来看，Raft 相较于 Paxos 确实更易于理解。为了提升可理解性，Raft 将一致性算法分解成了几个关键模块，例如领导人选举、日志复制和安全性。

而和一致性最相关的就是前面 2 个模块：领导人选举和日志复制。

## 2. 领导人选举

Raft 通过选举一个高贵的领导人，然后给予他全部的管理复制日志的责任来实现一致性。

而每个 server 都可能会在 3 个身份之间切换：

- 领导者
- 候选者
- 跟随者

而影响他们身份变化的则是 `选举`。
当所有服务器初始化的时候，都是 `跟随者`，这个时候需要一个 `领导者`，所有人都变成 `候选者`，直到有人成功当选 `领导者`。

角色轮换如下图：

![角色变化图](https://upload-images.jianshu.io/upload_images/4236553-56571f0026939037.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而领导者也有宕机的时候，宕机后引发新的 `选举`，所以，整个集群在选举和正常运行之间切换，具体如下图：

![image.png](https://upload-images.jianshu.io/upload_images/4236553-484b50efc95c219a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

从上图可以看出，选举和正常运行之间切换，但请注意， 上图中的 term 3 有一个地方，后面没有跟着 `正常运行` 阶段，为什么呢?

答：当一次选举失败（比如正巧每个人都投了自己），就执行一次 `加时赛`，每个 Server 会在一个随机的时间里重新投票，这样就能保证不冲突了。所以，当 term 3 选举失败，等了几十毫秒，执行 term 4 选举，并成功选举出领导人。

接着，领导者周期性的向所有跟随者发送心跳包来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是选举超时，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者。

要开始一次选举过程，跟随者先要增加自己的当前任期号并且**转换到候选人状态**。然后请求其他服务器`为自己投票`。那么会产生 3 种结果：

a. 自己成功当选

b. 其他的服务器成为领导者

c. 僵住，没有任何一个人成为领导者

注意：

1. 每一个 server 最多在一个任期内投出一张选票（有任期号约束），先到先得。
2. 要求最多只能有一个人赢得选票。
3. 一旦成功，立即成为领导人，然后广播所有服务器停止投票阻止新得领导产生。

僵住怎么办？ Raft 通过使用随机选举超时时间（例如 150 - 300 毫秒）的方法将服务器打散投票。每个候选人在僵住的时候会随机从一个时间开始重新选举。

以上，就是 Raft 所有关于领导选举的策略。

## 3. 日志复制

一旦一个领导人被选举出来，他就开始为客户端提供服务。

客户端发送日志给领导者，随后领导者将日志复制到其他的服务器。如果跟随者故障，领导者将会尝试重试。直到所有的跟随者都成功存储了所有日志。

下图表示了当一个客户端发送一个日志给领导者，随后领导者复制给跟随者的整个过程。

![image.png](https://upload-images.jianshu.io/upload_images/4236553-dced1da5a414c2a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

4 个步骤：

1. 客户端提交
2. 复制数据到所有跟随者
3. 跟随者回复 `确认收到`
4. 领导者回复客户端和所有跟随者 `确认提交`。

可以看到，直到第四步骤，整个事务才会达成。中间任何一个步骤发生故障，都不会影响日志一致性。

## 4. 总结

总结一下本文吧：

Raft 算法如同他的论文名字一样：`寻找一种易于理解的一致性算法`，这里的 `易于理解` 是相对于 Paxos 的，的确，Paxos 实在过于复杂了。

而如何实现易于理解？

答：Raft 将一致性算法分成了2部分：领导选举，日志复制。

领导选举基于一个随机的时间来保证不会冲突（如果冲突的话）。
而日志复制则类似于 2PC。

通常 5 个节点，只要不超过 2 个节点死亡都不会影响系统的运行。保证了系统的可用性，通过领导者的日志复制，实现了系统的一致性。

似乎 CAP 定理已经不起作用了，当然这又是一个重大的话题。

最后，以 Raft 论文的结尾结束本位：

> 算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。

## 引用

[寻找一种易于理解的一致性算法（扩展版）Raft 中文翻译](https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md)
[Raft 英文原文](https://github.com/maemual/raft-zh_cn)
[Raft 为什么是更易理解的分布式一致性算法](http://www.cnblogs.com/mindwind/p/5231986.html)



# [分布式理论(七)—— 一致性协议之 ZAB](https://www.cnblogs.com/stateis0/p/9062133.html)



## 前言

在前面的文章中，我们说了很多一致性协议，比如 Paxos，Raft，2PC，3PC等等，今天我们再讲一种协议，ZAB 协议，该协议应该是所有一致性协议中生产环境中应用最多的了。为什么呢？因为他是为 Zookeeper 设计的分布式一致性协议！

## 1. 什么是 ZAB 协议？ ZAB 协议介绍

1. ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。
2. Zookeeper 是一个为分布式应用提供高效且可靠的分布式协调服务。在解决分布式一致性方面，Zookeeper 并没有使用 Paxos ，而是采用了 ZAB 协议。
3. ZAB 协议定义：**ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持 崩溃恢复 和 原子广播 协议**。下面我们会重点讲这两个东西。
4. 基于该协议，Zookeeper 实现了一种 `主备模式` 的系统架构来保持集群中各个副本之间`数据一致性`。具体如下图所示：

![image.png](https://upload-images.jianshu.io/upload_images/4236553-a351e61d6ab6a3cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图显示了 Zookeeper 如何处理集群中的数据。所有客户端写入数据都是写入到 主进程（称为 Leader）中，然后，由 Leader 复制到备份进程（称为 Follower）中。从而保证数据一致性。从设计上看，和 Raft 类似。

1. 那么复制过程又是如何的呢？复制过程类似 2PC，ZAB 只需要 Follower 有一半以上返回 Ack 信息就可以执行提交，大大减小了同步阻塞。也提高了可用性。

简单介绍完，开始重点介绍 `消息广播` 和 `崩溃恢复`。**整个 Zookeeper 就是在这两个模式之间切换。** 简而言之，当 Leader 服务可以正常使用，就进入消息广播模式，当 Leader 不可用时，则进入崩溃恢复模式。

## 2. 消息广播

ZAB 协议的消息广播过程使用的是一个原子广播协议，类似一个 **二阶段提交过程**。对于客户端发送的写请求，全部由 Leader 接收，Leader 将请求封装成一个事务 Proposal，将其发送给所有 Follwer ，然后，根据所有 Follwer 的反馈，如果超过半数成功响应，则执行 commit 操作（先提交自己，再发送 commit 给所有 Follwer）。

**基本上，整个广播流程分为 3 步骤：**

1.将数据都复制到 Follwer 中

![image.png](https://upload-images.jianshu.io/upload_images/4236553-c2cb3f3b2b8b5841.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

1. 等待 Follwer 回应 Ack，最低超过半数即成功

![image.png](https://upload-images.jianshu.io/upload_images/4236553-2fe8ee18eef3701c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

1. 当超过半数成功回应，则执行 commit ，同时提交自己

![image.png](https://upload-images.jianshu.io/upload_images/4236553-8fe90fc287f2faca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

通过以上 3 个步骤，就能够保持集群之间数据的一致性。实际上，在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，避免同步，实现异步解耦。

**还有一些细节：**

1. Leader 在收到客户端请求之后，会将这个请求封装成一个事务，并给这个事务分配一个全局递增的唯一 ID，称为事务ID（ZXID），ZAB 兮协议需要保证事务的顺序，因此必须将每一个事务按照 ZXID 进行先后排序然后处理。
2. 在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞。
3. zookeeper集群中为保证任何所有进程能够有序的顺序执行，只能是 Leader 服务器接受写请求，即使是 Follower 服务器接受到客户端的请求，也会转发到 Leader 服务器进行处理。
4. 实际上，这是一种简化版本的 2PC，不能解决单点问题。等会我们会讲述 ZAB 如何解决单点问题（即 Leader 崩溃问题）。

## 3. 崩溃恢复

刚刚我们说消息广播过程中，Leader 崩溃怎么办？还能保证数据一致吗？如果 Leader 先本地提交了，然后 commit 请求没有发送出去，怎么办？

实际上，当 Leader 崩溃，即进入我们开头所说的崩溃恢复模式（崩溃即：Leader 失去与过半 Follwer 的联系）。下面来详细讲述。

假设1：Leader 在复制数据给所有 Follwer 之后崩溃，怎么办？
假设2：Leader 在收到 Ack 并提交了自己，同时发送了部分 commit 出去之后崩溃怎么办？

**针对这些问题，ZAB 定义了 2 个原则：**

1. ZAB 协议确保那些已经在 Leader 提交的事务最终会被所有服务器提交。
2. ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务。

所以，ZAB 设计了下面这样一个选举算法：
**能够确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务。**

针对这个要求，如果让 Leader 选举算法能够保证新选举出来的 Leader 服务器拥有集群总所有机器编号（即 ZXID 最大）的事务，那么就能够保证这个新选举出来的 Leader 一定具有所有已经提交的提案。
而且这么做有一个好处是：**可以省去 Leader 服务器检查事务的提交和丢弃工作的这一步操作。**

![image.png](https://upload-images.jianshu.io/upload_images/4236553-992f9f7b32c4df32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这样，我们刚刚假设的两个问题便能够解决。假设 1 最终会丢弃调用没有提交的数据，假设 2 最终会同步所有服务器的数据。这个时候，就引出了一个问题，如何同步？

## 4. 数据同步

当崩溃恢复之后，需要在正式工作之前（接收客户端请求），Leader 服务器首先确认事务是否都已经被过半的 Follwer 提交了，即是否完成了数据同步。目的是为了保持数据一致。

当所有的 Follwer 服务器都成功同步之后，Leader 会将这些服务器加入到可用服务器列表中。

实际上，Leader 服务器处理或丢弃事务都是依赖着 ZXID 的，那么这个 ZXID 如何生成呢？

答：在 ZAB 协议的事务编号 ZXID 设计中，ZXID 是一个 64 位的数字，其中低 32 位可以看作是一个简单的递增的计数器，针对客户端的每一个事务请求，Leader 都会产生一个新的事务 Proposal 并对该计数器进行 + 1 操作。

而高 32 位则代表了 Leader 服务器上取出本地日志中最大事务 Proposal 的 ZXID，并从该 ZXID 中解析出对应的 epoch 值，然后再对这个值加一。

![image.png](https://upload-images.jianshu.io/upload_images/4236553-47e58920c2d33f33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

高 32 位代表了每代 Leader 的唯一性，低 32 代表了每代 Leader 中事务的唯一性。同时，也能让 Follwer 通过高 32 位识别不同的 Leader。简化了数据恢复流程。

基于这样的策略：当 Follower 链接上 Leader 之后，Leader 服务器会根据自己服务器上最后被提交的 ZXID 和 Follower 上的 ZXID 进行比对，比对结果要么回滚，要么和 Leader 同步。

## 5. 总结

到了总结的时刻了。

ZAB 协议和我们之前看的 Raft 协议实际上是有相似之处的，比如都有一个 Leader，用来保证一致性（Paxos 并没有使用 Leader 机制保证一致性）。再有采取过半即成功的机制保证服务可用（实际上 Paxos 和 Raft 都是这么做的）。

ZAB 让整个 Zookeeper 集群在两个模式之间转换，消息广播和崩溃恢复，消息广播可以说是一个简化版本的 2PC，通过崩溃恢复解决了 2PC 的单点问题，通过队列解决了 2PC 的同步阻塞问题。

而支持崩溃恢复后数据准确性的就是数据同步了，数据同步基于事务的 ZXID 的唯一性来保证。通过 + 1 操作可以辨别事务的先后顺序。

好了，关于 ZAB 协议就介绍到这里，篇幅有限，难免疏漏。

good luck！！！！

## 引用

[《从 Paxos 到 Zookeeper——分布式一致性原理和实践》](https://search.jd.com/Search?keyword=%E4%BB%8E%20Paxos%20%E5%88%B0%20Zookeeper%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5&enc=utf-8&wq=%E4%BB%8E%20Paxos%20%E5%88%B0%20Zookeeper%E2%80%94%E2%80%94%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5&pvid=776b62609b5a42fe875f5e520404cb0d)





# [分布式理论(八)—— Consistent Hash（一致性哈希算法）](https://www.cnblogs.com/stateis0/p/9062134.html)



## 前言

在分布式系统中，常常需要使用缓存，而且通常是集群，访问缓存和添加缓存都需要一个 hash 算法来寻找到合适的 Cache 节点。但，通常不是用取余hash，而是使用我们今天的主角—— 一致性 hash 算法。

今天楼主就来说说这个一致性 hash 算法。

## 1. 为什么普通的 hash 算法不行？

普通的 hash 算法通常都是对机器数量进行取余，比如集群环境中有 3 台 redis，当我们放入对象的时候，通常是对 3 进行取余。这种做法在大部分情况下是没有问题的。但是，注意：如果缓存机器需要增减，问题就来了。

什么问题呢？

假设原本是 3 个 redis，这时候，加了一台 redis，那么取余算法就变成了取余 4。

这样有什么问题呢？
答：当使用负载均衡的时候，负载均衡器根据对象的 key 对机器进行取余，这个时候，原有的 key 取余现有的机器数 4 就找不到那台机器了！笨一点的办法，就是在增加机器的时候，清除所有缓存，但这会导致缓存击穿甚至缓存雪崩，严重情况下引发 DB 宕机。

## 2. 一致性 hash 怎么解决这个问题？

很简单，既然问题出在对机器取余上，那么就不对机器取余。

具体怎么做呢？

答：我们可以假设有一个 2 的 32 次方的环形，缓存节点通过 hash 落在环上。而对象的添加也是使用 hash，但很大的几率是 hash 不到缓存节点的。怎么办呢？**找离他最近的那个节点。** 比如顺时针找前面那个节点。

能解决问题吗？想象一下：当增减机器时，环形节点变化的只会影响一个节点，就是新节点的顺时针方向的前面的节点。这个时候，我们只需要清除那一个节点的数据就足够了，不用想取余 hash 那样，清除所有节点的数据。

具体类似于下图：

![image.png](https://upload-images.jianshu.io/upload_images/4236553-cb43963c48dac733.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图中，节点中的五角星代表对象，红绿黄代表节点，每个对象都会找他的上一个节点。如有增减，只影响一个节点。

如下图所示：

![image.png](https://upload-images.jianshu.io/upload_images/4236553-3c6371dba3a417bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

红色和绿色节点不受影响。

## 3. 一致性 hash 有什么问题呢？

是否这么做就完美了呢？

不是的。

如果认真看是上面的图的话，会发现，黄色节点的负载压力最大，这个集群环境负载不够均衡。

![image.png](https://upload-images.jianshu.io/upload_images/4236553-0dfd0d3194269df1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

什么原因导致的呢？原因是：如果缓存节点分布不均匀，就会出现这样的情况。但是，你不能奢望是均匀的。

怎么办呢？

我们可以在不均的地方给他弄均匀。在空闲的地方加入 **虚拟节点**，这些节点的数据映射到真实节点上，就可以了，如下图所示：

![image.png](https://upload-images.jianshu.io/upload_images/4236553-a567df4d45038e90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图中，我们给每个节点都做了虚拟节点（虚线），从而让整个集群在 hash 环比较均匀，从图中也可以看出，这样现对比之前均匀多了，黄色节点的负载和绿色节点额的负载相同。

## 4. 总结

总的来说，一致性 hash 还是比较简单的。核心思想是，不使用对机器取余的算法。这样就能避免机器增减带来的影响。

同时，使用 `就近寻址` 的方式找到最近的节点。当然，这会引起负载不均衡，所以需要引入虚拟节点的方式，变相的增加节点，让整个集群的负载能够均衡。

后面，我们将自己写一个一致性 hash 算法以加深印象。

good luck！！！！

