
# 深入理解支持向量机 - 喜欢打酱油的老鸟 - CSDN博客


2019年01月02日 09:14:24[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：69


[https://www.toutiao.com/a6641484699816444420/](https://www.toutiao.com/a6641484699816444420/)
2019-01-01 19:12:37
支持向量机是机器学习领域中最常用的监督分类之一。让我们了解支持向量机（SVM）背后的直觉。请注意，在本文中，支持向量机将被称为SVM。
# 让我们建立直觉
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/a4485e2df65444c4a76999521943821c)
图1：不正确的分类器
考虑上面的分类器。有两个类：
类'+1 ' ：深色数据点。
类' -1 '：浅色数据点。
那么，以上的分类器有什么问题呢?如果查看图1中的分类器，就无法确定这两个类的适当区域。超平面(超平面是对这两个类进行分类的平面。表示为虚线)在分类器A看起来非常接近类' -1 '。超平面在分类器B看起来更接近类' +1 '。而分类器C中的超平面看起来更接近类“-1”。看起来，如果超平面更接近某个类，那么它更倾向于这个特定的类，而不是另一个类。如果是这样，那么分类器出错的几率将更大，如图2所示。
![深入理解支持向量机](http://p1.pstatp.com/large/pgc-image/718a8d749beb437db13bdb88ebdbe64e)
图2：红色数据点被分类器错误地分类
可以观察到，最佳分类器将是超平面与两个类相等的距离（无偏好）。如果类和超平面之间的距离最大，那将会好得多。这就是SVM的情况，如图3所示。
![深入理解支持向量机](http://p3.pstatp.com/large/pgc-image/33a09ec6fd164dd5ab6ad3e785154467)
图3：红色虚线是最佳超平面
绿色虚线为每个类定义了边界。类边界上的绿色粗轮廓的数据点称为支持向量。因此，命名为支持向量机。关于SVM的有趣之处在于只有支持向量可以决定最佳的超平面。我们很快就会证明这个说法。
# 问题1：如果数据点不能被直超平面分开，怎么办呢？
![深入理解支持向量机](http://p3.pstatp.com/large/pgc-image/b876358644bc4b93b7d23c3327bf9343)
图4
如果我们考虑图4，则在二维空间中不存在可以对图中所示的数据点进行分类的超平面。在这种情况下，给定空间被转换为更高维空间，使得数据点在新空间中是可分离的。因此，数据点被分类在较高维空间中并映射回原始空间。这个概念似乎很棘手。因此，仔细查看下面显示的可视化。
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/3c8f4f3604cf49c2a104b59659d89f44)

> 图5：从给定空间到更高维度的转换的可视化，在新空间中对数据点进行分类，然后将所有内容映射回原始空间。
图5中的可视化给出了关于从给定空间到高维空间的转换的清晰概念。在上述可视化中，粉红色超平面可以在较高维空间中容易地对数据点进行线性分类。因此，图4中的数据点可以分类如图6所示。
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/750450da236c43aea58424a10065ff21)
图6
# 问题2：核函数的作用
SVM中的另一个问题是转换到更高维空间，然后在新空间中确定最佳超平面然后转换回原始空间的过程非常复杂并且成本高。例如，如果在一个100维空间中有1000个特征，如果这个100维空间被转换成1000维空间，那么每个特征向量将有1000个成分和1000 * 1000个计算(因为超平面被描述为WX + b = 0，其中X是特征向量)来确定最优的超平面，同样，超平面将被映射回原始空间。整个过程的成本很高。
上述问题的解决方案是核函数。关于核函数的有趣事实是核函数执行上述映射而不实际进入更高维空间。换句话说，核函数执行上述映射而不实际在较高维空间中执行所有上述计算。
使用多项式核函数φ（（a，b））=（a，b，a²+b²）完成图5所示的可视化变换，如图7所示。
![深入理解支持向量机](http://p3.pstatp.com/large/pgc-image/708abdea0b404c6a9df5af26957e0ee9)
图7：SVM的训练示例，其核由φ（（a，b））=（a，b，a²+b²）给出
请注意，核函数只适用于由点积或内积构成的问题。幸运的是，SVM的公式依赖于点积。
# 数学建模
让我们了解支持向量机背后的数学。给定机器学习训练集{（Xᵢ，Yᵢ），其中i = 1,2,3，...，n}，Xᵢ∈ℜᵐ，Yᵢ∈{+ 1，-1}。这里，Xᵢ是iᵗʰ数据点的特征向量，Yᵢ是iᵗʰ数据点的标签。对于正类，标签可以是“+1”，对于负类，标签可以是“-1”。
设Wᵢ是垂直于决策边界（最佳超平面）的向量，Xᵢ是未知向量。然后，Xᵢ向量在Wᵢ的单位向量上的投影将确定该未知点是否属于正类或负类，如图8所示。
![深入理解支持向量机](http://p9.pstatp.com/large/pgc-image/f22fdde273ca4f81a967320815ea3cc0)
图8
注意：在接下来的部分中，Wᵗ表示W转置。两个向量W和X之间的点积与Wᵗ和X之间的矩阵乘法相同。
基本上，对于具有由WᵗXᵢ+ b = 0给出的决策边界的分类器，可以说：
![深入理解支持向量机](http://p3.pstatp.com/large/pgc-image/ceec2d85f3ce4c6985212b3eeca2252b)
图9
![深入理解支持向量机](http://p1.pstatp.com/large/pgc-image/c6fca0cb24224d78adccde8fcaedc1b3)
图10
现在，如果我们采用所有'+1' pattern的最小值，那么它将是一些常数ε，如果我们取所有'-1' pattern的最大值，那么它将是一些常数-ε。这里，ε和-ε表示支持向量。在图10中，具有粗边框的数据点是支持向量。所以，它可以写成：
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/0395376dfe934442aa8d45f94f599f39)
图11
现在，如果我们用1 / {ε}来缩放上述不等式，那么最优超平面将保持相同。因此，我们对缩放矢量使用相同的符号。所以，它可以写成：
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/1549dacc5bbf40e09ce2b10ca6d74177)
图12
可以观察到，如果上面的不等式与相应的Yᵢ相乘，则上述断言可以用单个表达式来描述：
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/ed1271a579ef49d6834a3bc1bd2327e8)
图13
在图12的第一个方程中，Yᵢ= 1，当1乘以时，我们得到1。在第二个方程中Yᵢ= -1，当-1乘以RHS （-1）时，我们得到1。
设 X⁺为正类下的支持向量，X⁻为负类下的支持向量。然后，WX⁺ + b =1 ⇒ WX⁺ = 1 - b。类似地，WX⁻ + b =-1 ⇒ WX⁻ = -1 - b。然后，向量（X⁺-X - ）在W向量的单位向量上的投影给出了分离间隙的宽度或两类支持向量之间的边界。边距的宽度由下式给出：
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/9eee8cb2ec65405b8196a6005d0ad844)
图14：分离间隙的宽度
SVM的目标是最大化分离间隙的宽度。这意味着最大化2 / || W || ，这与最小化|| W ||相同， 这与最小化|| W ||²相同，与最小化（1/2）|| W ||²相同，同样可以写为（1/2）WᵗW。这样做是为了方便数学。此外，这样做会将问题转化为二次优化问题，二次优化问题没有局部最大值。除此之外，二次优化问题的原始和对偶总是相同的。SVM的基本问题可以简单地写成：
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/3467562c79584ebb95cd376a419f8886)
图15：hard SVM的问题
在实际场景中，数据不是严格线性可分的。因此，通过引入松弛变量'ξ'和惩罚项'C'来修改问题。这里，'C'是一种正则化参数。实际上，如果'C'的值很大，则意味着它不会忽略任何错误，而是会惩罚几乎每一个错误，在这种情况下，它可能无法找到最佳的W和b。然而，如果'C'的值非常小，则意味着它忽略了几乎所有的错误，然后W和b将获得任何随机值。因此，我们必须使用交叉验证来选择合适的“C”值，它必须介于非常小和非常大之间。每个错误都不是同样糟糕，它使用松弛变量'ξ'，它是数据点与另一侧的类边距之间的距离，如图16所示。如果数据点的ξᵢ小，那么错误不那么糟糕，C *ξᵢ会更少，所以，对错误的惩罚会更少。如果数据点的ξᵢ大，则错误严重，因此C *ξᵢ将更大，所以，对错误的惩罚会很高。
![深入理解支持向量机](http://p3.pstatp.com/large/pgc-image/5711179c1d154454b063713643815c24)
图16：具有最佳超平面和松弛变量的SVM
因此，图15中的问题可以改写为：
![深入理解支持向量机](http://p3.pstatp.com/large/pgc-image/a6bc09bc08c34974b3f0d3baef39e49f)
图17：Soft SVM的问题
hard SVM尝试严格对数据点进行分类，但实际上这是不可能的。因此，就需要了解soft SVM了。
在进一步研究之前，请考虑以下几点：
了解等高线图对于理解拉格朗日的概念非常重要。
请记住以下几点：（i）如果问题是找到极限（最大值或最小值），则使用拉格朗日。（ii）拉格朗日将约束优化问题转换为无约束优化问题。
约束优化问题的拉格朗日函数包括主要问题以及与拉格朗日乘数相结合的所有约束。对于每个约束，使用拉格朗日乘数。
对偶性：根据对偶性，每个优化问题都有另一个与之相关的问题，因此可以从前者得出。最初的问题称为'原始问题'，派生的问题称为'对偶问题'。在二次优化问题中，原始问题和对偶问题的解是相同的。
希望你考虑过上述几点。因此，图17中的等式是原始问题。但是，我们实际上对对偶问题很感兴趣。因为对偶问题是由特征向量与核函数的点积构成的，可以用于非线性扩展(即对于非线性可分的数据点的非线性扩展)。
在这里，拉格朗日乘数用于二元化。拉格朗日乘子的总数等于原始问题中约束的总数。如图17所示，方程中有两个约束条件，拉格朗日函数将使用两个拉格朗日乘数。拉格朗日函数为:
![深入理解支持向量机](http://p3.pstatp.com/large/pgc-image/29e80cf519e643b5a13ca5314d0fbba6)
图18：拉格朗日
图18中的等式（L）是拉格朗日，μ，λ是拉格朗日乘数，μ，λ≥0。拉格朗日方程的最小值是通过将它们的偏导数与变量相乘并将它们设置为零来得到的（如同Karush-Kuhn-Tucker（KKT）条件）。给定问题的KKT条件如下：
![深入理解支持向量机](http://p9.pstatp.com/large/pgc-image/c4632278338a40d6b9beb82bf8d5af96)
图19：KKT条件
注意：此处，1 /Yᵢ始终等于Yᵢ 。因为，Yᵢ=1⇒1/Yᵢ= 1/1 = 1 =Yᵢ，Yᵢ=-1⇒1/Yᵢ= 1 /（ - 1）= -1 =Yᵢ 。所以，在接下来的部分中，1 /Yᵢ总是写成Yᵢ。
从图19中的第一个方程式，它可以写成：
W =ΣᵢⁿμᵢYᵢXᵢ对于i = 1,2，...，n - - - - - - - - - - - - - - >（A）
现在，从图19中的第三个方程，可以写出C =μᵢ+λᵢ。因此，对于0 <μᵢ<C，λᵢ= 0，因此ξᵢ= 0。因此，图19中的第六个方程：μᵢ⋅[Yᵢ（WᵗXᵢ+ b）-1 +ξᵢ] = 0可以改写为：
Yᵢ（WᵗXᵢ+ b）= 1 - - - - - - - - - - - - - - - - - - - - - - >（B）
等式（B）描述了支持向量。对于Yᵢ=1⇒WᵗXᵢ+ b = 1，即位于“+1”类边界上的点。对于Yᵢ=-1⇒WᵗXᵢ+ b = -1，即位于类'-1'的边界上的点。因此，最佳超平面取决于支持向量。现在，等式（B）可以改写为：
b =Yᵢ-WᵗXᵢ使得0 <μᵢ<C - - - - - - - - - - - - - - >（C）
因此，方程（A）和方程（C）给出最优W和最优C，因此得到最优超平面（WᵗXᵢ+ b = 0）。此外，在方程（A）中可以观察到W可以完全描述为训练模式Xᵢ的线性组合。现在，将最优W和b的值放在拉格朗日方程中，得到对偶优化问题为:
![深入理解支持向量机](http://p3.pstatp.com/large/pgc-image/6cf49cf58b3a422791757223e5f111f8)
图20:对偶优化问题
注意，根据图20中的等式，可以根据数据点之间的点积（XᵢᵗXⱼ）来描述完整算法。因此，核技巧可用于非线性扩展，即任何核函数都可用于在高维空间中找到点积，如XᵢᵗXⱼ= K（φ（Xᵢ），φ（Xⱼ））。一些流行的核函数是：
![深入理解支持向量机](http://p99.pstatp.com/large/pgc-image/e2117d4c654a4d3caef2589e69650822)
核函数

