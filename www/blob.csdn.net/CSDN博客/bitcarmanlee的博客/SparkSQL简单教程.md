
# SparkSQL简单教程 - bitcarmanlee的博客 - CSDN博客


2017年12月28日 09:32:19[bitcarmanlee](https://me.csdn.net/bitcarmanlee)阅读数：1569


当面对一堆格式化的数据需要做一些统计分析的时候，awk是个非常不错的选择。但是当数据量上来以后，通过单机awk的方式处理就显得有些力不从心，这个时候我们可以通过SparkSQL来模拟sql的方式来处理这些海量数据，现在就给大家举个实例，看看怎么通过简单的几行代码用SparkSQL的方式来分析海量数据。
## 1.原始数据
在hdfs上有个路径为XXX，数据规模大概为100G左右，都是格式化的标准数据，每一行四个字段，其中某一行格式为`u1 20170916    20171203    OPPO R11`。现在我们想针对第四个字段做group by的操作。如果数据量比较小，例如1G左右的规模，用awk来处理是很方便的。但是100G的数据规模，用awk显然就不合适了。现在我们试着用SparkSQL来搞定这个需求。
## 2.在spark-shell中使用SparkSQL
2.1 先启动spark-shell，没啥好说的。
2.2 启动spark-shell以后，默认有SparkContext的入口，即sc变量，但是没有SparkSQL的入口，这个时候需要我们新建一个SparkSql的入口。
```python
scala> val sqlContext = new org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.SQLContext
```
```python
(sc)
```
```python
warning:
```
```python
there was one deprecation warning
```
```python
; re-run with -deprecation for details
```
```python
sqlContext:
```
```python
org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.SQLContext
```
```python
= org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.SQLContext
```
```python
@407
```
```python
e10a9
```
2.3 同时要将sqlContext的隐变量导入，因为在Scala中使用反射方式，进行RDD到DataFrame的转换，需要手动导入一个隐式转换：
```python
scala>
```
```python
import
```
```python
sqlContext.implicits._
```
```python
import
```
```python
sqlContext.implicits._
```
2.4 然后针对输入生成RDD:
```python
scala> val rdd = sc
```
```python
.textFile
```
```python
(
```
```python
"XXX"
```
```python
)
```
```python
.filter
```
```python
(
```
```python
x
```
```python
=>
```
```python
x
```
```python
.split
```
```python
(
```
```python
"\t"
```
```python
)
```
```python
.length
```
```python
==
```
```python
4
```
```python
)
```
```python
.map
```
```python
(
```
```python
x
```
```python
=>
```
```python
x
```
```python
.split
```
```python
(
```
```python
"\t"
```
```python
)(
```
```python
3
```
```python
))
```
```python
rdd:
```
```python
org
```
```python
.apache
```
```python
.spark
```
```python
.rdd
```
```python
.RDD
```
```python
[String] = MapPartitionsRDD[
```
```python
3
```
```python
] at map at <console>:
```
```python
29
```
2.5 为了方便后面使用，定义一个case class:
```python
scala>
```
```python
case
```
```python
class
```
```python
Model
```
```python
(
```
```python
model
```
```python
:
```
```python
String
```
```python
)
```
```python
defined
```
```python
class
```
```python
Model
```
2.6 接下来是关键的一步：将我们将前面生成的rdd转化为DataFrame：
```python
scala> val df = rdd
```
```python
.map
```
```python
(arr => Model(arr))
```
```python
.toDF
```
```python
()
```
```python
df:
```
```python
org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.DataFrame
```
```python
= [model: string]
```
2.7 注册一个临时表：
```python
scala> df.registerTempTable(
```
```python
"tmp"
```
```python
)
warning: there was
```
```python
one
```
```python
deprecation warning; re-run
```
```python
with
```
```python
-deprecation
```
```python
for
```
```python
details
```
2.8 接下来就可以使用类似的Sql语句来分析了：
```python
scala> val res = sqlContext
```
```python
.sql
```
```python
(
```
```python
"select upper(model), count(*) from tmp group by upper(model)"
```
```python
)
```
```python
.collect
```
```python
()
```
```python
res:
```
```python
Array[org
```
```python
.apache
```
```python
.spark
```
```python
.sql
```
```python
.Row
```
```python
] = Array([OPPO
```
```python
R11
```
```python
,
```
```python
1
```
```python
xxx], [OPPO A57,
```
```python
1
```
```python
xxxxxxx], [VIVO Y66,
```
```python
1
```
```python
xxxxxxxx], [VIVO X9,
```
```python
1
```
```python
xxxx])
```
通过上面简单的几行代码，就能在spark-shell上用SparkSql分析海量数据了！

