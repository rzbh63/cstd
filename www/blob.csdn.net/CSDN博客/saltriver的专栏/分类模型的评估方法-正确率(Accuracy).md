
# 分类模型的评估方法-正确率(Accuracy) - saltriver的专栏 - CSDN博客


2017年06月29日 21:32:49[saltriver](https://me.csdn.net/saltriver)阅读数：9615


我们知道，机器学习的一大任务是”分类”。我们构建了一个分类模型，通过训练集训练好后，那么这个**分类模型到底预测效果怎么样呢**?那就需要进行**评估验证**。
评估验证当然是在**测试集**上。问题是，我通过什么评估这个分类模型呢?也就是说我们怎么给这个模型打分呢?
想想我们上学时的考试，总分100分，总共100道题，作对1题给1分，最后会有一个得分，例如80分，90分，换算成百分比就是80%，90%，这是我们自然而然能想到的评估方法。在测试集上，假定有10000个样本数据，这个模型进行正确分类的样本数据是9000个，其它1000个都分错了，那么得分就是9000/10000=90%。
这种很简单很直观的评估方法就是**正确率(Accuracy）**。在一般情况下，这种方式就很好了，注意：**正确率(Accuracy)也经常翻译成准确率**。
但是在很多情况下，这种方式就不那么好了。例如考试中的100道题，不是每道题都是1分，而是前90道题是选择题，每题1分，后10道题是应用题，每题20分。A同学答对了前80道题，B同学答对了后80道题，按照答对题的数量，都是80道题，**正确率都是80%**。但是，后面的题更重要，B同学理应比A同学得分更高。
要知道我们的试卷就是这么设计的，也就是说有些题比其它题更重要，比如应用题就比选择题重要。同样，在数据样本中，**有些数据样本就是比其它数据样本更重要**。
例如：地震局的地壳活动数据，100万个测试数据样本中，只有10个是有地震的，另外的999990个数据是没有地震的.如果我们的分类模型预测对了这999990个没有地震的数据，另外10个有地震的都预测错了，那么按照**正确率(Accuracy)的计算**，得分仍然高达99.999%，这显然是离谱的评估。
再例如：医院的肺癌检查数据，10万个测试数据样本中，只有100个是有癌症的；信用卡交易数据中，1亿个测试数据样本，只有1万个是欺诈交易；像预测地震、癌症、欺诈交易写等等这些例子可是我们机器学习领域经常遇到的状况。
因此，**正确率(Accuracy)虽然简单直观，但在很多时候并不是一个真正正确的评估指标**。

