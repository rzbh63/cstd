
# 大数据面试题总结（附答案） - Magician的博客 - CSDN博客


2018年01月02日 15:02:27[春雨里de太阳](https://me.csdn.net/qq_16633405)阅读数：3744所属专栏：[自学大数据之路](https://blog.csdn.net/column/details/18514.html)




### 文章目录
[前言](#_2)
[hadoop相关试题](#hadoop_7)
[Hive相关试题](#Hive_103)
[Hbase相关试题](#Hbase_123)
[Storm相关试题](#Storm_153)
[Spark相关试题](#Spark_162)
[Java基础试题](#Java_172)
[其他](#_177)

# 前言
最近由于要准备面试就开始提早看些面试、笔试题。以下是自己总结的一些经常出现、有价值的试题，包含hadoop、hive、hbase、storm、spark等。答案仅供参考，如有错误，请指出。试题不定时更新。
## hadoop相关试题
MapTask并行机度是由什么决定的？
由切片数量决定的。
MR是干什么的？
MR将用户编写的业务逻辑代码和自带的默认组件结合起来组成一个完整的分布式应用程序放到hadoop集群上运行。
MR的实例进程：
driver(mr的job提交客户端)
MRAppMaster
MapTask
ReduceTask
combiner和partition的作用：
combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量
partition的默认实现是hashpartition，是map端将数据按照reduce个数取余，进行分区，不同的reduce来copy自己的数据。
partition的作用是将数据分到不同的reduce进行计算，加快计算效果。
什么是shuffle
map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle；
shuffle: 洗牌、发牌——（核心机制：数据分区，排序，缓存）；
具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序；
MR原理(详细解释参照：[MR运行原理剖析](http://blog.csdn.net/qq_16633405/article/details/78924165))：
InputFormat来读取数据，按行读取，返回KV值传到map方法中，
context.write方法将处理后数据输出到outputCollector中，
当outputCollector中的数据累计到一定数量后再将数据传到内存的环形缓冲区做处理，
当环形缓冲区中的数据累积到一定数量后再将数据通过Splier多次溢出到本地磁盘的多个文件中，期间会对各个溢出的数据进行分区、排序
然后对多个文件进行merge（归并排序）形成一个输出结果大文件
ruduceTask根据自己的分区号去各个mapTask机器上取输出结果文件
将得到的各个结果文件进行merge，然后进入reduce阶段，
context.write将最终结果输出到outPutformat上，进而输出到本地文件中。
举一个简单的例子说明mapreduce是怎么来运行的 ?
wd例子。详细解释参考：[Wd详解](http://blog.csdn.net/qq_16633405/article/details/78404018)
什么是yarn？
Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统之上的应用程序。
namenode的safemode是怎么回事？如何才能退出safemode？
namenode在刚启动的时候元数据只有文件块信息，没有文件所在datanode的信息，需要datanode自己向namenode汇报。如果namenode发现datanode汇报的文件块信息没有达到namenode内存中所有文件块的总阈值的一个百分比，namenode就会处于safemode。
只有达到这个阈值，namenode才会推出safemode。也可手动强制退出。
secondarynamenode的主要职责是什么？简述其工作机制
sn的主要职责是执行checkpoint操作
每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）
如果namenode宕机，datanode节点是否也会跟着挂掉？
否
一个datanode 宕机,怎么一个流程恢复？
Datanode宕机了后，如果是短暂的宕机，可以实现写好脚本监控，将它启动起来。如果是长时间宕机了，那么datanode上的数据应该已经被备份到其他机器了，
那这台datanode就是一台新的datanode了，删除他的所有数据文件和状态文件，重新启动
hadoop 的 namenode 宕机,怎么解决？
先分析宕机后的损失，宕机后直接导致client无法访问，内存中的元数据丢失，但是硬盘中的元数据应该还存在，如果只是节点挂了，
重启即可，如果是机器挂了，重启机器后看节点是否能重启，不能重启就要找到原因修复了。
但是最终的解决方案应该是在设计集群的初期就考虑到这个问题，做namenode的HA。
简述hadoop安装
改IP，修改Host文件；
装JDK配置环境变量；
装Hadoop配置环境变量；
修改hadoop的配置文件如core-site、marp-site、yarn-site、dfs-site等；
namenode进行格式化；
start-all；
请列出hadoop正常工作时要启动那些进程，并写出各自的作用。
namenode:管理集群并记录datanode的元数据，相应客户端的请求。
seconder namenode：对namenode一定范围内的数据做一份快照性备份。
datanode：存储数据。
jobTracker：管理客户端提交的任务，并将任务分配给TaskTracker。
TaskTracker：执行各个Task。
JobTracker和TaskTracker的功能
JobTracker是一个master服务，软件启动之后JobTracker接收Job，负责调度Job的每一个子任务task运行于TaskTracker上，
并监控它们，如果发现有失败的task就重新运行它。一般情况应该把JobTracker部署在单独的机器上。
TaskTracker是运行在多个节点上的slaver服务。TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务。
用mapreduce怎么处理数据倾斜问题？
数据倾斜：map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，
这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，
从而导致某几个节点迟迟运行不完，此称之为数据倾斜。
解决：自己实现partition类，用key和value相加取hash值。
Mapreduce 的 map 数量 和 reduce 数量 怎么确定 ,怎么配置？
map的数量有数据块决定，reduce数量随便配置。
hdfs的体系结构
hdfs有namenode、secondraynamenode、datanode组成。
namenode负责管理datanode和记录元数据
secondraynamenode负责合并日志
datanode负责存储数据
说下对hadoop 的一些理解,包括哪些组件
详谈hadoop的应用，包括的组件分为三类，分别说明hdfs，yarn，mapreduce。
一些传统的hadoop 问题,mapreduce 他就问shuffle 阶段,你怎么理解的
Shuffle意义在于将不同map处理后的数据进行合理分配，让reduce处理，从而产生了排序、分区。
NameNode 负责管理 metadata，client 端每次读写请求，它都会从磁盘中读取或则会写入 metadata信息并反馈client 端。（错误）
修改后分析：
NameNode 不需要从磁盘读取 metadata，所有数据都在内存中，硬盘上的只是序列化的结果，只有每次
namenode 启动的时候才会读取。
Hive相关试题
你的数据库是不是很大么,有没有分表,分区,你是怎么实现的
hive内部表和外部表的区别
内部表：加载数据到hive所在的hdfs目录，删除时，元数据和数据文件都删除
外部表：不加载数据到hive所在的hdfs目录，删除时，只删除表结构。
分桶的作用
最大的作用是提高join的效率。（1）获得更高的查询处理效率。（2）使取样（sampling）更高效。
Hive 你们用的是外部表还是内部表,有没有写过UDF。
UDF：
1、写对应的java代码自定义函数的逻辑
2、将代码打成jar包上传到hive
3、在hive创建临时函数与对应的class类相关联
4、在hive中调用临时函数。
Hbase相关试题
hbase的rowkey怎么创建好？列族怎么创建比较好？（重点）
hbase存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分利用排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)
一个列族在数据底层是一个文件，所以将经常一起查询的列放到一个列族中，列族尽量少，减少文件的寻址时间。
Redis,传统数据库,hbase,hive 每个之间的区别？(问的非常细)
redis：分布式缓存，强调缓存，内存中数据
传统数据库：注重关系
hbase：列式数据库，无法做关系数据库的主外键，用于存储海量数据，底层基于hdfs
hive：数据仓库工具，底层是mapreduce。不是数据库，不能用来做用户的交互存储
hdfs 和 hbase 各自使用场景。
整理总结：
首先一点需要明白：Hbase 是基于 HDFS 来存储的。
HDFS：
1、一次性写入，多次读取。
2、保证数据的一致性。
3、主要是可以部署在许多廉价机器中，通过多副本提高可靠性，提供了容错和恢复机制。
Hbase：
1、瞬间写入量很大，数据库不好支撑或需要很高成本支撑的场景。
2、数据需要长久保存，且量会持久增长到比较大的场景
3、hbase 不适用与有 join，多级索引，表关系复杂的数据模型
4、大数据量 （100s TB 级数据） 且有快速随机访问的需求。
如：淘宝的交易历史记录。数据量巨大无容置疑，面向普通用户的请求必然要即时响应。
5、容量的优雅扩展
大数据的驱使，动态扩展系统容量的必须的。例如：webPage DB。
6、业务场景简单，不需要关系数据库中很多特性（例如交叉列、交叉表，事务，连接等等）
7、优化方面：合理设计 rowkey。因为 hbase 的查
Storm相关试题
公司技术选型可能利用storm 进行实时计算,讲解一下storm
描述下storm的设计模式，是基于nimbus、supervisor、work、task的方式运行代码，由spout、bolt组成等等。
实时流式计算框架,几个人,多长时间,细节问题,包括讲flume ,kafka ,storm
的各个的组件组成,你负责那一块,如果需要你搭建你可以完成么?（多次提到）
Spark相关试题
你觉得spark 可以完全替代hadoop 么?
spark会替代mr,不会代替yarn和hdfs.
公司之后倾向用spark 开发,你会么(就用java代码去写)
会，spark使用scala开发的，在scala中可以随意使用jdk的类库，可以用java开发，但是最好用原生的scala开发，兼容性好，scala更灵活。
介绍下Spark
基于内存的分布式计算框架，基于Master、Worker、Executer模式运行，基本数据抽象为RDD（分布式数据集）。Spark Streaming类似于Apache Storm，用于流式数据的处理
Java基础试题
请参考
[http://blog.csdn.net/qq_16633405/article/details/79211002](http://blog.csdn.net/qq_16633405/article/details/79211002)
其他
简单介绍下flume、sqoop、azkaban
flume：分布式数据采集框架，核心校色agent由source、channel、sink组成。
sqoop:hadoop和关系型数据库传送数据的工具。
azkaban:工作流调度器。

