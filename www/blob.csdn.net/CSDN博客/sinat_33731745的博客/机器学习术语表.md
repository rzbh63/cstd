
# 机器学习术语表 - sinat_33731745的博客 - CSDN博客

2018年03月14日 08:19:51[Tao-Tao-Tao](https://me.csdn.net/sinat_33731745)阅读数：64标签：[机器学习																](https://so.csdn.net/so/search/s.do?q=机器学习&t=blog)个人分类：[机器学习																](https://blog.csdn.net/sinat_33731745/article/category/7400838)



# A
### accuracy (准确率)
分类模型预测准确的比例。在多类别分类中，准确率定义如下：
accuracy=
$$
accuracy =\frac{correct predictions}{total number of examples}
$$
在二分类中，准确率定义为：
accuracy=
$$
accuracy = \frac{true positives +true negatives}{total number of examples}
$$

### activation function (激活函数)
一种函数（例如ReLU或Sigmoid），将前一层所有神经元激活值的加权和输入到一个非线性函数中，然后向下一层传递该函数的输出值（典型的非线性）。
### adagrad
### auc （曲线下面积）
一种考虑到所有可能的分类阈值的评估标准。ROC 曲线下面积代表分类器随机预测真正类（Ture Positives）要比假正类（False Positives）概率大的确信度。
# B
### backpropagation (反向传播)
神经网络中完成梯度下降的重要算法。首先，在前向传播的过程中计算每个节点的输出值。然后，在反向传播的过程中计算与每个参数对应的误差的偏导数。
### baseline (基线)
被用为对比模型表现参考点的简单模型。基线帮助模型开发者量化模型在特定问题上的预期表现。
### batch（批量）
模型训练中一个迭代（指一次梯度更新）使用的样本集。
### batch size（批量大小）
一个批量中样本的数量。例如，SGD 的批量大小为 1，而 mini-batch 的批量大小通常在 10-1000 之间。批量大小通常在训练与推理的过程中确定，然而 TensorFlow 不允许动态批量大小。
### bias（偏置）
与原点的截距或偏移量。偏置（也称偏置项）被称为机器学习模型中的 b 或者 w0。例如，偏置项是以下公式中的 b：y=b+w1x1+w2x2+…wnxn
$$
y′=b+w_1x_1+w_2x_2+…w_nx_n
$$

### binary classification(二元分类器)
一类分类任务，输出两个互斥（不相交）类别中的一个。例如，一个评估邮件信息并输出「垃圾邮件」或「非垃圾邮件」的机器学习模型就是一个二元分类器。
### binning/bucketing
根据值的范围将一个连续特征转换成多个称为 buckets 或者 bins 二元特征，称为 buckets 或者 bins。例如，将温度表示为单一的浮点特征，可以将温度范围切割为几个离散的 bins。假如给定的温度的敏感度为十分之一度，那么分布在 0.0 度和 15.0 度之间的温度可以放入一个 bin 中，15.1 度到 30.0 度放入第二个 bin，30.1 度到 45.0 度放入第三个 bin。
# C
### calibration layer(标定层)
一种调整后期预测的结构，通常用于解释预测偏差。调整后的预期和概率必须匹配一个观察标签集的分布。
### candidate sampling(候选采样)
一种优化训练时间的，使用 Softmax 等算法计算所有正标签的概率，同时只计算一些随机取样的负标签的概率。例如，有一个样本标记为「小猎兔狗」和「狗」，候选取样将计算预测概率，和与「小猎兔狗」和「狗」类别输出（以及剩余的类别的随机子集，比如「猫」、「棒棒糖」、「栅栏」）相关的损失项。这个想法的思路是，负类别可以通过频率更低的负强化（negative reinforcement）进行学习，而正类别经常能得到适当的正强化，实际观察确实如此。候选取样的动力是计算有效性从所有负类别的非计算预测的得益。
### checkpoint(检查点)
在特定的时刻标记模型的变量的状态的数据。检查点允许输出模型的权重，也允许通过多个阶段训练模型。检查点还允许跳过错误继续进行（例如，抢占作业）。注意其自身的图式并不包含于检查点内。
### class(类别)
所有同类属性的目标值作为一个标签。例如，在一个检测垃圾邮件的二元分类模型中，这两个类别分别是垃圾邮件和非垃圾邮件。而一个多类别分类模型将区分狗的种类，其中的类别可以是贵宾狗、小猎兔狗、哈巴狗等等。
### class-imbalanced data set（类别不平衡数据集）
这是一个二元分类问题，其中两个类别的标签的分布频率有很大的差异。比如，一个疾病数据集中若 0.01% 的样本有正标签，而 99.99% 的样本有负标签，那么这就是一个类别不平衡数据集。但对于一个足球比赛预测器数据集，若其中 51% 的样本标记一队胜利，而 49% 的样本标记其它队伍胜利，那么这就不是一个类别不平衡数据集。
### classification（分类模型）
机器学习模型的一种，将数据分离为两个或多个离散类别。例如，一个自然语言处理分类模型可以将一句话归类为法语、西班牙语或意大利语。分类模型与回归模型（regression model）成对比。
### classification threshold（分类阈值）
应用于模型的预测分数以分离正类别和负类别的一种标量值标准。当需要将 logistic 回归的结果映射到二元分类模型中时就需要使用分类阈值。例如，考虑一个确定给定邮件为垃圾邮件的概率的 logistic 回归模型，如果分类阈值是 0.9，那么 logistic 回归值在 0.9 以上的被归为垃圾邮件，而在 0.9 以下的被归为非垃圾邮件。
### confusion matrix（混淆矩阵）
总结分类模型的预测结果的表现水平（即，标签和模型分类的匹配程度）的 NxN 表格。混淆矩阵的一个轴列出模型预测的标签，另一个轴列出实际的标签。N 表示类别的数量。在一个二元分类模型中，N=2。例如，以下为一个二元分类问题的简单的混淆矩阵：
![混淆矩阵](https://pic.36krcnd.com/201710/05065402/esikeg3sh1e2nnwa)
上述混淆矩阵展示了在 19 个确实为肿瘤的样本中，有 18 个被模型正确的归类（18 个真正），有 1 个被错误的归类为非肿瘤（1 个假负类）。类似的，在 458 个确实为非肿瘤的样本中，有 452 个被模型正确的归类（452 个真负类），有 6 个被错误的归类（6 个假正类）。
多类别分类的混淆矩阵可以帮助发现错误出现的模式。例如，一个混淆矩阵揭示了一个识别手写数字体的模型倾向于将 4 识别为 9，或者将 7 识别为 1。混淆矩阵包含了足够多的信息可以计算很多的模型表现度量，比如精度（precision）和召回（recall）率。
### continuous feature（连续特征）
拥有无限个取值点的浮点特征。和离散特征（discrete feature）相反。
### convergence(收敛)
训练过程达到的某种状态，其中训练损失和验证损失在经过了确定的迭代次数后，在每一次迭代中，改变很小或完全不变。换句话说就是，当对当前数据继续训练而无法再提升模型的表现水平的时候，就称模型已经收敛。在深度学习中，损失值下降之前，有时候经过多次迭代仍保持常量或者接近常量，会造成模型已经收敛的错觉。
### concex function（凸函数）
一种形状大致呈字母 U 形或碗形的函数。然而，在退化情形中，凸函数的形状就像一条线。例如，以下几个函数都是凸函数：
L2 损失函数
Log 损失函数
L1 正则化函数
L2 正则化函数
凸函数是很常用的损失函数。因为当一个函数有最小值的时候（通常就是这样），梯度下降的各种变化都能保证找到接近函数最小值的点。类似的，随机梯度下降的各种变化有很大的概率（虽然无法保证）找到接近函数最小值的点。
两个凸函数相加（比如，L2 损失函数+L1 正则化函数）后仍然是凸函数。
深度模型通常是非凸的。出乎意料的是，以凸优化的形式设计的算法通常都能在深度网络上工作的很好，虽然很少能找到最小值。
### cost（成本）
loss 的同义词。
### cross-entropy（交叉熵）
多类别分类问题中对 Log 损失函数的推广。交叉熵量化两个概率分布之间的区别。参见困惑度（perplexity）。
# D
### data set（数据集）
样本的集合。
### decision boundary（决策边界）
在一个二元分类或多类别分类问题中模型学习的类别之间的分离器。例如，下图就展示了一个二元分类问题，决策边界即橙点类和蓝点类的边界。
![决策边界](https://pic.36krcnd.com/201710/05065402/82zx22s46l7nikp0)
### deep model（深度模型）
一种包含多个隐藏层的神经网络。深度模型依赖于其可训练的非线性性质。和宽度模型对照（wide model）。
### dense feature（密集特征）
大多数取值为非零的一种特征，通常用取浮点值的张量（tensor）表示。和稀疏特征（sparse feature）相反。
### derived feature（派生特征）
合成特征（synthetic feature）的同义词。
## discrete feature（离散特征）
只有有限个可能取值的一种特征。例如，一个取值只包括动物、蔬菜或矿物的特征就是离散（或类别）特征。和连续特征（continuous feature）对照。
### dropout regularization（dropout 正则化）
训练神经网络时一种有用的正则化方法。dropout 正则化的过程是在单次梯度计算中删去一层网络中随机选取的固定数量的单元。删去的单元越多，正则化越强。
### dynamic model（动态模型）
以连续更新的方式在线训练的模型。即数据连续不断的输入模型。
# E
### early stopping（早期停止法）
一种正则化方法，在训练损失完成下降之前停止模型训练过程。当验证数据集（validation data set）的损失开始上升的时候，即泛化表现变差的时候，就该使用早期停止法了。
### embeddings（嵌入）
一类表示为连续值特征的明确的特征。嵌入通常指将高维向量转换到低维空间中。例如，将一个英语句子中的单词以以下任何一种方式表示：
拥有百万数量级（高维）的元素的稀疏向量，其中所有的元素都是整数。向量的每一个单元表示一个单独的英语单词，单元中的数字表示该单词在一个句子中出现的次数。由于一个句子中的单词通常不会超过 50 个，向量中几乎所有的单元都是 0。少量的非零的单元将取一个小的整数值（通常为 1）表示句子中一个单词的出现次数。
拥有数百个（低维）元素的密集向量，其中每一个元素取 0 到 1 之间的浮点数。
在 TensorFlow 中，嵌入是通过反向传播损失训练的，正如神经网络的其它参量一样。
### empirical risk minimization，ERM（经验风险最小化）
选择能最小化训练数据的损失的模型函数的过程。和结构风险最小化（structual risk minimization）对照。
### ensemble（集成）
多个模型预测的综合考虑。可以通过以下一种或几种方法创建一个集成方法：
设置不同的初始化；
设置不同的超参量；
设置不同的总体结构。
深度和广度模型是一种集成。
### Estimator（评估器）
tf.Estimator 类的一个例子，封装 logic 以建立一个 TensorFlow 图并运行一个 TensorFlow session。你可以通过以下方式创建自己的评估器：[https://www.tensorflow.org/extend/estimators](https://www.tensorflow.org/extend/estimators)
### example（样本）
一个数据集的一行内容。一个样本包含了一个或多个特征，也可能是一个标签。参见标注样本（labeled example）和无标注样本（unlabeled example）。
# F
### false negative，FN(假负类)
被模型错误的预测为负类的样本。例如，模型推断一封邮件为非垃圾邮件（负类），但实际上这封邮件是垃圾邮件。
### false positive，FP(假正类)
被模型错误的预测为正类的样本。例如，模型推断一封邮件为垃圾邮件（正类），但实际上这封邮件是非垃圾邮件。
### false positive rate，FP rate(假正类率)
ROC 曲线（ROC curve）中的 x 轴。FP 率的定义是：假正率=假正类数/(假正类数+真负类数)
### feature(特征)
输入变量，用于做出预测。
### feature columns/FeatureColumn(特征列)
具有相关性的特征的集合，比如用户可能居住的所有可能的国家的集合。一个样本的一个特征列中可能会有一个或者多个特征。
TensorFlow 中的特征列还可以压缩元数据比如下列情况：
特征的数据类型；
一个特征是固定长度的或应该转换为嵌入。
一个特征列可以仅包含一个特征。「特征- 列」是谷歌专用的术语。在 VW 系统（Yahoo/Microsoft）中特征列的意义是「命名空间」（namespace），或者场（field）。
### feature cross(特征交叉)
将特征进行交叉（乘积或者笛卡尔乘积）运算后得到的合成特征。特征交叉有助于表示非线性关系。
### feature engineering(特征工程)
在训练模型的时候，决定哪些特征是有用的，然后将记录文件和其它来源的原始数据转换成上述特征的过程。在 TensorFlow 中特征工程通常意味着将原始记录文件输入 tf.Example 协议缓存中。参见 tf.Transform。特征工程有时候也称为特征提取。
### feature set(特征集)
机器学习模型训练的时候使用的特征群。比如，邮政编码，面积要求和物业状况可以组成一个简单的特征集，使模型能预测房价。
### feature spec(特征定义)
描述所需的信息从 tf.Example 协议缓存中提取特征数据。因为 tf.Example 协议缓存只是数据的容器，必须明确以下信息：
需要提取的数据（即特征的关键信息）
数据类型（比如，浮点数还是整数）
数据长度（固定的或者变化的）
Estimator API 提供了从一群特征列中生成一个特征定义的工具。
### full softmax(完全 softmax)
参见 softmax。和候选采样对照。
# G
### generalization（泛化）
指模型利用新的没见过的数据而不是用于训练的数据作出正确的预测的能力。
### generalized linear model（广义线性模型）
最小二乘回归模型的推广/泛化，基于高斯噪声，相对于其它类型的模型（基于其它类型的噪声，比如泊松噪声，或类别噪声）。广义线性模型的例子包括：
logistic 回归
多分类回归
最小二乘回归
广义线性模型的参数可以通过凸优化得到，它具有以下性质：
最理想的最小二乘回归模型的平均预测结果等于训练数据的平均标签。
最理想的 logistic 回归模型的平均概率的预测结果等于训练数据的平均标签。
广义线性模型的能力局限于其特征的性质。和深度模型不同，一个广义线性模型无法「学习新的特征」。
### gradient（梯度）
所有变量的偏导数的向量。在机器学习中，梯度是模型函数的偏导数向量。梯度指向最陡峭的上升路线。
### gradient clipping（梯度截断）
在应用梯度之前先修饰数值，梯度截断有助于确保数值稳定性，防止梯度爆炸出现。
### gradient descent（梯度下降）
通过计算模型的相关参量和损失函数的梯度最小化损失函数，值取决于训练数据。梯度下降迭代地调整参量，逐渐靠近权重和偏置的最佳组合，从而最小化损失函数。
### graph（图）
在 TensorFlow 中的一种计算过程展示。图中的节点表示操作。节点的连线是有指向性的，表示传递一个操作（一个张量）的结果（作为一个操作数）给另一个操作。使用 TensorBoard 能可视化计算图。

