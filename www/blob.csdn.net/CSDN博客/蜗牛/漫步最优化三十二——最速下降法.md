
# 漫步最优化三十二——最速下降法 - 蜗牛 - CSDN博客


2017年10月26日 23:21:53[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：1084



$\textbf{爱需要勇气，}$
$\textbf{但是也许是天意，}$
$\textbf{让我爱上你，}$
$\textbf{也许轮回里早已注定，}$
$\textbf{今生就该还给你。}$
$\textbf{一颗心在风雨里，}$
$\textbf{飘来飘去，}$
$\textbf{一直都在等待你。}$
$\textbf{——畅宝宝的傻逼哥哥}$
考虑优化问题

$$
\text{miminize}\ F=f(\mathbf{x})\quad\text{for}\ \mathbf{x}\in E^n
$$
根据泰勒级数

$$
F+\Delta F=f(\mathbf{x}+{\delta})\approx f(\mathbf{x})+\mathbf{g}^T{\delta}+\frac{1}{2}{\delta}^T\mathbf{H}{\delta}
$$
当$\lVert{\delta}\rVert\to0$，由于${\delta}$引起的$F$变化量为

$$
\Delta F\approx\mathbf{g}^T{\delta}
$$
右边是向量$\mathbf{g},{\delta}$的点乘，如果

$$
\begin{align*}
\mathbf{g}&=[g_1\ g_2\ \cdots\ g_n]^T\\
{\delta}&=[\delta_1\ \delta_2\ \cdots\ \delta_n]^T
\end{align*}
$$
那么

$$
\Delta F\approx\sum_{i=1}^ng_i\delta_i=\lVert\mathbf{g}\rVert\lVert{\delta}\rVert\cos\theta
$$
其中$\theta$是向量$\mathbf{g},{\delta}$之间的夹角且

$$
\lVert\mathbf{g}\rVert=(\mathbf{g}^T\mathbf{g})^{1/2}=\left(\sum_{i=1}^ng_i^2\right)^{1/2}
$$
$\textbf{上升与下降方向}$
考虑图1的图像，如果$\mathbf{x},\mathbf{x}+{\delta}$是轮廓$A$上相邻的两个点，那么当$\lVert{\delta}\rVert\to0$

$$
\Delta F\approx\lVert\mathbf{g}\rVert\lVert{\delta}\rVert\cos\theta=0
$$
这是因为在该轮廓上$F$是常数，由此可得向量$\mathbf{g},{\delta}$之间的夹角$\theta$等于$90^\circ$。从效果上看，点$\mathbf{x}$处得梯度与$A$正交。对于任意向量${\delta}$，要想$\Delta F$取得最大正值，那么$\theta=0$，即${\delta}$必须与$\mathbf{g}$同向。另一方面，要想$\Delta F$取得最大负值，那么$\theta=\pi$，即${\delta}$必须与$-\mathbf{g}$同向。梯度$\mathbf{g}$与它的负$-\mathbf{g}$分别称为梯度上升与梯度下降方向，这些概念如图1和图2所示。

![这里写图片描述](https://img-blog.csdn.net/20171026225127778?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)[ ](https://img-blog.csdn.net/20171026225127778?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图1

## \textbf{基本方法}
假设函数$f(x)$在点$\mathbf{x}$的邻域内是连续的，如果$\mathbf{d}$是点$\mathbf{x}$处的最速下降方向，即

$$
\mathbf{d}=-\mathbf{g}
$$
那么$\mathbf{x}$的变化量${\delta}$

$$
{\delta}=\alpha\mathbf{d}
$$
会减少$f(\mathbf{x})$的值，其中$\alpha$是一个很小的正常数。通过求解一维优化问题

$$
\text{minimize}_\alpha\ F=f(\mathbf{x}+\alpha\mathbf{d})
$$
我们可以最大化的减少$f(\mathbf{x})$，如图3所示。
如果点$\mathbf{x}$处的最速下降方向朝向$f(\mathbf{x})$的最小值$x^*$，那么存在$\alpha$值，使得$f(\mathbf{x}+\alpha\mathbf{d})$对$\alpha$最小化，$f(\mathbf{x})$对$\mathbf{x}$最小化，因此这时候多维优化问题通过求解一维问题即可。不幸的是，大部分实际问题中，$\mathbf{d}$没有指向$\mathbf{x}^*$，所以为了求出解需要进行多次迭代。首先从初始点$\mathbf{x}_0$开始，计算方向$\mathbf{d}=\mathbf{d}_0=-\mathbf{g}$，确定最小化$f(\mathbf{x}_0+\alpha\mathbf{d}_0)$的$\alpha$值，用$\alpha_0$表示，那么可得到新的点$\mathbf{x}_1=\mathbf{x}_0+\alpha_0\mathbf{d}_0$，最小化的过程可以用前面介绍的任何一种方法，接下来在点

$$
\mathbf{x}_{k+1}=\mathbf{x}_k+\alpha_k\mathbf{d}_k
$$
处重复执行上面的过程直到收敛为止，其中$k=1,2,\ldots$。这个过程的终止条件可以是$\lVert\alpha_k\mathbf{d}_k\rVert$变得足够小或者$\alpha_k\leq K\alpha_0$，其中$K$是一个很小的正常数。对于最速下降法来说，典型的求解轨迹如图4所示。

![这里写图片描述](https://img-blog.csdn.net/20171026225203625?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)[ ](https://img-blog.csdn.net/20171026225203625?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图2

## \textbf{方向正交}
对于最速下降法，解的轨迹服从zig-zag模式，如图4所示。如果每次迭代所选的$\alpha$都使得$f(\mathbf{x}_k+\alpha\mathbf{d}_k)$最小，那么相邻的方向是正交的。为了证明这个结论，注意到

$$
\begin{align*}
\frac{df(\mathbf{x}_k+\alpha\mathbf{d}_k)}{d\alpha}
&=\sum_{i=1}^n\frac{\partial f(\mathbf{x}_k+\alpha\mathbf{d}_k)}{\partial x_{ki}}\frac{d(x_{ki}+\alpha d_{ki})}{d\alpha}\\
&=\sum_{i=1}^ng_i(\mathbf{x}_k+\alpha\mathbf{d}_k)d_{ki}\\
&=\mathbf{g}(\mathbf{x}_k+\alpha\mathbf{d}_k)^T\mathbf{d}_k
\end{align*}
$$
其中$\mathbf{g}(\mathbf{x}_k+\alpha\mathbf{d}_k)$是点$\mathbf{x}_k+\alpha\mathbf{d}_k$处的梯度，如果$\alpha^*$是最小化$f(\mathbf{x}_k+\alpha\mathbf{d}_k)$的$\alpha$值，那么

$$
\mathbf{g}(\mathbf{x}_k+\alpha^*\mathbf{d}_k)^T\mathbf{d}_k=0
$$
或者

$$
\mathbf{d}_{k+1}^T\mathbf{d}_k=0
$$
其中

$$
\mathbf{d}_{k+1}=-\mathbf{g}(\mathbf{x}_k+\alpha^*\mathbf{d}_k)
$$
是点$\mathbf{x}_k+\alpha^*\mathbf{d}_k$处的最速下降方向。从效果上看，相邻方向$\mathbf{d}_k,\mathbf{d}_{k+1}$如图4那样是正交的。
## \textbf{消除线搜索}
如果可以求出$f(\mathbf{x})$的海森矩阵，那么我们可以用解析法求出最小化$f(\mathbf{x}_k+\alpha\mathbf{d})$的$\alpha$值，用$\alpha_k$表示。如果${\delta}_k=\alpha\mathbf{d}_k$，那么根据泰勒级数可知

$$
f(\mathbf{x}_k+{\delta}_k)\approx f(\mathbf{x}_k)+{\delta}_k^T\mathbf{g}_k+\frac{1}{2}{\delta}_k^T\mathbf{H}_k{\delta}_k
$$
如果$\mathbf{d}_k$是最速下降方向，例如

$$
{\delta}_k=-\alpha\mathbf{g}_k
$$
那么我们可以得出

$$
f(\mathbf{x}_k-\alpha\mathbf{g}_k)\approx f(\mathbf{x}_k)-\alpha\mathbf{g}_k^T\mathbf{g}_k+\frac{1}{2}\alpha^2\mathbf{g}_k^T\mathbf{H}_k\mathbf{g}_k
$$
对其求导并等于零得

$$
\frac{f(\mathbf{x}_k-\alpha\mathbf{g}_k)}{d\alpha}\approx-\mathbf{g}_k^T\mathbf{g}_k+\alpha\mathbf{g}_k^T\mathbf{H}_k\mathbf{g}_k=0
$$
或者

$$
\alpha=\alpha_k\approx\frac{\mathbf{g}_k^T\mathbf{g}_k}{alpha\mathbf{g}_k^T\mathbf{H}_k\mathbf{g}_k}
$$

![这里写图片描述](https://img-blog.csdn.net/20171026225340121?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)[ ](https://img-blog.csdn.net/20171026225340121?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图3

![这里写图片描述](https://img-blog.csdn.net/20171026225309539?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)[ ](https://img-blog.csdn.net/20171026225309539?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图4
接下来，如果我们假设$\alpha=\alpha_k$最小化$f(\mathbf{x}_k+\alpha\mathbf{d}_k)$，那么我们得到

$$
\mathbf{x}_{k+1}=\mathbf{x}_k-\frac{\mathbf{g}_k^T\mathbf{g}_k}{alpha\mathbf{g}_k^T\mathbf{H}_k\mathbf{g}_k}\mathbf{g}_k
$$
$\alpha_k$的精确性依赖于${\delta}_k$的模长，因为泰勒级数的二次近似只在$\mathbf{x}_k$的邻域内有效。刚开始$\lVert{\delta}_k\rVert$相对比较大，所以$\alpha_k$将不准确，但不管怎样，因为在最速下降方向最小化$f(\mathbf{x}_k+\alpha\mathbf{d}_k)$，所以$f(\mathbf{x})$一直在减少，故$\alpha_k$的精确性在慢慢提高，甚至每次迭代都能实现最大减少$f(\mathbf{x})$，因此能实现收敛。对于二次函数来说，上面的近似符号可以改成等号，因此每次迭代$\alpha=\alpha_k$都能最大程度减少$f(\mathbf{x})$。
如果无法得到海森矩阵，那么我们可以计算点$\mathbf{x}_k,\mathbf{x}_k-\hat{\alpha}\mathbf{g}_k$处的$f(\mathbf{x})$值来确定$\alpha_k$，其中$\hat{\alpha}$是$\alpha_k$的估计值。如果

$$
f_k=f(\mathbf{x}_k)\quad\hat{f}=f(\mathbf{x}_k-\hat{\alpha}\mathbf{g}_k)
$$
那么根据泰勒级数可得

$$
\hat{f}\approx-\hat{\alpha}\mathbf{g}_k^T\mathbf{g}_k+\frac{1}{2}\hat{\alpha}^2\mathbf{g}_k^T\mathbf{H}_k\mathbf{g}_k
$$
或者

$$
\mathbf{g}_k^T\mathbf{H}_k\mathbf{g}_k\approx\frac{2(\hat{f}-f_k+\hat{\alpha}\mathbf{g}_k^T\mathbf{g}_k)}{\hat{\alpha}^2}
$$
从上式可得

$$
\alpha_k\approx\frac{\mathbf{g})k^T\mathbf{g}_k\hat{\alpha}^2}{2(\hat{f}-f_k+\hat{\alpha}\mathbf{g}_k^T\mathbf{g}_k)}
$$
合适的$\hat{\alpha}$为$\alpha_{k-1}$，即前一个迭代中的最优$\alpha$。对于第一次迭代我们用$\hat{\alpha}=1$。
## \textbf{收敛}
如果函数$f(\mathbf{x})\in C^2$有局部最小点$\mathbf{x}^*$且$\mathbf{x}=\mathbf{x}^*$处的海森矩阵是正定的，那么可以说明如果$\mathbf{x}_k$足够靠近$\mathbf{x}^*$，我们有

$$
f(\mathbf{x}_{k+1})-f(\mathbf{x}^*)\leq\left(\frac{1-r}{1+r}\right)^2[f(\mathbf{x}_k)-f(\mathbf{x}^*)]
$$
其中

$$
r=\frac{\mathbf{H}_k\text{的最小特征值}}{\mathbf{H}_k\text{的最大特征值}}
$$
更进一步，如果$f(\mathbf{x})$是二次函数，那么上面的不等式对所有$k$都成立。从效果上看，如果条件成立，那么最速下降法线性收敛，其收敛率为

$$
\beta=\leq\left(\frac{1-r}{1+r}\right)^2
$$
显然，如果$\mathbf{H}_k$的特征值基本都相等，那么收敛率比较高；如果至少有一个比最大值特征值小，那么收敛率就比较低。
$\mathbf{H}$的特征值$\lambda_i,i=1,2,\ldots,n$确定几何平面

$$
\mathbf{x}^T\mathbf{H}\mathbf{x}=\text{常数}
$$
这个等式给出了$\mathbf{x}^T\mathbf{H}\mathbf{x}$的轮廓并且如果$\mathbf{H}$是正定的，那么该轮廓是椭球，其轴长为$1/\sqrt{\lambda_i}$。如果变量个数为二，那么轮廓是椭圆，轴长分别为$1/\sqrt{\lambda_1},1/\sqrt{\lambda_2}$，因此如果在二维问题上使用最速下降法，当轮廓接近圆时收敛最快，如果就是圆即$r=1$，那么一次迭代就达到收敛。另一方面，如果轮廓近似椭圆或者说函数存在细长的谷底，那么收敛就非常慢，尤其是在靠近解的地方。$r$对收敛的影响通过比较图4与图5就能明显的看出来。

![这里写图片描述](https://img-blog.csdn.net/20171026225456116?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)[ ](https://img-blog.csdn.net/20171026225456116?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE4MjYzMw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
图5
从效果上看，最速下降法试着让梯度减小到零，因为鞍点处的梯度也是零，所以如果这样的点是解的话可能存在问题。然而这样的解基本是不可能，首先，将鞍点作为下次的迭代点这种概率非常低，其次，鞍点邻域内始终有下降方向。
## \textbf{尺度}
对某个特定的优化问题，$\mathbf{H}$的特征值或者说最速下降法的性能很大程度上依赖于所选的变量，例如在一维或二维问题中，轮廓是偏向于圆还是椭圆依赖于单位的选择，因此通过伸缩变量这种变量变换的方式可以提高收敛速率。
一种可能的伸缩方式是令

$$
\mathbf{x}=\mathbf{Ty}
$$
其中$\mathbf{T}$是对角矩阵，然后求解问题

$$
\text{minimize}_y\ h(\mathbf{y})=f(\mathbf{x})|_{\mathbf{x=Ty}}
$$
新问题的梯度与海森矩阵分别为

$$
\mathbf{g}_h=\mathbf{Tg}_x\quad\mathbf{H}_h=\mathbf{T}^T\mathbf{HT}
$$
因此最速下降方向以及与问题相关的特征值都发生了变化。但是不幸的是，$\mathbf{T}$的选择严重依赖具体的问题，对此没有一般的法则。不过有个小技巧，那就是尽量平衡二阶导

$$
\frac{\partial^2f}{\partial x_i^2}\quad\text{for}\ i=1,2,\ldots,n
$$


