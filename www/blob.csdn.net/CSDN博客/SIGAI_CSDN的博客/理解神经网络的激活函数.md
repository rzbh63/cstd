
# 理解神经网络的激活函数 - SIGAI_CSDN的博客 - CSDN博客
# [SIGAI_CSDN的博客](https://blog.csdn.net/sigai_csdn)


[博客首页](https://blog.csdn.net/SIGAI_CSDN)
[关于我们](https://me.csdn.net/SIGAI_CSDN)

2018年06月18日 14:34:56[SIGAI_csdn](https://me.csdn.net/SIGAI_CSDN)阅读数：2172


![](https://img-blog.csdn.net/2018061814354739?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
本文及其它机器学习、深度学习算法的全面系统讲解可以阅读《机器学习与应用》，清华大学出版社，雷明著，由SIGAI公众号作者倾力打造，自2019年1月出版以来已重印3次。
[书的购买链接](https://link.zhihu.com/?target=https%3A//item.jd.com/12504554.html)
[书的勘误，优化，源代码资源](https://link.zhihu.com/?target=http%3A//www.tensorinfinity.com/paper_78.html)
```python
更多干货请关注V X公众号：SIGAI更多干货请关注V X公众号：SIGAI导言
激活函数在神经网络中具有重要的地位，对于常用的函数如sigmoid，tanh，ReLU，不少读者都已经
非常熟悉。但是你是否曾想过这几个问题：
为什么需要激活函数？
什么样的函数可以做激活函数？
什么样的函数是好的激活函数？
在这篇文章中，SIGAI将和大家一起讨论这几个问题，以加深对激活函数的理解，如果对本文的观点持
有不同的意见，欢迎向我们的公众号发消息一起讨论。
```
**为什么需要激活函数**
从数学上看，神经网络是一个多层复合函数。激活函数在很早以前就被引入，其作用是保证神经网络的非线性，因为线性函数无论怎样复合结果还是线性的。假设神经网络的输入是n维向量x，输出是m维向量y，它实现了如下向量到向量的映射：
![](https://img-blog.csdn.net/20180618142908818?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
我们将这个函数记为：
![](https://img-blog.csdn.net/20180618142918166?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
除输入层之外，标准的前馈型神经网络第I层实现的变换可以分为线性组合、激活函数两步。在某些开源框架中，这两步可能会拆分成不同的层，以利于代码复用和灵活组合。例如Caffe中线性组合由内积层InnerProductLayer类实现，激活函数由神经元层NeuronLayer类实现。神经网络第层的变换写成矩阵和向量形式为：
![](https://img-blog.csdn.net/20180618142929456?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中W是权重矩阵，b是偏置向量，u是临时结果，x是神经网络每一层的输出。激活函数分别作用于向量u的每一个分量，产生一个向量输出x。在正向传播阶段，反复用上面的公式进行计算，最后得到网络的输出。对于一个3层的网络，整个映射可以写成：
![](https://img-blog.csdn.net/20180618142942375?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这是一个3层的复合函数。从这里可以清晰的看到，如果没有激活函数，整个函数将是一个线性函数：
![](https://img-blog.csdn.net/20180618142955261?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
因此对激活函数最基本的要求是必须是非线性的。在早期，普遍使用的是sigmoid函数和tanh函数。sigmoid函数的计算公式为：
![](https://img-blog.csdn.net/20180618143006775?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
这个函数的图像为：
![](https://img-blog.csdn.net/20180618143019735?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
tanh函数的计算公式为：
![](https://img-blog.csdn.net/20180618143030213?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
它的图像为：
![](https://img-blog.csdn.net/20180618143040283?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
前者的值域为(0,1)，单调递增且有界；后者的值域为(-1,+1)，是一个中心对称的奇函数，同样也是单调递增且有界。

## 什么样的函数可以做激活函数
前面已经说过，为保证非线性，激活函数必须为非线性函数，但仅仅具有非线性是不够的。神经网络在本质上是一个复合函数，这会让我们思考一个问题：这个函数的建模能力有多强？即它能模拟什么样的目标函数？已经证明，只要激活函数选择得当，神经元个数足够多，使用3层即包含一个隐含层的神经网络就可以实现对任何一个从输入向量到输出向量的连续映射函数的逼近，这个结论称为万能逼近（universal approximation）定理。万能逼近定理的表述为：
![](https://img-blog.csdn.net/20180618143058511?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![](https://img-blog.csdn.net/20180618143113947?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
万能逼近定理的直观解释是可以构造出上面这种形式的函数，逼近定义在单位立方体空间中的任何一个连续函数到任意指定的精度。这个定理对激活函数的要求是必须非常数、有界、单调递增，并且连续。
文献[1]对使用sigmoid激活函数时的情况进行了证明：如果是一个连续函数，并且满足下面条件：
![](https://img-blog.csdn.net/20180618143130750?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
则函数族：
![](https://img-blog.csdn.net/20180618143139930?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
在空间n维单位立方体空间![C^{n}[0,1]](https://www.zhihu.com/equation?tex=C%5E%7Bn%7D%5B0%2C1%5D)中是稠密的，即这样的函数可以逼近定义在单位立方体空间中的任意连续函数到任意指定的精度。显然sigmoid函数就满足对![\sigma](https://www.zhihu.com/equation?tex=%5Csigma)的要求。上面这些结论的函数输出值都是一个标量，但我们可以把它推广的向量的情况，神经网络的输出一般是一个向量。
只要网络规模设计得当，使用sigmoid函数和ReLU函数作为激活函数的逼近能力都能够得到保证。ReLU函数定义为：
![](https://img-blog.csdn.net/2018061814315165?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
它是一个分段线性函数。文献[7][8]分析了使用ReLU激活函数的神经网络的逼近能力。下图是一个非线性分类问题的例子，说明了神经网络确实能处理这种非线性问题：
![](https://img-blog.csdn.net/20180618143202167?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
在上图中，用图中的圆圈（红色和蓝色）训练样本训练出来的神经网络模型成功的将蓝色和红色两类样本分开了，分界面是两条曲线。
仅仅满足万能逼近定理的要求也是不够的。神经网络的训练一般采用反向传播算法+梯度下降法。反向传播算法从复合函数求导的链式法则导出，因为神经网络是一个多层的复合函数。在反向传播时，误差项的计算公式为：
![](https://img-blog.csdn.net/20180618143214301?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
由于使用梯度下降法需要计算损失函数对参数的梯度值，这个梯度值根据上面的误差项计算，而误差项的计算又涉及到计算激活函数的导数，因此激活函数必须是可导的。实际应用时并不要求它在定义域内处处可导，只要是几乎处处可导即可。“几乎处处可导”看上去是一个比较有文学味道的词，但实际上是数学中一个严格的概念，这涉及到实变函数的知识。它的严格定义是这样的：
![](https://img-blog.csdn.net/20180618143233204?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
上面这个定义过于晦涩。我们可以简单的将几乎处处可导理解成不可导点只有有限个，或者无限可列个（即可用自然数对这些点进行编号，某一区间上的实数就是无限不可列的），即不可导点的测度是0。可以将测度理解成一维直线的一些点的集合的长度，或者二维平面上一些点的集合的面积。在概率论中我们知道，连续型随机变量取任何一个点处的值的概率为0，如果将激活函数输入值x看做是随机变量，则它落在这些不可导点处的概率是0。在计算机实现时，浮点数是进行了离散化的，分成尾数和阶码表示，计算机能表示的浮点数是有限个，因此有一定的概率会落在不可导点处，但概率非常小。ReLU函数在0点处就不可导。

## 什么样的函数是好的激活函数
反向传播算法计算误差项时每一层都要乘以本层激活函数的导数。如果激活函数导数的绝对值值小于1，多次连乘之后误差项很快会衰减到接近于0，参数的梯度值由误差项计算得到，从而导致前面层的权重梯度接近于0，参数没有得到有效更新，这称为梯度消失问题。与之相反的是梯度爆炸问题，如果激活函数导数的绝对大于1，多次乘积之后权重值会趋向于非常大的数，这称为梯度爆炸。梯度消失问题最早在1991年发现，文献[10]对深层网络难以训练的问题进行了分析，在很长一段时间内，梯度消失问题是困扰神经网络层次加深的一个重要因素。
文献[10]对深层神经网络难以训练的问题进行了理论分析和实验验证。在实验中，作者训练了有1-5个隐藏层的神经网络，每个隐藏层有1000个神经元，输出层使用softmax logistic回归函数。激活函数使用了sigmoid，tanh，以及softsign：
![](https://img-blog.csdn.net/20180618143249326?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
权重初始化公式为：
![](https://img-blog.csdn.net/20180618143258798?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
其中，U[a, b]是[a,b]中的均匀分布，n是前一层的尺寸。理论分析和实验结果都证明，随着网络层数的增加，反向传播的作用越来越小，网络更加难以训练和收敛。
文献[11]中定义了激活函数饱和性的概念，并对各种激活函数进行了分析，给出了改进措施。如果一个激活函数满足：
![](https://img-blog.csdn.net/20180618143309690?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
即在正半轴函数的导数趋向于0，则称该函数为右饱和。类似的如果满足：
![](https://img-blog.csdn.net/20180618143317746?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
即在负半轴函数的导数趋向于0，则称该函数左饱和。如果一个激活函数既满足左饱和又满足右饱和，称之为饱和。如果存在常数c，当x>c时有：
![](https://img-blog.csdn.net/20180618143328279?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
则称函数右硬饱和；当x<c时有：
![](https://img-blog.csdn.net/20180618143337634?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
则称函数左硬饱和。既满足左硬饱和又满足右硬饱和的激活函数为硬饱和函数。饱和性和梯度消失问题密切相关。在反向传播过程中，误差项在每一层都要乘以激活函数导数值，一旦x的值落入饱和区间，多次乘积之后会导致梯度越来越小，从而出现梯度消失问题。
sigmoid函数的输出映射在(0,1)之间，单调连续，求导容易。但是由于其软饱和性，容易产生梯度消失，导致训练出现问题；另外它的输出并不是以0为中心的。
tanh函数的输出值以0为中心，位于(-1,+1)区间，相比sigmoid函数训练时收敛速度更快，但它还是饱和函数，存在梯度消失问题。
ReLU函数其形状为一条折线，当x<0时做截断处理。该函数在0点出不可导，如果忽略这一个点其导数为sgn。函数的导数计算很简单，而且由于在正半轴导数为1，有效的缓解了梯度消失问题。在ReLU的基础上又出现了各种新的激活函数，包括ELU、PReLU等。如果对各种激活函数深入的比较和分析感兴趣，可以阅读文献[11]。

## 常用的激活函数
下表列出了Caffe中支持的激活函数和它们的导数：
![](https://img-blog.csdn.net/20180618143411254?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NJR0FJX0NTRE4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
感兴趣的读者可以分析它们的实现细节。
参考文献
[1] Cybenko, G. Approximation by superpositions of a sigmoid function. Mathematics of Control, Signals, and Systems, 2, 303-314, 1989.
[2] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. 1991, Neural Networks.
[3] Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal approximators. Neural Networks, 2, 359-366, 1989.
[4] Hornik, K., Stinchcombe, M., and White, H. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural networks, 3(5), 551-560, 1990.
[5] Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6, 861-867, 1993.
[6] Barron, A. E. Universal approximation bounds for superpositions of a sigmoid function. IEEE Transactions on Information Theory, 39, 930-945, 1993.
[7] Montufar, G. Universal approximation depth and errors of narrow belief networks with discrete units. Neural Computation, 26, 2014.
[8] Raman Arora, Amitabh Basu, Poorya Mianjy, Anirbit Mukherjee. Understanding Deep Neural Networks with Rectified Linear Units. 2016, Electronic Colloquium on Computational Complexity.
[9] Nair, V. and Hinton. Rectified linear units improve restricted Boltzmann machines. In L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML 2010).
[10] X. Glorot, Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTATS, 2010.
[11] Caglar Gulcehre, Marcin Moczulski, Misha Denil, Yoshua Bengio. Noisy Activation Functions. ICML 2016.

```python
推荐文章
[1] 机器学习-波澜壮阔40年 SIGAI 2018.4.13.
[2] 学好机器学习需要哪些数学知识？SIGAI 2018.4.17.
[3] 人脸识别算法演化史 SIGAI 2018.4.20.
[4] 基于深度学习的目标检测算法综述 SIGAI 2018.4.24.
[5] 卷积神经网络为什么能够称霸计算机视觉领域？ SIGAI 2018.4.26.
[6] 用一张图理解SVM的脉络SIGAI 2018.4.28.
[7] 人脸检测算法综述 SIGAI 2018.5.3.
```

