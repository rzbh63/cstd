
# 漫步数理统计三十——依概率收敛 - 蜗牛 - CSDN博客


2017年06月12日 22:29:57[会敲键盘的猩猩](https://me.csdn.net/u010182633)阅读数：6349


本篇博文我们将正式地陈述一系列随机变量靠近某个随机变量。
$\textbf{定义1：}$$\{X_n\}$是一系列随机变量，$X$是定义在样本空间上的随机变量。我们说$X_n$依概率收敛到$X$，如果对于$\epsilon>0$

$$
\lim_{n\to\infty}P[|X_n-X|\geq\epsilon]=0
$$
或者等价的

$$
\lim_{n\to\infty}P[|X_n-X|<\epsilon]=1
$$
如果成立，我们一般写成

$$
X_n\overset{P}\to X
$$
如果$X_n\overset{P}\to X$，我们常说$X_n-X$的差收敛到0。极限随机变量$X$经常是一个常数；例如$X$是一个退化的随机变量。
说明依概率收敛的一种方法是用切比雪夫定理，具体会在下面的证明中给出，为了强调我们是一系列随机变量，我们在随机变量上给出下标，像$\bar{X}$写成$\bar{X}_n$。
$\textbf{定理1：}$(弱大数定理)$\{X_n\}$是一系列独立同分布的随机变量，均值为$\mu$，方差为$\sigma^2<\infty$，$\bar{X}_n=n^{-1}\sum_{i=1}^nX_i$，那么

$$
\bar{X}_n\overset{P}\to\mu
$$
$\textbf{证明：}$回忆一下$\bar{X}_n$的均值与方差分别为$\mu,\sigma^2/n$，因此根据切比雪夫定理，对于任意的$\epsilon>0$

$$
P[|\bar{X}-\mu|\geq\epsilon]=P[|\bar{X}-\mu|]\geq(\epsilon\sqrt{n}/\sigma)(\sigma/\sqrt{n})\leq\frac{\sigma^2}{n\epsilon^2}\to 0
$$
$||$
这个定理说明，当$n$取向$\infty$时，$\bar{X}$分布的所有质量收敛到$\mu$。也就时候对于大的$n$，$\bar{X}$接近$\mu$，但是多接近呢？例如如果我们用$\bar{X}_n$估计$\mu$，那么估计误差是多少？这个问题留到下篇博文讲解。
还有一个强大数定理，它弱化了定理1的假设：随机变量$X_i$独立且都有有限的均值$\mu$，因此强大数定理是一阶矩定理，而弱大数定理需要二阶矩存在。
还有些关于依概率收敛的定理，我们在后面会用到，首先是两个关于依概率收敛对线性封闭的定理。
$\textbf{定理2：}$假设$X_n\overset{P}\to X,Y_n\overset{P}\to Y$，那么$X_n+Y_n\overset{P}\to X+Y$。
$\textbf{证明：}$$\epsilon>0$已给定，利用三角不等式可得

$$
|X_n-X|+|Y_n-Y|\geq|(X_n+Y_n)-(X+Y)|\geq\epsilon
$$
因为$P$是单调的，所以我们有

$$
\begin{align*}
P[(X_n+Y_n)-(X+Y)\geq\epsilon]
&\leq P[|X_n-X|+|Y_n-Y|\geq\epsilon]\\
&\leq P[|X_n-X|\geq\epsilon/2]+P[|Y_n-Y|\geq\epsilon/2]
\end{align*}
$$
根据定理的假设，后两项收敛到0，从而得证。$||$
$\textbf{定理3：}$假设$X_n\overset{P}\to X$且$a$是一个常数，那么$aX_n\overset{P}\to aX$。
$\textbf{证明：}$如果$a=0$，结论明显成立。假设$a\neq 0$，令$\epsilon>0$，那么

$$
P[|aX_n-aX|\geq\epsilon]=P[|a||X_n-X|\geq\epsilon]=P[|X_n-X|\geq\epsilon/|a|]
$$
根据假设最后一项趋于0。$||$
$\textbf{定理4：}$假设$X_n\overset{P}\to a$且函数$g$在$a$点连续，那么$g(X_n)\overset{P}\to g(a)$。
$\textbf{证明：}$令$\epsilon>0$，那么因为$g$在$a$点连续，所以存在$\delta>0$使得如果$|x-a|<\delta,|g(x)-g(a)|<\epsilon$，所以

$$
|g(x)-g(a)|\geq\epsilon\Rightarrow|x-a|\geq\delta
$$
代入$X_n$可得

$$
P[|g(X_n)-g(a)|\geq\epsilon]\leq P[|X_n-a|\geq\delta]
$$
根据假设，最后一项在$n\to\infty$时趋于0，得证。$||$
这个定理给出了许多有用的结论。例如，如果$X_n\overset{P}\to a$，那么

$$
\begin{align*}
X_n^2&\overset{P}\to a^2\\
1/X_n&\overset{P}\to 1/a,\textrm{假设}a\neq 0\\
\sqrt{X_n}&\overset{P}\to \sqrt{a},\textrm{假设}a\geq0
\end{align*}
$$
实际上，如果$X_n\overset{P}\to X$且$g$是连续函数，那么$g(X_n)\overset{P}\to g(X)$，下面的定理就用了这个结论。
$\textbf{定理5：}$假设$X_n\overset{P}\to X,Y_n\overset{P}\to Y$，那么$X_nY_n\overset{P}\to XY$。
$\textbf{证明：}$利用上面的结论，我们有

$$
\begin{align*}
X_nY_n
&=\frac{1}{2}X_n^2+\frac{1}{2}Y_n^2-\frac{1}{2}(X_n-Y_n)^2\\
&\overset{P}\to\frac{1}{2}X^2+\frac{1}{2}Y^2-\frac{1}{2}(X-Y)^2=XY 
\end{align*}
$$
现在回到采样与统计的讨论，考虑这么一种情况，随机变量$X$的分布有未知参数$\theta\in\Omega$，我们要基于样本找到一个统计量来估计$\theta$，上篇博文我们介绍了无偏性，现在介绍一致性：
$\textbf{定义2：}$$X$是cdf为$F(x,\theta),\theta\in\Omega$的随机变量，$X_1,\ldots,X_n$是$X$分布的样本且$T_n$表示一个统计量。我们说$T_n$是$\theta$的一致估计，如果

$$
T_n\overset{P}\to\theta
$$
如果$X_1,\ldots,X_n$是有限均值$\mu$和方差$\sigma^2$分布的随机样本，那么根据弱大数定理，样本均值$\bar{X}$是$\mu$的一致估计。
$\textbf{例1：}$$X_1,\ldots,X_n$表示均值为$\mu$方差为$\sigma^2$分布的随机样本，定理1说明$\bar{X}\overset{P}\to\mu$。为了说明样本均值依概率收敛到$\sigma^2$，假设$E[X_1^4]<\infty$，这样的话$var(S^2)<\infty$。根据前面的结论可得：

$$
\begin{align*}
S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2
&=\frac{n}{n-1}\left(\frac{1}{n}\sum_{i=1}^nX_i^2-\bar{X}_n^2\right)\\
&\overset{P}\to1\cdot[E(X_1^2)-\mu^2]=\sigma^2
\end{align*}
$$
因此样本方差是$\sigma^2$的一致估计。
不像上面的例子，有时候我们可以用分布函数得出收敛，如下例所示：
$\textbf{例2：}$$X_1,\ldots,X_n$是均匀分布$(0,\theta)$的随机样本，$Y_n=\max\{X_1,\ldots,X_n\}$，从$Y_n$的cdf中很容易看出$Y_n\overset{P}\to\theta$且样本最大值是$\theta$的一致估计。注意无偏估计$((n+1)/n)Y_n$也是一致的。
接下里扩展下例2，根据定理1可得$\bar{X}_n$是$\theta/2$的一致估计，所以$2\bar{X}_n$是$\theta$的一致估计，注意$Y_n,2\bar{X}_n$依概率收敛到$\theta$的区别。对$Y_n$而言我们用的是$Y_n$的cdf，但对$2\bar{X}_n$而言，我们用的是弱大数定理。事实上$2\bar{X}_n$的cdf非常复杂。在许多情况下，统计量的cdf无法得到但是我们可以用近似理论来建立结论。其实还有许多其他$\theta$的估计量，那么哪个是最好的呢？后面的文章会继续介绍。
一致性是估计量非常重要的性质，当样本数量增大时差的估计量不可能靠近目标。注意这对无偏性是不成立的。例如我们不用样本方差来估计$\sigma^2$，假设用$V=n^{-1}\sum_{i=1}^n(X_i-\bar{X})^2$，那么$V$是$\sigma^2$的一致估计，但是是有偏的，因为$E(V)=(n-1)\sigma^2/n$，所以$V$的偏置为$\sigma^2/n$，当$n\to\infty$时该项消失。

