# 强化学习系列1：强化学习简介 - kittyzc的博客 - CSDN博客
2018年12月24日 16:24:03[IE06](https://me.csdn.net/kittyzc)阅读数：333所属专栏：[强化学习系列](https://blog.csdn.net/column/details/33845.html)
2015年10月，AlphaGo在和欧洲冠军进行的围棋赛上获得了5:0的完胜，其后的深度强化学习也随之火了起来。从本期开始开个新坑，一步步把强化学习的内容捋一遍。

# 1. 基本概念

强化学习(reinforcement learning)是用来解决连续决策问题的一种方法。针对的模型是马尔科夫决策过程（Markov Decision Process，MDP）。所谓马尔科夫决策过程，指的是下一时刻的状态仅由当前阶段（包括状态、决策）决定，不依赖于以往的状态，用数学化的语言表达为：
- 问题是多阶段决策过程，阶段数为$III$
- 每个阶段可能的状态为集合$SiS_iSi​$。
- 每个阶段可以做的决策为集合$AiA_iAi​$
- 从当前阶段状态到下一阶段状态的转移函数为$PPP$，有$si+1=P(si,ai)s_{i+1}=P(s_i,a_i)si+1​=P(si​,ai​)$
- 决策完成之后，当前阶段对应的成本（或者奖赏）为$ci=C(si,ai)c_i=C(s_i,a_i)ci​=C(si​,ai​)$。求解变量为$aia_iai​$，目标函数为最小化总成本（或者最大化总奖赏）$Σi∈Ici\Sigma_{i\in I}c_iΣi∈I​ci​$

如果阶段之间的转移存在随机性，那么状态转移函数为转移概率$psi+1=P(si+1,si,ai)p_{s_{i+1}}=P(s_{i+1},s_i,a_i)psi+1​​=P(si+1​,si​,ai​)$，求解变量为$ai=π(si)a_i = \pi(s_i)ai​=π(si​)$，当前阶段期望成本为$ci=Σsi+1C(si+1,si,ai)psi+1c_i=\Sigma _{s_{i+1}}C(s_{i+1},s_i,a_i)p_{s_{i+1}}ci​=Σsi+1​​C(si+1​,si​,ai​)psi+1​​$，目标函数为最小化期望总成本$Σi∈Ici\Sigma_{i\in I}c_iΣi∈I​ci​$。

模型在进行决策的时候有多种方法，这里列举常用的三种：
- 确定性的贪婪策略$π(s)=arg⁡max⁡aqs,a∗\pi(s) = \arg\max_a q^*_{s,a}π(s)=argmaxa​qs,a∗​$
- $ϵ\epsilonϵ$-greed策略：以$1−ϵ1-\epsilon1−ϵ$的概率取最优策略，其他等概率。这样可以平衡利用和探索($ϵ\epsilonϵ$的部分)
- 高斯策略：在连续系统中，行动为$a+ϵa+\epsilona+ϵ$，后面为零均值正态分布的高斯随机噪声。

下面介再简单描述一些重要概念：
- 马尔科夫过程（MP）MP可以用(S,P)序列表示，其中S表示环境状态，而P表示概率转移的边。MP的前进过程用图表示为：…$⟶st⟶pstst+1⟶\longrightarrow s_t \stackrel{p_{s_t}}{\longrightarrow} s_{t+1} \longrightarrow⟶st​⟶pst​​​st+1​⟶$…- 马尔科夫决策过程（MDP）MDP是(S,A,P,R)，多了一个决策A和回报R（跨时间的回报需要带上折扣$γ\gammaγ$），可以看做是MDP比MP多出来的D(ecision)。MDP的目标是min $GGG$ = min $ΣkγkRk+1\Sigma_k \gamma^k R^{k+1}Σk​γkRk+1$ = min $Eπ{ΣkγkRk+1}E_{\pi}\{\Sigma_k \gamma^k R^{k+1}\}Eπ​{Σk​γkRk+1}$，求解变量为 $π\piπ$。MDP的前进过程用图表示如下：…$⟶st⟶πat,rt⟶pst+1⟶\longrightarrow s_t \stackrel{\pi}{\longrightarrow}a_t ,r_t\stackrel{p}{\longrightarrow} s_{t+1} \longrightarrow⟶st​⟶π​at​,rt​⟶p​st+1​⟶$…- 值函数MDP使用迭代的方法求解，定义**状态值函数$vvv$**和**状态行为值函数$qqq$**。$vvv$的参数是s，而$qqq$的参数是(s,a)。值函数的前进过程图如下：…$⟶v(st)⟶πq(st,at),rt⟶pv(st+1)⟶\longrightarrow v(s_t) \stackrel{\pi}{\longrightarrow} q(s_t,a_t) ,r_t\stackrel{p}{\longrightarrow} v(s_{t+1})\longrightarrow⟶v(st​)⟶π​q(st​,at​),rt​⟶p​v(st+1​)⟶$…- 贝尔曼方程$vvv$和$qqq$都有对应的贝尔曼方程(Bellman Equation)：$vs∗=max⁡a{r′+Σps,a,s′vs′∗}v^*_{s}=\max_{a} \{r&#x27;+\Sigma p_{s,a,s&#x27;}v^*_{s&#x27;}\}vs∗​=maxa​{r′+Σps,a,s′​vs′∗​}$，其中$r′r&#x27;r′$和$s′s&#x27;s′$是根据$sss$和$aaa$算出来的。$qs,a∗=Σps,a,s′max⁡a′{r′+qs′,a′∗}q^*_{s,a}=\Sigma p_{s,a,s&#x27;}  \max_{a&#x27;} \{r&#x27;+q^*_{s&#x27;,a&#x27;}\}qs,a∗​=Σps,a,s′​maxa′​{r′+qs′,a′∗​}$，其中$rrr$和$s′s&#x27;s′$是根据$sss$和$aaa$算出来的。- 有模型MDP和无模型MDP区别在于P、R是否有确定的表达式。有模型可以简单使用$VVV$来求解，而无模型必须使用$QQQ$来求解。# 2. 基本算法![在这里插入图片描述](https://img-blog.csdnimg.cn/20190225133132792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tpdHR5emM=,size_16,color_FFFFFF,t_70)# 2.1 有模型MDP的算法有模型情况下，$PPP$、$RRR$是已知的，可以通过Bellman方程求解$VVV$。然而求解Bellman的过程比较麻烦，实际操作中使用迭代的方法，分为以下两种：- 策略迭代方法：策略评估和策略提升分开进行。1：对所有$sss$，迭代计算当前策略表格的值函数$VVV$，评估$V=ΠP(R′+γV′)V= \Pi P(R&#x27;+\gamma V&#x27;)V=ΠP(R′+γV′)$；2：对所有$sss$，计算所有$aaa$的$Q=P(R′+γV′)Q= P(R&#x27;+\gamma V&#x27;)Q=P(R′+γV′)$，更新策略$Π=arg⁡max⁡aQ\Pi = \arg \max_{a} QΠ=argmaxa​Q$。- 价值迭代方法：对所有的$sss$，迭代$V=arg⁡max⁡aP(R′+γV′)V=\arg \max_a P(R&#x27;+\gamma  V&#x27;)V=argmaxa​P(R′+γV′)$。最后结果中策略没有表格形式，需要自己根据$VVV$来算。- 广义策略迭代方法：由策略迭代和价值迭代组合而成的算法族，比如说先执行10轮价值迭代，再执行策略迭代。# 2.2 无模型MDP的算法- 表格型强化学习（即使用表格存储对应值函数的方法）。具体又分两种：1.**基于蒙特卡洛的算法**，策略评估阶段使用采样的方法，评估时又分every visit和first visit；2. **时间差分方法**，结合了采样和动态规划方法，又分online的**SARSA**方法和offline的**Q-learning**(使用下一阶段最大的q代替实际的q)。- 值函数逼近强化学习（使用机器学习模型替代表格的方法）。首先是DQN，使用了replay buffer和target network两个技巧；然后是double Q-learning，使用了priority replay buffer；再者是Dueling DQN，将值函数q分解为了v和A两部分，A称为优势函数；然后是DQN from demonstration，冷启动时引入专家策略；然后是分布式DQN，从原先的Q函数变成Q分布；Noisy Network则将参数添加随机项来进行探索与开发；最后则是Rainbow模型，集成了前面的诸多方法。- 直接搜索策略（将策略参数化，在参数空间中直接搜索最优策略）：1. 策略梯度方法；2. TRPO策略更新方法；3. 确定性策略；4. 基于引导策略搜索- 近期发展：1. 策略梯度与值函数组合方法；2. 值迭代网络；3. PILCO方法# 3. 需掌握的知识# 3.1. 线性代数相关- 特征值、特征向量矩阵$AAA$特征值$λ\lambdaλ$和特征向量$eee$: $Ae=λeAe=\lambda eAe=λe$。对于变换$AAA$，只有伸缩而没有旋转。对称矩阵的特征向量相互正交，根据这个特性，可以将对称矩阵**对角化**（对角线为特征值，其余位置都为0）。对角化之后，矩阵求阶乘简单多了：$An=EΛnEA^n = E \Lambda^n EAn=EΛnE$- 正定矩阵：满足$xAx&gt;0xAx&gt;0xAx>0$的矩阵称为正定矩阵（若存在等于0的，则城半正定）。若A的特征值均为正数，则A是正定的。- 正交矩阵：满足$AAT=EAA^T=EAAT=E$的矩阵称为正交矩阵，其中$EEE$为单位矩阵。- 奇异值分解（SVD）：将$AAA$分解为$UΣVTU\Sigma V^TUΣVT$形式，其中$UUU$和$VVV$是正交矩阵，$Σ\SigmaΣ$为对角阵，相当于进行了翻转->缩放->翻转。- 线性方程求解的高斯-赛德尔迭代法：与简单迭代法的不同点在于计算$xikx^{k}_ixik​$时，利用了刚刚迭代出的$x1kx^{k}_1x1k​$~$xi−1kx^{k}_{i-1}xi−1k​$# 3.2. 概率论相关- 概率首先抽象出随机变量的概念，概率看做是随机变量的取值，即事件到实数的一个映射。频率派将概率理解为无数次事件发生之后，随机变量取某个值的频度；贝叶斯派将概率理解为事件发生的信心。最为常用的概率分布莫过于离散的二项分布(伯努利分布)和连续的正态分布(高斯分布).- 贝叶斯公式贝叶斯公式算是概率历史上最重要的公式了：$p(Y∣X)=p(X∣Y)∗p(Y)/p(X)p(Y|X)=p(X|Y)*p(Y)/p(X)p(Y∣X)=p(X∣Y)∗p(Y)/p(X)$，其中$p(Y)p(Y)p(Y)$称为先验概率，$p(X∣Y)p(X|Y)p(X∣Y)$称为似然概率，$p(Y∣X)p(Y|X)p(Y∣X)$称为后验概率。$XXX$是过程信息，其概率一般是一个标准化常量。- 最大似然估计很多时候我们用概率模型来刻画事件，通过观察到的取值来推测模型的参数，推测的方法一般是用最大似然估计法，即以最大的概率产生样本为目标函数，求解得到参数。一般假设各个变量是独立同分布，然后采用对数将目标函数中的乘法转为加法，例如二项分布求出来的$P(X=1)=pP(X=1)=pP(X=1)=p$等于观测值的均值，正态分布求出来的$uuu$等于观测值的均值，$σ\sigmaσ$等于观测值的标准差。- 重要性采样有些时候，概率密度函数不太好表示，例如我们的数据概率密度函数为$p(x)p(x)p(x)$，想要计算数据$xxx$的$fff$函数下的期望，即$Ex∼p(x)f(x)E_{x\sim p(x)}f(x)Ex∼p(x)​f(x)$，常规的方法是求 $∫xp(x)f(x)\int_x p(x)f(x)∫x​p(x)f(x)$，而重要性采样是采样$NNN$次（蒙特卡罗法），然后求$Σxi∼p~(x)f(x)∗p(x)/p~(x)/N\Sigma_{x_i \sim \tilde{p}(x)}f(x)*p(x)/\tilde{p}(x)/NΣxi​∼p~​(x)​f(x)∗p(x)/p~​(x)/N$，其中$p(x)和p~(x)p(x)和\tilde{p}(x)p(x)和p~​(x)$也可以用频度近似计算。# 3.3. 信息论相关- 熵又称为不确定度，$H(P)=−Σplog(p)H(P) = -\Sigma p log(p)H(P)=−Σplog(p)$- 交叉熵$H(P,Q)=−Σplog(q)H(P,Q) = -\Sigma plog(q)H(P,Q)=−Σplog(q)$- KL散度用于衡量两个概率分布的差异，等于交叉熵与熵的差值：KL$(p∣∣q)=Σp(x)log(p(x)/q(x))=H(P,Q)−H(P)(p||q) = \Sigma p(x)log(p(x)/q(x)) = H(P,Q) - H(P)(p∣∣q)=Σp(x)log(p(x)/q(x))=H(P,Q)−H(P)$# 3.4. 优化相关- 凸函数与Jensen不等式凸函数的定义公式称为Jensen不等式。在凸函数下，局部最优值等价于全局最优值。- 优化的目标函数一般分类问题使用交叉熵作为优化目标，其损失的梯度只和正确分类的预测结果有关；回归问题使用平方损失作为优化目标，其损失的梯度和所有样本相关。- 梯度下降法在每一点，梯度下降法有3个值：当前值x，当前点的梯度grad，以及当前点向前前进的步长step。- 动量算法所谓动量，指的是将之前阶段的梯度pre_grad以一定的折扣$γ\gammaγ$叠加到当前的梯度上。在强化学习中目标函数就使用了动量算法。Nesterov算法计算梯度的时候基于future点来计算的。极限情况下，某个梯度贡献的总能量为$1/(1−γ)1/(1-\gamma)1/(1−γ)$- 共轭梯度法每一次前进时，使得前进方向$rtr_trt​$和误差项$ete_tet​$正交。使用Gram-Schmidt方法，即每次加一个新的向量进来，并去掉和已有的正交系统重合的部分，构成新的正交系统。具体实现还没怎么看懂，等以后看明白了再更新。- 自然梯度法将每一轮迭代中对参数的更新变为对模型效果的更新。同样没怎么看懂。
