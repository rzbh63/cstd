
# 自然语言处理6 -- 情感分析 - 谢杨易的博客 - CSDN博客

2018年09月09日 15:30:17[谢杨易](https://me.csdn.net/u013510838)阅读数：3914


系列文章，请多关注
[Tensorflow源码解析1 – 内核架构和源码结构](https://blog.csdn.net/u013510838/article/details/84103503)
[带你深入AI（1） - 深度学习模型训练痛点及解决方法](https://blog.csdn.net/u013510838/article/details/79835563)
[自然语言处理1 – 分词](https://blog.csdn.net/u013510838/article/details/81673016)
[自然语言处理2 – jieba分词用法及原理](https://blog.csdn.net/u013510838/article/details/81738431)
[自然语言处理3 – 词性标注](https://blog.csdn.net/u013510838/article/details/81907121)
[自然语言处理4 – 句法分析](https://blog.csdn.net/u013510838/article/details/81976427)
[自然语言处理5 – 词向量](https://blog.csdn.net/u013510838/article/details/82108381)
[自然语言处理6 – 情感分析](https://blog.csdn.net/u013510838/article/details/82558797)
# 1 概述
情感分析是自然语言处理中常见的场景，比如淘宝商品评价，饿了么外卖评价等，对于指导产品更新迭代具有关键性作用。通过情感分析，可以挖掘产品在各个维度的优劣，从而明确如何改进产品。比如对外卖评价，可以分析菜品口味、送达时间、送餐态度、菜品丰富度等多个维度的用户情感指数，从而从各个维度上改进外卖服务。
情感分析可以采用基于情感词典的传统方法，也可以采用基于深度学习的方法，下面详细讲解
# 2 基于情感词典的传统方法
### 2.1 基于词典的情感分类步骤
基于情感词典的方法，先对文本进行分词和停用词处理等预处理，再利用先构建好的情感词典，对文本进行字符串匹配，从而挖掘正面和负面信息。如下图
![屏幕快照 2018-09-09 下午1.39.29.png | left | 747x209](https://cdn.nlark.com/lark/0/2018/png/9304/1536471625491-39ce4173-e52f-4fe0-bec3-df26b44353a5.png)
### 2.2 情感词典
情感词典包含正面词语词典、负面词语词典、否定词语词典、程度副词词典等四部分。如下图
![image.png | left | 747x243](https://cdn.nlark.com/lark/0/2018/png/9304/1536471425217-c4b6e571-3b36-403c-8c81-31613991aff4.png)
词典包含两部分，词语和权重，如下
```python
正面：
很快  1.75
挺快  1.75
还好  1.2
很萌  1.75
服务到位    1
负面：
无语  2
醉了  2
没法吃  2
不好  2
太差  5
太油  2.5
有些油  1
咸   1
一般  0.5
程度副词：
超级  2
超  2
都   1.75
还   1.5
实在  1.75
否定词：
不   1
没   1
无   1
非   1
莫   1
弗   1
毋   1
```
情感词典在整个情感分析中至关重要，所幸现在有很多开源的情感词典，如BosonNLP情感词典，它是基于微博、新闻、论坛等数据来源构建的情感词典，以及知网情感词典等。当然我们也可以通过语料来自己训练情感词典。
### 2.3 情感词典文本匹配算法
基于词典的文本匹配算法相对简单。逐个遍历分词后的语句中的词语，如果词语命中词典，则进行相应权重的处理。正面词权重为加法，负面词权重为减法，否定词权重取相反数，程度副词权重则和它修饰的词语权重相乘。如下图
![屏幕快照 2018-09-09 下午2.05.48.png | left | 747x490](https://cdn.nlark.com/lark/0/2018/png/9304/1536474696424-415244ab-0888-42a0-abec-f91791909b49.png)
利用最终输出的权重值，就可以区分是正面、负面还是中性情感了。
### 2.4 缺点
基于词典的情感分类，简单易行，而且通用性也能够得到保障。但仍然有很多不足
精度不高。语言是一个高度复杂的东西，采用简单的线性叠加显然会造成很大的精度损失。词语权重同样不是一成不变的，而且也难以做到准确。
新词发现。对于新的情感词，比如给力，牛逼等等，词典不一定能够覆盖
词典构建难。基于词典的情感分类，核心在于情感词典。而情感词典的构建需要有较强的背景知识，需要对语言有较深刻的理解，在分析外语方面会有很大限制。
# 3 基于深度学习的算法
近年来，深度学习在NLP领域内也是遍地开花。在情感分类领域，我们同样可以采用深度学习方法。基于深度学习的情感分类，具有精度高，通用性强，不需要情感词典等优点。
### 3.1 基于深度学习的情感分类步骤
基于深度学习的情感分类，首先对语句进行分词、停用词、简繁转换等预处理，然后进行词向量编码，然后利用LSTM或者GRU等RNN网络进行特征提取，最后通过全连接层和softmax输出每个分类的概率，从而得到情感分类。
![image.png | left | 747x135](https://cdn.nlark.com/lark/0/2018/png/9304/1536475906730-786dca18-d3b5-401e-9170-0d499dea7c3e.png)
### 3.2 代码示例
下面通过代码来讲解这个过程。下面是我周末写的，2018年AI Challenger细粒度用户评论情感分析比赛中的代码。项目数据来源于大众点评，训练数据10万条，验证1万条。分析大众点评用户评论中，关于交通，菜品，服务等20个维度的用户情感指数。分为正面、负面、中性和未提及四类。代码在验证集上，目前f1 socre可以达到0.62。
#### 3.2.1 分词和停用词预处理
数据预处理都放在了PreProcessor类中，主函数是process。步骤如下
读取原始csv文件，解析出原始语句和标注
错别字，繁简体，拼音，语义不明确等词语的处理
stop words停用词处理
分词，采用jieba分词进行处理。分词这儿有个trick，由于分词后较多口语化的词语不在词向量中，所以对这部分词语从jieba中del掉，然后再进行分词。直到只有为数不多的词语不在词向量中为止。
构建词向量到词语的映射，并对词语进行数字编码。这一步比较常规。
```python
class
```
```python
PreProcessor
```
```python
(
```
```python
object
```
```python
)
```
```python
:
```
```python
def
```
```python
__init__
```
```python
(
```
```python
self
```
```python
,
```
```python
filename
```
```python
,
```
```python
busi_name
```
```python
=
```
```python
"location_traffic_convenience"
```
```python
)
```
```python
:
```
```python
self
```
```python
.
```
```python
filename
```
```python
=
```
```python
filename
        self
```
```python
.
```
```python
busi_name
```
```python
=
```
```python
busi_name
        self
```
```python
.
```
```python
embedding_dim
```
```python
=
```
```python
256
```
```python
# 读取词向量
```
```python
embedding_file
```
```python
=
```
```python
"./word_embedding/word2vec_wx"
```
```python
self
```
```python
.
```
```python
word2vec_model
```
```python
=
```
```python
gensim
```
```python
.
```
```python
models
```
```python
.
```
```python
Word2Vec
```
```python
.
```
```python
load
```
```python
(
```
```python
embedding_file
```
```python
)
```
```python
# 读取原始csv文件
```
```python
def
```
```python
read_csv_file
```
```python
(
```
```python
self
```
```python
)
```
```python
:
```
```python
reload
```
```python
(
```
```python
sys
```
```python
)
```
```python
sys
```
```python
.
```
```python
setdefaultencoding
```
```python
(
```
```python
'utf-8'
```
```python
)
```
```python
print
```
```python
(
```
```python
"after coding: "
```
```python
+
```
```python
str
```
```python
(
```
```python
sys
```
```python
.
```
```python
getdefaultencoding
```
```python
(
```
```python
)
```
```python
)
```
```python
)
```
```python
data
```
```python
=
```
```python
pd
```
```python
.
```
```python
read_csv
```
```python
(
```
```python
self
```
```python
.
```
```python
filename
```
```python
,
```
```python
sep
```
```python
=
```
```python
','
```
```python
)
```
```python
x
```
```python
=
```
```python
data
```
```python
.
```
```python
content
```
```python
.
```
```python
values
        y
```
```python
=
```
```python
data
```
```python
[
```
```python
self
```
```python
.
```
```python
busi_name
```
```python
]
```
```python
.
```
```python
values
```
```python
return
```
```python
x
```
```python
,
```
```python
y
```
```python
# todo 错别字处理，语义不明确词语处理，拼音繁体处理等
```
```python
def
```
```python
correct_wrong_words
```
```python
(
```
```python
self
```
```python
,
```
```python
corpus
```
```python
)
```
```python
:
```
```python
return
```
```python
corpus
```
```python
# 去掉停用词
```
```python
def
```
```python
clean_stop_words
```
```python
(
```
```python
self
```
```python
,
```
```python
sentences
```
```python
)
```
```python
:
```
```python
stop_words
```
```python
=
```
```python
None
```
```python
with
```
```python
open
```
```python
(
```
```python
"./stop_words.txt"
```
```python
,
```
```python
"r"
```
```python
)
```
```python
as
```
```python
f
```
```python
:
```
```python
stop_words
```
```python
=
```
```python
f
```
```python
.
```
```python
readlines
```
```python
(
```
```python
)
```
```python
stop_words
```
```python
=
```
```python
[
```
```python
word
```
```python
.
```
```python
replace
```
```python
(
```
```python
"\n"
```
```python
,
```
```python
""
```
```python
)
```
```python
for
```
```python
word
```
```python
in
```
```python
stop_words
```
```python
]
```
```python
# stop words 替换
```
```python
for
```
```python
i
```
```python
,
```
```python
line
```
```python
in
```
```python
enumerate
```
```python
(
```
```python
sentences
```
```python
)
```
```python
:
```
```python
for
```
```python
word
```
```python
in
```
```python
stop_words
```
```python
:
```
```python
if
```
```python
word
```
```python
in
```
```python
line
```
```python
:
```
```python
line
```
```python
=
```
```python
line
```
```python
.
```
```python
replace
```
```python
(
```
```python
word
```
```python
,
```
```python
""
```
```python
)
```
```python
sentences
```
```python
[
```
```python
i
```
```python
]
```
```python
=
```
```python
line
```
```python
return
```
```python
sentences
```
```python
# 分词，将不在词向量中的jieba分词单独挑出来，他们不做分词
```
```python
def
```
```python
get_words_after_jieba
```
```python
(
```
```python
self
```
```python
,
```
```python
sentences
```
```python
)
```
```python
:
```
```python
# jieba分词
```
```python
all_exclude_words
```
```python
=
```
```python
dict
```
```python
(
```
```python
)
```
```python
while
```
```python
(
```
```python
1
```
```python
)
```
```python
:
```
```python
words_after_jieba
```
```python
=
```
```python
[
```
```python
[
```
```python
w
```
```python
for
```
```python
w
```
```python
in
```
```python
jieba
```
```python
.
```
```python
cut
```
```python
(
```
```python
line
```
```python
)
```
```python
if
```
```python
w
```
```python
.
```
```python
strip
```
```python
(
```
```python
)
```
```python
]
```
```python
for
```
```python
line
```
```python
in
```
```python
sentences
```
```python
]
```
```python
# 遍历不包含在word2vec中的word
```
```python
new_exclude_words
```
```python
=
```
```python
[
```
```python
]
```
```python
for
```
```python
line
```
```python
in
```
```python
words_after_jieba
```
```python
:
```
```python
for
```
```python
word
```
```python
in
```
```python
line
```
```python
:
```
```python
if
```
```python
word
```
```python
not
```
```python
in
```
```python
self
```
```python
.
```
```python
word2vec_model
```
```python
.
```
```python
wv
```
```python
.
```
```python
vocab
```
```python
and
```
```python
word
```
```python
not
```
```python
in
```
```python
all_exclude_words
```
```python
:
```
```python
all_exclude_words
```
```python
[
```
```python
word
```
```python
]
```
```python
=
```
```python
1
```
```python
new_exclude_words
```
```python
.
```
```python
append
```
```python
(
```
```python
word
```
```python
)
```
```python
elif
```
```python
word
```
```python
not
```
```python
in
```
```python
self
```
```python
.
```
```python
word2vec_model
```
```python
.
```
```python
wv
```
```python
.
```
```python
vocab
```
```python
:
```
```python
all_exclude_words
```
```python
[
```
```python
word
```
```python
]
```
```python
+=
```
```python
1
```
```python
# 剩余未包含词小于阈值，返回分词结果，结束。否则添加到jieba del_word中，然后重新分词
```
```python
if
```
```python
len
```
```python
(
```
```python
new_exclude_words
```
```python
)
```
```python
<
```
```python
10
```
```python
:
```
```python
print
```
```python
(
```
```python
"length of not in w2v words: %d, words are:"
```
```python
%
```
```python
len
```
```python
(
```
```python
new_exclude_words
```
```python
)
```
```python
)
```
```python
for
```
```python
word
```
```python
in
```
```python
new_exclude_words
```
```python
:
```
```python
print
```
```python
word
```
```python
,
```
```python
print
```
```python
(
```
```python
"\nall exclude words are: "
```
```python
)
```
```python
for
```
```python
word
```
```python
in
```
```python
all_exclude_words
```
```python
:
```
```python
if
```
```python
all_exclude_words
```
```python
[
```
```python
word
```
```python
]
```
```python
>
```
```python
5
```
```python
:
```
```python
print
```
```python
"%s: %d,"
```
```python
%
```
```python
(
```
```python
word
```
```python
,
```
```python
all_exclude_words
```
```python
[
```
```python
word
```
```python
]
```
```python
)
```
```python
,
```
```python
return
```
```python
words_after_jieba
```
```python
else
```
```python
:
```
```python
for
```
```python
word
```
```python
in
```
```python
new_exclude_words
```
```python
:
```
```python
jieba
```
```python
.
```
```python
del_word
```
```python
(
```
```python
word
```
```python
)
```
```python
raise
```
```python
Exception
```
```python
(
```
```python
"get_words_after_jieba error"
```
```python
)
```
```python
# 去除不在词向量中的词
```
```python
def
```
```python
remove_words_not_in_embedding
```
```python
(
```
```python
self
```
```python
,
```
```python
corpus
```
```python
)
```
```python
:
```
```python
for
```
```python
i
```
```python
,
```
```python
sentence
```
```python
in
```
```python
enumerate
```
```python
(
```
```python
corpus
```
```python
)
```
```python
:
```
```python
for
```
```python
word
```
```python
in
```
```python
sentence
```
```python
:
```
```python
if
```
```python
word
```
```python
not
```
```python
in
```
```python
self
```
```python
.
```
```python
word2vec_model
```
```python
.
```
```python
wv
```
```python
.
```
```python
vocab
```
```python
:
```
```python
sentence
```
```python
.
```
```python
remove
```
```python
(
```
```python
word
```
```python
)
```
```python
corpus
```
```python
[
```
```python
i
```
```python
]
```
```python
=
```
```python
sentence
```
```python
return
```
```python
corpus
```
```python
# 词向量，建立词语到词向量的映射
```
```python
def
```
```python
form_embedding
```
```python
(
```
```python
self
```
```python
,
```
```python
corpus
```
```python
)
```
```python
:
```
```python
# 1 读取词向量
```
```python
w2v
```
```python
=
```
```python
dict
```
```python
(
```
```python
zip
```
```python
(
```
```python
self
```
```python
.
```
```python
word2vec_model
```
```python
.
```
```python
wv
```
```python
.
```
```python
index2word
```
```python
,
```
```python
self
```
```python
.
```
```python
word2vec_model
```
```python
.
```
```python
wv
```
```python
.
```
```python
syn0
```
```python
)
```
```python
)
```
```python
# 2 创建词语词典，从而知道文本中有多少词语
```
```python
w2index
```
```python
=
```
```python
dict
```
```python
(
```
```python
)
```
```python
# 词语为key，索引为value的字典
```
```python
index
```
```python
=
```
```python
1
```
```python
for
```
```python
sentence
```
```python
in
```
```python
corpus
```
```python
:
```
```python
for
```
```python
word
```
```python
in
```
```python
sentence
```
```python
:
```
```python
if
```
```python
word
```
```python
not
```
```python
in
```
```python
w2index
```
```python
:
```
```python
w2index
```
```python
[
```
```python
word
```
```python
]
```
```python
=
```
```python
index
                    index
```
```python
+=
```
```python
1
```
```python
print
```
```python
(
```
```python
"\nlength of w2index is %d"
```
```python
%
```
```python
len
```
```python
(
```
```python
w2index
```
```python
)
```
```python
)
```
```python
# 3 建立词语到词向量的映射
```
```python
# embeddings = np.random.randn(len(w2index) + 1, self.embedding_dim)
```
```python
embeddings
```
```python
=
```
```python
np
```
```python
.
```
```python
zeros
```
```python
(
```
```python
shape
```
```python
=
```
```python
(
```
```python
len
```
```python
(
```
```python
w2index
```
```python
)
```
```python
+
```
```python
1
```
```python
,
```
```python
self
```
```python
.
```
```python
embedding_dim
```
```python
)
```
```python
,
```
```python
dtype
```
```python
=
```
```python
float
```
```python
)
```
```python
embeddings
```
```python
[
```
```python
0
```
```python
]
```
```python
=
```
```python
0
```
```python
# 未映射到的词语，全部赋值为0
```
```python
n_not_in_w2v
```
```python
=
```
```python
0
```
```python
for
```
```python
word
```
```python
,
```
```python
index
```
```python
in
```
```python
w2index
```
```python
.
```
```python
items
```
```python
(
```
```python
)
```
```python
:
```
```python
if
```
```python
word
```
```python
in
```
```python
self
```
```python
.
```
```python
word2vec_model
```
```python
.
```
```python
wv
```
```python
.
```
```python
vocab
```
```python
:
```
```python
embeddings
```
```python
[
```
```python
index
```
```python
]
```
```python
=
```
```python
w2v
```
```python
[
```
```python
word
```
```python
]
```
```python
else
```
```python
:
```
```python
print
```
```python
(
```
```python
"not in w2v: %s"
```
```python
%
```
```python
word
```
```python
)
```
```python
n_not_in_w2v
```
```python
+=
```
```python
1
```
```python
print
```
```python
(
```
```python
"words not in w2v count: %d"
```
```python
%
```
```python
n_not_in_w2v
```
```python
)
```
```python
del
```
```python
self
```
```python
.
```
```python
word2vec_model
```
```python
,
```
```python
w2v
```
```python
# 4 语料从中文词映射为索引
```
```python
x
```
```python
=
```
```python
[
```
```python
[
```
```python
w2index
```
```python
[
```
```python
word
```
```python
]
```
```python
for
```
```python
word
```
```python
in
```
```python
sentence
```
```python
]
```
```python
for
```
```python
sentence
```
```python
in
```
```python
corpus
```
```python
]
```
```python
return
```
```python
embeddings
```
```python
,
```
```python
x
```
```python
# 预处理，主函数
```
```python
def
```
```python
process
```
```python
(
```
```python
self
```
```python
)
```
```python
:
```
```python
# 读取原始文件
```
```python
x
```
```python
,
```
```python
y
```
```python
=
```
```python
self
```
```python
.
```
```python
read_csv_file
```
```python
(
```
```python
)
```
```python
# 错别字，繁简体，拼音，语义不明确，等的处理
```
```python
x
```
```python
=
```
```python
self
```
```python
.
```
```python
correct_wrong_words
```
```python
(
```
```python
x
```
```python
)
```
```python
# stop words
```
```python
x
```
```python
=
```
```python
self
```
```python
.
```
```python
clean_stop_words
```
```python
(
```
```python
x
```
```python
)
```
```python
# 分词
```
```python
x
```
```python
=
```
```python
self
```
```python
.
```
```python
get_words_after_jieba
```
```python
(
```
```python
x
```
```python
)
```
```python
# remove不在词向量中的词
```
```python
x
```
```python
=
```
```python
self
```
```python
.
```
```python
remove_words_not_in_embedding
```
```python
(
```
```python
x
```
```python
)
```
```python
# 词向量到词语的映射
```
```python
embeddings
```
```python
,
```
```python
x
```
```python
=
```
```python
self
```
```python
.
```
```python
form_embedding
```
```python
(
```
```python
x
```
```python
)
```
```python
# 打印
```
```python
print
```
```python
(
```
```python
"embeddings[1] is, "
```
```python
,
```
```python
embeddings
```
```python
[
```
```python
1
```
```python
]
```
```python
)
```
```python
print
```
```python
(
```
```python
"corpus after index mapping is, "
```
```python
,
```
```python
x
```
```python
[
```
```python
0
```
```python
]
```
```python
)
```
```python
print
```
```python
(
```
```python
"length of each line of corpus is, "
```
```python
,
```
```python
[
```
```python
len
```
```python
(
```
```python
line
```
```python
)
```
```python
for
```
```python
line
```
```python
in
```
```python
x
```
```python
]
```
```python
)
```
```python
return
```
```python
embeddings
```
```python
,
```
```python
x
```
```python
,
```
```python
y
```
#### 3.2.2 词向量编码
词向量编码步骤主要有：
加载词向量。词向量可以从网上下载或者自己训练。网上下载的词向量获取简单，但往往缺失特定场景的词语。比如大众点评菜品场景下的鱼香肉丝、干锅花菜等词语，而且往往这些词语在特定场景下还十分重要。而自己训练则需要几百G的语料，在高性能服务器上连续训练好几天，成本较高。可以将两种方法结合起来，也就是加载下载好的词向量，然后利用补充语料进行增量训练。
建立词语到词向量的映射，也就是找到文本中每个词语的词向量
对文本进行词向量编码，可以通过keras的Embedding函数，或者其他深度学习库来搞定。
前两步在上面代码中已经展示了，词向量编码代码示例如下
Embedding(input_dim=len(embeddings),output_dim=len(embeddings[0]),weights=[embeddings],input_length=self.max_seq_length,trainable=False,name=embeddings_name))3.2.3 构建LSTM网络
LSTM网络主要分为如下几层
两层的LSTM。
dropout，防止过拟合
全连接，从而可以输出类别
softmax，将类别归一化到[0, 1]之间
LSTM网络是重中之重，这儿可以优化的空间很大。比如可以采用更优的双向LSTM，可以加入注意力机制。这两个trick都可以提高最终准确度。另外可以建立分词和不分词两种情况下的网络，最终通过concat合并。
classModel(object):def__init__(self,busi_name="location_traffic_convenience"):self.max_seq_length=100self.lstm_size=128self.max_epochs=10self.batch_size=128self.busi_name=busi_name
        self.model_name="model/%s_seq%d_lstm%d_epochs%d.h5"%(self.busi_name,self.max_seq_length,self.lstm_size,self.max_epochs)self.yaml_name="model/%s_seq%d_lstm%d_epochs%d.yml"%(self.busi_name,self.max_seq_length,self.lstm_size,self.max_epochs)defsplit_train_data(self,x,y):x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.1)\# 超长的部分设置为0，截断x_train=sequence.pad_sequences(x_train,self.max_seq_length)x_val=sequence.pad_sequences(x_val,self.max_seq_length)\# y弄成4分类，-2未提及，-1负面，0中性，1正面y_train=keras.utils.to_categorical(y_train,num_classes=4)y_val=keras.utils.to_categorical(y_val,num_classes=4)returnx_train,x_val,y_train,y_valdefbuild_network(self,embeddings,embeddings_name):model=Sequential()model.add(Embedding(input_dim=len(embeddings),output_dim=len(embeddings[0]),weights=[embeddings],input_length=self.max_seq_length,trainable=False,name=embeddings_name))model.add(LSTM(units=self.lstm_size,activation='tanh',return_sequences=True,name='lstm1'))model.add(LSTM(units=self.lstm_size,activation='tanh',name='lstm2'))model.add(Dropout(0.1))model.add(Dense(4))model.add(Activation('softmax'))returnmodeldeftrain(self,embeddings,x,y):model=self.build_network(embeddings,"embeddings_train")model.compile(optimizer="adam",loss="categorical_crossentropy",metrics=["accuracy"])\# 训练，采用k-folder交叉训练foriinrange(0,self.max_epochs):x_train,x_val,y_train,y_val=self.split_train_data(x,y)model.fit(x_train,y_train,batch_size=self.batch_size,validation_data=(x_val,y_val))\# 保存modelyaml_string=model.to_yaml()withopen(self.yaml_name,'w')asoutfile:outfile.write(yaml.dump(yaml_string,default_flow_style=True))\# 保存model的weightsmodel.save_weights(self.model_name)defpredict(self,embeddings,x):\# 加载modelprint'loading model......'withopen(self.yaml_name,'r')asf:yaml_string=yaml.load(f)model=model_from_yaml(yaml_string)\# 加载权重print'loading weights......'model.load_weights(self.model_name,by_name=True)model.compile(optimizer="adam",loss="categorical_crossentropy",metrics=["accuracy"])\# 预测x=sequence.pad_sequences(x,self.max_seq_length)predicts=model.predict_classes(x)\# 得到分类结果，它表征的是类别序号\# 转换classes=[0,1,-2,-1]predicts=[classes[item]foriteminpredicts]np.set_printoptions(threshold=np.nan)\# 全部打印print(np.array(predicts))returnpredicts3.2.4 softmax输出类别
这一部分上面代码已经讲到了，不在赘述。softmax只是一个归一化，讲数据归一化到[0, 1]之间，从而可以得到每个类别的概率。我们最终取概率最大的即可。
3.3 基于深度学习的情感分析难点
基于深度学习的情感分析难点也很多
语句长度太长。很多用户评论都特别长，分词完后也有几百个词语。而对于LSTM，序列过长会导致计算复杂、精度降低等问题。一般解决方法有进行停用词处理，无关词处理等，从而缩减文本长度。或者对文本进行摘要，抽离出语句主要成分。
新词和口语化的词语特别多。用户评论语句不像新闻那样规整，新词和口语化的词语特别多。这个问题给分词和词向量带来了很大难度。一般解决方法是分词方面，建立用户词典，从而提高分词准确度。词向量方面，对新词进行增量训练，从而提高新词覆盖率。
4. 总结
文本情感分析是NLP领域一个十分重要的问题，对理解用户意图具有决定性的作用。通过基于词典的传统算法和基于深度学习的算法，可以有效的进行情感分析。当前情感分析准确率还有待提高，任重而道远！
系列文章，请多关注
[Tensorflow源码解析1 – 内核架构和源码结构](https://blog.csdn.net/u013510838/article/details/84103503)
[带你深入AI（1） - 深度学习模型训练痛点及解决方法](https://blog.csdn.net/u013510838/article/details/79835563)
[自然语言处理1 – 分词](https://blog.csdn.net/u013510838/article/details/81673016)
[自然语言处理2 – jieba分词用法及原理](https://blog.csdn.net/u013510838/article/details/81738431)
[自然语言处理3 – 词性标注](https://blog.csdn.net/u013510838/article/details/81907121)
[自然语言处理4 – 句法分析](https://blog.csdn.net/u013510838/article/details/81976427)
[自然语言处理5 – 词向量](https://blog.csdn.net/u013510838/article/details/82108381)
[自然语言处理6 – 情感分析](https://blog.csdn.net/u013510838/article/details/82558797)

