
# 随笔列表第2页 - simplex - 博客园|[simplex](https://www.cnblogs.com/simplex/)
|
|[博客园](https://www.cnblogs.com/)|::|[首页](https://www.cnblogs.com/simplex/)|::|[新随笔](https://i.cnblogs.com/EditPosts.aspx?opt=1)|::|[联系](https://msg.cnblogs.com/send/simplex)|::|[订阅](https://www.cnblogs.com/simplex/rss)![订阅](//www.cnblogs.com/images/xml.gif)|::|[管理](https://i.cnblogs.com/)|17 				Posts ::				0 Stories				::				5 Comments				::				0 Trackbacks|
|

|公告

|共2页:|[上一页](https://www.cnblogs.com/simplex/default.html?page=1)|[1](https://www.cnblogs.com/simplex/default.html?page=1)|2
|2017年4月27日|[\#](https://www.cnblogs.com/simplex/archive/2017/04/27.html)
|摘要: 先列参考书 Gaussian Processes for Machine Learning MLAPP PRML 笔记回头再整理|[阅读全文](https://www.cnblogs.com/simplex/p/6777890.html)
|posted @ 2017-04-27 23:35 simplex 阅读(52) 评论(0)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=6777890)

|摘要: 下降单纯形法(downhill simplex method)是一个广泛使用的“derivative free”的优化算法。一般来说它的效率不高，但是文献[1]提到“the downhill simplex method may frequently be the *best* method to|[阅读全文](https://www.cnblogs.com/simplex/p/6777705.html)
|posted @ 2017-04-27 22:36 simplex 阅读(2463) 评论(2)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=6777705)

|2017年4月6日|[\#](https://www.cnblogs.com/simplex/archive/2017/04/06.html)
|摘要: 概述 优化问题就是在给定限制条件下寻找目标函数$f(\mathbf{x})，\mathbf{x}\in\mathbf{R}^{\mathbf{n}}$的极值点。极值可以分为整体极值或局部极值，整体极值即函数的最大/最小值，局部极值就是函数在有限邻域内的最大/最小值。通常都希望能求得函数的整体极值，但|[阅读全文](https://www.cnblogs.com/simplex/p/6671343.html)
|posted @ 2017-04-06 00:18 simplex 阅读(2779) 评论(0)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=6671343)

|摘要: 0. 前言 MC方法的关键在于如何对想要的分布进行采样，获得采样点。换句话说就是如何生成满足指定分布的随机数。在该系列文章中，我们有一个默认的假设就是已经有了一个能产生均匀分布随机数的机制，不管它是硬件生成的真随机数，还是算法模拟的伪随机数。关于伪随机数的生成算法，如线性同余法或者移位寄存器法，请参|[阅读全文](https://www.cnblogs.com/simplex/p/6671337.html)
|posted @ 2017-04-06 00:14 simplex 阅读(783) 评论(0)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=6671337)

|2016年8月31日|[\#](https://www.cnblogs.com/simplex/archive/2016/08/31.html)
|摘要: SVD 定义 假设\(A\)为\(M\times N\)矩阵，则存在\(M\times M\)维正交矩阵\(U=[u_1,u_2,\cdots,u_m]\)，\(N\times N\)维正交矩阵\(V=[v_1,v_2,\cdots,v_n]\)和\(M\times N\)对角矩阵\(\Sigma=|[阅读全文](https://www.cnblogs.com/simplex/p/5826999.html)
|posted @ 2016-08-31 17:56 simplex 阅读(2923) 评论(0)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=5826999)

|2016年8月20日|[\#](https://www.cnblogs.com/simplex/archive/2016/08/20.html)
|摘要: 问题描述 我们需要解决的问题可以描述如下：\(D\)维特征向量\(X=[x_1,x_2,\cdots,x_D]\)，\(x_i\in{1,2,\cdots,K}\)属于给定的\(C\)个类别之一，用\(y\)标记\(X\)属于的类别。如何根据已知的\(N\)个被正确分类的特征向量\(S=\{(x_1|[阅读全文](https://www.cnblogs.com/simplex/p/5791059.html)
|posted @ 2016-08-20 19:20 simplex 阅读(497) 评论(0)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=5791059)

|2016年8月18日|[\#](https://www.cnblogs.com/simplex/archive/2016/08/18.html)
|摘要: 假设我们在做一个抛硬币的实验，硬币出现正面的概率是\(\theta\)。在已知前\(n\)次结果的情况下，如何推断抛下一次硬币出现正面的概率呢？ 当\(n\)很大的时候，我们可以直接统计正面出现的次数，假设为\(n_1\)，然后可以做出推断\(\theta=\frac{n_1}{n}\)。 但是，如|[阅读全文](https://www.cnblogs.com/simplex/p/5785874.html)
|posted @ 2016-08-18 23:09 simplex 阅读(2214) 评论(0)|[编辑](https://i.cnblogs.com/EditPosts.aspx?postid=5785874)

|共2页:|[上一页](https://www.cnblogs.com/simplex/default.html?page=1)|[1](https://www.cnblogs.com/simplex/default.html?page=1)|2
|
|Copyright @
	simplex
|Powered by:|[.Text](http://scottwater.com/blog)|and|[ASP.NET](http://asp.net)
|Theme by:|[.NET Monster](http://www.DotNetMonster.com)
