
# 神经网络-常用激活函数 - 喜欢打酱油的老鸟 - CSDN博客


2018年08月25日 08:58:32[喜欢打酱油的老鸟](https://me.csdn.net/weixin_42137700)阅读数：59


[https://www.jianshu.com/p/b8ded87e0c0c](https://www.jianshu.com/p/b8ded87e0c0c)
常用的激活函数有三种：
1.Sigmoid or Logistic
2.Tanh-双曲正切
3.ReLu-线性整流函数
Sigmoid 激活函数：**f(x) = 1 / 1 + exp(-x)**
该函数的取值范围为0到1。这是一个S型曲线，容易理解和使用，但是有缺陷：1）消失的梯度问题， 2）输出不是以0为中心的，它会让梯度更新在不同的方向上走太远，输出范围在（0，1），这使优化更困难，3）Sigmoids易饱和杀死梯度，4）Sigmoid收敛速度慢。
![](https://img-blog.csdn.net/20180825085719388?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
sigmoid.png

双曲正切函数 tanh:**f(x) = 1 -exp(-2x) / 1 + exp(-2x)**
该函数的输出以0位中心，且范围为（-1，1），所以优化会更容易些，所以在实际中会用的比Sigmoid多，但是还是存在梯度消失的问题
![](https://img-blog.csdn.net/20180825085740267?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

线性整流函数ReLu：f(x)=max(0,x),if x<0.f(x)=0,if x>0,f(x)=x.
已被证明，在收敛速度上比tanh函数快6倍。它避免和矫正了梯度消失问题，基本所有的深度学习模型都用ReLu.
但ReLu的局限性在于只能被用于神经网络模型的隐藏层。所以对于输出来说，对于分类问题，我们会使用Softmax函数来计算类别概率，对于回归问题，我们会用linear函数来拟合。
ReLu的另一个问题就是一些梯度在训练的时候比较脆弱并且死亡。这会引起一个权重的更新，这会导致它在任何数据点上都不会被再激活。简言之，ReLu可能会导致死亡神经元。
为了修正神经元死亡这个问题，引入一个改进版本的ReLu：Leaky ReLu，它引入一个小区间，来保持更新的活力。
然后，我们还有另一种变体，由ReLu和Leaky ReLu组成，成为Maxout 函数
![](https://img-blog.csdn.net/20180825085759559?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjEzNzcwMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
所以，在深度学习的神经网络中，我们通常使用ReLu作为隐藏层函数，如果你的模型会受困于神经元死亡问题，那么可以采用ReLu的变体形式：Leaky ReLu或者Maxout 函数

作者：想跳舞的兔子
链接：https://www.jianshu.com/p/b8ded87e0c0c
來源：简书
简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。


